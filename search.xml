<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[The oxford 3000 by cefr level]]></title>
    <url>%2F2023%2F06%2F14%2Feng_oxford_3000%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[A1 concert: 音乐会 I’m going to a concert tonight featuring my favorite band. 今晚我要去看我最喜欢的乐队的音乐会。 carrot: 胡萝卜 Carrots are a good source of vitamins and fiber. 胡萝卜是维生素和纤维素的良好来源。 dish: 菜肴/盘子 This Italian restaurant serves delicious pasta dishes. 这家意大利餐厅供应美味的意面菜肴。 Can you hand me that dish? I need to set the table. 你能递给我那个盘子吗？我需要摆餐具。 euro: 欧元 The price of the item is 20 euros. 这个物品的价格是20欧元。 routine: 日常事务/例行公事 I have a morning routine that helps me start the day off right. 我有一个早上的例行公事，帮助我开始美好一天。 shower: 淋浴 I always take a shower in the morning to wake myself up. 我总是在早上冲个淋浴来唤醒自己。 A2 airline: 航空公司 I always choose this airline for my international flights. 我总是选择这家航空公司乘坐国际航班。 architect: 建筑师 The architect designed a beautiful and functional building. 建筑师设计了一座美丽而实用的建筑。 architecture: 建筑学 I’m studying architecture at university. 我在大学学习建筑学。 athlete: 运动员 The athlete broke the world record in the 100-meter dash. 运动员在100米短跑中打破了世界纪录。 camp: 露营 We’re going to camp in the mountains this weekend. 我们这个周末要去山里露营。 camping: 露营的 We need to buy some camping gear(物品/设备/齿轮) before we go. 我们需要在出发前购买一些露营用品。 campus: 校园 The university campus is very large and has many buildings. 这所大学的校园非常大，有很多建筑。 carpet: 地毯 We’re going to replace the old carpet in the living room. 我们要更换客厅里的旧地毯。 charity: 慈善机构 I donate to a local charity every year. 我每年都向当地的慈善机构捐款。 clerk: 职员 The clerk at the store was very helpful. 商店的职员非常乐于助人。 climate: 气候 The climate in this region is very hot and dry. 这个地区的气候非常炎热和干燥。 coach: 教练 The basketball coach is very strict but also very supportive. 篮球教练非常严格，但也很支持我们。 coast: 海岸 We took a walk along the coast and watched the sunset. 我们沿着海岸散步，看着日落。 compete: 竞争 The two companies are competing for the same market share. 这两家公司在争夺同一市场份额。 competition: 竞争 The competition between the two teams was intense. 两支队伍之间的竞争非常激烈。 conference: 会议 I’m attending a conference on renewable energy next week. 下周我将参加一场关于可再生能源的会议。 continent: 大陆 Africa is the second largest continent in the world. 非洲是世界上第二大的大陆。 credit: 信用 It’s important to have good credit when applying for a loan. 申请贷款时拥有良好的信用记录非常重要。 crowd: 人群 The crowd at the concert was enthusiastic and energetic. 演唱会上的观众热情而充满活力。 crowded: 拥挤的 The subway was very crowded during rush hour. 高峰期地铁非常拥挤。 curly: 卷曲的 Her hair is naturally curly and very beautiful. 她的头发天生卷曲，非常漂亮。 detective: 侦探 The detective solved the case and arrested the criminal. 侦探破案并逮捕了罪犯。 drama: 戏剧 I love watching dramas and going to the theater. 我喜欢看戏剧和去剧院。 enormous: 巨大的 The elephant was enormous and could barely fit through the door. 大象非常巨大，几乎无法通过门口。 essay: 文章 I have to write an essay for my English class. 我得为我的英语课写一篇文章。 fiction: 小说 I enjoy reading science fiction novels. 我喜欢阅读科幻小说。 figure: 数字/人物 The figure on the left represents the number of new cases. 左侧的数字代表新增病例数。 The figure of a superhero stood tall in the plaza. 广场上豪杰的雕像高耸入云。 grocery: 食品杂货 I need to go to the grocery store to buy some milk and bread. 我需要去杂货店买些牛奶和面包。 gallery: 画廊 The art gallery features works from famous artists around the world. 这个艺术画廊展示了来自世界各地著名艺术家的作品。 flu: 流感 I got the flu and had to stay home for a few days. 我得了流感，不得不在家休息几天。 hockey: 曲棍球 Hockey is a popular sport in Canada. 曲棍球是加拿大的一项流行运动。 insect: 昆虫 The butterfly is a beautiful insect with colorful wings. 蝴蝶是一种拥有彩色翅膀的美丽昆虫。 jam: 果酱/堵塞 I like to spread jam on my toast in the morning. 我喜欢在早餐时将果酱涂在面包上。 The traffic jam delayed our arrival at the airport. 交通堵塞延误了我们到达机场的时间。 lamp: 灯 I need to buy a new lamp for my desk. 我需要为我的桌子买一盏新的灯。 ordinary: 普通的 The sandwich was just an ordinary ham and cheese sandwich. 这个三明治只是一个普通的火腿芝士三明治。 poster: 海报 The poster advertised a new movie coming out next month. 海报宣传了下个月即将上映的一部新电影。 reception: 接待处/招待会 I went to the reception desk to check in to my hotel. 我去接待处办理入住手续。 The company held a reception to welcome its new employees. 公司举行招待会欢迎新员工。 refrigerator: 冰箱 I need to put the leftovers in the refrigerator to keep them fresh. 我需要把剩菜剩饭放到冰箱里保持它们的新鲜。 route: 路线 We took a scenic route to enjoy the beautiful views. 我们选择了一条景色优美的路线来欣赏美景。 stamp: 邮票 I collect rare stamps from around the world. 我收集来自世界各地的稀有邮票。 thick: 厚的/浓的 The book was so thick that it took me weeks to finish it. 这本书太厚了，我花了几周时间才读完。 The soup was too thick for my liking. 这汤太浓了，不太合我口味。 B1 alcoholic [ˌælkəˈhɒlɪk]: He is an alcoholic and needs to undergo alcoholism treatment. 他是一个酗酒者，需要戒酒治疗。 assignment [əˈsaɪnmənt]: Our teacher assigned a homework that needs to be completed before next week. 我们的老师布置了一份作业，需要在下周之前完成。 atmosphere [ˈætməsfɪə(r)]: The atmosphere of this restaurant is very comfortable and peaceful. 这个餐厅的氛围非常舒适和宁静。 authority [ɔːˈθɒrəti]: The government has the authority to make laws and regulations. 政府有权利制定法律和规定。 cd clause [klɔːz]: This contract has a special clause that needs to be paid attention to. 这个合同有一个特别的条款，需要注意。 chemical [ˈkemɪkəl]: This chemical substance has a significant impact on the environment. 这种化学物质对环境有很大的影响。 ceremony [ˈserəməni]: The ceremony is very solemn and grand. 这个仪式非常庄重和隆重。 cap [kæp]: He is wearing a red cap that is very eye-catching. 他戴着一顶红色的帽子，非常醒目。 campaign [kæmˈpeɪn]: They are conducting an anti-corruption campaign. 他们正在进行一项反贪腐的宣传活动。 cable [ˈkeɪbl]: They use a cable to connect the TV and the computer. 他们用电缆将电视机和电脑连接起来。 bury [ˈberi]: We need to bury this garbage to avoid polluting the environment. 我们需要将这些垃圾埋掉，以免对环境造成污染。 comparison [kəmˈpærɪsn]: The comparison between these two products is very interesting and can help people better understand their pros and cons. 这两个产品的比较非常有趣，让人可以更好地了解它们的优缺点。 competitive [kəmˈpetɪtɪv]: This is a very competitive market, and one needs to constantly improve oneself to achieve success. 这是一个非常有竞争力的市场，需要不断提高自己的水平才能获得成功。 competitor [kəmˈpetɪtə(r)]: The company’s competitors are very powerful, and better strategies are needed to deal with them. 这个公司的竞争对手很强大，需要制定更好的策略来应对。 concentrate [ˈkɒnsəntreɪt]: We need to concentrate on completing this task. 我们需要集中精力完成这项任务。 conclude [kənˈkluːd]: After considering all aspects, we have come to a conclusion. 经过多方考虑，我们得出了一个结论。 conclusion [kənˈkluːʒn]: This conclusion is based on facts and evidence. 这个结论是基于事实和证据得出的。 consequence [ˈkɒnsɪkwəns]: Irresponsible actions will bring about adverse consequences. 不负责任的行为会带来不良后果。 convince [kənˈvɪns]: He convinced me with facts and logic and made me change my mind. 他用事实和逻辑说服了我，让我改变了观点。 corn [kɔːn]: Corn is a very popular cereal crop. 玉米是一种非常受欢迎的粮食作物。 costume [ˈkɒstjuːm]: The costume design of this stage play is very exquisite and allows the audience to better experience the plot. 这个舞台剧的服装设计非常精美，让观众可以更好地体验剧情。 cotton [ˈkɒtn]: This piece of clothing is made of pure cotton and is very comfortable to wear. 这件衣服是用纯棉制成的，穿着非常舒适。 cruel [kruːəl]: Animal abuse is a very cruel behavior. 虐待动物是一种非常残忍的行为。 cupboard [ˈkʌbəd]: We need a cupboard to store miscellaneous items. 我们需要一个储物柜来存放零散的物品。 currency [ˈkʌrənsi]: The US dollar is one of the most popular currencies in the world. 美元是世界上最流行的货币之一。 curtain [ˈkɜːtn]: We need some curtains to block the sunlight and protect the privacy. 我们需要一些窗帘来遮挡阳光和保护隐私。 departure [dɪˈpɑːtʃə(r)]: Our flight will depart tomorrow morning. 我们的航班将在明天早上出发。 diagram [ˈdaɪəɡræm]: This flowchart can help us better understand the process of the entire project. 这个流程图可以帮助我们更好地理解整个项目的流程。 edfg exhibition [ˌeksɪˈbɪʃn]: This museum is exhibiting some very precious cultural relics. 这个博物馆正在展出一些非常珍贵的文物。 examine [ɪɡˈzæmɪn]: Doctors need to carefully examine the patient’s physical condition. 医生需要仔细检查病人的身体状况。 essential [ɪˈsenʃl]: This is a very important issue that needs to be resolved in a timely manner. 这是一个非常重要的问题，需要及时解决。 episode [ˈepɪsəʊd]: Each episode of this TV series is very interesting and captivating. 这个电视剧的每一集都非常有趣和引人入胜。 engaged [ɪnˈɡeɪdʒd]: We are engaged in an important project and cannot participate in other activities for the time being. 我们正在忙于一项重要的项目，暂时无法参加其他活动。 district [ˈdɪstrɪkt]: This city is divided into many districts, and each district has its own characteristics. 这个城市分为很多个区域，每个区域都有自己的特点。 divide [dɪˈvaɪd]: We need to divide this task into several parts to complete it. 我们需要将这个任务分成几个部分来完成。 documentary [ˌdɒkjʊˈmentəri]: This documentary is very meaningful and allows people to better understand a historical event. 这个纪录片非常有意义，让人可以更好地了解一个历史事件。 drum [drʌm]: The sound of the drums is enthusiastic and joyous. 鼓声响起，让人感到非常热烈和欢快。 fasten [ˈfɑːsn]: Please fasten your seatbelts as we are about to take off. 请系好安全带，我们即将起飞。 favor [ˈfeɪvə(r)]: He asked me for a small favor, and I am happy to help him. 他向我请求一个小小的帮助，我很乐意帮助他。 federal [ˈfedərəl]: The federal government has the power to make some national policies and laws. 联邦政府有权力制定一些全国性的政策和法律。 fence [fens]: This garden has a beautiful fence that protects the flowers and plants from external interference. 这个花园有一道美丽的篱笆，保护了花草不受外界干扰。 fuel [fjuːəl]: Cars need sufficient fuel to run. 汽车需要足够的燃料才能够行驶。 garage [ˈɡærɑːʒ]: I parked the car in the garage. 我把车停在了车库里。 glove [ɡlʌv]: I need a pair of gloves to protect my hands. 我需要一双手套来保护我的手。 grain [ɡreɪn]: These grains are one of the essential foods in our daily life. 这些谷物是我们日常生活中必不可少的食物之一。 hijk hire [haɪə(r)]: We need to hire some professional workers to help us complete this project. 我们需要雇佣一些专业的工人来帮助我们完成这个项目。 historic [hɪˈstɒrɪk]: This historic event will be recorded in the history books. 这个历史性的事件将会被记录在历史书上。 historical [hɪsˈtɒrɪkəl]: This museum has many historical relics and exhibits. 这个博物馆收藏了很多历史性的文物和展品。 hurricane [ˈhʌrɪkən]: A hurricane is a very strong weather phenomenon that can bring destructive impacts. 飓风是一种非常强烈的天气现象，可以带来破坏性的影响。 imaginary [ɪˈmædʒɪnəri]: Children often have various imaginations and fantasies. 小孩子们常常有各种各样的幻想和想象。 immigrant [ˈɪmɪɡrənt]: Immigrants are an important source of economic and cultural diversity for a country. 移民是一个国家经济和文化多样性的重要来源。 impact [ˈɪmpækt]: This event has a significant impact on our lives. 这个事件对我们的生活产生了很大的影响。 indicate [ˈɪndɪkeɪt]: The data indicates that the sales of this product are gradually decreasing. 数据显示，这个产品的销售额正在逐渐下降。 indoors [ˌɪnˈdɔːz]: Indoor exercises are also a great way to keep fit. 在室内运动也是一种很好的健身方式。 ingredient [ɪnˈɡriːdiənt]: This recipe requires many different ingredients to make. 这个菜谱需要很多不同的食材来制作。 intend [ɪnˈtend]: I intend to go on vacation next month and enjoy some relaxing time. 我打算下个月去旅游，享受一下放松的时光。 intention [ɪnˈtenʃən]: My intention is to help others and make their lives better. 我的意图是帮助别人，让他们过得更好。 invest [ɪnˈvest]: Investing in stocks is a high-risk and high-return investment method. 投资股票是一种风险和收益都很高的投资方式。 investigate [ɪnˈvestɪɡeɪt]: The police are investigating this case and hope to find the murderer. 警方正在调查这个案件，希望能够找到凶手。 involved [ɪnˈvɒlvd]: This project requires the involvement of many people to work together to complete. 这个项目需要很多人一起合作才能够完成。 journal [ˈdʒɜːnl]: This scientific journal published a valuable research paper. 这个科学杂志发表了一篇很有价值的研究论文。 killing [ˈkɪlɪŋ]: Killing animals for meat is a very cruel and inhumane act. 屠宰动物是一种非常残忍和不人道的行为。 lmno laboratory [ləˈbɒrətəri]: Science laboratories are important places for scientific research and development. 科学实验室是进行科学研究和开发的重要场所。 lack [læk]: Our company lacks an effective marketing strategy. 我们公司缺乏一个有效的市场营销策略。 leather [ˈleðə(r)]: This leather bag is made of high-quality leather and is very durable and beautiful. 这个皮包是用高质量的皮革制作的，非常耐用和美观。 leisure [ˈleʒə(r)]: Leisure time can be used to do things you enjoy and relax. 闲暇时间可以用来做一些自己喜欢的事情，放松身心。 literature [ˈlɪtərətʃə(r)]: Literature can bring us infinite imagination and emotion. 文学作品可以带给我们无限的想象力和感受力。 mental [ˈmentl]: Mental health is crucial for our life and work. 心理健康对于我们的生活和工作都非常重要。 mild [maɪld]: The climate in this area is mild and pleasant throughout the year. 这个地区的气候温和，四季宜人。 mixture [ˈmɪkstʃə(r)]: This medicine is made up of a mixture of different ingredients. 这个药是由多种不同的成分混合而成的。 nail [neɪl]: I need a nail to fix this wooden board. 我需要一把钉子来固定这个木板。 narrative [ˈnærətɪv]: This novel tells a touching story that is engaging. 这个小说讲述了一个感人的故事，引人入胜。 neat [niːt]: This room is neat and tidy, making it very comfortable and pleasant. 这个房间整洁有序，非常舒适宜人。 nuclear [ˈnjuːkliə(r)]: Nuclear weapons are a very dangerous weapon, posing a great threat to humanity and the environment. 核武器是一种非常危险的武器，对人类和生态造成极大的威胁。 occasion [əˈkeɪʒən]: This special occasion requires us to wear formal dress to attend. 这个特殊的场合需要我们穿礼服出席。 ought [ɔːt]: You ought to start preparing for the exam early so that you can achieve better results. 你应该尽早开始准备考试，这样才能更好地取得好成绩。 outdoors [ˈaʊtdɔːz]: Outdoor activities can allow us to breathe fresh air and enjoy the beauty of nature. 在户外活动可以让我们呼吸新鲜空气，享受自然的美景。 overseas [ˌəʊvərˈsiːz]: Our company has many overseas clients and needs to constantly expand our overseas market. 我们公司有很多海外客户，需要不断拓展海外市场。 pq participate [pɑːˈtɪsɪpeɪt]: Participating in community activities can help us better understand the community and interact with others. 参与社区活动可以让我们更好地了解社区和与他人互动。 particularly [pəˈtɪkjʊlə(r)li]: I particularly like the historical relics and culture of this city. 我特别喜欢这个城市的历史遗迹和文化。 persuade [pəˈsweɪd]: We need to persuade them to accept our proposal, which is crucial for the success of the project. 我们需要说服他们接受我们的建议，这对项目的成功非常重要。 pleasant [ˈpleznt]: The atmosphere of this restaurant is very pleasant, and the service is also attentive. 这个餐厅的氛围非常愉快，服务也很周到。 plot [plɒt]: This novel has a thrilling plot that keeps you wanting to read on. 这个小说有一个扣人心弦的情节，让人一直想看下去。 poet [ˈpəʊət]: 诗人 William Shakespeare is a famous poet in England, and his poetry is still widely recited to this day. William Shakespeare 是英国著名的诗人，他的诗歌作品至今仍然被广泛传诵。 poetry [ˈpəʊətri]: 诗歌 Poetry can express the poet’s deep emotions and thoughts through words. 诗歌可以通过文字表达出诗人内心深处的情感和思想。 poem [ˈpəʊəm]: 诗篇 This poem describes a beautiful landscape, allowing people to feel the magic and beauty of nature. 这首诗篇描述了一个美丽的风景，让人感受到大自然的神奇和美丽。 portrait [ˈpɔːtrət]: 肖像画 This portrait vividly depicts the appearance and temperament of the celebrity, and is very lifelike. 这幅肖像画生动地展现了这位名人的面容和气质，非常逼真。 pot [pɒt]: He cooked a pot of noodles in the pot. 他在锅里煮了一锅面条。 pour [pɔːr]: She poured a glass of water. 她倒了一杯水。 poverty [ˈpɒvəti]: Many people live in poverty. 许多人生活在贫困中。 powder [ˈpaʊdə(r)]: She made a cake with flour powder. 她用面粉制作了蛋糕。 prayer [preə(r)]: They prayed in the church. 他们在教堂里祷告。 priest [priːst]: The priest conducted mass in the church. 牧师在教堂里主持弥撒。 profit [ˈprɒfɪt]: This business brought in a lot of profit. 这项业务带来了很多利润。 promote [prəˈməʊt]: The company promoted its new product to increase sales. 公司为了提高销售额而推广新产品。 protest [ˈprəʊtest]: People held a protest march to protest the government’s policies. 人们举行示威游行，抗议政府的政策。 prove [pruːv]: He proved his point with an experiment. 他用实验证明了自己的观点。 qualification [ˌkwɒlɪfɪˈkeɪʃən]: This job requires relevant qualifications and certifications. 这份工作需要相关的学历和资格证书。 qualified [ˈkwɒlɪfaɪd]: She is a qualified doctor with many years of medical experience. 她是一名合格的医生，拥有多年的医疗经验。 qualify [ˈkwɒlɪfaɪ]: He qualified for a driver’s license by passing the exam. 他通过考试，获得了驾驶执照。 quotation [kwəʊˈteɪʃən]: He quoted Martin Luther King’s words in his speech. 他在演讲中引用了马丁·路德·金的名言。 rs race [reɪs]: They raced to win first place in the competition. 他们在比赛中争夺第一名。 receipt [rɪˈsiːt]: She received a receipt for her purchase. 她收到了购物小票。 religion [rɪˈlɪdʒən]: Religious belief is a personal freedom. 宗教信仰是个人自由。 religious [rɪˈlɪdʒəs]: She is a devout Christian and goes to church every week. 她是一名虔诚的基督徒，每周都会去教堂。 rent [rent]: She rented an apartment and needs to pay rent every month. 她租了一套公寓，每个月需要付房租。 represent [ˌreprɪˈzent]: He represented the company and attended the international conference. 他是公司的代表，参加了国际会议。 reservation [ˌrezəˈveɪʃən]: We made a reservation for a dinner table. 我们预订了一个晚餐位子。 revise [rɪˈvaɪz]: She revised her paper to get a better grade. 她修改了论文，以便获得更好的成绩。 rough [rʌf]: The road is very rough and requires careful driving. 这条路很崎岖，需要小心驾驶。 royal [ˈrɔɪəl]: Members of the royal family attended the grand ceremony. 王室成员出席了这次盛大的仪式。 sculpture [ˈskʌlptʃə(r)]: The sculpture was created by a famous artist. 这座雕塑是由著名艺术家创作的。 sensible [ˈsensəbl]: She is sensible and doesn’t act impulsively. 她做事很明智，不会轻易冲动。 similarity [ˌsɪmɪˈlærəti]: There are many similarities in the appearance of these two people. 这两个人的外表有很多相似之处。 similarly [ˈsɪmɪləli]: This issue applies similarly to other similar situations. 这个问题同样适用于其他类似的情况。 soil [sɔɪl]: The soil is very fertile and suitable for growing crops. 这片土地很肥沃，适合种植庄稼。 spicy [ˈspaɪsi]: This dish is very spicy and needs to be eaten carefully. 这道菜很辣，需要小心食用。 spirit [ˈspɪrɪt]: Her spirit is very good and she is full of energy every day. 她的精神状态很不错，每天都充满活力。 spot [spɒt]: We stayed at this spot for an hour. 我们在这个景点停留了一个小时。 stadium [ˈsteɪdiəm]: This stadium can accommodate tens of thousands of spectators. 这个体育场可以容纳数万观众。 staff [stɑːf]: The company has an efficient staff team. 公司雇佣了一支高效的员工团队。 statue [ˈstætʃuː]: The statue was built to commemorate a famous war hero. 这座雕像是为了纪念著名的战争英雄而建造的。 stick [stɪk]: She stirred the bonfire with a stick to make the fire stronger. 她用树枝搅拌着火堆，让火更旺盛。 studio [ˈstjuːdiəʊ]: The artist creates in his own studio. 这个艺术家在自己的工作室里创作。 substance[ˈsʌbstəns]: This medication contains active substances that can alleviate pain. 这种药物含有有效成分，可以缓解疼痛。 summarize [ˈsʌməraɪz]: Please summarize your research findings in one or two sentences. 请用一两句话简要概括你的研究成果。 summary [ˈsʌməri]: The report provides a summary of the company’s performance. 这份报告提供了对公司业绩的简要概述。 symptom [ˈsɪmptəm]: She has symptoms of headache and fever. 她出现了头痛和发热的症状。 tvw tend [tend]: She tended the plants well and made them grow strong. 她照顾得很好，让植物茁壮成长。 tent [tent]: They set up a tent in the wilderness and rested overnight. 他们在野外搭建了帐篷，过夜休息。 throughout [θruːˈaʊt]: This project will be carried out throughout the country. 这个项目将在全国范围内展开。 tire [taɪə(r)]: My tire is flat and needs to be replaced. 我的轮胎瘪了，需要更换。 toe [təʊ]: She is wearing high heels and her toes are uncomfortable. 她穿着高跟鞋，脚趾很不舒服。 ton [tʌn]: This truck can carry ten tons of goods. 这辆卡车可以装载十吨货物。 tour [tʊə(r)]: We went on a city tour. 我们参加了一次城市游览。 trend [trend]: This style of clothing has become a trend in fashion. 这种服装风格已经成为了时尚的潮流。 tube [tjuːb]: She squeezed the paint out of the tube and began painting. 她用管子将颜料挤出来，开始涂画。 various [ˈveəriəs]: This store has various products, and you can choose what you like. 这家商店有各种各样的商品，你可以挑选你喜欢的。 wing [wɪŋ]: The bird spreads its wings and soars in the blue sky. 这只鸟展翅高飞，翱翔在蓝天之上。 worldwide [ˈwɜːldwaɪd]: This news event has attracted attention and discussion worldwide. 这个新闻事件引起了全球的关注和讨论。 B2a abroad [əˈbrɔːd] adv. I want to study abroad to broaden my horizons and experience different cultures. 我想出国留学，开阔视野，体验不同的文化。 accuse [əˈkjuːz] v. He was accused of stealing the money from the company’s safe. 他被指控从公司保险箱里偷了钱。 affair [əˈfeə(r)] n. The scandalous affair between the two celebrities was all over the news. 两位名人之间的丑闻引起了媒体的广泛报道。 agency [ˈeɪdʒənsi] n. I hired a travel agency to help me plan my vacation. 我雇了一家旅行社帮我计划度假。 agenda [əˈdʒendə] n. The agenda for today’s meeting includes discussing the new budget proposal. 今天会议的议程包括讨论新的预算提案。 aggressive [əˈɡresɪv] adj. The aggressive driver cut off other cars on the highway. 这个咄咄逼人的司机在高速公路上削减了其他车辆。 aid [eɪd] n., v. The Red Cross provided aid to the victims of the natural disaster. 红十字会向自然灾害的受害者提供援助。 aircraft [ˈeəkrɑːft] n. The aircraft took off from the runway and soared into the sky. 飞机从跑道起飞，冲上了天空。 anniversary [ˌænɪˈvɜːsəri] n. We celebrate our wedding anniversary every year with a romantic dinner. 我们每年都会用浪漫的晚餐庆祝我们的结婚周年纪念日。 anxious [ˈæŋkʃəs] adj. I’m feeling anxious about the job interview tomorrow. 我对明天的面试感到焦虑不安。 approach [əˈprəʊtʃ] n., v. We need to approach this project with a clear plan and timeline. 我们需要有一个清晰的计划和时间表来处理这个项目。 appropriate [əˈprəʊpriət] adj. It’s not appropriate to wear casual clothes to a formal event. 在正式场合穿休闲服装是不合适的。 approval [əˈpruːvəl] n. I need to get my manager’s approval before taking time off work. 在休假之前，我需要得到经理的批准。 approve [əˈpruːv] v. The board of directors approved the new budget proposal. 董事会通过了新的预算提案。 armed [ɑːmd] adj. The police officers were armed with guns to protect the public. 警察携带着枪支来保护公众。 arms [ɑːmz] n. The country has a large stockpile of nuclear arms as a deterrent. 这个国家有大量的核武器库，作为威慑力量。 artificial [ˌɑːtɪˈfɪʃəl] adj. Some people prefer real flowers, while others don’t mind artificial ones. 有些人喜欢真正的花，而有些人不介意人造花。 artistic [ɑːˈtɪstɪk] adj. She has a natural talent for artistic expression and loves to paint. 她有天生的艺术表达才华，喜欢画画。 aside [əˈsaɪd] adv. Aside from his work, he enjoys playing tennis in his free time. 除了工作之外，他在空闲时间喜欢打网球。 aspect [ˈæspekt] n. One important aspect of the job is good communication skills. 工作的一个重要方面是良好的沟通技巧。 assess [əˈses] v. The teacher needs to assess the students’ understanding of the material before moving on. 老师需要评估学生对材料的理解情况，然后再继续教学。 assessment [əˈsesmənt] n. The assessment of the damage caused by the hurricane will take weeks. 对飓风造成的损害进行评估需要几周时间。 associate [əˈsəʊʃieɪt] v. I don’t want to associate with people who are negative and bring me down. 我不想和那些消极的人为伍，他们会让我失落。 associated [əˈsəʊʃieɪtɪd] adj. The company’s profits are closely associated with economic conditions. 公司的利润与经济状况密切相关。 attorney [əˈtɜːni] n. The attorney provided legal advice to his clients. 律师为他的客户提供了法律咨询。 b bacteria [bækˈtɪəriə] n. 细菌 Bacteria can be harmful to humans, but they also play an important role in the ecosystem. 细菌对人类有害，但它们在生态系统中也扮演着重要的角色。 bent [bent] adj. 弯曲的 The tree was bent over by the strong wind. 树被强风吹弯了。 bitter [ˈbɪtər] adj. 苦的 The medicine tasted very bitter, but it helped me feel better. 药品非常苦，但它帮助我感觉更好。 bond [bɒnd] n. 债券；纽带 The bond between the two sisters was unbreakable. 两个姐妹之间的纽带是牢不可破的。 breast [brest] n. 乳房 Breastfeeding is a natural way to feed a newborn baby. 母乳喂养是一种天然的喂养新生儿的方式。 broad [brɔːd] adj. 宽的；广泛的 The road was too narrow for the large truck, so it had to find a broad one. 这条路对于大卡车来说太窄了，所以它必须找到一条宽阔的路。 broadcast [ˈbrɔːdkæst] v., n. 广播 The news was broadcast on TV and radio. 新闻通过电视和广播播出。 budget [ˈbʌdʒɪt] n. 预算 The company had to cut its budget because of the economic downturn. 由于经济下滑，公司不得不削减其预算。 bunch [bʌntʃ] n. 串；一群 She picked a bunch of flowers from the garden to decorate the table. 她从花园里采了一串花来装饰桌子。 bush [bʊʃ] n. 灌木丛 The rabbit disappeared into the bush and we couldn’t find it. 兔子跑进了灌木丛中，我们找不到它了。 c capable [ˈkeɪpəbl] adj. 有能力的 She is capable of handling multiple tasks at the same time. 她有能力同时处理多项任务。 capacity [kəˈpæsəti] n. 容量；能力 The stadium has a seating capacity of 50,000 people. 这个体育场有5万个座位。 chief [tʃiːf] adj., n. 主要的；首席 The chief executive officer of the company announced his resignation. 公司首席执行官宣布辞职。 circumstance [ˈsɜːrkəmstæns] n. 环境；情况 I cannot control the circumstance, but I can control my attitude towards it. 我无法控制环境，但我可以控制自己对它的态度。 cite [saɪt] v. 引用 He cited several examples to support his argument. 他引用了几个例子来支持他的论点。 civil [ˈsɪvl] adj. 文明的；民事的 It is important to have a civil conversation even if you disagree with someone. 即使你不同意对方的观点，也很重要要有一次文明的对话。 closely [ˈkləʊsli] adv. 密切地 The two countries have a closely connected history. 这两个国家有着密切相关的历史。 commission [kəˈmɪʃn] n., v. 委员会；委托 The commission was established to investigate the causes of the accident. 委员会成立以调查事故的原因。 commitment [kəˈmɪtmənt] n. 承诺；投入 He has a strong commitment to his work and always strives to do his best. 他对工作有很强的投入感，并始终努力做到最好。 committee [kəˈmɪti] n. 委员会 The committee meets every month to discuss current issues in the community. 委员会每个月都会开会讨论社区当前的问题。 commonly [ˈkɒmənli] adv. 通常地 It is commonly believed that exercise is good for your health. 人们通常认为运动对健康有益。 concentration [ˌkɒnsənˈtreɪʃn] n. 集中；浓度 She needs to improve her concentration if she wants to do well on the exam. 如果她想在考试中表现良好，就需要提高她的注意力集中度。 concept [ˈkɒnsept] n. 概念 The concept of time is difficult for young children to understand. 时间的概念对于年幼的孩子来说是很难理解的。 concern [kənˈsɜːn] n., v. 关注；担心 The safety of our employees is a major concern for the company. 员工的安全是公司的重要关注点。 concerned [kənˈsɜːnd] adj. 担心的；有关的 I am concerned about the impact of pollution on the environment. 我担心污染对环境的影响。 conduct [kənˈdʌkt] v., n. 进行；行为 He conducted a thorough investigation into the matter. 他对这件事进行了彻底的调查。 confidence [ˈkɒnfɪdəns] n. 自信；信心 Having confidence in yourself is important for achieving your goals. 对自己有信心对于实现目标很重要。 congress [ˈkɒŋɡres] n. 国会；代表大会 The congress will vote on the proposed legislation next week. 国会将在下周对拟议中的法案进行投票。 conscious [ˈkɒnʃəs] adj - &gt; . 有意识的；清醒的roundings and knew exactly where she was. 她清醒地意识到周围的环境知道自己在哪里。 conservative [kənˈsɜːvətɪv] adj., n. 保守的；保守主义者 He has conservative views on social issues such as abortion and same-sex marriage. 他在社会问题，如堕胎和同性婚姻方面持保守观点。 consideration [kənˌsɪdəˈreɪʃn] n. 考虑；关注 Take all factors into consideration before making a decision. 在做出决定之前，考虑所有因素。 consistent [kənˈsɪstənt] adj. 一致的；持续的 She has been consistent in her efforts to improve her performance at work. 她一直致力于提高自己在工作中的表现，持续不断。 constant [ˈkɒnstənt] adj. 不断的；恒定的 The noise from the construction site was a constant source of annoyance for the residents. 建筑工地的噪音是居民不断的困扰。 constantly [ˈkɒnstəntli] adv. 不断地；经常地 He is constantly looking for ways to improve his skills. 他经常寻找提高自己技能的方法。 construct [kənˈstrʌkt] v. 建造；构建 The company plans to construct a new office building next year. 公司计划明年建造一座新的办公楼。 construction [kənˈstrʌkʃn] n. 建筑；构造 The construction of the bridge was completed ahead of schedule. 桥梁的建造提前完成。 contemporary [kənˈtempəreri] adj. 当代的；现代的 The artist’s work is considered to be a masterpiece of contemporary art. 这位艺术家的作品被认为是当代艺术的杰作。 contest [ˈkɒntest] n., v. 竞赛；争议 He won first place in the singing contest last year. 他去年在歌唱比赛中获得了第一名。 convinced [kənˈvinst] adj. 确信的；信服的 I am convinced that this is the right decision. 我确信这是正确的决定。 corporate [ˈkɔːpərət] adj. 公司的；企业的 The corporate culture of the company emphasizes teamwork and collaboration. 公司的企业文化强调团队合作和协作。 council [ˈkaʊnsəl] n. 委员会；议会 The city council passed a new law to protect the environment. 市议会通过了一项新的法律以保护环境。 credit [ˈkredɪt] v. 归功于；赞扬 She credited her success to hard work and dedication. 她把自己的成功归功于勤奋和专注。 crew [kruː] n. 全体工作人员；船员 The film crew worked long hours to complete the movie on time. 电影制作组加班赶工，按时完成了电影制作。 crisis [ˈkraɪsɪs] n. 危机；危急关头 The company is facing a financial crisis and may have to lay off employees. 公司面临财务危机，可能不得不裁员。 criterion [kraɪˈtɪərɪən] n. 标准；准则 The selection committee used several criteria to evaluate the candidates. 选拔委员会使用了几个标准来评估候选人。 critic [ˈkrɪtɪk] n. 评论家；批评者 The film received mixed reviews from critics. 这部电影在评论家中获得了好坏参半的评价。 critical [ˈkrɪtɪkəl] adj. 批评的；关键的 The project is at a critical stage and needs to be carefully managed. 这个项目处于关键阶段，需要仔细管理。 criticism [ˈkrɪtɪsɪzəm] n. 批评；指责 He faced a lot of criticism for his handling of the situation. 他因应对这种情况而受到了很多批评。 crucial [ˈkruːʃl] adj. 至关重要的；决定性的 The next few months will be crucial for the success of the project. 接下来的几个月对于项目的成功至关重要。 d debate [dɪˈbeɪt] n. 辩论；讨论，v. 辩论；讨论 The candidates engaged in a heated debate over healthcare reform. 候选人们就医疗改革展开了激烈的辩论。 decline [dɪˈklaɪn] v. 下降；减少，n. 下降；衰退 The company’s profits have been declining for several years. 该公司的利润已经连续几年下降。 defend [dɪˈfend] v. 辩护；保卫 The lawyer will defend his client in court. 律师将在法庭上为他的客户辩护。 defense [dɪˈfens] n. 防御；防护 The army is responsible for the defense of the country. 军队负责国家的防卫。 deliberate [dɪˈlɪbərət] adj. 故意的；深思熟虑的，v. 慎重考虑 The jury took several hours to reach a deliberate verdict. 陪审团花了几个小时才作出慎重的判决。 deliberately [dɪˈlɪbərətli] adv. 故意地；蓄意地 She deliberately ignored my calls. 她故意不理我的电话。 demand [dɪˈmænd] n. 需求，v. 要求；需要 The demand for organic food has increased in recent years. 近年来，有机食品的需求增加了。 demonstrate [ˈdɛmənstreɪt] v. 示范；演示 The teacher demonstrated how to solve the math problem on the board. 老师在黑板上演示了如何解决数学问题。 depressed [dɪˈprɛst] adj. 沮丧的；消沉的 She has been feeling depressed since she lost her job. 自从失业后，她一直感到沮丧。 depressing [dɪˈprɛsɪŋ] adj. 令人沮丧的；令人失望的 The news about the economy was very depressing. 关于经济的消息非常令人沮丧。 desert [dɪˈzɜːt] v. 抛弃，n. 沙漠 He deserted his wife and children and ran away with another woman. 他抛弃了妻子和孩子，和另一个女人私奔了。 deserve [dɪˈzɜːv] v. 应得；值得 She deserves to be praised for her hard work. 她应该因她的努力工作而受到赞扬。 desperate [ˈdɛspərət] adj. 绝望的；拼命的 The refugees were in a desperate situation and needed immediate help. 难民们处于绝望的境地，需要立即的帮助。 discipline [ˈdɪsəplɪn] n. 纪律；训练，v. 训练；惩罚 The school has a strict discipline policy. 学校有一套严格的纪律政策。 distribute [dɪsˈtrɪbjuːt] v. 分发；分配 The charity organization distributed food and blankets to the homeless. 慈善组织向无家可归者分发食物和毛毯。 distribution [ˌdɪstrɪˈbjuːʃn] n. 分配；分发 The distribution of wealth in the country is very unequal. 该国的财富分配非常不均衡。 divide [dɪˈvaɪd] n. 分开；分裂 The river divides the city into two parts. 这条河把城市分成了两部分。 division [dɪˈvɪʒn] n. 分开；分裂；部门 The company has several divisions that specialize in different products. 该公司有几个专门从事不同产品的部门。 domestic [dəˈmɛstɪk] adj. 国内的；家庭的 She prefers to buy domestic products to support the local economy. 她更喜欢购买国内产品来支持当地经济。 dominate [ˈdɒmɪneɪt] v. 主导；占优势 The team has dominated the league for the past few years. 这支队伍在过去几年里主导了联赛。 downward [ˈdaʊnwəd] adj. 向下的，adv. 向下地 The stock market experienced a downward trend last week. 股市上周经历了一个下降趋势。 dozen [ˈdʌzn] n. 一打，det. 十二个 I need a dozen eggs to make a cake. 我需要一打鸡蛋来做蛋糕。 draft [drɑːft] n. 草稿；草案，v. 起草；征召 Please send me the latest draft of the report. 请把最新的报告草稿发给我。 dramatic [drəˈmætɪk] adj. 戏剧性的；引人注目的 The play had a dramatic ending that left the audience in tears. 这部戏剧有一个引人注目的结局，让观众流泪。 e edition [ɪˈdɪʃn] n. 版本；版本号 I have the latest edition of the Oxford English Dictionary. 我有牛津英语词典的最新版本。 elderly [ˈɛldərli] adj. 年长的；年老的 The elderly man needed help crossing the street. 那位年长的男子需要帮助过马路。 elect [ɪˈlɛkt] v. 选举；选择 The people will elect a new president next year. 人民将在明年选举新总统。 elsewhere [ˈɛlsweə] adv. 在别处；到别处 If they don’t have it here, you might find it elsewhere. 如果他们这里没有，你可能会在别处找到它。 emerge [ɪˈmɜːdʒ] v. 出现；浮出水面 The sun emerged from behind the clouds. 太阳从云层后面出现了。 emotional [ɪˈməʊʃənl] adj. 情绪的；感情的 She gave an emotional speech at the funeral. 她在葬礼上发表了感人的演讲。 emphasis [ˈɛmfəsɪs] n. 强调；重点 The emphasis in this sentence is on the word “important”. 这个句子中重点在于“重要”这个词。 emphasize [ˈɛmfəsaɪz] v. 强调；着重说明 The teacher emphasized the importance of studying hard. 老师强调了努力学习的重要性。 encounter [ɪnˈkaʊntə] v. 遇到；邂逅，n. 遭遇；邂逅 She encountered an old friend on the street. 她在街上遇见了一位老朋友。 engage [ɪnˈɡeɪdʒ] v. 与订婚 I’m planning to engage in some volunteer work this summer. 我计划在这个夏天参加一些志愿者工作。 ensure [ɪnˈʃʊr] v. 确保；保证 Please ensure that the door is locked before you leave. 请确保在离开前门已经锁好了。 enthusiasm [ɛnˈθjuːziæzəm] n. 热情；热忱 Her enthusiasm for the project was contagious. 她对这个项目的热情是具有感染力的。 enthusiastic [ɛnˌθjuːziˈæstɪk] adj. 热情的；热心的 The students were enthusiastic about the field trip. 学生们对这次实地考察充满热情。 estate [ɪˈsteɪt] n. 房地产；财产 He inherited a large estate from his grandfather. 他从祖父那里继承了一大笔财产。 estimate [ˈɛstɪmeɪt] v. 估计；评估，n. 估计；评估 Can you give me an estimate of how long the project will take? 你能给我一个项目需要多长时间的估计吗？ ethical [ˈɛθɪkəl] adj. 道德的；伦理的 It is important to make ethical decisions in the workplace. 在工作场所做出道德决策是很重要的。 evaluate [ɪˈvæljueɪt] v. 评价；评估 The teacher will evaluate the students’ essays based on several criteria. 老师将根据几个标准评价学生的论文。 evil [ˈiːvl] adj. 邪恶的；恶毒的，n. 邪恶；罪恶 The evil queen in the fairy tale was jealous of Snow White’s beauty. 童话中的邪恶女王嫉妒白雪公主的美貌。 examination [ɪɡˌzæmɪˈneɪʃn] n. 考试；检查 He passed the final examination with flying colors. 他以优异的成绩通过了期末考试。 executive [ɪɡˈzɛkjʊtɪv] n. 执行官；高管，adj. 行政的；执行的 The executive team is responsible for making important decisions. 执行团队负责做出重要决策。 exhibit [ɪɡˈzɪbɪt] v. 展示；展览，n. 展览品；展览会 The museum will exhibit a collection of ancient artifacts next month. 博物馆下个月将展出一组古代文物。 existence [ɪɡˈzɪstəns] n. 存在；实在 The existence of aliens is still a topic of debate among scientists. 外星人的存在仍然是科学家们争论的话题。 expense [ɪkˈspɛns] n. 费用；支出 The company had to cut expenses in order to stay in business. 公司不得不削减开支以保持运营。 exploration [ˌɛkspləˈreɪʃn] n. 探索；勘探 The astronauts conducted an exploration of the surface of the moon. 宇航员们对月球表面进行了一次探索。 expose [ɪkˈspəʊz] v. 暴露；揭露 The journalist exposed the corruption scandal. 记者揭露了腐败丑闻。 extend [ɪkˈstɛnd] v. 扩展；延伸 The company plans to extend its operations to other countries. 公司计划将业务展到其他国家。 extent [ɪkˈstɛnt] n. 程度；范围 The extent of the damage caused by the earthquake was enormous. 地震造成的破坏范围极大。 external [ɪkˈstɜːrnəl] adj. 外部的；外在的 The company faced external pressures from competitors and economic conditions. 公司面临来自竞争对手和经济环境的外部压力。 extraordinary [ɪkˈstrɔːdnri] adj. 非凡的；特别的 The athlete’s performance was extraordinary and broke the world record. 这位运动员的表现非凡，打破了世界纪录。 extreme [ɪkˈstriːm] n. 极端；极度，adj. 极端的；极度的 The weather conditions were extreme during the hurricane. 飓风期间天气条件极端。 f facility [fəˈsɪlɪti] n. 设施；场所 The new facility has state-of-the-art equipment. 新设施拥有最先进的设备。 favor [ˈfeɪvər] v. 偏爱；支持 I would favor a candidate who has experience in the field. 我会支持一个在这个领域有经验的候选人。 feather [ˈfɛðər] n. 羽毛 The bird’s feathers were brightly colored. 鸟的羽毛色彩鲜艳。 fee [fiː] n. 费用；酬金 The lawyer’s fee for the consultation was quite high. 律师咨询的费用相当高。 feed [fiːd] n. 饲料；喂养，v. 喂食 The farmer feeds the cows with fresh hay. 农民用新鲜的干草喂牛。 fellow [ˈfɛləʊ] adj. 同伴的；同事的 My fellow colleagues and I worked on the project together. 我和我的同事们一起合作完成了这个项目。 figure [ˈfɪɡər] v. 认为；描绘，n. 数字；图形 I can’t figure out how to solve this problem. 我想不出如何解决这个问题。 finance [ˈfaɪnæns] n. 财政；金融，v. 资助；融资 The company needs to finance its new project. 公司需要为新项目提供资金。 finding [ˈfaɪndɪŋ] n. 发现；调查结果 The findings of the study were surprising. 研究结果令人吃惊。 firm [fɜːm] n. 公司；企业，adj. 坚定的；牢固的 The law firm specializes in corporate law. 这个律师事务所专门从事公司法律事务。 flame [fleɪm] n. 火焰；烈焰 The flame of the candle flickered in the breeze. 蜡烛的火焰在微风中摇曳。 flexible [ˈflɛksəbl] adj. 灵活的；可弯曲的 The yoga instructor taught us how to be more flexible. 瑜伽教练教我们如何更加灵活。 float [fləʊt] v. 漂浮；飘动 The boat floated down the river. 小船漂下了河流。 forgive [fəˈɡɪv] v. 原谅；宽恕 I hope you can forgive me for my mistake. 我希望你能原谅我犯的错误。 former [ˈfɔːmər] adj. 以前的；从前的 The former president was known for his diplomatic skills. 前总统以其外交手腕而著称。 fortune [ˈfɔːtʃuːn] n. 运气；财富 He made his fortune in the stock market. 他在股市里赚了大钱。 forward [ˈfɔːwəd] adj. 向前的；前进的 The team made a forward pass to score the winning goal. 球队进行一次向前传球以进入胜利的进球。 found [faʊnd] v. 创立；建立 The company was founded in 2001. 这家公司成立于2001年。 frequency [ˈfriːkwənsi] n. 频率；频繁 The frequency of earthquakes in this region is quite high. 这个地区地震的频率相当高。 fuel [fjuːəl] v. 加油；加燃料 I need to fuel up my car before we go on our road trip. 我们去公路旅行前我需要给汽车加油。 fully [ˈfʊli] adv. 完全地；彻底地 I fully support your decision. 我完全支持你的决定。 fund [fʌnd] n. 基金；资金，v. 提供资金；资助 The company raised funds to support the new project. 公司筹集资金支持新项目。 fundamental [ˌfʌndəˈmɛntl] adj. 基本的；根本的 The fundamental principles of mathematics are the building blocks of higher-level concepts. 数学的基本原理是高层次概念的基石。 funding [ˈfʌndɪŋ] n. 资金；资助 The school received funding from the government for a new science lab. 学校获得政府的资助建设了一座新的科学实验室。 furthermore [ˌfɜːrðəˈmɔːr] adv. 而且；此外 The company is expanding its operations, furthermore, it plans to enter new markets. 公司正在扩大其业务，而且计划进入新市场。 ghi gain [ɡeɪn] v. 获得；增加，n. 增益；收益 I hope to gain more knowledge through my studies. 我希望通过学习获得更多知识。 gang [ɡæŋ] n. 帮派；团伙 The police arrested several members of the gang. 警方逮捕了该团伙的几名成员。 genre [ˈʒɑːnrə] n. 类型；流派 I enjoy reading books of different genres. 我喜欢阅读不同类型的书籍。 govern [ˈɡʌvn] v. 统治；管理 The government is responsible for governing the country. 政府负责管理国家。 governor [ˈɡʌvnər] n. 州长；省长 The governor of California is a former movie star. 加州州长是一位前电影明星。 grade [ɡreɪd] v. 给…评分；分等级，n. 等级；年级 The teacher will grade our essays based on grammar and content. 老师将根据语法和内容为我们的文章评分。 gradually [ˈɡrædʒuəli] adv. 逐渐地；逐步地 My mood gradually lightened. 我的心情渐渐好起来。 grand [ɡrænd] adj. 壮丽的；宏伟的；重大的 The grand entrance to the building was decorated with ornate carvings. 建筑的宏伟入口装饰着华丽的雕刻。 grant [ɡrænt] v. 授予；允许；同意，n. 拨款；授予 The university granted her a scholarship to study abroad. 这所大学授予她一笔留学奖学金。 guarantee [ˌɡærənˈtiː] v. 保证；担保，n. 保证；担保 The company guarantees the quality of its products. 公司保证其产品的质量。 hearing [ˈhɪrɪŋ] n. 听力；听觉；审讯 My hearing isn’t as good as it used to be. 我的听力不如以前。 heel [hiːl] n. 脚后跟；鞋跟 She twisted her ankle when she landed on her heel. 她着陆时脚后跟着地扭伤了脚踝。 hire [haɪr] v. 雇佣；租借，n. 租用；雇佣 The company decided to hire a new marketing manager. 公司决定雇佣一位新的市场经理。 hollow [ˈhɑːloʊ] adj. 空心的；凹陷的；空洞的 The old tree had a hollow trunk. 那棵老树的树干是空心的。 household [ˈhaʊshoʊld] n. 家庭；家居，adj. 家庭的；家居的 She manages the household finances. 她管理家庭财务。 housing [ˈhaʊzɪŋ] n. 住房；住宅 The city needs more affordable housing for its residents. 这个城市需要更多供居民负担得起的住房。 ideal [aɪˈdiːl] n. 理想；典范，adj. 理想的；完美的 She is the ideal candidate for the job. 她是这份工作的理想人选。 illustrate [ˈɪləstreɪt] v. 阐述；举例说明；给…画插图 The professor used diagrams to illustrate his point. 教授用图表来阐述他的观点。 illustration [ˌɪləˈstreɪʃn] n. 插图；说明；例证 The book contains many beautiful illustrations. 这本书包含许多漂亮的插图。 imagination [ɪˌmædʒɪˈneɪʃn] n. 想象力；幻想 Her imagination was sparked by the fantasy novel. 她的想象力因为这本幻想小说而被激发。 imply [ɪmˈplaɪ] v. 暗示；意味着 His tone implied that he wasn’t happy with the decision. 他的语气暗示他对这个决定不满意。 impose [ɪmˈpoʊz] v. 强制实行；征税；把…强加于 The new law imposes stricter regulations on the industry. 新法规对该行业实行更严格的监管。 impress [ɪmˈpres] v. 使印象深刻；给…留下印象 The teacher’s dedication impressed the students. 老师的奉献精神给学生留下了深刻的印象。 impressed [ɪmˈprest] adj. 印象深刻的；感到钦佩的 I was impressed by his ability to speak five languages fluently. 他能流利地讲五种语言让我印象深刻。 inch [ɪntʃ] n. 英寸 The length of the box is twelve inches. 盒子的长度是十二英寸。 incident [ˈɪnsɪdənt] n. 事件；事故 The police are investigating the incident. 警方正在调查这起事件。 jlmn junior [ˈdʒuːniə(r)] adj. （年龄、职位等）较低的，（学校）初年级的 My sister is a junior in high school. （我）妹妹在高中读初三。 justify [ˈdʒʌstɪfaɪ] v. 证明…是正当的，为…辩护 Can you justify your actions? 你能证明你的行为是正当的吗？ labor [ˈleɪbə(r)] n. 劳动，工作 The workers were doing hard labor all day. 工人们整天都在进行艰苦的劳动。 landscape [ˈlændskeɪp] n. 风景，景色 The landscape of the countryside is beautiful. 乡村的风景很美丽。 largely [ˈlɑːdʒli] adv. 大部分地，主要地 The success of the project was largely due to her efforts. 该项目的成功主要归功于她的努力。 launch [lɔːntʃ] v., n. 发射，推出，开始 We will launch the new product next month. 我们将在下个月推出新产品。 league [liːɡ] n. 联盟，联合会 The two teams are in the same league. 这两支队伍在同一个联盟中。 lean [liːn] v. 倾斜，靠着 She leaned against the wall and closed her eyes. 她靠在墙上闭上了眼睛。 lively [ˈlaɪvli] adj. 活泼的，热闹的 The party was a lively affair. 派对很热闹。 loan [ləʊn] n. 贷款，借款 He took out a loan to buy a new car. 他借款买了一辆新车。 logical [ˈlɒdʒɪkl] adj. 合乎逻辑的，有道理的 His argument seemed logical. 他的论点似乎是有道理的。 long-term [ˌlɒŋˈtɜːm] adj., adv. 长期的，长期地 We need to think about the long-term consequences of our actions. 我们需要考虑我们行动的长期影响。 loose [luːs] adj. 宽松的，不紧的 The dress was too loose for her. 这件裙子对她来说太宽松了。 lord [lɔːd] n. 贵族，上帝 He was made a lord for his services to the government. 他因对政府的服务而被封为贵族。 lower [ˈlaʊə(r)] v. 降低，减少 The government has lowered taxes. 政府已经降低了税收。 lung [lʌŋ] n. 肺 Smoking can damage your lungs. 吸烟会损害你的肺部。 maintain [meɪnˈteɪn] v. 维持，保持 It’s important to maintain a healthy lifestyle. 保持健康的生活方式很重要。 major [ˈmeɪdʒə(r)] n. 主修科目，少校 My major in college was English. 我在大学的主修科目是英语。 majority [məˈdʒɒrəti] n. 大多数，大部分 The majority of people in this country speak English. 这个国家的大多数人说英语。 mass [mæs] n., adj. 大量的，质量，群众 There was a mass of people on the street. 街上有很多人。 massive [ˈmæsɪv] adj. 巨大的，大规模的 The earthquake caused massive damage. 地震造成了巨大的破坏。 matching [ˈmætʃɪŋ] adj. 相配的，一致的 She wore a dress and shoes that were perfectly matching. 她穿着完美搭配的裙子和鞋子。 material [məˈtɪəriəl] adj. 物质的，重要的 The books provided valuable material for the research. 这些书为研究提供了有价值的材料。 means [miːnz] n. 手段，方法 We need to find a means of solving this problem. 我们需要找到解决这个问题的方法。 measurement [ˈmeʒəmənt] n. 测量，度量 The measurement of the room is 5 meters by 6 meters. 房间的尺寸是5米乘6米。 melt [melt] v. 融化，熔化 The ice melted in the sun. 冰在阳光下融化了。 military [ˈmɪlɪtri] adj., n. 军事的，军队 He joined the military when he was 18. 他18岁时加入了军队。 mineral [ˈmɪnərəl] n. 矿物质 Minerals are important for our health. 矿物质对我们的健康很重要。 minister [ˈmɪnɪstə(r)] n. 部长，牧师 The prime minister met with the foreign minister. 总理会见了外交部长。 minor [ˈmaɪnə(r)] adj. 较小的，次要的 The problem was only a minor one. 这个问题只是个小问题。 minority [maɪˈnɒrəti] n. 少数派 The minority of people in this country speak a different language. 这个国家的少数人说其他语言。 mission [ˈmɪʃn] n. 任务，使命 Their mission was to rescue the hostages. 他们的任务是解救人质。 mixed [mɪkst] adj. 混合的，杂色的 The paint was a mixed color of blue and green. 油漆是蓝色和绿色混合的颜色。 moral [ˈmɒrəl] adj., n. 道德的，寓意 The story had a moral lesson. 这个故事有一个道德教训。 motor [ˈməʊtə(r)] n., adj. 发动机，机动的 The car had a problem with its motor. 这辆车发动机有问题。 mount [maʊnt] v. 登上，安装 He mounted the horse and rode away. 他骑上马匹离开了。 multiply [ˈmʌltɪplaɪ] v. 乘，增加 If you multiply 5 by 3, you get 15. 如果你把5乘以3，得到15。 negative [ˈneɡətɪv] n. 否定，负数，消极的 He answered with a negative. 他回答了一个否定。 nerve [nɜːv] n. 神经，勇气 He had the nerve to ask for more money. 他有胆量要求更多的钱。 nevertheless [ˌnevəðəˈles] adv. 然而，不过 He was injured, nevertheless he continued to play. 他受伤了，然而他继续比赛。 notion [ˈnəʊʃn] n. 概念，想法 I have no notion of what to do next. 我不知道下一步该怎么做。 numerous [ˈnjuːmərəs] adj. 众多的，许多的 There are numerous reasons why I can’t come. 有很多原因我不能来。 o object v. - /ˈɒbdʒɛkt/ - 动词 She will object to the proposal at the meeting tomorrow. 她明天在会议上会反对这个提议。 objective n. - /əbˈdʒɛktɪv/ - 名词 The objective of the project is to increase sales by 20%. (这个项目的目标是将销售额增加2 。 objective adj. - /əbˈdʒɛktɪv/ - 形容词 The report provides an objective analysis of the situation. 报告提供了对情况的客观分析。 obligation n. - /ˌɒblɪˈɡeɪʃən/ - 名词 It is your obligation to pay your bills on time. 按时支付你的账单是你的义务。 observation n. - /ˌɒbzəˈveɪʃən/ - 名词 The scientist made some interesting observations about the behavior of the monkeys. 科学家对猴子的行为作了一些有趣的观察。 observe v. - /əbˈzɜːv/ - 动词 We can observe the stars with a telescope. 我们可以用望远镜观察星星。 obtain v. - /əbˈteɪn/ - 动词 It is easy to obtain a visa if you have all the necessary documents. 如果你拥有所有必要的文件，获得签证很容易。 occasionally adv. - /əˈkeɪʒənəli/ - 副词 I occasionally go to the gym to work out. 我时不时去健身房锻炼身体。 offend v. - /əˈfɛnd/ - 动词 I’m sorry if I offended you with my remark. 如果我说的话冒犯了你，我很抱歉。 offense n. - /əˈfɛns/ - 名词 Parking in a disabled spot is a serious offense. 停车在残疾人位上是一个严重的违法行为。 offensive adj. - /əˈfɛnsɪv/ - 形容词 His language was very offensive and hurtful. 他的语言非常冒犯和伤人。 opponent n. - /əˈpəʊnənt/ - 名词 The two opponents will face each other in the boxing ring tonight. 今晚两名对手将在拳击比赛中交锋。 oppose v. - /əˈpəʊz/ - 动词 The protesters opposed the new law. 抗议者反对新法律。 opposed adj. - /əˈpəʊzd/ - 形容词 The two sides were opposed to each other’s plans. 双方对彼此的计划持反对态度。 opposition n. - /ˌɒpəˈzɪʃən/ - 名词 The opposition party has been criticizing the government’s policies. 反对党一直在批评政府的政策。 organ n. - /ˈɔːɡən/ - 名词 The heart is an important organ in the human body. 心脏是人体中重要的器官。 origin n. - /ˈɒrɪdʒɪn/ - 名词 The origin of the universe is still a mystery. 宇宙的起源仍然是一个谜。 otherwise adv. - /ˈʌðəwaɪz/ - 副词 You need to finish your homework, otherwise you won’t pass the test. 否则你就无法通过考试，你需要完成家庭作业。 outcome n. - /ˈaʊtkʌm/ - 名词 The outcome of the election was a surprise to many people. 选举的结果让许多人感到惊讶。 outer adj. - /ˈaʊtə/ - 形容词 The outer planets in our solar system are all gas giants. 我们太阳系中的外行星都是气态巨型行星。 outline n. - /ˈaʊtlaɪn/ - 名词 The professor gave us an outline of the course syllabus. 教授给我们提供了课程大纲的概述。 outline v. - /ˈaʊtlaɪn/ - 动词 Please outline your proposal for the new project. 请概述你对新项目的提议。 overall adj. - /ˌəʊvərˈɔːl/ - 形容词 The overall goal of the company is to increase profits. 公司的总体目标是增加利润。 overall adv. - /ˌəʊvərˈɔːl/ - 副词 Overall, it was a great experience. 总的来说，这是一次很好的经历。 owe v. - /əʊ/ - 动词 I owe my success to my parents’ support. 我把我的成功归功于父母的支持。 p pace n. - /peɪs/ - 名词 He walked at a slow pace. 他走得很慢。 pace v. - /peɪs/ - 动词 She paced back and forth nervously. 她紧张地来回踱步。 panel n. - /ˈpænəl/ - 名词 The panel of judges will announce the winner tomorrow. 评委会将于明天宣布获胜者。 participant n. - /pɑːˈtɪsɪpənt/ - 名词 All participants will receive a certificate of attendance. 所有参与者将获得出席证书。 partly adv. - /ˈpɑːtli/ - 副词 The delay was partly due to bad weather. 延误部分是由于恶劣天气。 passage n. - /ˈpæsɪdʒ/ - 名词 The novel has many poetic passages. 这部小说有许多诗意的段落。 permanent adj. - /ˈpɜːmənənt/ - 形容词 The ink is permanent and cannot be erased. 这种墨水是永久的，无法擦除。 permit v. - /pəˈmɪt/ - 动词 The company will not permit employees to smoke on the premises. 公司不允许员工在场地内吸烟。 permit n. - /ˈpɜːmɪt/ - 名词 You need a permit to park in this area. 你需要一个许可证才能停车在这个区域内。 perspective n. - /pəˈspɛktɪv/ - 名词 The artist used perspective to create a sense of depth in the painting. 艺术家利用透视法在画中营造出深度感。 phase n. - /feɪz/ - 名词 The project is currently in its final phase. 该项目目前正在进行最后阶段。 phenomenon n. - /fɪˈnɒmɪnən/ - 名词 The aurora borealis is a natural phenomenon in the polar regions. 极光是极地地区的自然现象。 philosophy n. - /fɪˈlɒsəfi/ - 名词 Her philosophy on life is to live in the moment. 她的人生哲学是活在当下。 pile n. [paɪl] 名词：一堆，一叠； verb. [paɪl] 动词：堆积，积累。 There was a pile of books on the table. 桌子上堆着一叠书。 pitch n. [pɪtʃ] 名词：音高，场地，推销； verb. [pɪtʃ] 动词：投掷，推销。 The pitch of the song was too high for me to sing. 这首歌的音高对我来说太高了。 plain adj. [pleɪn] 形容词：简单的，朴素的。 She wore a plain white shirt. 她穿了一件朴素的白衬衫。 plot v. [plɑt] 动词：密谋，策划，绘制； 名词：情节，图表。 He is plotting revenge against his enemies. 他正在密谋着对他的敌人复仇。 pointed adj. [pɔɪntɪd] 形容词：尖锐的，尖角的。 The pointed roof of the tower was visible from far away. 塔楼的尖顶从远处可见。 popularity n. [ˌpɑpjuˈlærəti] 名词：受欢迎，流行。 The popularity of this brand among young people is increasing. 这个品牌在年轻人中的受欢迎程度正在增加。 pose v. [poʊz] 动词：摆姿势，引起困难，提出问题。 He posed for a photograph in front of the Eiffel Tower. 他在艾菲尔铁塔前摆着姿势照相。 positive n. [ˈpɑzətɪv] 名词：肯定，正面，阳极。 He has a positive attitude towards life. 他对生活持积极的态度。 possess v. [pəˈzɛs] 动词：拥有，具有。 She possesses a great talent for singing. 她具有非常出色的歌唱天赋。 praise n. [preɪz] 名词：赞扬，夸奖； verb. [preɪz] 动词：赞美，夸奖。 The teacher praised the student for his hard work. 老师因为学生的努力工作而表扬了他。 presence n. [ˈprɛzəns] 名词：出席，存在，氛围。 His presence at the meeting was required. 他需要出席会议。 preserve v. [prɪˈzɜrv] 动词：保存，保护，腌制。 She learned how to preserve fruits and vegetables for the winter. 她学会了如何保存水果和蔬菜过冬。 prime adj. [praɪm] 形容词：主要的，最好的，黄金时期的。 He was in his prime when he won the championship. 他在赢得冠军时正值黄金年华。 principal adj. [ˈprɪnsəpl] 形容词：首要的，主要的，校长。 The principal of the school greeted the students on the first day of class. 学校的校长在上课的第一天向学生们问好。 principle n. [ˈprɪnsəpl] 名词：原则，准则，道义。 He always adheres to the principle of honesty. 他始终坚持诚实的原则。 priority n. [praɪˈɔrəti] 名词：优先权，重要性。 Safety is our top priority. 安全是我们最重要的事情。 procedure n. [prəˈsidʒər] 名词：程序，步骤。 The procedure must be followed in a specific order. 步骤必须按照特定的顺序进行。 produce n. [ˈprɑdus] 名词：产品，农产品； verb. [prəˈdus] 动词：生产，制造。 The factory produces 1000 cars every day. 工厂每天生产1000辆汽车。 proof n. [pruf] 名词：证明，证据，酒精度数。 He showed me the proof of his innocence. 他向我展示了他清白的证据。 proposal n. [prəˈpoʊzəl] 名词：提议，建议。 I accepted his proposal of marriage. 我接受了他的求婚。 propose v. [prəˈpoʊz] 动词：建议，提议，求婚。 He proposed a new idea for the company’s marketing strategy. 他提出了公司的市场营销策略的新想法。 prospect n. [ˈprɑspɛkt] 名词：前景，展望，可能性。 The prospect of finding a job after graduation is uncertain. 毕业后找到工作的前景是不确定的。 psychologist n. [saɪˈkɑlədʒɪst] 名词：心理学家。 The psychologist helped him deal with his anxiety. 心理学家帮助他处理他的焦虑。 psychology n. [saɪˈkɑlədʒi] 名词：心理学。 He studied psychology at university. 他在大学学习了心理学。 publication n. [ˌpʌblɪˈkeɪʃn] 名词：出版物，刊物。 The publication of the new novel has been delayed. 新小说的出版被推迟了。 pursue v. [pərˈsu] 动词：追求，追赶，继续进行。 He decided to pursue his dreams and become a musician. 他决定追求自己的梦想，成为一名音乐家。 r realistic adj. [riəˈlɪstɪk] 形容词：现实的，实际的。 She has a realistic approach to problem-solving. 她对解决问题有一种现实的方法。 recall v. [riˈkɔl] 动词：回忆起，召回。 He couldn’t recall the name of the movie he saw last night. 他忘记了昨晚看的电影的名字。 recover v. [rɪˈkʌvər] 动词：康复，恢复。 He is recovering from a cold. 他正在从感冒中康复。 reduction n. [rɪˈdʌkʃən] 名词：减少，削减。 The company announced a reduction in staff. 公司宣布裁员。 regard v. [rɪˈɡɑrd] 动词：关心，看待，尊重； 名词：关注，尊敬。 He regards his family and friends as the most important thing in his life. 他认为他的家人和朋友是他生活中最重要的事情。 regional adj. [ˈridʒənl] 形容词：地区的，区域的。 The regional government is responsible for local affairs. 地方政府负责本地事务。 regulation n. [ˌrɛɡjʊˈleɪʃn] 名词：规定，条例，管理。 The government has introduced new regulations to improve traffic safety. 政府出台了新的规定以提高交通安全。 relatively adv. [ˈrɛlətɪvli] 副词：相对地，比较地。 The price of this product is relatively low. 这种产品的价格相对较低。 relevant adj. [ˈrɛləvənt] 形容词：相关的，切题的。 His work experience is not relevant to the job he is applying for. 他的工作经验与他申请的工作无关。 relief n. [rɪˈlif] 名词：宽慰，救济，浮雕。 He felt a sense of relief when he passed the exam. 当他通过考试时，他感到一种宽慰的感觉。 rely v. [rɪˈlaɪ] 动词：依靠，信赖。 She can rely on her friends for help. 她可以依靠她的朋友来寻求帮助。 remark n. [rɪˈmɑrk] 名词：备注，评论； verb. [rɪˈmɑrk] 动词：评论，谈论。 His remark about her weight was uncalled for. 他关于她体重的评论是不必要的。 representative n. [ˌrɛprɪˈzɛntətɪv] 名词：代表，代理人； 形容词：代表性的。 The representative from the company gave a presentation about their products. 该公司的代表介绍了他们的产品。 reputation n. [ˌrɛpjʊˈteɪʃn] 名词：声誉，名誉。 The company has a good reputation for quality products. 该公司以产品质量好而著称。 reserve n. [rɪˈzɜrv] 名词：储备，保留； verb. [rɪˈzɜrv] 动词：预订，保留。 She made a reservation for a hotel room. 她预订了一间酒店房间。 resident n. [ˈrɛzədənt] 名词：居民，住户； 形容词：居住的。 The residents of the neighborhood are complaining about the noise. 社区居民正在抱怨噪音。 resist v. [rɪˈzɪst] 动词：抵抗，抗拒。 He can’t resist eating chocolate. 他忍不住要吃巧克力。 resort n. [rɪˈzɔrt] 名词：度假胜地，手段； verb. [rɪˈzɔrt] 动词：诉诸于。 They decided to take a beach resort for their holiday. 他们决定去海滩度假。 retain v. [rɪˈteɪn] 动词：保持，保留，记住。 He has the ability to retain information easily. 他有记忆信息的能力。 reveal v. [rɪˈvil] 动词：揭示，透露。 The investigation revealed some surprising facts. 调查揭示了一些令人惊讶的事实。 revolution n. [ˌrɛvəˈluʃn] 名词：革命，旋转。 The industrial revolution had a significant impact on society. 工业革命对社会产生了重大影响。 reward n. [rɪˈwɔrd] 名词：报酬，奖励； verb. [rɪˈwɔrd] 动词：奖励，报答。 He received a reward for his hard work. 他因工作努力而获得奖励。 rid v. [rɪd] 动词：摆脱，除去。 He wanted to rid himself of the bad habit of smoking. 他想摆脱吸烟的不良习惯。 rise n. [raɪz] 名词：上升，涨价； verb. [raɪz] 动词：上升，增加。 The price of oil has risen sharply. 油价急剧上涨。 round n. [raʊnd] 名词：圆形，回合； 形容词：圆形的； verb. [raʊnd] 动词：环绕，转一圈。 The table was round and could seat eight people. 桌子是圆形的，能坐八个人。 routine adj. [ruˈtin] 形容词：例行的，常规的。 He followed a daily routine of exercise and healthy eating. 他按照每天锻炼和健康饮食的例行程序进行。 rub v. [rʌb] 动词：擦，摩擦，揉。 His sore muscles needed some rubbing to ease the pain. 他的酸痛肌肉需要一些摩擦来缓解疼痛。 rubber n. [ˈrʌbər] 名词：橡胶，橡皮； 形容词：橡胶制的。 He wants to replace the rubber soles on his shoes. 他想要更换鞋子上的橡胶鞋底。 rural adj. [ˈrʊrəl] 形容词：农村的，乡下的。 She enjoys the peace and quiet of rural life. 她喜欢乡村生活的安静和祥和。 s satellite [ˈsætəlaɪt] n. 卫星 The satellite is used for communication between Earth and space. 卫星被用来进行地球和空间之间的通讯。 satisfied [ˈsætɪsfaɪd] adj. 满意的 He was satisfied with his work. 他对他的工作感到满意。 satisfy [ˈsætɪsfaɪ] v. 满足 A good meal can satisfy your hunger. 一顿美食可以满足你的饥饿感。 saving [ˈseɪvɪŋ] n. 储蓄 He has a saving account at the bank. 他在银行有一个储蓄账户。 scream [skriːm] v., n. 尖叫 The baby started to scream when she saw the spider. 女婴看到蜘蛛就开始尖叫起来。 sector [ˈsektər] n. 部门 The IT sector has been growing rapidly in recent years. 近年来IT部门一直在快速发展。 senate [ˈsenət] n. 参议院 The bill was approved by the Senate. 这项议案已经获得参议院的批准。 senator [ˈsenətər] n. 参议员 The senator was re-elected for another term. 这位参议员已连任第二个任期。 senior [ˈsiːniər] adj. 高级的，年长的 He is the senior member of the team. 他是团队中最资深的成员。 sense [sens] v. 感知 I sense something strange about this place. 我感应到这个地方有些奇怪。 sensitive [ˈsensɪtɪv] adj. 敏感的 She is very sensitive to the cold weather. 她对寒冷的天气很敏感。 sentence [ˈsentəns] v. 宣判 The judge will sentence the criminal tomorrow. 法官将在明天宣判罪犯。 sequence [ˈsiːkwəns] n. 顺序 Please write down the sequence of the events. 请写下事件的顺序。 session [ˈseʃn] n. 会议，一段时间 She attended a training session last week. 上周她参加了培训会议。 settle [ˈsetl] v. 定居 They decided to settle in a small town in the countryside. 他们决定在乡村的一个小镇上定居。 severe [sɪˈvɪr] adj. 严重的，剧烈的 He suffered severe injuries in the car accident. 他在车祸中受了严重伤。 shall [ʃəl] modal v. 将，应该 Shall I book the tickets for you? 我要帮你预订这些票吗？ shallow [ˈʃæloʊ] adj. 浅的 The water in this pond is very shallow. 这个池塘的水很浅。 shift [ʃɪft] v. 转移 The company will shift its focus to green energy in the future. 未来公司将转向绿色能源领域。 ship [ʃɪp] v. We need to ship these goods overseas. 我们需要运送这些货物到国外。 shock [ʃɑk] n., v. The news was too much of a shock to hear. 新闻听起来太震惊了。 shocked [ʃɑkt] adj. We were all shocked when we heard the news. 我们听到这个消息时都吓了一跳。 shooting [ˈʃutɪŋ] n. The shooting caused multiple casualties. 那场枪击事件造成了多个人员伤亡。 significant [sɪɡˈnɪfɪkənt] adj. This discovery is significant for the research findings. 这个发现对研究结果非常重要。 significantly [sɪɡˈnɪfɪkəntli] a His improvement plan significantly increased efficiency.dv. 他的改进方案显着提高了效率。 sincere [sɪnˈsɪr] adj. He sincerely appreciates your help. 他发自内心地感谢你的帮助。 slide [slaɪd] v., n. The children slid across the ice rink. 孩子们滑过那个冰场。 slight [slaɪt] adj. This issue is not very serious, just a slight misunderstanding. 这个问题不是很严重，只是轻微的误会。 slip [slɪp] v. The ground under her feet was a bit slippery, so she almost fell. 她脚下的地面有些湿滑，所以她差点摔倒。 slope [sloʊp] n., v. We hiked along the slope in the canyon. 我们在峡谷中沿着山坡行进。 solar [ˈsoʊlər] adj. Solar energy is a very promising energy source. 太阳能是一种非常有前途的能源。 somewhat [ˈsʌmˌwʌt] ad This issue is somewhat challenging.v. 这个问题有些棘手。 specialist [ˈspɛʃəlɪst] n., adj. He is a highly experienced eye specialist. 他是一位很有经验的眼科专家。 species [ˈspiʃiz] n. Humans are one of the greatest species on the earth. 人类是地球上最伟大的物种之一。 spiritual [ˈspɪrɪtʃuəl] adj. We need some spiritual support to keep us going. 我们需要一些精神寄托来支撑我们前行。 split [splɪt] v., n. The country eventually split into several independent states. 那个国家最后分裂成了几个独立的州。 sponsor [ˈspɑnsər] v., n. That company sponsored our competition. 那家公司赞助了我们的比赛。 spot [spɑt] v. Can you spot our location on the map? 你能在地图上找到我们的位置吗？ spread [sprɛd] n. The epidemic continues to spread, and we need more preventive measures. 这场传染病继续扩散，我们需要更多的防控措施。 stable [ˈsteɪbl] adj. This system is very stable. 这个系统是非常稳定的。 stage [steɪdʒ] v. He is staging a massive banquet that requires careful preparation. 他策划了一个巨大的宴会，需要精心筹备。 stare [stɛr] v. He stared at me for a long time, which made me feel uncomfortable. 他盯着我看了好久，让我觉得很不自在。 status [ˈsteɪtəs] n. We need a comprehensive evaluation of the status of this plan. 我们需要对这个计划做一次全面的评估。 steady [ˈstɛdi] adj. He is always very steady and calmly faces any difficulties. 他总是十分稳重，冷静地面对任何困难。 steel [stil] n. Steel is one of the basic materials for modern construction. 钢材是当代建筑的基础材料之一。 steep [stip] adj. The mountain is very steep and requires careful climbing. 这座山非常陡峭，需要小心攀爬。 step [stɛp] v. We need to take some steps to prevent this issue from happening. 我们需要采取一些措施来防止这个问题的发生。 sticky [ˈstɪki] adj. The glass is sticky with some juice on it and needs to be wiped with a wet cloth. 这个杯子粘了些果汁，需要用湿布擦拭。 stiff [stɪf] adj. The wooden chair is not very comfortable, and it feels stiff to sit on. 那个木制的椅子不太舒服，坐起来很生硬。 stream [striːm] n. Along this stream, you can see various wild animals. 沿着这片河流，你可以看到各种各样的野生动物。 stretch [strɛtʃ] v., n. In the morning stretching exercise, you can stretch your muscles and stay energized. 在早晨的伸展活动中，你可以舒展肌肉，保持活力。 strict [strɪkt] adj. Our rules and regulations are very strict, and there can be no violation. 我们的规章制度非常严格，不能有任何违反。 strike [straɪk] v., n. The strike affected the entire city’s transportation. 那场罢工影响了整个城市的交通。 structure [ˈstrʌktʃər] v. We need to structure a more stable bridge. 我们需要构建一个更加稳固的桥梁。 struggle [ˈstrʌɡəl] v., n. The country is struggling for democracy and freedom. 这个国家正在为民主自由而奋斗。 stuff [stʌf] v. My backpack is stuffed with essentials and common items. 我的背包里装满了必需品和常用物品。 subject [ˈsʌbdʒɛkt] adj. The subject discussed in the article is very important. 那篇文章讨论的主题非常重要。 surgery [ˈsɜrdʒəri] n. His surgery was very successful, and he recovered quickly. 他的手术非常成功，恢复得很快。 surround [səˈraʊnd] v. They stood around the room and enjoyed the beautiful view. 他们站在房间四周欣赏那美丽的景色。 surrounding [səˈraʊndɪŋ] adj. The surrounding landscape of this city is very beautiful. 这座城市的周边景观非常美丽。 survey [sɜrˈveɪ] v. We need to conduct a comprehensive survey of the market. 我们需要对市场进行一次全面的调查。 suspect [səsˈpɛkt] v., n. The suspect was arrested by the police and jailed. 那个嫌疑犯被警方抓捕并关进监狱。 sympathy [ˈsɪmpəθi] n. We should help those in need with sympathy. 我们应该带着同情之心帮助那些需要帮助的人。 tuvw tale [teɪl] n. Grandfather always tells some wonderful tales. 祖父总是讲着一些奇妙的故事。 term [tɜrm] v. After each term, we need to summarize and evaluate. 每个学期结束后，我们需要进行一次总结和评估。 therapy [ˈθɛrəpi] n. Psychotherapy is a very effective way to help people deal with difficulties. 心理治疗是一种非常有效的方式来帮助人们应对困难。 thus [ðʌs] adv. His logical reasoning is rigorous and thus received a high score. 他的逻辑推理很严密，因此得到了高分。 tone [toʊn] n. His tone sounded very confident and impressive. 他的语气听起来非常自信，令人印象深刻。 tough [tʌf] adj. This job requires tough mental and physical strength. 这项工作需要非常坚韧的精神和体力。 track [træk] v. We need to track this project to ensure that it progresses as planned. 我们需要跟踪这个项目，确保它按计划进行。 transform [trænsˈfɔrm] v. We need to transform our work methods to adapt to this new market. 我们需要改变我们的工作方式，以适应这个新市场。 transition [trænˈzɪʃən] n. We must smoothly complete the transition process of this new plan. 我们必须顺利地完成这个新计划的转型过程。 trial [ˈtraɪəl] n. This vaccine is undergoing human trials. 这项疫苗正在进行人体试验。 trip [trɪp] v. We arranged a trip to relax. 我们安排了一次旅行来放松精神。 tropical [ˈtrɑpɪkəl] adj. The climate in tropical regions is very comfortable. 热带地区的气候非常舒适。 truly [ˈtruːli] adv. His words were very sincere and touched us all. 他的话非常真诚，让我们都非常感动。 tune [tun] n. The melody of that song is very beautiful. 那首歌曲的旋律非常动听。 tunnel [ˈtʌnl] n. That tunnel is very deep and requires careful crossing. 那个隧道非常深，需要小心穿越。 ultimately [ˈʌltɪmətli] adv. Ultimately, they reached a compromise. 最终，他们达成了妥协。 unconscious [ʌnˈkɑnʃəs] adj. He was unconscious for several hours after the accident. 他在车祸中昏迷了好几个小时。 upper [ˈʌpər] adj. This room is upstairs and very quiet. 这个房间位于楼上，很安静。 upward [ˈʌpwərd] adv. Overall, our sales are trending upward. 总体来说，我们的销售额呈上升趋势。 urban [ˈɜrbən] adj. Urban development has brought many conveniences to people’s lives. 城市的发展为人们的生活带来了很多便利。 urge [ɜrdʒ] v. He strongly advocates environmental protection and urges the government to take action. 他坚决主张保护环境，敦促政府采取行动。 van [væn] n. This van can be used to transport furniture and other items. 这辆面包车可以用来搬运家具和其他物品。 vary [ˈvɛri] v. Temperatures vary in different seasons and regions. 气温在不同季节和不同地区会有所不同。 vast [væst] adj. The vast grassland feels infinitely wide. 那片广阔的草原让人感到无限宽广。 venue [ˈvɛnju] n. That venue is perfect for concerts and various other performances. 那个场馆非常适合举办音乐会和各种其他演出活动。 via [ˈviːə] prep. We will arrive at the destination via this route. 我们将通过这条路线到达目的地。 visual [ˈvɪʒuəl] adj. Videos, charts, and other visual elements are essential parts of this presentation. 视频、图表和其他视觉元素是这个演示文稿的重要部分。 vital [ˈvaɪtl] adj. Sleep is vital to physical health. 睡眠对身体健康至关重要。 volume [ˈvɑljuːm] n. This book has three volumes and is very detailed. 这本书有三卷，内容非常详细。 wage [weɪdʒ] n. We need to pay our employees a fair and reasonable wage. 我们需要支付员工一个公平、合理的薪水。 way [weɪ] adv. We need to develop better plans to accomplish tasks more effectively. 我们需要制定更好的计划来更有效地完成任务。 whereas [wɛrˈæz] conj. Whereas I understand your concerns, I think we should carry on. 虽然我了解你的担忧，但我认为我们应该坚持下去。 wherever [wɛrˈɛvər] conj. Wherever you go, keep a positive attitude. 无论你去哪里，都要保持乐观的态度。 widely [ˈwaɪdli] adv. The company’s products are widely accepted and used. 该公司的产品已经广泛被人们接受和使用。 wildlife [ˈwaɪldlaɪf] n. The country’s abundant and diverse wildlife is very attractive to tourists. 那个国家丰富多样的野生动物对游客来说非常吸引人。 willing [ˈwɪlɪŋ] adj. They are very willing to help us through difficult times. 他们非常愿意帮助我们度过难关。 wire [waɪr] n. I need to fix the wires in my apartment. 我需要修理一下公寓的电线。 witness [ˈwɪtnɪs] n., v. The witness proved the truth of the event. 那个目击者证明了这个事件的真相。 worst [wɔrst] n. The worst injured in the squirrel attack was the little bird. 松鼠袭击时伤势最重的是那只小鸟。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日常英语集]]></title>
    <url>%2F2023%2F06%2F02%2Flife_eng_set%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[起床 Hey, honey, it’s time to wake up and get out of bed. 中文=嘿, 亲爱的, 该醒醒并起床啦. Do I have to get up now? 中文=我一定要现在起床吗? You’d better get up, or you’ll be late. 中文=你最好现在起床, 不然你就会迟到嘞. What are you talking about? My alarm hasn’t even gone off yet. 中文=你在说啥?我闹钟都还没响 Yes, it did. It went off thirty minutes ago. You slept right through it. You’re like a dead person while you sleep. 中文=它响过了. 它三十分钟前就响过了. 你睡过头了. 你睡得像个死人 I must have slept right through it. 中文=我一定是睡过头了. Rise and shine! Sleeepyhead! 中文=太阳晒屁股啦! 贪睡虫! Just let me sleep five more minutes. 中文=就让我再多睡五分钟嘛. The early bird gets the worm. 中文=早起的鸟儿有虫吃. I know, I know. But I don’t want any worms. 中文=我晓得, 我晓得. 但是我不想要任何虫子. Haha! If you don’t make an effort now, later on it’ll be a lot thougher for you. 中文=哈哈, 如果你现在不努力一把, 等一哈之后, 对于你来说就更加艰难了. . . . 睡觉 It’s time for bed. 中文=是时候睡觉了哈. But I’m not sleepy, honey. Can I Stay up and watch more TV? 中文=但是我一点也不困, 亲爱的. 我能再熬夜并且看多一点电视吗? It’s already past eleven. You have school tomorrow. You’ll be sleepy all day tomorrow if you don’t sleep well tonight. 中文=已经超过十一点了. 你明天还要上学. 你明天会很困的如果你今晚不睡好的话. I know, but I promise I won’t be sleepy tomorrow. I work really hard. 中文=我晓得, 但我保证明天不困. 我学习很用功的. Don’t you have exams comming up soon? I’m sure you can spend more time preparing for those. 中文=你不是有个考试马上就要来了吗? 我确信你能花更多时间在它们上面. Oh, I nearly forgot the exams. 中文=噢, 我几乎忘记了考试了. You do well in math, but your history is not good enough. Am I right? 中文=你在数学上做的不错, 但你的历史还不够好. 我说的对吗? Yeah, I guess so, maybe I could go to sleep now and get up early to review the lessons tomorrow. 中文=是哈, 我觉得是这样, 也许我可以现在去睡觉并且明天起早点复习课程. An excellent idea, work now and play later. 中文=好主意, 先工作后玩耍. I know you’re right, school is more important than TV. But can I watch one more show then go to bed? 中文=我知道你是对的, 上学比电视重要. 但是我能再多看一个节目再去床上吗? No, now is bedtime. 中文=不行, 现在是睡觉时间. 打电话-Making a Telephone Call Good morning. This is Wilson association. 中文=早上好，这里是威尔逊协会 This is Brown speaking. I’d like to speak to Mr.Thomas. 中文=我是布朗，我想和托马斯先生通话. I’m sorry, but Mr.Thomas left here just a few minutes ago. 中文=很抱歉，托马斯先生刚在几分钟前离开了这里. I’ve been trying to call him for the last ten minutes. 之前十分钟我一直都在给他打电话. But your line was busy. Will he be back soon? 中文=但总是占线，他很快能回来吗？ I’m afraid not. He is away for the rest of the day. 中文=恐怕不能，他今天一天都不在. Is there any other way I can reach him? 中文=还有别的方法能联系他吗? I’m afraid not. He has gone out of this town on business. May I take a message? 中文=恐怕不行，他出差了，我可以捎个口信吗？ I have a business appointment with him at 10 o’clock tomorrow morning. But I’m afraid I can’t make it. 中文=明天上午10点我和他有个上午约会，我恐怕不能参加了. Would you like to make another appointment? 中文=您想另外再约个时间吗？ Unfortunately, I’m leaving here unexpectedly, and I may be away for several days. 中文=遗憾的是，我凑巧有事要办，可能要出去几天. I see. I’ll tell Mr.Thomas that you’ve called. 中文=我明白了，我会告诉托马斯先生您来过电话. Thank you. 中文=谢谢. 接电话-Answering the Telephone Hello? Who is speaking? 中文=你好，哪位？ Hello! THis is John. I want to speak to Linda. 中文=喂，我是约翰.我想找琳达. This is her. 中文=这就是她（我就是）. Hi, Linda. I’m just calling to invite you to a dinner party tomorrow evening. 中文=我打电话是想邀请你参加明天晚上的一个宴会. Really? What time and where? 中文=真的吗？什么时候，在哪里? 7:30 pm, at Longding Chinese restaurant. I’ll be at your place at 7:00 to pick you up if you need a ride. 中文=晚上七点半，龙鼎中餐厅，要是你需要搭车的话，我会在七点的时候去你那里接你. Yes, please. I’ll need a ride. I’ll be waiting for you then. 中文=好的，那请你来接我吧.到时我在家等你. See you tomorrow at 7:00. Make sure you dress a little formally. I heard the restaurant is kind of upscale. 中文=明天七点见，一定穿得正式点，我听说那家餐厅比较上档次 Thank you. See you then. 中文=谢谢，到时见. 关于时间-About the time What time is it? ten to nine.(8:50) Your watch is ten minutes slow. So it should be nine sharp(9:00). I’m gonna be late again. What will my excuse be this time? We should’ve set the alarm for seven thirty(7:30). Don’t cry over spilt milk. Honey, could you stay with me a few more minutes? What? Why? What’s wrong with you? I just hate being alone at home with nothing to do but listen to the clock ticking. It’s so lonely and boring. My sickness makes it worse. When will I recover? You’ll be fine in a couple of months. Just be patient. I’ve got to go to work now or I’ll be late. You only care about your work. Sorry, baby. You’re everything to me. All I do is for this family. Stay just a few minutes longer, okay? Ok. I’ll leave at nine fifteen(9:15). Is that all right? Yes. I’ll spend more time with you in the future, honey. Thank you. 回家后-After Getting Home Home at last. Tonight we have a lot of homework though. Are you saying we don’t have time to watch our favorite show tonight? Jane, you know I really don’t like our teacher all that much. He gives far too much homework. He criticizes me in front of everyone all the time. Tell the truth, I don’t really like hime either. He’s kind of boring and not very active. He always looks unhappy too. Yes, and he also … Do you think we should be talking about him like this behind his back? Probably not. After all he is our teacher. We should try to find something nice to say. If you can’t say something nice you shouldn’t say anything at all. I absolutely agree. It’s getting dark. We should finish our homework now. All right. I want to take a shower first. I’m exhausted! Falling into love I saw Lily for the first time and felt like I had been struck by lightning. She is the girl of my dreams. I have a date with her on Friday. but I really can’t decide what to do. Any other ideas? What about taking her for a dinner or a movie? Not bad. Any other ideas? Well, there’s a play on campus this weekend. Good idea. That way she’ll think that I have culture. Do you have any other suggestions? What’s the matter with these ideas? They are good, but I still need a few more. What about going on a picnic? Cool, but what if the weather isn’t ideal? You could take her to the art museum or a cafe. Certainly I must have come up with something you can use. How do you like the ideas i’ve given you? They’re all good.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多邻国英语学习攻略]]></title>
    <url>%2F2023%2F04%2F22%2Fduolingo_english_learning%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[本文由 简悦 SimpRead 转码， 原文地址 zhuanlan.zhihu.com . . . 本人背景： 双非法学本科 四级 428 分 六级没过 曾准备过半年的考研，发现卷不过别人。 去年 9 月计划出国 没考过雅思，无其他英语成绩 前前后后 1 个月多一点准备多邻国 最后 120 分过。. . . 下文全文 8897 字大概花时间 7 分钟 保姆级全攻略多邻国 13 天带你背完 7000＋个多邻国单词 26 天过多邻国考试 本文花费 3 天时间整理 内涵含 10 篇手打实战资料 —– 以下是我的经验分享 —– 疫情肆虐的这几年，确实是给我们打算留学的友友们带来了很大的不便。1 月时候订的 3 月初广州考雅思也因为疫情反弹，中介觉得考试大概率可能取消。 很巧，拿着有条件录取很郁闷的我在网上认识一个同样录取的朋友，听他说了以后才知道有多邻国这个考试，后来就去简单了解一下。起初并不打算转移阵地，毕竟在雅思这上面已经花了准备时间了。 但是当我看到多邻国的优点（费用廉价、可官网多次模拟考、更重要是足不出户只要在家里有电脑、花个一个小时即可以完成！！！！口语时间还贼短，并且电脑打分。）因此巨大的应试可操作空间，友友们你懂的！）以上总总，顿时让我这个懒癌患者兼社恐患者欣喜若狂。立刻取消了雅思报名，转战多邻国！ 但是，当我深入研究多邻国以后，我发现，多邻国并不比雅思简单太多，甚至有些题目，比如口语和阅读，题目要求看起来比雅思简短，看似是容易了，但是！！！！对于我们来说容错率的要求就高了。要求我们在更短的时间在有限的回答里面尽量的全对。天哪，这也不简单吧？！ 比如假设雅思 100 个题目，要考 90 分，允许我们错 10 个。现在多邻国是 10 个题目，要考 90 分，我们只能错 1 个！ 是不是突然觉得选择多邻国又是进坑了？ 没关系，跟着我大处女座的步伐，带你细致解决备考路上每一个小问题。本文从这里开始下面全是干货。各位友友们请认真看哦，如有不懂，随时私信我。 一、雅思托福与多邻国的对比。 另外刚才收到信息，多邻国又要增加新题型了，最近可能转战多邻国的人太多了，规则变动太大。因此，备考的友友们，抓紧啦！ 上图原推文链接：https://mp.weixin.qq.com/s/pP9iH6Mytx22TUlEDaMNPw 二、13 天搞定多邻国所有单词 每天只需 10 分钟 多邻国考试我觉得词汇是特别特别重要的。在考试第一部分的听音辩词和单词识词中，由于没有上下文的引导，只有单独的单词显示在屏幕上，另外该单词还可能是原单词的变体（如加前后缀，形式变换，此行变换等）因此，想获得高分必须提高对单词的熟记程度，因为单词是最容易提分的一个模块。 （一）多邻国背什么单词：CEFR 单词（A1-C2 去重后总计 7243 个单词，在这 7000 多个单词里面有一部分不是常考重点词，但是我的处女座强迫症逼迫我一定要背完整个词单。挑重点背总会让我觉得没有安全感。万一没背的就考到了呢？） （二）官方 CEFR 单词出处链接：http://www.englishprofile.org/american-english（该链接显示的是官方外语机构出示的 CEFR 等级单词划分，刚接触多邻国考试的友友们可以上这个网看看哦） （三）10 分钟背单词方法：其实背单词是一件很枯燥的事情，特别是我们还要背 7000 多个，因此如何最高效的并又不会太无聊的背单词成了我思考的问题。 针对多邻国考试当中只考单个单词的辨析和发音，因此我把我自己总结好并按 A1 到 C2 不同等级单词区分好，制作成 Excel 表格，划分每天的学习范围。 以下是 A1-C2 各个不同难度的单词数量汇总。 具体操作就是把制成后当日要背的 Excel 的内容导入手机词典中（例如有道词典等等其他词典）并生成自己自定义的单词本，在单词本上反复听。以上这个过程每天只花 10 分钟即可把当天单词过一遍，时间短，效率高。 重点来了！ 重点来了！ 下图是花了很多时间总结的单词背诵表。该表用于导入上面提及的背单词软件当中。 本人以及几个好友实测，很好用！！！ 且（市面各个机构老师似乎没有这种 Excel 表格版 并且总结这么好的！！！） 首先说一下表格的优点： 按单词 topic 划分，方便系统记忆。 按单词 A1-C2 不同等级归类，方便背诵以及查缺补漏，看得到成就感，使背诵不那么枯燥。 按单词字母个数区分。特别便利于提高多邻国中完形填空部分的能力。 以上各个划分可以按自己的爱好，在 Excel 中二次自行生成排列，方便于每个人。 Excel 格式展示利于直接复制到手机或电脑的词典软件中，生成自定义单词本，随身背诵。 （四）学习单词的时间分配为：A1 A2 两个等级的分别一天过完，B1 两天，B2 三天，C1 三天，C2 三天，一共 13 天过一遍。如果仅有一个月的复习时间，单词建议过两遍。我一般在学习累了以后，去整理家务，收拾房子，一边听着耳机播放单词，既能学习，又劳逸结合。一天下来零零碎碎的时间一个单词表能听 6、7 遍（其实我们每天零碎时间是不少的，至少超出我们想象。）连续半个月这样学习，你还怕记不下来？ 在过单词的过程中，把不会的，听不懂的，特别容易混淆的单词记下来，后面的几天反复听，反复记忆，相信我，到最后你会发现在做模拟题的时候豁然开朗，顿时打通了任督二脉。 友友们可以按照我上面的表格的格式自己花时间整理一下，整理资料的过程也是练习沉下性子的过程。自己辛苦整理的资料自己会背的更用心。当然考试时间临近或者想偷懒的同学可以私信我领取哈。 （五）单词表学习建议：因为我们中国学生，从小学习英语的就是应试能力很厉害，但是口语和写作这类就差一点。所以我们在词汇这类靠努力记忆就能提高的环节，我们必须得好好记忆。特别是想考 120 分以上的朋友，建议 c1、c2 都要好好的记忆，毕竟出国后这些都是常用词。目标分数 110 的朋友 A1-C2 都要认识（C1-C2 可认识 80%）。另外，因为是自适应考试，选错倒扣分，所以务必只选 100% 有把握的，宁可少选，避免错选。 想考 125 分以上的朋友单词表内单词的含义、拼写、发音都请记熟。 （六）关于词根词缀词根词缀，就像汉字的部首，因此熟悉常用的词根词缀以后，看到熟悉的词根词缀就可以大致判断是否是真实存在的单词以及其含义了。因此，当友友们时间允许，还是建议记一下词根词缀，可用词根词缀字典 APP 查询（无网页版），网页版可以用百度翻译来查，虽然不如上面这个 APP 全，但是网页版的好用。https://fanyi.baidu.com/ 以下是我自己总结的在真题中出现高频的单词的词根词缀助记总结，这只是节选其中一部分。友友们可以按照我这个格式自己制作一份。方便记忆。 二点①、客观答题部分的复习策略（占总分的 100 分） 单词辨析、听音辩词、完型填空、听写句子、朗读句子这 5 类，我把他们称为客观题部分。以上这 5 类题型不能有明显的短板，否则总体分数是上不去的。并且，如果这类题都上不区分，你难道指望口语和写作提分吗？？ 其中单词辨析和完型填空据说是最最重要的。同时也是最考验我们的耐心的。 （一）其中单词辨析和听音辩词部分攻略结合上面总结好的 CEFR（A1-C2）单词 Excel 表格＋登登多邻国官网的免费资料库刷题。二者同时进行。在学习的过程中你会发现你在 Excel 学习的单词出现在登登多邻国的真题测试中。 打开 “登登多邻国” 的官网（https://det.91ddedu.com/#/practice）的时候。点击词汇专栏，下面会有对应不同难度的选择。我们可以选择与我们所背的 Excel 单词等级相同的单词进行学习。 总共登登多邻国内词汇的题目要练 2 遍。其中有错的部分刷 3-5 遍。与此同时，仍要继续的巩固 Excel 内的单词，总结常错词。到后面你会发现，题目中出的单词你能够一下子很快的分辨出真假对错。这种喜悦感能够给我们慢慢的踏实感去完成接下去的题目。另外，在选单词时候选自己有把握的单词！因为选正确得分，选错的会倒扣分。 其中注意：国家，月份，星期等词汇必须大写，小写是错的！！！ 下面是我总结真题常出现的部分常错的单词，同样也是 Excel 表格制成，方便我们后续的按需编辑加工。同时提高吸收效率。结构简单明了，逻辑清晰。 纯手输总结的常见错词，需要完整版请私信我。 附：正确的单词参考的是 牛津词典（https://www.oxfordlearnersdictionaries.com/） 柯林斯词典（https://www.collinsdictionary.com/zh/） 听音辩词的美式发音可以参考韦氏词典的发音：https://www.merriam-webster.com/ 以上网址均不需要翻墙。 （二）完型填空部分攻略1. 大量刷题 + 总结整理 先说大量刷题，登登上的所有阅读题目加上自己找的题目一共 400 + 道题，基础量刷了 1 遍，有错题的额外又复习了 1-3 遍。 为什么要大量刷题？ 因为可以遇到原题呀！！ 之前我看知乎一个学姐说。考试时她遇到了 3 道完型填空的原题（但可悲的有两道是她考完试再复习才发现它躺在登登的题库中还没有被做到！！！）因此，我在复习完形填空的时候都是保质保量地去完成，保证刷过的都对，填的都对。终于功夫不负有心人，最后出分 120。考试中，我遇到了 4 道完型原题，其中两道是超长翻页的难题，并且在考完试对答案时候我发现，4 篇完型正确率 100% 3. 方法论：完形填空如何总结整理。 我们在填空的时候，联想单词，根据上下文找到逻辑，有时候题目本身不难。 注意：当我们做题时想到了填的单词时，如果发现空格不是刚好合适，不要急着放弃这个单词，想一想有没有其他形式可以填进去。 比如 n. /v. /adj. /adv. 当填入动词的时候要注意人称和时态，来决定动词的形式。要注意读题，从哪些方面读呢？——— ①读指代：（it, she, he, they, you, I, its, her, his/him, them, your, my/me, we, us. 等等……..），大写（国家，月份，星期等词汇必须大写，小写是错的），时态，单词重复（文中多次重复同一单词），对比（black and white）并列 (and, 以及逻辑上的并列关系) 这些上下文关系！ ②固定短语：这个太多了，就要看个人平时的积累了，比如：be good at, consist of, be made up of,Look after, look for, prefer to 等等……. ③时态：填入动词时，注意人称和时态，找准文章的基本时态。 ④送分小词：be 动词，定冠词，连接词，序数词，数量词，介词：on, of, in, with, at, from, as, after,before, beside, above, under, down, upon, behind, to, by. ⑤从句：注意倒装和从句中的关系 第一遍做完形填空题我会把题目截图到 word 里，并整理题目中的考点（固定搭配，单词的熟词僻义等）和关键的生词，如果是填不出来的，也写在 word 中写下来提醒自己。那么在第二遍复习的时候可以直接对这个 word 文档看，可以快速浏览知识点，减少自己的记忆量。并且节约很多时间，同时提高效率。（截图在下方） 基本所有的 “登登多邻国” 上的真题都截图并做好总结，花了很多时间。总共 230 多页的 word 文档，有这份完形填空就稳了，之前发给身边的其他同学，其中一个同学甚至最后 15 天 130 分数上岸。简直不要太爽。 另外还总结了做过的完型题目中错词所对应的 CEFR 的 level，见下表，发现主要考点集中在 A1-B2，这也就是我在背单词的那部分为什么说 A1-B2 的词大家要掌握含义、拼写和发音了。 下图明显表示了完形填空中出题主要位于 A1-B2，同时，我错误的区间也是集中在 A1-B2。多背多记多熟练友友们。 下图是我根据挖空制作而成的完形填空积累。同样也是很好用，一目了然让我们知道错在哪里，而不需要找回当初的整篇题目去看其中错误地地方。省下了很大的篇幅，节约时间。 在备考的时候，时间难道不是最重要的嘛？ 其次我在练习题目中整理了所有的题库中出现的固定搭配，下图是部分内容截图。想考高分 120 分以上的友友们必须重视这部分。我总共在题库里总结了 130 道固定搭配，多看多记也能够让我们运用到口语和写作中，一举两得。并且里面很多都是高级词汇，很容易帮助我们得到高分。 如果时间充足，那我们就仔细的刷一遍题库 + 复习错题，如果时间不充足，可以直接看我整理的完型考点，直接当阅读材料来看就好，有个印象。 （三）听写句子 &amp; 朗读句子部分攻略1. 听写句子部分方法论： 听写句子综合考察我们对单词的熟练程度，连读弱读等技巧的理解程度，时态语态句式和熟练，以及打字的速度。 练习方法：CEFR 单词 Excel 表格导入词典生成单词本以后设置为美式发音。（整个词表反复刷四遍吧，最后最好掌握在 80% 左右）+ 同时在登登多邻国官网进行 “听写句子” 跟读。 登登听写的题库总共刷了 3-5 遍，前两遍就听写并背里面的生词和单词拼写（写到 word 中，第二遍只练错题），第三遍不写跟着题库的录音跟读（挑错题跟读），直到读到你和录音的节奏一致为止，可以 10 句一组来练习，大概一组跟读就花 15-20 分钟。总结题目中出现的让你没有听出来的连读技巧。 下图是我的听写 word 标记。 练完了登登的题库后，我还按照一个学姐的推荐，找了微信小程序 “PTE 黑科技学习平台”。其中，里面 listening 板块有 WFD。集中练了三天，每天 60 个，发现 3 天后真的基本上能听懂题目再讲什么了。 2. 听写句子练习心得： 考试一般出现 4 句简单的句子加 1-2 句长难句。 总共每道题目可以听三遍。 听第一遍后，立马打字，打首字母或简写，能打多少打多少。 第二遍听完，立马补全。 第三遍听完，继续补全 + 检查。 （最后一遍特别注意基本语法：大小写，时态，人称，单复数。） 一个有意思的发现是，多邻国的听写句子最长不超过 14 个单词（基于登登多邻国现在的真题 3. 朗读句子部分的方法论： 朗读句子是有时间限制的，所以要是句子比较长，就大概浏览一下，把不好发音或者不认识的个别单词熟悉一下，但是注意速度要快。如果想练习读完一整句再录的话，是会没有时间录完的。假如句子短，录完了还有时间，觉得自己第一遍没读好的话，可以接着继续重新读，算分是看第二遍的，但是最好可以一次就读好！ A. 读错了不能停顿，因为是机器阅卷，多读的音会自动判到下一词。 B. 读错了或者没读准就一笔带过吧，小细节不影响总体判分。 C. 遇到生词， 用拼读法大致拼出来。 D.And，while, however, but 作为连接词，重 (zhong) 读拖长。 E. 检测方法：用微信 “文件传输助手”，打开系统自带语音输入功能， 对着它读，如果都能识别出来，说明你读对了。反之标记出未识别的音，查单词，跟读，重来。 最后最后，一定一定读慢一点，吐词清晰！！！！嘴型夸张，需要拖长的词一定拖长，因为机器会判不出来。 另外朗读句子需要知道重读、弱读、连读、弱读、浊化、断句、语调七大发音规则（下面是整理的资料，内容都是出自真题语料库，参考性极强） 三、主观答题部分的复习策略（占总分的 60 分） 主观答题部分分为两大块：写作和口语。 写作题目包括：看图写句子（3 句连着出）、小作文、面试作文 口语题目包括：看图说句子、听录音演讲、看题演讲、面试口语 先说写作部分，写作部分的占分要比口语部分高，对大部分小伙伴来说也比口语要简单，可准备性高。会影响分数的关键点：全文的词汇是否高级、文章连接词的使用、时态的复杂性，句型的复杂性。 （一）作文部分（小作文和面试作文均有参考意义）作文要求 50-100 字，考试只有 5 分钟，时间有限。所以，这个题型拼的就是手速。建议拿到题目不假思索开始写，最开始想到什么就写什么，避免因为要构思导致的紧张。 同时，因为是 50 字左右的作文，不要求你写很多或者写很长的句子。因此，注意单词大小写和标点符号，保证写的尽量都对即可。 公式：简单句 + 较复杂的句子 + 无拼写错误 + 无语法错误 = 高分 另外在观点解释部分可以利用： If 假设论证（如果这么做了会怎样） 对比论证（反方的后果） 深层推理，一层推一层 （锻炼 - 让我们身体好 - 身体好之后更有精力工作学习 - 为工作学习好了能为社会创造价值 - 这样能使祖国富强） 举例论证 与此同时，友友们可以整理自己的模板。我有两个写作通用模板，模板字数 50 字 +，最后在考试中往里面填相关的词句即可，其中模板里面 C 级词汇 10 个 + （因此 100 字左右的文章，能保证文章的整体词汇水平在 C 级）。 把这些模板背的滚瓜烂熟，并且练习限时默写，最好能练到 1 分十几秒就默写出来，考试看完题目直接先套模板。然后再具体的补充内容，补充的内容就不用着重考虑词汇的级别了，因为模板里的那些 C 级就已经够了，想到什么就写什么吧，如果能用出来复杂句当然更好。全文整体能达到 100 字 + 应该就不错了。 看图写句子 这个题我也觉得较难，刚开始练的时候很考验我的耐心，特别是我还是时间紧任务中。只有两周不到时间准备。并且正式考试中这种题型出现 3 道，占比较大。考试中 3 道都是连续出现的，还没有准备时间，只写 1 分钟，通常要看图思考组句，所以难度大时间紧。 考试中第一题时间满了系统会立马切换到下一题。因此建议平时一定要练习打字速度和反应能力，计时练习。 2. 看图写句子攻略 看图写句子练习的目标是写出两句话，字数 30 字左右。由于题目是三题连着一起出，我就准备了三套开头句，每个开头句 7-9 个词，都是复杂句。第①句：描述主要人物 / 事物的主要动作，第②句：描述补充的人物 / 事物 / 天气 / 你的猜测等。 3. 运用高级词代替常用词也是提分的重点。 4. 另外，看图写句子主要分为人物类和事物类两大类，相关句式的技巧方式如下。 人物图： 介绍人物加动作，还剩时间就写周围环境。用万能非谓语或定语从句 who/ which 描述穿着、表情。如下： 答： Holding a piece of tissue, the girl who wears a red jacket is crying. The boy (who is) sitting next to her is comforting her, putting his arm and hand on her shoulder. 解析如下： Holding a piece of tissue, （非谓语解释动作） the girl who wears a red jacket is crying.（定从解释穿着） The boy (who is) sitting next to her（定从解释动作） is comforting her, putting his arm and hand on her shoulder.（非谓语解释动作） 非谓语与定从可以互换，用在人物身上万能。 事物图： 从近到远、从主到次描述图中所有的事物，加形容词。 巧用介词连接图中各个细节。 答： A brown horse with a white stripe is standing next to white fences and trees on green grasslandin cloudy weather. A thatched cottage is faraway. 解析：形容词做特征描写利用介词说细节，且不用断句。 5. 如果遇到难图，脑袋一片空白，那就： 窍门 1 挑你最熟悉的事物的单词开始写，因为机器踩点给分，图中出现的都有分。 窍门 2 用 There are 囊括所有事物：There are a brown horse with a white stripe,white fences and trees, green grassland and a thatched cottage in thepicture.（这样在动词方面会丢分，但能保证形容词和名词的分全拿下。） 总的来说，这个题得分率想在短期内提上来是有挑战的，但是我们只能保证训练 “看到图能快速地有东西可写” 这种能力。一般来说分也不会低到哪里去。友友们还是得有信心！ （二）口语部分 看图说句子 这题考试中占比非常小，所以不用花大力气复习。保证所说的话无语法错误即可。 假设如果是人物图，看到图片后的描述步骤：（1 分钟准备时间） 1. 个数、性别、年龄、体态。（1 个人，女，中年 30 岁或 40 岁，中等身材） 2. 从上到下的穿着描述。 （戴白帽子，穿红衣服绿裤子，白鞋子） 3. 从上到下的动作描述。主要是面部表情，手脚在干嘛。（她在笑，手里拿着相机拍照，蹲在地上） 4. 周围环境。主要是地点，周围有什么东西。（她在广场上，周围有很多树和商 店。） 5. 全程用简单句，现在时。例如： There is only one person in the picture. She is…wearing/smiling/holding/…. There is a tall building… 因为是机器判分，不需向考官展示语言水平。只要出现踩分的点，都算对。 发音清晰、标准, 声音一定要大！！！！且不要长时间停顿。 时间说满。以上 4 个板块细节说满之后还有剩余时间的话，就利用想象力猜想： What 人们在做什么 why 她为什么要这么做 result 这么做带来啥结果 等等其他的，结合具体图片展开想象力吧！遇到奇怪的图片，就说确定的部分，剩下的全猜想，把能猜到的都说出来去碰分。 以上是大框架上的方法论，针对这个题型我也做了相应的答题模板。模板只包含 5 句话，简单直接好记。 这五句的逻辑关系如下： 第①句：区别于看图写句子的开头句 + 主要人物的主要动作 第②句：描述补充的人物 / 事物，介词开头 Behind/Around/Below/Opposite , there is /are 第③句：大环境：天气、景色很美呀之类的（关键词可以按需自己替换） 第④句：你根据图片的猜测，比如他很擅长这份工作 第⑤句：这个图片让你想到了什么，引起了你什么回忆 ③④⑤句我都有刻意准备高分的词和常用的表达句型 下面是模板的部分截图。（多熟练模板，基本上题库里面的题目都能套上） 另外这个模板里面一些关键单词都是 C2 等级的单词，这也能更好地帮助我们在这个版块获得更高的分数。 2. 听题演讲 &amp; 看题演讲 &amp; 面试口语攻略 我其实是很害怕口语的，毕竟以前学的都是哑巴英语。但是后来为了考试不得不认真起来。仔细分析了一下我不敢张口的原因：一单词的发音拿不准，二没有基本句型做支撑。因此，多邻国复习的口语我就着重从这两方面入手，单词发音通过背 CEFR 单词来搞定，基本句型我就准备了话题通用素材。 口语题题目我把手边的 200 多个题目按分类用思维导图梳理了一遍，帮助我有一个直观的了解会有什么类型的题目，基本上是有 7 大类：【如自己有关 - 事件类】、【与人有关的】（人的行为（微笑、撒谎、贪婪等）、健康、家庭）、【工作和公司】、【国家 / 城市 / 政府】、【文化】、【环境】、【教育】口语题没有采取题海战术，就准备了每个分类下的一篇素材（与自己有关 - 事件类我准备了 5 篇 提高注意力、游览故宫、名人、历史、买新能源车），一共 14 篇素材 (7 篇必备 + 7 篇扩充，素材太多了也记不下来），考到这个分类起码有话可说。素材先通读，再记忆，练到真的像再说自己的故事一样的程度。 以下是多篇模板素材中的一例： 听题演讲我额外用登登过了一遍题库中的问题的录音，熟悉问题的发音，知道他在问什么。再加上你们前面复习听写句子的收获，听题这部分问题基本能听懂或关键词能听懂。 口语部分我没有说满时间（剩 10-20 秒不等），但是按照分类素材好好准备，基本上都能说到 1 分钟以上。 下图是总结好的口语锦囊节选，方便记忆卡壳时候使用。 四、关于模考 模考我一共做了差不多 20 遍吧。我的情况是模考的平均分和我最后的出分非常接近，大家可以根据自己的目标分来练习模考。模考的题目我都会截图的，做完统一看一下错误在哪里，下次再遇到就要做对了。听音辩词没有答案，拿不准的结合金山词霸的语音识别来确认是否是正确的单词。 五、短时间突破的时间规划 复习时长：多邻国 1 个月，学习日日均 6H+，其中中间摸索的方法的走了不少弯路，如果知道方法完全再来一次的话，通过速度会更快（每天有效学习时间 7-8h）。需要完整学习规划的可以留言并且私信找我。当然如果你基础比我好，目标分又比我低，CEFR 单词过 1-2 遍，复习半个月到一个月也是很有希望考出来的。 学习时间记录可以用 APP 番茄 ToDo。 以下是部分的 15 天学习备考规划清单 六、复习用到的网页 &amp; APP 网页： CEFR 词单：http://www.englishprofile.org/american-english 柯林斯字典：https://www.collinsdictionary.com/zh/ 牛津字典： https://www.oxfordlearnersdictionaries.com/ 韦氏字典：https://www.merriam-webster.com/ （辅助听美式发音） 百度翻译 https://fanyi.baidu.com/ （查词内置牛津和柯林斯字典，也用来查词根词缀） CEFR checker：https://cefr.duolingo.com/ （同于查看文章的词汇水平） 必应： https://cn.bing.com/ （搜完型原文） linggle： https://www.linggle.com/ （查词组搭配频率） 空缺单词查询：https://www.crosswordsolver.org/ （根据单词的开头字母和字数总数查询） 七、最后想说的话 关于试题难度自适应浮动： 以选单词题为例：如这一页的正确单词是 6 个，当考生正确率到达一半，也就是选对三个的时候，难度会增加上去，分数当然也是与之对应上涨。当考生做起来有困难，难度会降低一些，所有很智能人性化呀，完全按照考生的实际能力抛出题。或许在这里就有同学问了，在考试中如果感觉题变简单了，那么是不是分数会低呀？有些同学可能会出现恐慌。但完全不必担心，考试中我们要保持良好的做题心态，题简单的时候，我们可以做正确呀，就得分啦！！在考试过程中，难度设置是波动状态，所有同学们要专心做题，保持做题的良好心态，不必担心难度浮动~ 关于考试焦虑： 其实我这个人真的非常焦虑，我是个高考前一晚都失眠只睡三小时的人，备考多邻国的时候也掉过眼泪（嘘）我考试前有找老师和学长姐聊过，他们给了我很多鼓励，也让我觉得这个考试不是那么难通过的。回顾我自己的考试经历真的很坎坷（你肯定会比我顺利的！），首先我电脑进不去考试，换了三台电脑才能进去，后面又经历了三次不认证。我每次不认证的时候都特别焦虑，但我一般会强迫自己冷静下来，因为其实我们是有退路的，如果考不好可以不提交，如果不认证大不了隔两天考一次，慢慢来一定会上岸的，先不要着急，如果实在焦虑可以早睡一天。大家都要加油！ 最后– 以上就是上文提到的各个干货文件，点赞评论的朋友私信我，免费送一份 CERF 单词 Excel 手打表格， 如果还需要其他的就得有偿了哈。但是放心，也就是一杯咖啡，一杯奶茶的钱。毕竟码字不易。多邻国还是建议大家早点考过，因为指不定啥时候又换规则，会让我们很被动。 最后祝大家都能够上岸！外面的世界很精彩，等着我们去挖掘。当我们决定去做一件事情的时候，全世界都会给我们让路！相信自己哦！！！]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Duolingo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[点子与布局]]></title>
    <url>%2F2023%2F02%2F14%2Fideas_and_layout%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[我曾经在上篇文章《教你写笑话》中，提到过要写一篇点子与布局的文章，没想到读者会很感兴趣，经常有读者会问我什么时候可以看到这篇文章？ 无奈的是，我写完了《教你写笑话》之后，我发现幽默问题，其实就是一个推理问题，谈到推理，最正宗的当然是逻辑学，于是我就一头扎进了逻辑学当中，努力的把我的理论公式化。 但正像霍金所说的，你的小说里每增加一个公式，你的读者就会减少50%，我现在的文章，普通读者越来越没法读懂了，但请你们理解一个作者的心，一个作者总是想把自己的理论弄成科学的那种公式化，而不是玄学的那种，似是而非的东西。 这几年的研究其实很有成就的，有了许多的感悟，于是就把这篇点子与布局写了出来。 点子是一篇文章的灵魂，是作者最想传达给读者的东西，而布局则是写作的基本功了，一篇文章里可以没有点子，但是绝对不能没有布局，许多作者也想不出好的点子，但是他们的布局功夫好，文章写的非常流畅。其实一个作者刚开始写作的时候，总是想一些最精妙的点子，但是写着写着，就变得奸滑了，他们发现没有好的点子，也可以写出好的文章，这就是布局的精妙了。 可以把文章理解为由段子组成的，我这里所说的段子不仅指是笑话，而是泛指一个故事，一个段落，一个情节等等。但是如何把段子组成文章呢？这就需要用到主题了。什么叫主题呢？主题就是通过相同点把段子给组合起来。主题很类似于数学的集合，什么是集合你们可能已经忘了，我们高一学过的，集合就是由相同事物构成的一个整体，比如说马的集合，那这个集合中就会包括：大马，小马，公马，母马，黑马，斑马，白马，各种类型的马就构成了马的集合。 经常会听到电台的主持人用一些最简单的主题去策划他这一期的节目，比如说我们今天要播放刘德华演唱的歌，我们今天要播放那些经典的情歌对唱，我们今天要播放那些让人难忘的粤语歌曲，我们今天要播放90后演唱的歌，我们今天要播放抖音上最火的歌曲。几个简单的主题可以构成一个大的主题，几个大的主题又可以构成一个更大的主题，如果我们只是写一篇文章，那么随便找一个主题就可以了，但如果我们想写一部小说的话，那就必须要列提纲了，那种树状的提纲，一个主干的主题，主干开始分支，成为几个新的主题，分支又继续分支，就像一棵树一样。所谓的布局，其实就是用主题把段子串联起来的方法，这里会分两种情况，一种情况是有一些段子，我们想一个主题，把它给串联起来，另一种情况是，我们想一个主题，然后根据这个主题去写段子，这两种方法都很有用，有时候这两种方法是交替使用的。如果善于把握主题，写文章其实是一件很简单的事情，比如说描写一棵树，我们可以写树干，树枝，树叶，树根，树皮，树的形状，树的生长过程，以及与这棵树有关的任何事情，你想写一万字，我就可以写出一万字来，你想写100万字，我就可以写出100万字来，善于把握主题，其实写作不过就是往上堆段子而已。 我们这里只讨论幽默作品的布局，对于其他类型的文章，就不多做介绍了，如果全写的话，这个话题能让我写一辈子。自古道文无第一，武无第二，各种写法有各种写法的道理。写作这回事，不怕你不全面，就怕你没特点。各种写法有各种写法的道理，存在即是合理。如果只是写一个笑话，那些段子手都没有什么问题，连笑话都写不出来，还当什么段子手？但如果要写到一个相声或小品那种长度的文章，就没几个人能做到了。有些人会在网上找一些笑话，再加一个主题，就会形成一个作品了。曹云金连续三年上春晚，他的作品都是这么写出来的，到了吐槽大会，被李诞这些人一顿痛批。《爱情公寓》也是这种写法，这已经不能算是一种写法了，只能算是一种抄袭。而以前的相声作者用的都是谐音梗，现在几乎也是，类似于马三立那批人，谐音梗是最容易创作的笑话，汉字的同音字实在太多了，随便找一个就形成了谐音梗，在当年这种做法，观众还是能接受的，今天则不行了，今天的谐音梗观众都可以创作的出来，还要你们作者做什么？ 这里要推荐的是李诞这批人，他们是真正的创作者，他们做了两个栏目，一个是《吐槽大会》，另一个是《脱口秀大会》，吐槽大会非常成功，而脱口秀大会要求的是，那些脱口秀演员自己写文章，自己表演。这些脱口秀演员都是职业的段子手，写笑话当然没问题，就是靠这个吃饭的，但是要写到一篇文章的长度就很难了，连贯性很差，经常出现卡壳的情况，一篇脱口秀文章，至少应该是十个以上笑话的串联，那些脱口秀演员想着一两个笑话，还可以，想十个笑话，确实很难做到，尤其到了总决赛之后，那些演员有一种江郎才尽的感觉。其实根本的原因就是，他们缺少子弹了，或者说他们缺少原材料了。一个笑话的原材料是什么？就是乖讹，有了乖讹，才可以创造出笑话来，这可以参见我的《教你写笑话》这篇文章，可以到我的微博里去找，新浪微博@张扬大。其实吐槽大会是非常成功的，整期节目其实都是由各种段子组成的，怎么会产生这么多的段子呢？每期节目都会邀请5到6名明星嘉宾，这些嘉宾每位都会有自己的另类事件，这些事件就是一种乖讹，有了这些乖讹，就可以源源不断地创造出各种段子。有一类幽默表演形式就是新闻播报，其实每个新闻都是一个乖讹，作者把这些乖讹加工一下，就会形成许多新的段子，而且因为这些段子的即时创作性，更会吸引读者。这里要推荐的是马季的《五官争功》，将人类的五官都想象成几个人，眼人，鼻子人，耳朵人，嘴巴人，这就形成了几个乖讹，他们又可以构建大量的段子，整段相声里包袱不断。 其实用主题串联起来的，不应该是笑话，而是一个个的乖讹，乖讹才是笑话的原材料，有了这些基础材料，我们才可能产生大量的笑话。幽默很难写成长篇的作品，因为没有什么意义，一个乖讹，通常可以推理成三种状态，一种是点子，用点子可以解决困难，这在情节类小说中很重要，情节类小说通常是这种布局，遇到一个困难，找到一个点子解决困难，再遇到一个困难，再找到一个点子解决困难，形成了困难，解决困难，再困难，再解决困难，这种连贯的模式，非常有节奏感。而怪讹形成的另一种状态是悬念，这个情节类小说中也很重要，会形成一种悬念，答案，再悬念，再答案这种连贯模式，也会很有节奏感，这对情节类小说的情节推进是很有帮助的。而幽默呢，其实就是一种误会，误会对情节的推进是没有什么太大帮助的，你总不能误会，误会，误会，误会，那主人公是不是个大傻子呢？ 其实一些小说的作者对乖讹的推导比许多段子手还要擅长，比如说金庸，南派三叔这些人，他们会把乖讹推导成困难解决困难模式，或者悬念答案模式，让他们的作品节奏感非常强。而在于小说中，幽默仅仅能起到一种辅助作用，作者苦心串联起来的那些乖讹，可不舍得把它们构成一个个的误会。如果只是写一个笑话，学习幽默理论并没有什么意义，灵机一动就想出来了，但如果要写一篇类似于相声小品，这种有长度的幽默文章时候，幽默理论的价值就体现出来了，首先我们要搞清楚，创作笑话的原材料是什么，然后研究如何用主题，把这些原材料串联起来，再研究如何用这些原材料构成笑话，如果你能做到这一步，你才能成为幽默界大师级的人物。 转自作者张国强 新浪微博@张扬大 https://zhuanlan.zhihu.com/p/142191261]]></content>
      <categories>
        <category>Comedy</category>
      </categories>
      <tags>
        <tag>Humor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你写笑话]]></title>
    <url>%2F2023%2F02%2F14%2Fteach_you_to_write_jokes%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[简介 几乎很少有什么关于幽默创作方面的理论，你也很难找到这方面的书籍。对于幽默，几乎就是凭借着个人的天赋，比如说赵本山、郭德纲、周星驰，想弄出好的段子，几乎就是凭借这些人的个人能力。你如果问他们，你们是怎么想出好笑话的，他们也是莫名其妙，他们会这样说：灵机一动、灵感、一拍大腿、脑子里电光一闪，总之，他们不会说出具体的方法来。一些相声老师傅会口耳相传一些做包袱的手法，但那些方法听起来就有些可笑。 中国的幽默理论虽然很少，但西方却一直有研究幽默的传统。从亚里士多德到康德两千多年里，产生了二百多种幽默理论。其中著名的有三种，它们分别是优越论、宽慰论、乖讹论。当今的幽默理论界占主流地位的有两种理论，分别是脚本转换和乖讹论。脚本理论说的是笑话都是由两个脚本构成的，总是会由一个脚本转化到另一个脚本。乖讹的意思是与众不同的事物，乖讹论说的是笑话总是由这些与众不同的事物构成。我是在前人的理论基础上，将幽默理论向前推进了一大步。我的发明亮点是，前人虽然知道乖讹很重要，每个笑话里都有它的存在，但它究竟在笑话里起什么作用却搞不清楚。我首先提出了乖讹不是死物，它是活物。它可以实现转换，由一个乖讹转换成为另一个乖讹。从作者得到一个幽默素材开始，幽默素材就是一个乖讹，叫素材乖讹。作者将素材乖讹转换成为一个作品乖讹，我们看到的幽默作品中的乖讹，就是有作者用一个幽默素材转化而来的。我的这套幽默理论叫做乖讹推导理论，意思是乖讹总是会由一个乖讹推导出另外一个乖讹。我的理论完美的诠释了乖讹论，又巧妙的将脚本转换理论纳入我的理论当中，使它成为了乖讹推导的一种方式。 本文由于采用了许多的专业术语以及字母和公式，让读者读起来有些乏味。而且许多读者会认为，我也不想成为段子手，学这些写笑话的方法有什么用！我想对你说的是：你如果这样认为，那你就大错特错了。西方为什么那么热衷于幽默理论的研究？象亚里士多德、康德这种顶级哲学家都参与其中，难道他们只是为了研究写笑话的方法？难道这些人也是穷的需要向杂志社投些小段子，来获得一些稿费？其实幽默是人类思维的一种重要形式，与其说这些人在研究写笑话的方法，不如说他们在试图破解人类思维的奥秘。就以我的脚本转换理论为例，它不止适用于幽默，它更适用于文学的点子与计谋，科学的顿悟，发明和广告的创意。它其实就是把人类的最高思维形式创造力给公式化了，以前只有几个天才能够想到的创意，现在只要运用我的公式，一个普通人也可以把它推导出来。如果你是一个电脑高手，将我的推导理论编制成电脑程序，你就可以得到一台拥有创造力的超级电脑，人类一直梦想的研制出像人一样聪明的机器人的想法，其实很快就可以实现了。 以上所说的内容都是我的一些想法，但你不觉得我的想法很有道理吗？我希望你仔细的阅读全文，看看我的乖讹推导理论可不可以将人类的创造力公式化，如果你也觉得可以，我希望你和我站在一起，为了伟大的梦想而奋斗。 本文的第一章介绍了脚本理论和乖讹论，简单的阐述了虚假相同差异关系对的概念。第二、三、四章正式推出我的乖讹推导理论以及各种乖讹推导的推导公式。第五章讲述了如何得到素材乖讹。 关键词：脚本转换、乖讹论、乖讹推导理论、狭义乖讹推导理论、真实相同差异关系对简称真对、虚假相同差异关系对简称假对、乖讹点B。 第一章脚本理论一语义脚本理论 脚本这个概念起源于心理学，后来应用于认知学和人工智能。脚本不同的人有不同的称谓，有人称框架，有人称图示，还有人称场景。脚本指的是人们心里的认知框架。比如说动物的脚本包括有皮肤、能活动、吃食物、呼吸空气;鸟的脚本包括有翅膀、有羽毛、能飞等。总之,一个符号、一种物体等均可以看成是一种脚本。 Raskin首先应用脚本理论解释幽默，他于1985年提出了语义脚本理论。他的想法主要有三点。 1笑话是由两个脚本构成的。 2这两个脚本是相互对立的。 3通过触发词的作用，可以从一个脚本转换成另一个脚本，即所谓的脚本转换或框架转移。 例 “医生在家吗”？一位病人用带有支气管的病变的声音低声询问。“不在”，医生的年轻漂亮的老婆低声回答道，“直接进来吧” 这个笑话里，第一个脚本是病人看病的脚本，第二个脚本是通奸的脚本，触发词是直接进来吧。通过触发词，第一个脚本直接转换成为第二个脚本。 还可以举出其他的例子 例 你才是猪 某日老李一个人在山路上开车，正当他倘佯在山区美丽风景时，突然迎面开来一辆计程车，而且司机还摇下窗户对他大骂一声猪。此时老李越想越纳闷也越来越气，于是他也摇下车窗回头大骂：“你才是猪！”才刚骂完，老李便迎头撞上一群过马路的猪。 这个笑话里，第一个脚本是别人骂老李是猪，老李和他对骂。第二个脚本是真的碰上了一群猪。 例 某日,我开了一辆面包,一开奔驰的追上我,问:兄弟,开过大奔吗?把我气坏了，一踩油门就开走了，谁知他追上我，又问：兄弟,开过大奔吗?气死我也！我又开走了，在前面，他超了我，撞树上去了。我跑过去，问：兄弟,开过大奔吗?他带着哭腔答：我就是因为没开过才问你刹车在那，你TMD咋不告诉我啊！ 这个笑话里，第一个脚本是开奔驰的人向开面包的人炫富，第二个脚本是问他奔驰的刹车在那？ 二如何实现脚本转换 这个才是许多作者真正想知道的问题！按照Raskin的观点，触发词是实现脚本转换的关键。但实际上，双关语才是最重要的。在《你才是猪》的笑话里，猪是一个双关语，它的一个含义是骂人的脏话，另一个含义是一种动物。作者利用了这两种不同的意思，利用脏话的意思构建了一个骂人的脚本，利用动物的意思构建了一个交通事故的脚本。脚本转换的基础是误会，利用双关语字面相同的特点，分别构制了两个脚本。第二个脚本隐藏在第一个脚本中间，通过触发词的作用，实现了脚本的突然转换。在开奔驰的笑话里，双关语是“你开过奔驰吗”。“你开过奔驰吗”有两个含义，一个是你开不起奔驰吧。作者利用这个意思构制了第一个脚本，开奔驰的司机戏弄一个开面包的司机。“你开过奔驰吗”的另一个含义是你知道奔驰怎么开吗？作者利用这个意思构制了问奔驰刹车在哪的脚本另一个脚本。脚本转换始终是以误会为基础的，作者欺骗的其实是读者，在读者还没有反应过来的时候，突然从一个脚本转换成另一个脚本。 例 说好一起到白头，你却偷偷焗了油。 这个幽默语句中，“白头”一个是一个双关语，一个意思是白头偕老，作者利用这个意思构制了第一句白头偕老的脚本。另一个意思是真正的白头发，作者利用这个意思同焗油构制了另一个脚本。形成了脚本的转换。 例 护士对正在喝酒的病人说：“小心肝”。病人猥琐的对护士说：“小宝贝”。 这个段子里“小心肝”是一个双关语，护士的意思是小心你的肝。而病人理解为昵称的小心肝，构制了一个猥琐的脚本。 例 赵本山的一个段子，记者采访一位大妈：“大妈，你对过年燃放烟花爆竹怎么看。”大妈说：“问我怎么看啊，我爬窗户看。” 这个段子里，“看”是一个双关语,记者的看是看法的意思，而大妈理解的是如何的观看，作者利用两个意思分别构制了两个脚本，实现脚本转换。 如果不利用双关语可以进行这种脚本转换吗？当然可以了！ 下面我再举一个例子 有一个囚犯逃离了关了他１５年的监狱。他越狱后，闯入了一间民宅，看到了一对年轻的夫妻躺在床上。他把那位丈夫的赶下床，绑在椅子上，把太太绑在床上，他上了床，亲吻那太太的脖子，然后就进去了洗手间。当那逃犯在洗手间的时候，丈夫跟他的太太说：“听著，这个男的是囚犯，看看他的衣服就知道！他可能被关很久，而且很久没碰过女人了，假如他想要**，千万不要抵抗，也不要抱怨，就让他做他想做的事，让他满足就好。这个人非常的危险，假如让他生气的话，他可能会把我们杀了。” 这时候他太太说：“我很高兴你这样想，没错，他很久没碰女人了，但是他刚才不是在亲我的脖子，他是在我的耳边小声的说他觉得你很性感，问我洗手间里面有没有放润滑液。坚强点，亲爱的，我爱你！” 在这个笑话里，第一个脚本是一个逃犯想强奸妻子，第二个脚本是逃犯是同性恋，想强奸的是丈夫。这里面没有双关语，构制两个脚本的两个概念是“强奸犯”和“同性恋强奸犯”。两者之间看起来是相同的，让读者产生了误会，实现了脚本转换。 例 人家是在擦玻璃呢 老王对哥们说：“难道今天我走了桃花运了?” 哥们问：“怎么了?” 老王说：“对面楼有个美女挥舞白手帕，对我挥了一早上了。” 哥们伸头一看说：“尼玛的，人家是在擦玻璃呢。” 在这个笑话里，第一个脚本是美女冲老王挥手，第二个脚本是美女在擦玻璃。挥手和擦玻璃看起来是相同的，让人引起误会，实现了脚本转换。 读到这里，聪明的读者已经发现了其中的规律性，是的，这就是我要推出的一个新的概念，“虚假相同差异关系对” 三虚假相同差异关系对 如果两者之间看起来相同或听起来相同或某一部分相同，而两者之间本质上是不同的，那么我们就把这两者称为虚假相同差异关系对。 双关语的两个不同含义就是一对虚假相同差异关系对。例如双关语“猪”中，骂人的脏话猪和动物的猪就是一对虚假相同差异关系对。而在“你开过奔驰吗”中，炫富的“你开过奔驰吗”和你会开奔驰吗？形成一对虚假相同差异关系对。在双关语“白头”中，一个含义白头偕老和另一个含义白头发之间形成一个虚假相同差异关系对。虚假相同差异关系对这个名词太过绕嘴，主要是为了让读者理解它的真正含义。两者之间有相同的部分，也有差异的部分，为什么叫虚假呢，主要是为了将来还要推出一个真实相同差异关系对。为了省事，以后简称虚假关系对。 语言文字形成的关系对通常都是虚假关系对，比如说多音字，长（chang)和长(zhang）之间。两个字是相同的，但读音是不同的，含义也是不同的。 例 南京市长（chang)江大桥南京市长（zhang)江大桥 利用多音字，形成了另一个脚本。 我的家乡是尚志市，尚志市曾经自号为雪都，有一年的广告宣传语是“雪都欢迎您”。一个外地人看了好生感动：“你们尚志人太热情了，连雪都欢迎我”。他没有分清楚都du和都dou。 不只是语言文字可以形成虚假关系对，形态动作也可以，甚至许多抽象的道理也可以。在前面的擦玻璃的笑话里，擦玻璃和挥手之间就是一对虚假关系对，两者之间的相同点是看起来很像，不同点是各有不同的意义。而在同性恋强奸犯的笑话里，强奸犯和同性恋强奸犯构成虚假关系对，两者之间外表看起来没有什么不同，但是本质上却又重大的不同。 例 公交车上，一个青年在嚼着口香糖。坐在他对面的一个老头突然说：“你别说了，我听不清，听不清。 这个笑话里，嚼口香糖和说话之间；形成了一个虚假关系对。两者看起来是相同的，但本质上却又重大的不同。作者利用这个特点，形成了两个脚本，以相同点为轴，进行脚本转换。 现在我们知道，要想实现脚本转换，就需要一个虚假关系对。利用关系对的两个不同的概念构制两个不同的脚本，以相同点为轴，形成脚本转换。 四同一律 我们利用虚假关系对的两个不同的概念，分别构制两个脚本，从而形成了脚本转换的手法。如果把虚假关系对中的两个不同概念放在一个脚本里，那会是一个什么样子呢？ 同一律是一个逻辑学的概念，逻辑学有三大基本的原则。只有符合这三条基本的原则，逻辑推理才是成立的，否则，逻辑不成立。这三条基本原则是：同一律，矛盾律，排中律。同一律概念是在同一思维过程中，必须在同一意义上使用概念和判断，不能在不同意义上使用概念和判断。 只有符合同一律，逻辑才是成立的。而幽默恰恰总是违反了同一律，两个不同概念肆意的放在一个脚本里进行比较，作用，印证。不符合逻辑，却有了幽默的效果。 例 兜比脸还干净 兜的干净和脸的干净是一对虚假关系对，都是干净，但是概念确是不同的。兜的干净指的是兜里什么也没有，而脸的干净指的是脸上没有灰尘。两个不同的概念放在一起进行比较，逻辑上是没有意义的，但幽默效果十分明显。 例 一个男孩去商店里买飞机模型，售货员对他说：“你的钱是假的。”小孩分辨道：“你的飞机不也是假的吗？ 这个笑话里，飞机的假和钱的假不是一个概念，小孩将它们混为一谈，用来进行狡辩。 例 自从得了精神病以后，整个人都精神多了。 精神病和精神是一对虚假关系对，字面上是相同的，但是概念上是不同的。把两者混为一谈，达到幽默的效果。 例 儿子哭着对爸爸说：“他们都说我傻，我真的傻吗？”爸爸抚摸着儿子的头说：“你怎么会傻呢，傻孩子。” 傻孩子和傻是一对关系对，傻孩子是昵称，并不是真的傻。把这两个不同的概念放在一起，起到了一个自相矛盾的作用。 脚本转换的手法是利用虚假相同差异关系对中两个不同的概念，分别构制两个脚本。以相同点为轴，通过触发词的作用，突然进行脚本转换。违反同一律的手法是，将虚假相同差异关系对中的两个不同的概念放在同一个脚本里进行比较，印证，相互作用。虚假相同差异关系对在幽默中有着举足轻重的作用，或者可以说，一个关系对的好坏，就决定了一个段子的好坏。但我们如何能得到一个好的关系对呢？这就是我们在下面章节里要讨论的内容。 五乖讹论 幽默理论研究渊源流长，从亚里士多德开始，至今已有两千多年的历史。幽默理论层出不穷，其中以三大理论最为著名，它们分别是优越论，宽慰论，乖讹论。其中乖讹论以其独特的研究视角最为深入人心，今天乖讹论依然是幽默研究的主流。 乖讹论也被称为不和谐理论。在当代幽默心理面研究中占主导地位的乖讹论，是对幽默和笑的研究中最具影响力的一派理论。有相当久远的历史。乖讹这一概念大概可以追溯到古希腊时代，最起码始于文艺复兴时期。之后的康德和叔本华也曾从乖讹的角度提出了自己的理论。Beattie对乖讹的定义被广为引用，他说:“两个或更多不一致、不适合、不协调的部分或情况，在一个复杂的对象或集合中统一起来，或以一种头脑能注意到的方式获得某种相互关系，笑便源出于此。”18世纪的康德被认为是第一个从乖讹的角度为幽默下了完整定义的人。他指出，幽默来自于“从期待到期待落空的突然转换”。叔本华在给笑的定义中则明确地提到了乖讹/不和谐：“在每一个事例中，笑的原因不过是突然感觉到一个概念和借助这一概念表现的现实事物之间的不和谐，而笑本身正是这一不和谐的表现。 那么什么是乖讹呢？乖讹指的是不正常的，不和谐的，与众不同的。在我们的大脑中存在着一个对万事万物正常状态的框架，即所谓的脚本。当出现一个与正常脚本不同事物或现象的时候，我们就认为它是乖讹的。例如，正常人的身高是170左右，而当出现一个身高220的姚明的时候，我们就认为姚明是一个乖讹。例如，美国总统都是白人，当出现一个黑人总统奥巴马时，我们就认为他是乖讹的。乖讹是一个相对的概念，与它相对的总有一个正常的概念。乖讹也是分不同人群的，现代人和古代人对乖讹的认识就是不同的，我们会认为古代人留辫子是一个乖讹。而古代人则会认为我们留短发是一个乖讹。 康德说过：“两个或更多不一致、不适合、不协调的部分或情况，在一个复杂的对象或集合中统一起来，或以一种头脑能注意到的方式获得某种相互关系，笑便源出于此。” 一个不和谐就代表一个乖讹，一个笑话里通常会有两个或两个以上的乖讹，这些乖讹之间有什么关系呢？ 我在乖讹的研究方面比前人深入一些，下面就直接引入我的研究理论了，我的幽默理论叫做“乖讹 推导理论” 六乖讹推导理论 幽默创作不是无中生有的，它也需要一个写作素材，这个写作素材必须是一个乖讹，我把它叫做素材乖讹。或者可以这么理解，幽默创作其实就是一个对素材乖讹的推导过程。作者把素材乖讹推导成一个新的乖讹，这个乖讹叫做作品乖讹，作品乖讹和素材乖讹整体上才能算作一个笑话。比如说一个人走着走着突然摔了一跤，这看起来很好笑，但这只是一个素材乖讹，算是一个滑稽，称不上是幽默。幽默必须是一个推导公式，由一个已知的素材乖讹推导出一个新的乖讹。素材乖讹必须是作者和读者之间的一种共识，大家都承认它的存在或想象中的存在。读者欣赏幽默的过程，其实就是看着作者对素材乖讹的推导过程，如何利用一个已知乖讹推导出一个新的乖讹的过程。现在大家应该明白了为什么一个新闻出来了，会聚集大量的段子手进行pK。一个新闻就是一个好的素材乖讹，利用这个乖讹，段子手可以创作出大量的段子。生活中的幽默也是一样，总会出现一个乖讹事件，一个擅长幽默的人就会抓住这个乖讹，突然搞出一句爆笑的语言。如果一个人对你说：‘你能不能用一句话把大家给逗笑了。”这是一个很荒谬的提法，幽默创作不可能无中生有的，它需要有一个素材乖讹，作者创作的过程其实就是对这个素材乖讹推导的过程。一个好的幽默创作者，他首先是一个善于发现素材乖讹的人，其次才是一个善于进行乖讹推导的人。 那么应该如何对素材乖讹进行推导呢，下面我们就来具体研究一下乖讹推导的方法。 一个乖讹通常可以分解成三种状态，真实相同差异关系对，虚假相同差异关系对，以及一个乖讹作用生成另一个乖讹，利用另一个乖讹形成相同差异关系对。下面我们具体的对三种方法进行分析。 第二章真实相同差异关系对 乖讹是一个相对的概念，一个乖讹总是有一个和它相对的正常的概念，我把乖讹和与它相对正常的概念称为一对真实相同差异关系对。比如说姚明是一个乖讹，与他相对正常的是普通人，姚明与普通人构成了一对真实相同差异关系对。比如说哑语是一个乖讹，与它相对正常的是正常的语言，哑语与正常的语言之间构成了一对真实相同差异关系对。为了简便起见，以后我会把真实相同差异关系对简称为真对，虚假相同差异关系对简称为假对。 当我们得到一个素材乖讹的时候，很容易就把它化解为一个真对，如何对一个真对进行推导呢？ 一个乖讹与它相对正常的概念之间总会有一个差异，我把这个差异叫做乖讹点。比如说姚明和普通人之间的差异是高度，高度就是这个真对的乖讹点。比如说煎饼是一个乖讹，它和普通食物之间的差异是它特别的薄，薄就是煎饼和普通食物之间真对的乖讹点。 我把正常的概念记做A，乖讹点记做B,一个乖讹可以记做A+B。 A会与周围的物体或者环境发生作用，周围的物体或环境记做C。 单单有C显然还不够，C不会和B发生作用，A与C发生作用，得到的结果是E,而A+B与C发生作用得到的结果也是E。两个结果是相同的，也是毫无意义的。 我们还需要一个点D，D是C的特殊情况，也可以说是C的乖讹，记做C+D。D是一个特殊的点，它即会和A发生作用，也会和B发生作用。而且对两者之间的作用是完全不同的，D或者是允许A否定B,或者说允许B否定A。 造成的结果是A与C+D之间发生相互作用，得到的结果是E，A+B与C+D发生相互作用得到的结果是F。F与E之间产生的重大的差异。也就是说A与A+B之间的差异推导出了E和F之间的差异。我把这个公式叫做狭义乖讹推导公式。 例 医生：你的病需要进行隔离治疗，以后你只能吃煎饼了。 病人：吃煎饼对我的病情有帮助吗？ 医生：不是，这是唯一能从门缝里塞进来的食物。 在这个笑话里，A是正常的食物，A+B是煎饼，B是乖讹点，它特别的薄。遇到的环境乖讹C+D是一个封闭的房间，只留一个小门缝。D点会否定A，普通的食物塞不进来，D点会肯定A+B,只有煎饼能塞进来。在幽默创作中，作者首先得到的是一个素材乖讹A+B，这时需要构思的是与乖讹点B发生相互作用的环境乖讹D点。再把D点构制成可以与A发生作用的C+D。 例 一个人到游泳馆里对经理说：“我想应聘你这里的救生员。”经理问他：“你会游泳吗？”他说不会。经理大怒道：“你不会游泳，你当什么救生员，你是逗我玩吗？”他说：“你们游泳池的水深两米，我的身高是两米二，我不会游泳也淹不死啊！” 这个笑话肯定是因姚明而构制的，这里普通人是A，姚明是A+B，B点是姚明与普通人之间的高度差，环境是水池C，特殊的水深是D，这里的水深D点一定要是两米，如果水深是一米五，姚明和普通人都不会被淹死，这是没有意义的。如果水深是两米五，姚明和普通人都会被淹死，这也是没有意义的。只有水深是两米的时候，普通人会被淹死，姚明却不会，姚明和普通人的身高差被推导成为普通人会被淹死，姚明却不会。这里如果把姚明换成是一个侏儒，依然可以构制成一个笑话。 例 女巫追到了黑森林里，白雪公主带着七个小矮人迅速逃命，跑着跑着，前面出现了一条大河。幸好河水不深，刚好到白雪公主的腰部，白雪公主忍着冰冷刺骨的河水，逃到了对岸，回头一看，妈呀，七个小矮人被淹死了！ 小矮人与普通人之间的乖讹点B依然是高度差，水的高度D依然需要控制，需要控制在可以把小矮人淹死，普通人却不会。 研究这套理论的时候，我正在读《失控》这本书，给我很大的灵感。达尔文创立进化论以来，立即以它独特的观点深入人心，到了今天，人们不止将进化论应用于生物界，在自动化控制电脑编程，复杂系统研究等诸多领域，人们会将优胜劣汰应用到方方面面。就好像一只小怪兽，由于基因突变，它产生了。如果环境不改变，那它并没有什么发展，只不过是和正常的有些不同而已嘛，并没有什么了不起的。但是环境发生了改变，变得只适合小怪兽生存，而把正常的都干掉。这是人们才发现小怪兽的伟大。我们也可以把这个理论推广到万事万物，任何事物当出现一个它的乖讹的时候，人们总是会把它和正常事物放在一起进行比较，去面对正常事物应该面对的环境。当环境很正常的时候，人们不会注意这个乖讹的与众不同，一旦环境改变的时候，环境会变得否定正常现象而肯定乖讹，这时人们才发现乖讹的与众不同。我们以金庸小说的屠龙刀为例，屠龙刀是一个乖讹，它相对于普通刀的乖讹点是削铁如泥。当它和普通刀一样，面对的环境只是萝卜豆腐、鸡鸭鱼肉的时候，它和普通刀没有什么区别。当面对的环境是铜铁的时候，普通的刀已经无能为力了，这时才凸显出屠龙刀的威力。 为什么叫狭义乖讹推导理论呢？前文我们提到，素材乖讹可以分解成不同的关系对，这些关系对又 都可以进行推导，所以会有很多种不同的推导方式。而这些方式有些只适用于文学，甚至有些只适用于幽默。只有狭义乖讹推导公式既适用于文学，也适用于科学。它是人们进行思维活动的时候的一种实实在在的推理方式。逻辑学有三大推理方法，演绎推理、归纳推理，类比推理。而狭义乖讹推理是区别于这三种方式的一种新的推理方法，它更适用于人们进行创造性思维。如果你对思维或者对智慧很感兴趣，我希望你能掌握并熟练运用这种方法。我可以大言不惭的告诉你，如果你熟练了这种思维技巧，至少可以让你的智商提高二十个百分点。狭义乖讹推导公式必须是有一个真实相同差异关系对，B点必须和D点发生相互作用，所得到的E和F之间的差异必须是有合理性的。 狭义乖讹推导公式在文学中应用非常广泛，尤其是在故事当中。例如在金庸的《飞狐外传》中，主人公胡斐和众群侠被困在铁制的密室里，敌人又在密室的底部点上火，想把群侠给烤死。当时只有一个小孔与外界相连，群侠身材都很高大，很难从小孔中钻出去，只有胡斐当时只有十三岁，身材矮小，从小孔中钻了出去，杀死了敌人，解救了群侠。 这里胡斐与群侠相比是一个乖讹A+B，群侠是A，乖讹点B是身材矮小。作者构思了一个可以和乖讹点B发生作用的D点，D点是一个小孔，D可以否定群侠A，又可以肯定乖讹胡斐。 在《笑傲江湖》中，令狐冲只会武功，没有内力。遇到了一个以琴音作为攻击武器，专门攻击人的内力。令狐冲由于根本没有内力，所以免遭攻击，战胜了对手。 这里的乖讹点B是没有内力，根据乖讹点，作者构制了可以和B相互作用的D点。D是专门攻击人的内力，D可以否定普通人，却可以肯定令狐冲。 有一个关于曾国藩的段子，说曾国藩很笨，背书总是背很多遍也不会。一天，他在家里背书。有一个贼摸到他家里，躲在房梁上，只等他睡着了，好去偷东西。只见曾国藩背一小段文章，背了两个时辰还是不会，贼实在是忍不了了，从梁上跳了下来。对曾国藩说：“你这么笨，还当什么读书人。”说着，把曾国藩背的文章从头至尾背了一遍，然后扬长而去。 这里的乖讹点B是时间长，作者构制了一个D点是一个急等时间的贼。 狭义乖讹推导理论是可以反推的，在文学创作中，作者通常是得到一个素材乖讹，把它化解成一个真实相同差异关系对，找出乖讹点，通过乖讹点，设计出一个与它相互作用的D点。从而构制成一个段子或者是故事。而在科学发明中，这个乖讹是根本不存在的。研究者通常遇到的是一个困难，C+D与A相互作用得到的结果是E,E是一个困难，为了解决这个困难，需要找到一个A+B，A+B与C+D相互作用，得到的结果是F，F与E是不同的，从而解决了困难。研究者其实求解的是B。例如在爱迪生发明灯泡的过程中，遇到的一个困难是，在两千度的高温下，灯丝就会熔断。为了解决这个问题，必须找到灯丝的乖讹，让它在高温下不会熔断。爱迪生解决了这个问题，研究出了一种炭化的灯丝，在高温下不会熔断，从而解决了问题。 我曾经搞过一个发明，新型筷子。 筷子是中国人的餐具，非常实用。但使用它需要很大的技巧，儿童以及外国人就很难使用筷子。这里就是一个困难，为了解决这个困满，就需要对筷子进行改进。我把筷子的尾部用弹性材料连接，把筷子做成一个U型，更像是一个夹子，使用起来非常简便，从而解决了困难。 发明其实是研究者研究出了一个新的乖讹，把它放在困难当中，解决了困难。 在解决困难的模式中，会把C+D与A发生作用，得到E设置为一个困难。而把C+D与A+B作用得到结果F视为解决困难。在这个模式中，D是否定A，肯定B的。而在幽默中，更喜欢的是D否定B。 例 印度人用手抓饭吃，那他们吃火锅是怎么办呢！ 我们中国人使用筷子，所以我们认为印度人用手抓饭是一个乖讹，乖讹点B是用手抓。B会被D点环境乖讹即特殊的食物火锅所否定。D是直接否定B的。 例 在公交车上，一个小伙子给一个老大爷让座。老大爷非常感动，非要和小伙子结拜。两人跪在地上，不求同年同月生，但求同年同月死。小伙子问大爷多少岁了，大爷说：我都九十了。 和一个比自己大得多的人结拜，就会被环境乖讹D同年同月死所否定。 狭义乖讹推导公式对讽刺有十分明显的效果，主人公一个微小的缺点，都会被乖讹推导成巨大的差异。 一个胖子刚从公交车上下来，售票员就会喊道：有空座了，有四个空座了。 例 牧师问丈夫：如果再过五分钟，地球就会毁灭，你要做什么？丈夫说：我要疯狂的做爱。妻子冷冷的问：那剩下四分钟呢！ 早泄是一个乖讹，乖讹点B是时间太快。作者会安排一个D点是非常短的五分钟，主人公还给剩下了四分钟。 隐性的乖讹点 在一个简单的乖讹中，可以只有一个乖讹点。而在一个复杂的乖讹事件中，可以存在许多的乖讹点，它们通常死隐性的，不容易被发现，需要作者去发掘。 例 甲：我妻子骗了我，她说她昨天晚上和她妹妹在一起。 乙：你是怎么知道的？ 甲：昨天晚上我和她妹妹在一起！ 和小姨子偷情是一个乖讹是事件，在这个事件里，还存在一个乖讹点，主人公可以和小姨子呆在一起一个晚上，这是普通人无法做到的。利用这个乖讹点，设置了一个环境因素D妻子的谎话，妻子说和小姨子在一起，乖讹点B恰恰可以否定这个D点。 例 一个一丝不挂的美女钻进了出租车里，司机看了美女很久，美女生气的说：没见过美女呀！司机怒道：我看你他妈的从哪能把钱掏出来。 裸体美女是一个乖讹，这里还隐藏这一个乖讹点，没穿衣服就没有兜，没有地方装钱。这里设计一个需要钱的环境D，直接就把美女给否定了。 例 妻子对丈夫说：我要给你一个惊喜。丈夫问是什么？妻子说：我怀孕了。丈夫郁闷的说：我还不想要孩子。妻子笑着说：瞧你、瞧你！我说要给你一个惊喜，这只是一个惊。喜的是孩子不是你的，是隔壁老王的。 孩子不是自己的，这是一个乖讹，这里还隐藏一个乖讹点是，孩子不是自己的，就可以不用抚养了，然后有了不想要孩子的环境因素D，乖讹点B就可以否定环境因素D了。 在狭义乖讹推导理论中，乖讹点B是关键，一个段子的好坏，基本上取决于B的好坏。B点可以作用什么、攻击什么、否定什么，或者可以被什么攻击或否定，就可以把什么设定为环境因素D点。在这个公式中，关键是B点和D点的相互作用。 D点之所以可以区分A和A+B，主要的原因是B点和D点发生的巧妙作用。在前面的例子里，B点胡斐身材矮小，D点就设置为一个只有他能钻进去的洞。B点令狐冲没有内力，D点就设置为专门攻击内力的琴音。B点是可以和小姨子睡一宿，D点就是一个只有他可以揭穿的妻子的谎言。B点是孩子不是自己的，D点就设置为不想要小孩。A也可以和D发生作用，但A和D发生的作用只是没有象B一样和D发生巧妙作用的后果。在前面的例子中，A点众群侠没有钻过那个小洞，后果当然就是死。A点有内力的人遇到琴音，当然就是被攻击。A点普通人没有和小姨子睡一宿，当然是无法揭穿妻子的谎言。A点普通人孩子是自己的，当然无法解决不想要孩子的困难。 B的始终是一个段子能量点，D是根据B设计的。B是一个功能器，B可以做什么，就把D设置为什么。B如果是水，那么D就因该是火。B如果是一个苍蝇拍，D就应该是一只苍蝇。B如果是一只猫，D就应该是一只老鼠。在段子创作中，作者首先要在一个乖讹事件中寻找乖讹点，得到一个与众不同的乖讹点，通过乖讹点去设计D点，从而创作出一个完美的段子。 同样的B点，在不同的D点下，可以得到不同的结果。因为有些D点可以否定B，有些D点可以肯定B。我们以姚明为例，姚明是一个乖讹，乖讹点B是特别的高。如果是在D点篮球场上，姚明无疑是一个王者，所向披靡，无人能挡。但如果D点换成了战场，那情况就会大大的不同了，因为在战场上，姚明的目标太大，即使是枪法最差的士兵，也可以打中他。 一些看起来很差的B点，因为D点的作用，也会表现的很突出。例如胡斐身材矮小，在正常的环境下，他是很吃亏的，他不可能打败那些大侠。但在特殊的环境D点下，比如说那个小洞，胡斐就会表现出非凡的成就。令狐冲没有内力也是一个不好的乖讹点，在正常的环境里总是吃亏，但作者会安排一个只有他能过关的琴音环境。比如说一个盲人，正常环境下比常人差太多了，但如果在一个黑暗的环境里，他就会比正常人更加的适应。 乖讹的这种特点在文学中有着广泛的应用，一个非常好的乖讹点，在各种D点环境下风风火火，所向披靡。一旦遇到一个克制他的D点环境，立刻大败亏输，满盘皆负。而一个看起来很差的乖讹点，只要作者为他设计了好的D点，也可以如鱼得水，出人头地。这在文学创作中通常可以达到一个逆反的效果。 一些我们平时习以为常的乖讹点，在不同D点的作用下，就会发出不同的效果。如前面举得几个黄色笑话的例子，和小姨子偷情的乖讹，在妻子说谎的时候，竟然可以揭穿妻子的谎言。一个裸体美女，在遇到钱的问题时，竟然会被司机否定。孩子不是自己的，竟然可以解决不想抚养孩子的问题。大家注意没有，明星的签名字总是写的莫名其妙的，不像是正常的字。为什么会这样呢？ 很早就看过一部香港影片，名字我已经忘记了。说的是洪金宝很崇拜刘德华，想要刘德华给他签名，刘德华于是就签了，结果洪金宝说刘德华已经加入了他们的组织，拿出一份证书，上面有刘德华刚才的签名。我也构制过类似的段子，说一个人找明星签名，签完之后才发现，是一份账单。 明星签名是一个乖讹点，当重新构制一个D点的时候，就又成了一个新的段子。一个脑筋急转弯说的是为什么一个仁慈的皇帝，却要诛人九族呢？答案是如果人都死了，就不会有人会伤心了。诛人九族的典故大家已经知道了，是一种非常残忍的手段，但它却可以解决有人伤心的问题。 我以前经常给故事类的杂志投稿，许多写手们混在一起，在论坛上大家经常会讨论一个问题。要不要从已知的文学作品中或新闻中提取乖讹进行创作，这样算不算是抄袭呢？今天这个问题已经没有讨论的必要了，作为故事或者说是段子的创作，本来就是利用已知乖讹进行再创作的的过程。 从新闻或文学作品中提取的乖讹B，只要我们重新设计一个D点和它发生巧妙的作用，这就形成了一个新的作品。这哪里是抄袭，这是重新创作。如果只是指着作者对生活中所能遇到的几个乖讹进行创作，那这个作者一生能够写出几个作品！利用已知文学作品或新闻的乖讹进行创作，本来就是段子手的两大阵地。 段子会分射雕段子、西游段子、三国段子，越是大家耳熟能详的乖讹，创作的段子越多。一个已知的乖讹，加上不同的D点，就会变成新的段子。甚至许多大的乖讹点也是可以分类的，如根据北京雾霾形成的段子，根据弹琴让邻居很闹心形成的段子，根据丈夫怕老婆形成的段子，根据鹦鹉学舌形成的段子，根据口吃形成的段子数不胜数。这些个大的乖讹点下面都有数不清的段子，每个人都可以根据自己想到的新的D点，和它们配合形成新的段子。 在狭义乖讹推导公式中，如果得到的是A+B，自然可以推出C+D。如果得到C+D和A，也可以推出A+B。如果得到的是B和D，也可以推出A和C。 在几十年前，苏联人就发明了发明机。发明机的原理很简单，只是把任何不同的两个物品组合在一起，他们就认为完成了一个发明。从狭义乖讹推导理论中，我们明白了，他们只是形成了一个乖讹。只有当乖讹点B和困难点D发生相互作用时，一个发明才真正成立。而且在发明中，以乖讹点B求取困难点D并不可取。发明真正的方法是，以困难点D去求取乖讹点B。发明就是为了解决我们所遇到的一个一个的困难。而在文学创作中则恰恰相反，作者努力的去寻找一个一个的乖讹，在长篇小说中，作者还要自己去创作乖讹。通过这些乖讹，得到乖讹点。再利用乖讹点构制与它发生巧妙作用的D点。这是才会形成一个段子。 乖讹推导还有一个变式，可以没有D点，B点直接作用于A点。B通常是A的倍数，作者把A设置成一个事件的关键点，或者是一个因果关系的起因，总之是很重要。B作用于A点，把A的功能扩大无数倍。 例 蜈蚣爸爸看见小蜈蚣正在哭，于是问道：你想要什么，孩子？小蜈蚣说：我想要买耐克鞋。 蜈蚣的脚是一个乖讹，他是正常脚的无数倍，A点是普通的脚，想买耐克鞋会让爸爸很头疼，也就是说，脚会让爸爸头疼。然后就是无数支脚，可以让爸爸疯掉。 例 医生将针头插入病人的体内，病人痛的一声惨叫。医生不耐烦的说：安静点好不好，我们是在做针灸，还有一百多针呢！ 针灸可以看作是普通打针的乖讹，乖讹点B是普通打针的无数倍。A点是普通打针，A点可以给病人带来痛苦，B点是A点的超强版，病人应该昏过去了。 这种推导方法更适用于虚假相同差异关系对，下面我们开始讲解虚假相同差异关系对了。 第三章虚假相同差异关系对 上文提到，B点其实是一个功能器，B点可以做什么，就把D点设置为什么。从而让D可以区分A和A+B。那么如果B点不是一个功能器，B其实做不了什么，只是和A的样子有些不同而已，那么又会怎样呢？ 我们以哑语为例，哑语是一个乖讹，它相对的正常现象是正常的语言。但它和正常的语言已经不是很像了，它更像是一种舞蹈，于是哑语和正常的语言之间构成了一对真是相同差异关系对。哑语和舞蹈之间构成了一对虚假相同差异关系对。 狭义乖讹推导理论不止适用于文学，还适用于科学。而虚假相同差异关系对只适用于文学，甚至只适用于幽默。幽默本身就是一门研究误会的学问，如何让人产生误会？一定要存在看起来相似的两者，这两者就叫做虚假相同差异关系对。许多虚假相同差异关系对是天然的，如同音字、同音词、双关语，只要我们能够寻找出来就可以了。但更多的虚假相同差异关系对是需要我们去创造的。如何创造一个假对呢？自然是用乖讹的方法。 一个乖讹会和与它相对的正常现象构成一个真对，但乖讹通常不会和正常现象很像，而会像其他别的东西。乖讹就会和其他别的东西构成了一个假对。利用乖讹的这个特点，我们可以轻松地创造出许多新的假对。 例如郭德纲的段子：你行的，你和超人唯一的区别是你把裤衩穿里面了。 超人的服装是一个乖讹，尤其是裤子，看起来很像裤衩套在了裤子的外面。 赵本山的段子，宋丹丹：我年轻的时候，那是柳叶弯眉樱桃口，大人小孩都乐意瞅。就隔壁那吴老二，一见我就浑身发抖。赵本山：吴老二脑血栓，看谁都哆嗦。 脑血栓患者是一个乖讹，浑身总是不停的抖，看起来很像是一个人精神激动发抖。两者之间构成了一个假对。 长城汽车早期的商标是一个墙上有一个墙垛，不过画的已经不像墙垛了，而象是一个牙床上只有一颗牙。于是车迷们就把长城汽车戏称为一颗牙。 故事会里的一个故事，名字叫《你怎么学坏了》，说的是一个大学生毕业找工作，用人单位都要抽血体检。一个月下来，工作没找上，胳膊上却扎了许多的针孔。家里人发现了，以为他学坏了，开始吸毒了。 这种找工作体检是一个乖讹，会让胳膊上出现许多的针孔，看起来和吸毒的针孔很像，于是和吸毒构成了一个假对。 乖讹形成假对的手法，在动物笑话里有着普遍的应用。 例 赵本山的段子，老虎被蛇咬了，于是老虎去追蛇，追到了一条小河旁，蛇一下子钻进了水里。老虎在岸边等了半天，突然从水里爬上来一只王八，老虎上前一把把王八按住说：“好小子，你穿了马夹我就不认识你了。” 蛇穿了马夹是一个乖讹，看起来已经不像蛇了，而像是一只王八。穿了马夹的蛇和王八构成了一个假对。动物笑话的乖讹都是想象中的，事实上根本不存在。 例 母老鼠发现公老鼠偷偷的溜出了家门，于是在后面跟踪，走着走着，发现了一只刺猬。母老鼠上前按住了刺猬：“小样的，还说不是去约会，你打这么多的摩丝做什么。 打摩丝的老鼠是一个乖讹，看起来已经不像是一只老鼠了，而像是一只刺猬。打摩丝的老鼠和刺猬构成了一个假对。 例 饺子和馒头结婚了，洞房的时候，馒头发现饺子不见了，床上只躺着一个肉丸子。馒头于是要去找饺子，只听肉丸子说：‘死鬼，人家把衣服脱了，你就不认识人家了。” 脱了衣服的饺子和肉丸子形成了一个假对。 语言类假对的构成 语言只是对具体事物的描叙，语言是不存在乖讹推导的。语言形成的关系对没有真对，也就是说，语言含义形成的关系对通常都是假对，只有一些语法可以形成乖讹真对。 从天然的假对如多音字、同音词、双关语到曲解、歧义以及事物本身称呼的乖讹，都可以形成假对。在本文的一开头，我就介绍了假对的概念。假对是看起来相同，听起来相同，或其他部分相同，但本质上却是不同的两者之间。 谐音一直是早期幽默创作的主流， 如马三立的段子《逗你玩》。妈妈让小孩在门口看着晾晒的衣服，一个贼走了过来，贼对小孩说：“我姓逗，叫逗你玩。”贼开始偷衣服，小孩对妈妈喊道：“他把褂子拿走了。”妈妈：“谁啊”。小孩：“逗你玩”。等妈妈出门看的时候，衣服已经被偷光了。妈妈问谁拿走的，小孩：都你玩。 这里，名字的逗你玩和调皮话逗你玩是一对同音的假对。它们的含义是不同的。 马三立的相声《开药铺》也是谐音的典范，两个外行人开药铺，不懂装懂。把中药银珠曲解为银子打的珠子，把中药砂仁曲解为三个人，把陈皮曲解为姓陈的皮匠。 现代相声如苗圃的《满腹经纶》也大量运用了谐音，取得了不俗的成就。 《非诚勿扰》的主持人孟非，你听他说话，几乎每一句话里都有谐音的包袱存在，运用的非常娴熟。谐音笑话如今已经不能成为幽默创作的主流了，乖讹推导才是幽默创作的主要手段。谐音胜在它的简单，不用去深思熟虑的布局，随口就来。一般会用在需要现场表演的场合，如主持人特别喜欢这种手法。在现场主持紧张的氛围下，很难去运用乖讹创作一个好的段子，而谐音的简洁手法符合了这种需要。 但对于一个有时间进行创作的作者来说，如果你总是用谐音，效果就不好了，别人会以为你不懂幽默。 曲解也是形成语言类假对的主要手段，效果通常会很好。 谐音只是语音相同，而字面上是不同的。曲解则不然，一个词或者一句话通常会有一个引申含义，还会有一个字面上的意思，这两个不同含义之间就构成了一个假对。这样形成的假对，由于字面上是完全相同的，所以效果很好，很轻松的就能够把读者给骗到。 例 郭德纲的一个段子，郭德纲想要吃鸡，于是去找一家做鸡的饭店。在一群发廊中间，找到了一家饭店。郭德纲问服务员：“你这有鸡吗？”服务员说：“嘘，我就是！” 鸡的引申含义是妓女，鸡的本意是一种家禽，鸡的两个不同含义之间就构成了一个假对。 例 阿拉伯酋长的儿子在伦敦读书，他给父亲发电报：同学们都坐地铁上学，只有我开着纯金的奔驰，我感到很不好意思。父亲回电报：已经给你汇去两个亿，赶快买一辆地铁，不要再给我丢人了。 这也是一个曲解，作者把儿子想坐地铁上学硬性曲解为同学都买的起地铁，而儿子买不起。 妻子把刚买的自行车撞到了树上，撞得稀巴烂。丈夫对妻子说：你若安好，便是晴天。妻子感动的流下了眼泪。丈夫接着说：你若安不好，嘿嘿，你可小心了。 你若安好，便是晴天。是一句关心的话。突然曲解为你要把自行车安好。两个不同含义构成了一个假对。 一个帅哥遇见了一个美女，帅哥对美女说：可以啪啪啪不。于是两个人进入了树林里。两人啪啪啪了半个小时，美女问帅哥:还啪啪啪不？帅哥说：不了，脸疼。 啪啪啪的引申含义是做爱，把它曲解为打脸的啪啪啪，两个不同含义构成了一个假对。 肢体语言形成的假对 肢体语言指的是手势，它通常会跟动作之间形成一个假对。 例 小蚊子哭着对妈妈说：爸爸死了。妈妈惊道：你们不是去看演唱会了吗，爸爸怎么会死呢？小蚊子说：观众们鼓掌，爸爸没躲开。 鼓掌是一个肢体语言，它和拍蚊子之间构成了一个假对。 例 两个贼准备翻墙去偷东西，甲先爬了上去，乙问甲：有没有保安。甲做了一个OK的手势。乙也爬了上去，刚上去，就被保安给抓住了。乙问甲：你不是说没有保安吗？甲说：三个保安。 OK是一个手势，表示好的的意思。它和树三个手指表示山的含义之间构成了一个假对。 鹦鹉和老鹰打架，被老鹰把毛全都拔光了，鹦鹉说道：不脱光膀子还真干不过你。 脱光膀子是一个肢体语言，表示要大干一场。而鹦鹉被老鹰拔光了毛和脱光膀子之间，成了一个假对。 素材乖讹如何形成语言假对 在上面的例子里，大家也看到了，如果得到一个好的语言假对，购置一个段子其实是一件很简单的事情。但问题的关键是，语言假对也不是凭空出现的，也需要作者去创造的。如何将一个幽默素材构成一个语言假对，是作者所面临的实实在在的问题。 最简单的方法当然是谐音了，汉字的同音词实在是太多了。你如果用的是搜狗拼音输入法，随便拼出一个词来，下面的选择框里就会有许多的同音词在等着你。利用谐音，几乎任何的词语都可以形成假对。但谐音假对的质量实在是不高，只有初学者或有时间限制的场合，比如主持人主持节目的时候才会用到。 高明一些的手法是曲解，一个词语或一句俗语通常会有一个引申含义，它的字面意思又会是另一个含义，这两个不同含义就构成了一个语言假对。这种假对的质量通常会很高。一个幽默素材如果是一个引申含义，就可以考虑和它字面的意思构成一个假对。如赵本山在小品《相亲》中，女主角说：我们还是等下辈子吧。赵本山说：完了完了，一竿子直到三零零零年去了。下辈子引申含义是不可能了，而作者将它曲解为真的要到下辈子，到三零零零年。 肢体语言形成的假对也是一个好方法，它会和一些动作形成假对，或者说一些动作会和它形成假对。这一点非常有用，如果你的素材乖讹是一个动作，你就可以考虑和一个肢体语言构成一个假对。反之，如果你的素材乖讹是一个肢体语言，你就可以考虑和一个动作构成一个假对。例如赵本山的小品《卖拐三》中，赵本山饰演一个脑血栓患者，两只手摆出了特殊的姿势。范伟说：这是什么呀，非常六加七呀！ 素材乖讹转化成为什么关系对，如要是看这个素材乖讹本身有什么特点，所谓因才施用。如果素材乖讹适合形成一个真对，那就让它形成真对。如果适合形成乖讹假对那就形成乖讹假对。如果素材乖讹在语音方面很有特点，那就适合形成语言假对。 哈尔滨电台有位主持人叫莫红岩，莫是一个很有趣的姓，比如说莫吃，正常人的含义是不要吃。而对于莫姓的人来说，莫吃的意思是让姓莫的人来吃。于是我创作了一个段子。 台长买了一包耗子药，因为怕人误食了，于是在纸包上写了莫吃两个字。红岩看了脸上一红：台长就喜欢给人家开小灶。说着，把耗子药全吃了。 蔡明和潘长江的一个小品里，潘长江饰演的是一个喜欢跳舞的老年人，别人给他打电话，让他去参加一个节目，潘长江忙不迭的说：我去我去我去。蔡明在旁边冷冷的说：我去（拉长音）。拉长音的我去已经不是我去的意思，而是不去的意思，它和真正的我去之间构成了一个假对。 在小品《不差钱》里，小沈阳对毕姥爷说：我的中文名字叫小沈阳。毕姥爷问：你还有英文名字？小沈阳说：我的英文名字是小沈阳（学外国人的语调）。外国人语调的小沈阳是一个乖讹，但它并不是和真正的小沈阳构成假对，而是和英文名字构成假对。 假对的推导方式 假对的最简单的推导方式当然形成误会，我们构制假对的目的也是为了让它形成误会。许多简单的笑话，其实就是一个误会。但误会作为幽默的基本元素，还是可以继续推导的。 1误会在狭义乖讹推导理论中的应用 在狭义乖讹推导理论中我们知道，乖讹点B是一个功能器，B点决定了D点。B点可以做什么，就把D点设置为什么。而误会其实是一个最好的功能器，乖讹点B如果是样子和正常事物A不同，就可以引起D点的误会，从而让D点做出与众不同的表现。 例 老王上卫生间，进去了才发现自己进错了女卫生间。老王赶快往出走，正好碰上了一个要上卫生间的姑娘，姑娘看见了老王脸上一红，竟然扭头进了对面的男卫生间。 这个笑话里，老王进女卫生间是一个乖讹，乖讹点B看起来竟像是老王站在男卫生间的门口，从而引起了D点姑娘的误会，导致姑娘进了男卫生间。 例 一个浑身多处骨折的病人去看医生，医生问他是怎么弄得。病人说：我就用手扶着电线杆子去倒一倒鞋里的沙子，一个傻子就以为我触电了，用木棍疯狂的打我，结果就是这个样子了。 这个笑话里，用手扶着电线杆子不住的晃动，让人误以为是触电了，结果被D点一个傻子好炖打。 例 甲：你叫什么名字。 乙：我叫依洛、洛、洛、洛夫斯基。 甲：你是口吃吗？ 乙：我不是，我爸爸是口吃，户籍登记处的那个工作人员简直是一个白痴。 这个笑话里，爸爸户籍登记时口吃，导致儿子竟然有了一个类似口吃的名字。 这个公式也是可以反推的，如果把主人公换成了D点，即那个上当的人，也可以产生许多好笑话的。 例 在地铁里，奶奶对孙女说：你看现在的人多开放啊，那两个人在地铁里就跳起舞来了。孙女说：奶奶，你看好了，那两个是聋哑人，他们在说哑语呢！ 这个笑话里，两个人说哑语是一个乖讹，看起来好像是跳舞，引起了D点奶奶的误会。 2假对在D点的作用下会有怎样的变化 我们以前一直在讨论B点和D点的相互作用，那么一个假对在面对一个第三者D点的时候会有什么变化呢？我们可以把一个假对中的虚假的那个点设为假点A，而把假对中真实的那个点设为真点B。 当D点和假点A发生作用时得到的结果是E，当D点和真点B发生作用时得到的结果是F。 当看起来D点和假点A发生作用时，由于假点A和真点B是一个假对，事实上是D点在和真点B发生作用，这样得到的结果是，看起来D和A发生作用，实际上是D和B发生作用，看起来D和A发生作用得到的结果是E，而实际上却是D和B发生作用得到的结果是F。D和A发生作用得到的结果E在我们的脑海中是一个正常的脚本，而D和A发生作用得到的结果是F则是一个不正常的脚本，即产生了乖讹。说起来很绕嘴，还是举例说明吧。 例 两只青蛙却生出了一只癞蛤蟆，公青蛙勃然大怒，母青蛙安慰道：孩他爹，在嫁给你之前，我整过容。 在这个笑话里，整过容的癞蛤蟆和青蛙之间是一个假对。假点A是青蛙,真点B是整过容的癞蛤蟆。当遇到一个D点生孩子时，表面上是和假点A发生作用，也就是说青蛙和青蛙之间生孩子，但实际上是和真点B发生作用，也就是说青蛙和癞蛤蟆之间生孩子，得到的结果是生出了一只癞蛤蟆。表面上D和A发生作用，得到的结果却是生出了一只癞蛤蟆，这就是一个乖讹。 两个青蛙生出了一个癞蛤蟆，这是一个乖讹。答案是母青蛙其实是一只癞蛤蟆，也就是说是一只青蛙和一只癞蛤蟆生出了一只癞蛤蟆。 例 病人：医生，我感觉我有两个心脏在跳动。医生：啊，不好意思，我把手表落在你的肚子里了。 这个笑话里，手表和心脏是一个假对，假点A是心脏。真点B是手表。 乖讹是有两个心脏在跳动。手表在肚子里，病人误会它是心脏，于是有了两个心脏的乖讹。 例 孩子刚出世，喊了一声爷爷，爷爷阿的一声死了。喊了一声奶奶，奶奶阿的一声死了。喊了一声爸爸，爸爸阿的一声发现自己没死，叔叔阿的一声死了。 这个笑话里，假点A是表面上看起来是爸爸，真点B是其实不是爸爸。两者之间构成了一个假对。作者虚构了一个因果关系，孩子喊爷爷，爷爷就会死。喊奶奶，奶奶就会死。喊爸爸，爸爸没有死，这里形成了一个乖讹。因为正常现象爸爸因该死，爸爸没有死就是一个乖讹。答案是爸爸其实不是爸爸，叔叔才是爸爸。 例 小姨子把头枕在我的腿上，甜甜的睡着了，让我不忍心去叫醒她。这是妻子突然闯进房间里来了。我的脑袋突然翁了一下，不知道该怎么解释。妻子狠狠的瞪着我，突然说：你怎么还不送我妹妹上幼儿园。 这个笑话里，假点A是姐夫喜欢的小姨子，真点B是小姨子原来是上幼儿园的孩子。这种笑话是可以无限拓展的，比如说可以说小姨子偷偷的亲了我一下，被妻子发现了。小姨子和我在一起睡觉，被妻子发现了。 在D点和假对作用产生的乖讹其实都是虚假的乖讹，并不是真实存在的。如两只青蛙生出癞蛤蟆，其实并不是这样的，实际是一只青蛙和一只癞蛤蟆生出一只癞蛤蟆。如有两个心脏，其实是一只手表和一个心脏。而我们在狭义乖讹推导理论中推导出来的乖讹都是真实的乖讹。比如说由姚明身高的乖讹推导出的姚明在两米深的游泳池里淹不死的乖讹。比如说由煎饼形状的乖讹推出在一个门缝里，只有煎饼可以送进去。比如说在和小姨子偷情的笑话里，和小姨子呆在一起，却可以揭穿妻子的谎言。这些由狭义乖讹推导理论推导出来的乖讹都是真实有效的。而假对推出来的虚假乖讹却只是看起来是一个乖讹，其实是不存在的。假对可以推出虚假乖讹的方法在文学中有着广泛的应用，作者通常会用一个假对作为故事的底，利用它不断的推导出虚假的乖讹，最后再揭露答案。 如一个《开罚单》的故事，交警在给一辆路边违章停靠的车辆开罚单，一个人走了过去对交警说：能不能不开啊？交警没有理他，继续开罚单。这个人生气的说：有种你多开几张！交警真的就多开了几张。这人非常愤怒：有种你把车拖走。交警真的叫来了拖车把车给拖走了。这时这个人说道：这是谁的车这么倒霉，我还是骑我的自行车回家吧。 在这个笑话里，这个人不是车主，却装成是车主，形成了一个假对。作者利用这个假对不断地形成虚假乖讹，最后才揭露谜底。 乖讹-解讹模式 早期人们在研究乖讹论的时候，发现只有一个乖讹是不够的，单单的一个乖讹并不能引起多大的笑果。还需要在乖讹的后面加上一个为什么产生乖讹的原因，即解讹的模式。于是形成了一个笑话的标准模式，即乖讹-解讹模式。我们上面举得例子都属于乖讹-解讹模式，特点是先告诉读者一个乖讹，再告诉读者为什么会产生这个乖讹，即解讹。 真对形成的真实乖讹也可以构成乖讹-解讹模式，但真实乖讹的特点是B点和D点的巧妙作用，它的乖讹是真实存在的，即使不用乖讹-解讹模式，它的效果也会很好。而假对形成的虚假乖讹，如果不用乖讹-解讹的模式，而直接把假对的谜底揭露出来的话，那它形成的虚假乖讹几乎是毫无意义的，所以，乖讹-解讹模式更常用的是虚假乖讹。 3真点B是假点A的否定式 在前文中我们讨论过，乖讹推导理论有一个变式，即乖讹点B不用和环境点D发生作用，乖讹点B直接作用于正常点A。让正常点A成为一个事件的关键点，或一个因果关系中的起因，总之是很重要的部分。然后让B去否定A，从而否定了整个事件。 在假对中依然可以套用这个公式，让真点B成为假点A的否定式，而让假点A成为事件的关键点，或因果关系的起因部分，总之是很重要的部分。让真点B否定假点A从而否定了整个事件。 例 甲：打饭的队伍长吗? 乙：不长，很粗！ 这个笑话里，打饭的人不按队伍排列，而是挤作一团，是一个乖讹事件。因为没有排列，所以不会很长，就和真正的不长构成了一个假对。假点A是虚假的不长，真点B是很粗，人都挤作一团。用假点A去回答问题，不长。用真点B去否定它，很粗。 例 小丽去逛街，看见一家店铺门口写着，进店就有礼赠送。小丽高兴地进去了，希望得到什么礼物。只见服务员冲她微微一笑：小女子这厢有礼了。 这个笑话里，小女子这厢有礼了是一句古代的俗语，在电视剧里经常听到。它和真正的有礼构成了一个语言假对，假点A是有礼赠送，真点B是小女子这厢有礼了。用假点A构成一个事件的关键点，即进店就有礼赠送。用真点B去否定它，原来是小女子这厢有礼了。 例 唐僧一觉醒来，发现孙悟空眼含热泪跪在自己的床前，唐僧惊道：悟空，你这是怎么了？孙悟空哭着说：师傅，求你以后不要在说梦话的时候念紧箍咒了。 这个笑话里，梦话和真正的话构成了一个假对，假点A是念紧箍咒，真点B是在说梦话。 例 一个大款出五百万让我和我的女朋友分手，我真的好紧张，我该怎么办呢？我是应该砍我的左手还是右手呢？ 俗话说男人的手就是他最好的女朋友，于是手和女朋友构成了一个假对，假点A是女朋友，真点B是自己的手。利用假点A构成事件的关键点，即大款出五百万让我和我的女朋友分手。然后用真点B去否定，原来是要砍手。 例 一个水潭里养满了鳄鱼，一个富翁宣布，谁能跳下去游到对岸，我就把我的女儿嫁给他。话音未落，一个小伙子就跳了下去，他手足并用，很快的游到了对岸。富翁很激动，抓住小伙子的手说：我要把我的女儿嫁给你。小伙子甩开他的手冲着对岸说：刚才是谁他妈的把我推下去的？ 这个笑话里，自己跳下去和被人推下去是一个假对。假点A是自己跳下去，真点B是被人推下去。让假点A成为事件的关键点，即谁跳下去就把女儿嫁给他。用真点B去否定它，即原来是被人推下去的。 例如在蔡明和潘长江的小品中，有人问潘长江去不去，潘长江说：我去、我去、我去。蔡明说：我去（拉长音）。我去（拉长音）和我去构成一个假对，假点A是我去，真点B是我去（拉长音）。只要假点A构成事件的关键点，真点B就可以否定它。比如说有人问你晚上喝酒去不去，你说：我去(拉长音）。有人问你打麻将去不去，你说：我去（拉长音）。女朋友问你，晚上看电影去不去，你说：我去（拉长音），估计是一顿好打。 有一篇小小说，名字叫《永远的门》。说的是老李是剧院里的美工，是个老光棍，年纪很大了也没有老婆。他住在单位的宿舍里，住他隔壁的是一位离异的女士，和他年龄相仿。老李一直对这位女士有好感，但一直没有捅破这层窗户纸。后来老李得了癌症去世了，那位女士也搬走了。人们在为他收拾房间的时候，发现屋里有一扇屏风，把屏风撤掉以后，漏出了一扇门，这扇门是正对着女士的房间。人们这才恍然大悟，不禁发出了原来如此的感慨。有人去拉门，却发现没有门把手，原来这是一幅画在墙上的门。人们的心中都起了一种莫名的悲哀。 画在墙上的门和真正的门之间构成了一个假对，假点A是真正的门，真点B是画在墙上的门。利用假点A构成了故事的关键点，即他们偷情的工具。而利用真点B进行否定，把主人公的无奈的心里刻画的淋漓尽致。这个公式的关键是真点B对假点A的否定式，这种假对通常也是需要真点B对假点A的否定，即使是没有直接否定的关系对，也要把两者之间推导成否定式的关系对。 4脚本转换 脚本转换在本文开头已经介绍过，这里还要补充一些内容。 前面讲的几种假对的推导方式都是在一个脚本里进行的，而脚本转换则需要有两个脚本。上文提到的真点B对假点A的否定式，而在脚本转换中，需要的则是假对中假点A和真点B几乎是毫无关系的，没有任何关联的假对。作者需要对假点A和真点B分别构制一个脚本。对假点A构制的脚本叫做虚假脚本，对真点B构制的脚本叫做真实脚本。作者通常会把读者引入虚假脚本中，让读者深信不疑，然后突然地引出真实脚本。 例 一个脑筋急转弯 两个人掉进了陷井里，一个人摔死了，一个人还活着。摔死的人叫张三，活着的人叫什么？答案是：活着的人叫救命。 这个笑话里，叫什么是一个假对，假点A是叫什么名字，真点B是叫救命。作者利用假点A构制了一个虚假脚本让读者上当，读者会陷入到这个脚本当中，认为摔死的人叫张三，活着的人因该叫李四或者是王二麻子。最后作者推出真实脚本，其实不是什么名字，而是应该叫救命。 例 在小品《不差钱》中，毕姥爷问小沈阳：你叫什么名字。小沈阳：我的中文名字叫小沈阳。毕姥爷：你还有英文名字？小沈阳：我的英文名字叫小沈阳（学外国人腔调）。 这个笑话里，外国人腔调的小沈阳和英文名字之间构成了一个假对，假点A是真正的英文名字，真点B是学外国人腔调说出的名字。作者利用假点A构制了一个虚假脚本，让读者上当，读者会认为的英文名字是乔治、迈克、或汤姆之类的英文名。而作者却推出了真实脚本，原来是外国人腔调念出的小沈阳。 也可以先推出真实脚本，然后调侃式的推出虚假脚本。 例 一个女新手司机在等红灯，等了好几个红灯她也没有开过去。交警走过来问：这位女士，还没等到你喜欢的颜色吗？ 真实脚本是等灯，而交警却故意引出另一个脚本，女司机在挑颜色，真实情况当然不是这个样子的，所以说这是一个调侃的虚假脚本。 例 外面工地施工，一块大石头被砰地一声爆破了。唐僧对孙悟空说：悟空，你妈给你生二胎了。 石头爆破和孙悟空出世形成一个假对，作者利用假点A石头爆破构制了一个孙悟空生二胎的虚假脚本。 例 现在这物价涨的，以前拿一块钱上超市，可以买两根肠、三瓶醋、四代盐。现在不行了，按摄像头了。 这是典型的脚本转换，作者先把读者引入一个以前物价便宜的脚本，然后突然引出了盗窃未遂的脚本。 第四章乖讹的其他的推导方式 上面我们论述了素材乖讹的几种推导方式，可以把一个素材乖讹分解成为一个真实相同差异关系对，简称为真对。可以把一个素材乖讹分解成为一个虚假相同差异关系对，简称为假对。然后根据真对或假对的不同性质分别进行推导。然而许多的素材乖讹在笑话里是起着一些的辅助作用，并不会把它们直接分解为真对或假对。还有许多素材乖讹会作用于其他的事物，使其他的事物产生乖讹，形成关系对进行推导。许多的笑话并不止有一次的推导，素材乖讹会形成一个作品乖讹，利用作品乖讹再继续进行推导。对于各种复杂的情况，本章中将会一一的论述。 比如说雾霾是一个乖讹，根据北京雾霾构制的笑话是一筐一筐的，但通常不会把雾霾直接分解成为真对或假对，雾霾起着一个遮挡视线的作用，让人们会看不清楚。从而把许多正常情况下不会成为关系对的变成了关系对。 例 泼水节上，一个男子忽然大喊：他妈的谁用水泼我。众人忙向他解释，泼水是祝福的意思。男子说道：我知道，那他丫的也不能拿开水泼我呀。 泼水节是一个乖讹，但它在笑话里起的作用也是一个掩盖的作用，正常的情况不不可以向人泼水的，但在泼水节上却可以。真正形成的关系对是普通的水和开水。这个笑话如果把开水换成是洗脚水同样是成立的。例如，刚洗完脚，出门去倒洗脚水，突然想起今天是泼水节，直接把水泼到了对门大妈的脸上。 比如说口吃，口吃是一个乖讹，但通常不会把它分解为真对或假对。口吃会让说出的话变形，产生了一个语言的乖讹，这个语言的乖讹会和其他的语言构成一个语言假对，之后再对语言假对进行推导。口吃是一个系列，因为口吃会让说出的每一句话或每一个词语都产生变形，从而生出无数多的语言假对，所以根据口吃构制的段子是层出不穷的。 还有打岔的例子也是同样的道理，打岔是相声中经常用到的手法，听话者耳朵会很背，每句话都听错了，听错的话就会和正常的话形成假对，这样的假对会形成很多，所以打岔也是一个系列。 比如说姚明是一个乖讹，他的乖讹会作用于他身边的许多事物，形成许多新的乖讹。如姚明的衣服是一个乖讹，姚明的鞋子是一个乖讹，他的许多用品都会是乖讹。这些乖讹都可以单拿出来，形成真对或假对，进而形成一个笑话。 乖讹的因果关系 许多的因果关系也会是一个乖讹，而且这种乖讹的用途会很广。乖讹的因果关系直接就可以形成一个真对，利用乖讹点B寻找一个D点，就可以形成一个好点子。 例 一只狼把一群羊给抓住了，准备要吃他们，领头的羊对狼说：只要你能够数出我们一共有多少只，我们就让你把我们吃掉。狼于是开始数羊，一只羊、两只羊、三只羊·······数着数着，狼就睡着了。 数羊能让人睡着，这是一个乖讹的因果关系。利用这个乖讹点，作者构制了一个它可以否定的狼的存在，从而购置了一个笑话。 例 一个外地人来到尚志，坐在了公交车里，正好赶上一个下雨天，他叹气说：我一来尚志就下雨，没有一会不下雨的。一个乘客笑道：那好啊，以后尚志天旱了就找你过来，立马就下雨了。 一来尚志就下雨是一个乖讹的因果关系，其实只是一个巧合。而那位乘客却假设它是一条永远成立的定律，把它作为了乖讹点B，构制了一个乖讹点B可以否定的困难D，即天旱的时候。 例 白雪公主耐不住寂寞，把匹诺曹的头夹在了两腿中间，说道：说实话、说谎话······ 匹诺曹的鼻子说谎话的时候就会变长，这是一个乖讹的因果关系。把它作为一个乖讹点B，它的长短变化还可以实现一个功能，即作为一个jj。 乖讹的因果关系还有一个重要的功能，即成为一个放大器。什么是放大器呢？因果关系的起因是一个小小的变化，导致结果却是一个大大的变化。起因和结果之间有着巨大的差异，导致起因的小变化被结果给放大了，这就是一个放大器。比如说起因是数羊，却导致结果是睡着了。起因是说谎话，导致的结果是鼻子变长了。这有什么作用呢？在狭义乖讹推导理论中，乖讹点B是一个功能器，它能做什么，就会把什么设置为D点。所以需要乖讹点B尽量可以做一些伟大的事情，怎么才能办到呢，就需要这种放大器，B点也许只能做一些小事，通过放大器，却可以将它无限放大。在乖讹推导变式中，乖讹点B直接作用于正常点A，需要A成为事件的关键点，A如何成为关键点呢，也需要这种放大器，将A的小功能无限放大。在假对真点B是假点A的否定式中，要求假点A成为事件的关键点，也需要这种放大器才能够实现。说了放大器的这么多的作用，我们还是举例说明吧。 紧箍咒是一个乖讹的因果关系，说话就可以让一个人头疼，这种放大器把说话的功能给放大了。然后就有了一个唐僧说梦话念紧箍咒让孙悟空头疼的笑话。 比如说刚才数羊的笑话，在数羊会睡着的因果关系中，是利用睡着的乖讹点解决了狼要吃他们的困难。如果从放大器的角度考虑问题，从数羊上设置一个假对，也可以构制一个笑话。例如，地主让王二小放羊，对王二小说：如果丢了一只羊，就拔了他的皮。王二小很是担心，一边放羊，一边数羊，数着数着就睡着了，等他醒来的时候，羊都跑光了。 放大器的作用在计谋中有着很好的应用，计谋就是让人上当的，本来是骗的一件小事，通过放大器的作用，却可以扩大很多倍。计谋用的最多的书当然是三国演义，我举几个三国演义的例子。 望梅止渴 想到梅子就会让人流口水，这是一个乖讹的因果关系。作为放大器使用，只要曹操骗人说前面有梅林，就会得到流口水的效果。 草船借箭 如果晚上攻击敌人，敌人就会放出许多只箭。这是一个乖讹的因果关系。利用这种放大器的作用，只要装作攻击敌人，敌人就会放出很多只箭。 乖讹的因果关系，如果从结果入手，就会形成一个狭义乖讹推导公式。如果从起因入手，就会形成一个放大器。 复杂的乖讹推导 一个笑话通常不止一次的推导，基本上会有两次。而在一个复杂的故事中，甚至存在着多次的推导。 例如泼水节的笑话里，男人让开水给泼了，这已经是一个推导成功的作品乖讹。但还是会对它进行二次的推导，让男人被开水泼了的大喊和不懂泼水节规矩的大喊构成一个假对，然后进行一个脚本转换。 前面举过的姚明的笑话里，利用姚明的身高的素材乖讹推导出的姚明在两米深的游泳池里不会被淹死的作品乖讹。这个作品乖讹还会对它进行推导，让它和不会游泳之间形成一个假对，假点A是不会游泳，真点B是可以在水里行走不会淹死。假点A成为事件的关键点，让人认为你不会游泳当什么救生员，不是无理取闹吗，然后用真点B进行否定。 例 老张的老婆非常关心老张，每天总要打电话问老张中午想要吃什么。每天总是这样，搞得老张有些烦了。一天，老婆又打电话问老张吃什么，老张没好气的说：吃你。中午，老张回到家，发现老婆并没有做饭而是在院子里不停地跑步，跑得满头大汗的，老张奇怪的问：你在做什么？老婆说：我在给你热菜呢！ 这个笑话里，首先是有一个吃你的语言假对，假点A是要吃人，真点B是一句比喻的气话。把真点B曲解成为假点A时，真的吃人变成了一个乖讹。然后又对这个吃人的乖讹进行推导，推出了热菜的乖讹。 在故事的创作中，乖讹的推导更加的复杂，通常作者最初的点子叫做故事核，通过故事核继续推导出新的情节。一个好故事不只需要点子好，更需要写得好，即故事核的推导情节也很精彩。我将会在我的下一篇文章《点子与布局》中，详细论述这些复杂乖讹推导问题，敬请大家期待。 第五章如何得到素材乖讹 从前文的论述中大家可以知道，只要有了一个好的素材乖讹，要想形成一个段子，只不过是一些公式而已。也不是我自夸，像我这种级别的段子手，如果得到一个好的素材乖讹，形成一个段子，那是手到拈来的事情。段子手拼到后来，拼的也不过是素材乖讹的好坏，有了好的素材乖讹，自然可以形成好的段子。那么该如何得到好的素材乖讹呢？下面我们来讨论一下这个问题。 1生活中的乖讹 如果你不喜欢写作，也不喜欢文学，也不喜欢段子创作，你只是想成为一个生活中风趣幽默的人。那好了，我只要告诉你一个小方法，你就可以成为一个幽默的人了。 如果你从哪里得来的一个小段子，想讲给别人听，逗大家哈哈大笑，这基本上是不会成功的。因为我们根本没有大兵讲笑话的水准，而且这根本是没有什么意义的。一个真正好笑的笑话，是需要你和你的小伙伴们处在一个团体中，这时自然的发生了一个乖讹事件或出现了一个乖讹的事物，你把这个自然出现的素材乖讹推导成为一个作品乖讹，这才能引起大家的哄堂大笑。生活中幽默的人，首先是一个善于观察的人，可以观察到这种乖讹的出现，然后迅速的把它转化为作品乖讹。生活中的乖讹比比皆是，只要你细心观察，总会有所收获。 在这里我要提倡一种娱乐精神，许多的乖讹是在某一个人的身上出现的，可能是他的一个缺陷，或者是他的一个失误，作者就会利用这种素材乖讹，推导出作品乖讹逗大家开心。但有些人真的很小气，发现笑话的主角是自己，立刻就翻脸了。这就不好了吗，应该有一点娱乐的精神，作者利用他的缺陷构制段子，固然有一些讽刺的成分在里面，但更多的目的是娱乐大家。 我的名字叫张国强，在小学二年级的时候，就有同学发现我的名字和八仙之一的张果老很像，于是就给我起了一个外号叫张果老。这让我很郁闷，又没有什么办法。终于熬到了初中，我想这个外号可以消失了吧，没想到又有几个我小学同学和我到了一个班里，他们把我的外号也带来了。到了高中依然是这种情况，我的外号依然在沿用着。到了高中毕业，我的外号终于消失了，我还没有高兴几天，一个新的团队又给我起了一些新的外号，而且比原来更狗血。随着年纪的增大，我不再介意别人给我起的外号，没有外号不发家吗！其实这就是一种娱乐精神，你作为一个乖讹的存在，别人利用你创作了一个段子，其实是你和作者共同创作了这个段子，娱乐了大家，没有什么好介意的。 2动物笑话 如果把动物用拟人的方法去写，那么动物就变成了一个乖讹的人。比如说蜈蚣，蜈蚣如果用拟人的手法就变成了一个蜈蚣人，蜈蚣人和正常人的乖讹点是有无数条腿。不只是动物，植物、物体、物品都可以用拟人的手法去写，它们会变成各种乖讹的人。这里最值得推荐的是马季的相声《五官争功》，作者把五官利用拟人的手法，做成眼人、耳人、口人、鼻人。再利用四种乖讹，变化出了大量的段子，整个作品包袱不断，实在是上乘之作。 3文学笑话 利用文学作品中的乖讹构制笑话，也是笑话的一大门类。其中用的最广泛的是《西游记》，根据西游记购置的笑话也是层出不穷。比如说紧箍咒是一个乖讹，乖讹点是说一句话就可以让人头疼。金箍棒是一个乖讹，乖讹点是可以变大变小的棍子。唐僧肉是一个乖讹，一种吃了可以长生不老的人肉。利用武侠故事乖讹构制的笑话也有很多，比如说屠龙刀是一个乖讹，乖讹点是削铁如泥。点穴是一个乖讹，乖讹点是可以让人不动。轻功是一个乖讹，乖讹点是人可以在空中飞。 有人说周星驰的每一部古代题材的影片都是成功的，而现代题材的影片，成功的一半都没有。这是因为古代的人物或事件相对于现代人来说几乎都是乖讹，乖讹点会很多，能构成的段子也就多。如古代人的头发是乖讹，古代人的衣服是乖讹，古代人的交通工具是乖讹。而周星驰的喜剧电影就是靠段子取胜的，现代题材的影片里乖讹很少，形成的段子也就少，所以不会成功。当你看惯了现代电视剧的一些狗血元素，如大款、黑社会、同性恋、艾滋病、白血病。你会奇怪为什么用烂了还要用，不要怪那些编剧了，现代社会就这么些乖讹元素，不用又能怎么办。 4新闻笑话 新闻是现代笑话创作的第一素材乖讹库，段子手需要它，几乎每一个新闻出来以后，就会有大量的段子手聚集，产生出大量的段子。几年之前，哈尔滨电台有一档栏目叫《天天资讯》，主持人以另类的方式播报新闻，真的是很新颖，很奇特。每个新闻其实就是一个素材乖讹，主持人可以用它们推导出段子，但这还是有些难为主持人了。不过电台是一个很好的互动节目，主持人可以和听众互动。主持人刚说一个新闻，听众中的段子手早就构制成段子发给主持人了。过了几年，这种栏目已经是很普遍了。什么脱口秀、搞笑栏目都会用到新闻。这已经是搞笑类栏目的重要方式。 5黄色笑话 弗洛伊德从生理学的角度提出了一种幽默理论，叫做宽慰轮。他说普通的笑话只能让人浅浅的笑，只有和性有关的笑话才可以让人开怀大笑。有一份调查说，百分之六十三的笑话是黄色笑话，也不知道他的数据是怎么得出来的。但可以肯定的是，黄色笑话至少占据笑话的半壁江山。和性有关的那些事，只需简单地推导，就可以形成一个好的笑话。许多黄色笑话电台是不让讲的，但真的不需要特别黄的，只需要和性有关的一些小事，就可以弄出好的笑话。有时候也会让作者很为难，为什么总是要往这方面靠拢呢，但真的没有办法，这是制造幽默的最好手段。 6残疾人笑话 亚里士多德提出了一种幽默理论叫优越论，说的是笑话里总会有残疾人或傻子的存在，人们是因为看到他们做的傻事，产生了自己的优越感，所以会笑。其实残疾人也是一种很重要的素材乖讹门类，任何残疾都是一种乖讹。不过幽默也是有底线的，诸如盲人或没有手脚的严重残疾没有人会拿他们开玩笑的。我们使用的一些残疾乖讹，通常是一些小残疾，开开玩笑无伤大雅的。 7医生笑话 这也是一个很重要的素材乖讹门类，得了各种病的病人，总是会和正常人不一样的，是一种乖讹。医生手术，会把病人的这里或那里切掉，这也是不正常的现象。 素材乖讹还会分很多的门类，这里就不一一的介绍了。 小结 读到这里，我相信许多读者已将本文粗略的看了一遍。但还是感到懵懵懂懂，不知所以然，我也只能告诉大家，多读几遍。 我研究这套理论用了十年时间，如果你瞬间就理解它掌握它，那我只能说你是神而不是人了。 本文总结-简化版写笑话方法 我在这里弄了一个简化版出来，你可以试着从简化版入手，一点一点的掌握乖讹推导的方法。 其实很简单，我们首先要得到一个幽默素材，这个素材必须是一个乖讹，叫做素材乖讹。 然后看素材乖讹与它相对的正常现象之间的差异，这个差异就是乖讹点B。 有了乖讹点，然后看乖讹点可以做什么，也就是说乖讹点是一个功能器。乖讹点可以做什么，可以否定什么，或者可以被什么否定，我们就把什么设定为环境点D。D点和B点共同构成了一个点子。 如果乖讹点B并不能做什么，它只是和正常现象之间样子有些不同，那么它象什么，它就和什么构成一个虚假相同差异关系对，简称假对。然后我们再对假对进行推导。 你可以从简化版入手，试着找到一些乖讹，对它进行推导，渐渐地熟悉了以后，对你的智商是一个很大的提高。我在前面已经说过了，我的乖讹推导理论是对人类创造力的公式化处理。以前只有一些天才能够想出来的点子，普通人只要学会了我的推导方法，也可以轻松的推导出好的点子。 我的终极梦想还是希望用我的理论，让人工智能技术有一个飞跃，让人类可以早日研究出像人一样聪明的的机器人。如果你真正的懂了我的这套理论，我希望你加入进来，这是当今科学最有前途的一个突破点。 转自作者张国强 新浪微博@张扬大 https://zhuanlan.zhihu.com/p/44548925?utm_medium=social&amp;utm_oi=28712972058624&amp;utm_psn=1608463909458997248&amp;utm_source=wechat_session]]></content>
      <categories>
        <category>Comedy</category>
      </categories>
      <tags>
        <tag>Humor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[油管比利]]></title>
    <url>%2F2022%2F11%2F16%2Fyoutube_bilibili%2F</url>

    <encrypted>1</encrypted>

    <content type="text"><![CDATA[. . . 短期任务规划 2022.12.10 学习怎么写搞笑短视频剧本的基本方法论 研究账号”维维啊” 研究账号”睾丸” 研究”万宁叔” 研究”李宗恒” 单人的号也要能打: 琢磨做单人类的搞笑/离谱的方法, 以及适合题材 双人的号互动也要能打: 琢磨做双人类的搞笑/离谱的方法, 以及适合题材 情侣适合的带货方式与适用的货物选择 2022.11.21 这周末(11.27)上线一个双人对话的试水版本 b站和油管怎么能用他网站本身的字幕生成? 学习略微夸张的独立风格的演技来做反应 调研一个自媒体应该怎么运营的问题: 怎么拉流量 怎么逐步突破 变更题材的节奏 … 2022.11.14 这周末(11.20)上线一个试水版本的成品 2022.10.26 我们这周日23:59分之前先做出一个大概的样子, 各种基本元素要齐全, 然后去和他们的对比, 看看还差在哪里(比如一些表现力方面/台词/特效/转场方式的选择等等), 然后进一步学习, 在下周日的23.59分做出一个尽量达到成品级别的 因为因为这周我们主要是学习怎么剪辑, 那就用我们之前一起拍的那个视频和相关的素材来剪辑 每周发布时间安排参考备注: 如果不是质量很一般的流水账视频, 再目前这个形势还是建议放到大号里 大号至少一周两更, 只要不是整活和沙雕日常相邻发布即可, 至少隔一天 这个计划只是个参考, 并不是时间节点完全按照他来办事的, 完美的节奏如表, 我们执行的时候灵活一些 平时遇到可以拍或者可以借鉴啥的, 记得收藏 遇到神评论记得点赞 / 周一 周二 周三 周四 周五 周六 周日 W / 大号:情侣沙雕日常 / / 小号:单人作品/情侣普通日常 / / H / / / 小号:单人作品/情侣普通日常 / / 大号:整活 神评集合 情景 神评 示例链接 带有危险性 我去年就是干这个死的 https://v.douyin.com/hycEMMk/ 一个男子又带老婆又带兄弟玩 一夫一妻制 / 新冠疫情 开放前只是没客户，这开放后连同事都快没了 / 新冠疫情 人：为什么给我干到42℃了[辣眼睛] 免疫系统：刚才病毒骂我是细狗 / 新冠疫情 奥密克戎前期症状：嘴硬 / TODO 有何办法能一次性批量把简体字幕全部改为繁体? 得找找有啥办法能把我变小鲨鱼 如何找到各种高清无码无水印的素材? WIKI 做视频封面, 答疑: https://www.youtube.com/watch?v=Z6tKiEHXBKI https://www.youtube.com/watch?v=emyoZTORv2w 脸部变形, 然后变成静态画面之后, 3d 摇晃的效果: 先用鱼眼效果把脸变形 把图发到剪映里剪辑, 然后用它的”抖音玩法”中的3d运镜等效果制作 双时间线如何同时转场? 答疑: https://discussionschinese.apple.com/thread/253823415?login=true 多条时间线的操作技巧 答疑: https://www.bilibili.com/video/BV1nt4y1Q7Me/?vd_source=8a83b38420b65ac33aa101b7754630f6 https://www.bilibili.com/video/BV1Qa4y1j7ch/?vd_source=8a83b38420b65ac33aa101b7754630f6 自动对话字幕如何做? 答疑: https://www.youtube.com/watch?v=G6AXFkbNEkM 制作流程 平时收集各种素材收藏于b站和抖音快手, 尽量分门别类, 标好名称：内容+序列号 素材连接处理详细步骤如下: 各自把各自 (为什么要各自处理? 以为要尽量保证录制的时候对方没有看过) 收集的素材都放到剪辑软件里把每个素材的精彩部分都截取出来, 然后一个一个的剪辑连接起来, 然后找到相关素材的神评做成字幕放到每个素材里当做提词器, 此时你和我都会各自得到一份连接好的一长段视频 A 和 B 你和我剪辑好后的视频A和B再连接起来成为 C 录制时一台电脑播放视频，一台手机架起来横屏, 拍的设备比如手机本身要静音, 打开勿扰等, 要保证录制时候尽量安静, 开启手机的720P以及30帧的视频模式 录制前，把门窗关好尽量保证静音 一个耳机插在电脑上用来听素材的声音, 一个耳机/麦克风插在手机上用来收音才行 录像的时候最好用手势打好标记”下一个”, 剪辑的时候方便剪掉多余的 剪辑的时候 如果是抖音等竖屏素材, 最后的高斯模糊以及转场应该在剪辑完成之后再统一复制加上(提前加的话, 复制的时候还得花时间去除转场等多余的东西, 费时费力) 最后记得加上各种字幕出现时候的音效, 转场的时候也加个音效 我们纯说话点评的片段是否铺满了全屏, 以及可以用关键帧逐渐将整体画面放大, 效果就不会那么单调 做视频封面: b 站的视频封面不是 1080p 的比例, 而且会截断图上下两边的一部分, 所以相关内容不要放在最上和最下面 做两个版本, 国内发简体, 国外发繁体(youtube批量把简体字幕全部改为繁体) 剪映 字幕和各种字幕气泡都要往上提, 因为可能会被发布后的详情描述和名字 id 等挡住, &lt;&lt;如何控制你的愤怒&gt;&gt;就是反例 它的字幕默认位置偏下, 需要调整 他的封面标题默认位置偏上, 需要调整 每个视频的开头要加上这个视频的梗概配音, 建议用”广西表哥” 标题尽量要吸引眼球, 可以使用”最”字句, 比如”最硬气的求人办事” 使用教人办事的语句, 比如”如何硬气的求人” 使用疑问句, “最硬气的求人是怎样的?” 每个视频的开头要用文字和语音交代作品的标题, 并且相关文字要和封面一致 每日时间安排指引 / 周一 周二 周三 周四 周五 周六 周日 早上 - - - - - - / H学习 睡觉 中午 吃饭+刷抖音 吃饭+刷抖音 吃饭+刷抖音 吃饭+刷抖音 吃饭+刷抖音 吃饭+刷抖音 吃馆子+骑车爬山 18:30 W吃饭 / H剔牙 W吃饭 / - W吃饭 / H剔牙 W吃饭 / H剔牙 W吃饭 / H剔牙 吃饭 吃饭 19:00 W剔牙 / H学习 W剔牙 / - W剔牙 / H学习 W剔牙 / H洗澡 W剔牙 / H学习 剔牙 剔牙 19:30 跳操 W洗澡 / - 跳操 W洗澡 / H学习 跳操 W洗澡 / H学习 W洗澡 / H学习 20:00 W洗澡 / H学习 W学习 / - W洗澡 / H学习 学习 W洗澡 / H学习 W学习 / H洗澡 W学习 / H洗澡 20:30 W学习 / H洗澡 W学习 / - W学习 / H洗澡 学习 W学习 / H洗澡 学习 拍摄 21:00 下载素材和神评论+剪辑拼接 如左 如左 如左 如左 如左 剪辑 里程碑 2022.09.18 从b站看到”好奇伍先生”和”伍话可说”的如何成为一个up主的视频 2022.10.01 观看大量b站/youtube的各类视频, 以及网红分析网站 开始整理适合我们的素材, 考虑过: 类似老高与小茉的都市传说未解之谜类 类似Beast React类的反应类 2022.10.26 选定反应类, 制定短期计划 2022.10.30晚 学习完毕fcp基础教程 制作出第一个demo 2022.11.15晚 制作完毕第一个具备可正式上线质量的视频 2022.11.19晚10点 b 站上面投稿了人生中第一个视频! 2022.11.20凌晨1点 youtube上面投稿了油管第一个视频! 2022.11.24早上9:33 w写出了人生中第一个剧本! 2022.12.03早上00:05 w剪辑了人生中第一个像模像样的短视频! &lt;&lt;让我一次爱个狗&gt;&gt; 2022.12.07早上11:09 收获了第一个真实评论和真实粉丝! 2022.12.13早上12:09 抖音&lt;&lt;新冠和感冒的区别&gt;&gt;收获了第一个超过1万播放! 51个赞! 快手&lt;&lt;新冠和感冒的区别&gt;&gt;收获了第一个真实评论中有人at其他人! 2022.12.24中午14.08 b站&lt;&lt;发烧冒烟了&gt;&gt;破万播放且 99 个赞! 2022.12.27中午15.18 快手/视频号&lt;&lt;你最像什么动物&gt;&gt;破10万播放! 快手 500 个赞!70 个评论!45 个收藏! 视频号 100 个赞! 200 个红心!40 个转发!101 个评论! 2022.12.28 抖音&lt;&lt;他是懂变身的&gt;&gt;破150万播放! 7000+赞! 视频号&lt;&lt;他是懂变身的&gt;&gt;破120万播放! 2000转发+爱心! 2023.1.5上午12.18 抖音&lt;&lt;对浪漫的误解&gt;&gt;破600万播放! 3.8万个赞 2023.1.12上午21:00 w 第一个独立制作的作品&lt;&lt;如何控制你的愤怒&gt;&gt; !]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Youtube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个自动生成网站目录大纲的油猴脚本]]></title>
    <url>%2F2022%2F08%2F25%2Fauto_toc%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[auto-toc auto-tocDisplays a table of contents for any website, making it easier to read and navigate long articles, documentations, and online books. Generate TOC(table of contents) for any website. 可以为任何网站生成TOC网站目录大纲. Switch black/white themes automatically according to your compupter’s light/dark mode. 会根据你的电脑的黑夜白天模式自动切换黑白主题. Features Work properly on any website that conforms to the HTML standard and uses HTML heading tags properly (e.g. Wikipedia.com). Accurate article and heading detection A clean user interface Highlight current heading Click to jump to headings Drag the panel to your preferred position Only runs when you actually use it Dark mode, automatically 6 heading levels Expand all headings to a specific level Support HTML label: H1, H2, H3, H4, H5, H6 &lt;strong&gt; &lt;b&gt; Installation &amp; Usage English: https://greasyfork.org/en/scripts/458022-auto-toc?locale_override=1 中文: https://greasyfork.org/zh-CN/scripts/458022-auto-toc?locale_override=1 By default, it is not open. You need to go to the plug-in menu to open the switch for the website that wants to open the toc. The plug-in will remember this switch, and the toc will be generated automatically according to the switch when you open the website the next time. 可以为任何网站生成TOC网站目录大纲, 默认是不打开的, 需要去插件菜单里为想要打开 toc 的网站开启开关, 插件会记住这个开关, 下回再打开这个网站会自动根据开关来生成 toc 与否. Inspired by https://github.com/FallenMax/smart-toc https://chrome.google.com/webstore/detail/lifgeihcfpkmmlfjbailfpfhbahhibba https://greasyfork.org/en/scripts/415856-bc-smarttoc]]></content>
      <categories>
        <category>GitHub</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tab一下交互式搜一切文件且动态补全任何路径的shell插件]]></title>
    <url>%2F2022%2F08%2F22%2Ftab_to_find%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[tab-to-find tab-to-find一个tab就能交互式动态搜一切 !! 并且支持命令对于任何目录和文件的路径的 tab 动态补全 对于经常要找文件和烦恼输入非常长的路径的人来说极为好用, 不用一个路径一个路径慢慢敲了, 可以一步搜索到位, 即使有几百万的文件, 也非常丝滑, 丝毫不卡 简称: T T F Demo 功能跟平时一样按tab就完事 不输入任何东西, tab 也可直接动态搜索当前目录以及子目录的所有目录和文件, 只是当做一个搜索工具也很好用 tab来补全 可以不一定是头部完全匹配, 比如 输入 doc 然后 tab , 可以匹配 test_doc 也可以匹配 doc_test 也可以匹配 test_doc_test 可以递归匹配当前目录的子目录的所有 doc 的文件/目录, 也就是说你可以在 home 目录输入 cd doc 然后从 home 目录一步直接 cd 到 ~/github/test-proj/documents 里 ! 如果只有一个匹配项, 则自动补全 比如匹配到了 Documents/ , 但如果这不是你想要的, 你想要的是 ~/github/test-proj/documents , 那你可以再按一次tab 如果不只是有一个匹配项, 则会递归搜索子目录下的所有含有 doc 的文件夹 如果 cd 后面跟的不是目录, 则自动进入到那个文件所在的目录, 比如输入 cd ~/github/test.txt 则可以直接进入到 ~/github 这个文件夹里 如果输入以 // 结尾, 则可以只搜索一层目录的文件夹, 而不是递归搜索所有子目录的 如果输入以 .. 结尾, 则可以只搜索一层目录的文件, 而不是递归搜索所有子目录的 同理 vi, ln, mv, cp 等等其他命令也是如此 Usage Press tab to search everything Press tab for completion as usual 安装方法 先安装fd, 按照 fd的安装方法 安装一下就行(一般来说就是一行命令就搞定), 不用额外配置 再安装fzf, 按照 fzf的安装方法 安装一下就行(一般来说就是一行命令就搞定), 不用额外配置 然后再按照下方的手动方式 或者 走Oh-My-Zsh方式的方式来安装 tab-to-find 就可以了 手动方式先克隆一波这个项目 1git clone https://github.com/no5ix/tab-to-find ~/somewhere 然后把下面这行加到你的 ~/.zshrc. 1source ~/somewhere/tab-to-find.plugin.zsh OhMyZsh方式执行下面这行命令 1git clone https://github.com/no5ix/tab-to-find $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/tab-to-find 然后vim ~/.zshrc, 找到七八十行左右 plugins=(git) 的位置 比如原来是 plugins=(git) 则改为 plugins=(git tab-to-find) source ~/.zshrc 或者重启 zsh Installation Install fd by following its installation instruction. Install fzf by following its installation instruction. then install tab-to-find by following Manual or Oh-My-Zsh below Manualclone this repository. 1git clone https://github.com/no5ix/tab-to-find ~/somewhere Then add the following line to your ~/.zshrc. 1source ~/somewhere/tab-to-find.plugin.zsh OhMyZshClone this repository to your custom directory and then add tab-to-find to your plugin list. 1git clone https://github.com/no5ix/tab-to-find $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/tab-to-find]]></content>
      <categories>
        <category>GitHub</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac笔记]]></title>
    <url>%2F2022%2F05%2F07%2Fmac_notes%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[这是一份 mac 折腾配置以及各种改造的精华笔记, 可以帮你从头到尾打造一个极为顺手的 mac. 在Windows端配合sux 可以统一 win &amp; mac 的使用体验. . . . 改键Karabiner改键软件 Karabiner-Elements 的配置得直接覆盖他的配置文件 karabiner.json (它的配置文件路径在 app 里的 misc 里有) 参考: https://github.com/pqrs-org/Karabiner-Elements/issues/2711 https://github.com/pqrs-org/Karabiner-Elements/issues/2949 https://github.com/pqrs-org/KE-complex_modifications/issues/697#issuecomment-678677912 https://github.com/realliyifei/mac-karabiner-chinese-punctuations-to-halfwidth-forms 科学上网科学上网软件： ClashX下载地址: https://itlanyan.com/trojan-clients-download/ 步骤: 去 just my socks 拷贝那些服务节点的配置然后去google搜“ss配置转clash配置”的网站(但是似乎很有可能会泄露相关 ss 密码之类的)，比如 https://subconverter.speedupvpn.com ， 然后在线转换为clash的配置然后点击 ClashX 的菜单栏的图标， 然后 Config-Remote Config-Manage-Add 如果发现上不了网的话, 点击 ClashX 的图标, 然后 Config-Open Config Folder 查看生成的 config 文件是否和 本 github 项目的 clashx里的类似 请不要打开 clashx 的”设置为系统代理”, 否则剪映等一些软件无法联网, 但此时 safari 也会翻不了墙 (以下教程参考 https://www.youtube.com/watch?v=pAY8pNou9Gk) 此时需要先把 safari_proxy 文件夹中的 proxy.pac(这个是由 edge 的 SwitchyOmega插件里的配置生成的) 放到 /Library/WebServer/Documents 里 然后在设置-网络-高级-代理的Automatic proxy configuration 里输入 http://127.0.0.1/proxy.pac, 然后点击 右下角的 ok, 点击完ok之后会退回上一层菜单, 然后再点击 Apply 然后在 terminal 里输入命令 sudo apachectl start 去 safari 的地址栏输入http://127.0.0.1/proxy.pac 测试一下是否能访问这个, 有内容说明成功了, 此时再看看是否能谷歌/油管 此时还有个问题就是:可能会因为其它软件给关掉，如 ClashX 设置为系统代理的时候会把这个 pac 给清除掉, 所以我们需要检查一下 hammerspoon 里的 init.lua是否有 networksetup -setautoproxyurl 相关的代码, 有的话就会自动在激活 safari 的时候自动设置一下 pac 设置(相关代码其实是参考了 https://nowtime.cc/macos/1753.html , networksetup -setautoproxyurl &quot;Wi-Fi&quot; &quot;http://127.0.0.1/proxy.pac&quot; , 这个 “Wi-Fi” 是通过命令 networksetup -listallnetworkservices 拿到的) 触摸板增强软件： betterTouchTool破解版的得把下面这几句加到clashx当前所用的config文件的rules里来屏蔽下面这些地址1234- DOMAIN-KEYWORD,folivora.ai,REJECT- DOMAIN-KEYWORD,www.folivora.ai,REJECT- DOMAIN-KEYWORD,updates.boastr.net,REJECT- DOMAIN-KEYWORD,updates.folivora.ai,REJECT 加完之后去浏览器测试一下是否能打开这几个网址， 打不开则为屏蔽成功，然后再打开betterTouchTool。 jetbrain-crack用jetbrain_crack 文件夹里 的 ja-netfilter, 然后参考下方链接来破解即可, 适用于 jb2022.1 的全家桶 https://learnku.com/articles/67432 https://segmentfault.com/a/1190000041769901 clion使用的时候切记: 要把项目目录以及各种觉得要mark的目录右键Mark directory as一下, 然后再项目根目录Reload Cmake Project 搜狗输入法皮肤自己做了仨, 黑白灰 另外的好东西高仿 mac风格: Rime 鼠须管输入法皮肤效果 https://ssnhd.com/2022/01/11/rime-skin/ Alfred 配置: 把 Alfred.alfredpreferences 直接复制到 Alfred的配置文件夹(即 Advance-Reveal in Finder 出来的文件夹) 里覆盖即可 皮肤: 自己做了俩, 对应黑夜白天 配置中的 workflow 相关网址: https://github.com/zqzten/alfred-web-search-suggest https://github.com/iamstevendao/alfred-open-with-vscode https://github.com/LeEnno/alfred-terminalfinder https://github.com/wensonsmith/YoudaoTranslator hammerspoon装好 hammerspoon 之后, 把~下的.hammerspoon文件夹里的删除, 然后将本项目中的 hammerspoon 文件夹的内容放到 ~下的.hammerspoon文件夹里 禁止cleanmymacx即使退出它的HealthMonitor还一直后台运行问题：虽然CleanMyMacX软件被吹嘘的很厉害，在用起来感觉也可以帮助更好的清理电脑，但是它一直后台运行，还终止不了，一直监控Mac的信息情况，个人觉得很是鸡肋，在使用的时候打开用就行了，平常没必要一直常驻，对于内存紧张的朋友来说太吃内存了。 经过试验，可以修改权限解决，不需要删除文件，也不会出现system.log中的异常日志。 解决办法： 打开终端输入以下 chmod 400 &quot;/Applications/CleanMyMac X.app/Contents/Library/LoginItems/CleanMyMac X Menu.app/Contents/Library/LoginItems/CleanMyMac X HealthMonitor.app/Contents/MacOS/CleanMyMac X HealthMonitor&quot; chmod 400 &quot;/Applications/CleanMyMac X.app/Contents/Library/LoginItems/CleanMyMac X Menu.app/Contents/Library/LaunchServices/com.macpaw.CleanMyMac4.Agent&quot; chmod 400 &quot;/Applications/CleanMyMac X.app/Contents/Library/LaunchServices/com.macpaw.CleanMyMac4.Agent&quot; 然后去活动监视器终止CleanMyMacX所有进程即可。 启动台行列【终端】输入或粘贴以下命令，修改后面的数字更改行列数。 行数 defaults write com.apple.dock springboard-rows -int 9 列数 defaults write com.apple.dock springboard-columns -int 10 最后输入以下命令 killall Dock 程序坞降低显示延迟如果你习惯隐藏程序坞，鼠标放在屏幕底部，默认显示程序坞非常慢，你可以在【终端】输入或粘贴下面命令，将数值改为 0，这样，显示程序坞会变的很快。 defaults write com.apple.dock autohide-delay -int 0 最后输入以下命令 killall Dock 允许任何来源安装非 App Store 里应用，可能会出现无法安装的情况，需要打开任何来源。【终端】输入或粘贴以下命令，按回车键。 sudo spctl --master-disable 密码位数管理员默认密码至少为 4 位，【终端】输入或粘贴以下命令支持将密码改为 1 位。 pwpolicy -clearaccountpolicies 深色主题显示浅色窗口程序坞和菜单栏深色，窗口是浅色。 【终端】输入或粘贴以下命令，按回车键, 注销并重新登录 Mac, 系统偏好设置 - 通用 - 深色。defaults write -g NSRequiresAquaSystemAppearance -bool Yes 恢复原样: defaults delete -g NSRequiresAquaSystemAppearance Mac 删除原生 ABC 英文输入法删除系统英文 ABC，只保留一个输入法，这样搜狗输入法只需按 Shift 即可切换中英文。 (这一步可以不做, 因为 sip 关闭之后 iOS 程序在 mac 就不能运行了, 所以先不做等发现删不掉 abc再来做吧, 关闭 sip 之后删除 abc 输入法再打开sip也可以 )先关闭 系统完整性保护SIP, 参考 系统完整性保护SIP 前往自己home 目录下的 ~/Library/Preferences/ 文件夹，找到 com.apple.HIToolbox.plist 文件, 先备份 cp com.apple.HIToolbox.plist com.apple.HIToolbox.plist.bak 删除 AppleEnabledInputSources 下的 12345678&lt;dict&gt; &lt;key&gt;InputSourceKind&lt;/key&gt; &lt;string&gt;Keyboard Layout&lt;/string&gt; &lt;key&gt;KeyboardLayout ID&lt;/key&gt; &lt;integer&gt;252&lt;/integer&gt; &lt;key&gt;KeyboardLayout Name&lt;/key&gt; &lt;string&gt;ABC&lt;/string&gt;&lt;/dict&gt; 然后保存 重启 Mac。 恢复原样: 不必担心此操作给电脑带来异常，一切正常。还原打开系统偏好设置 → 键盘 → 输入法 → 添加 ABC 即可。 terminal相关autojump安装和tab不自动补全的办法安装: brew install autojump安装完之后在 .zshrc 最后加上一行:[ -f /opt/homebrew/etc/profile.d/autojump.sh ] &amp;&amp; . /opt/homebrew/etc/profile.d/autojump.sh tab不自动补全的办法: 在 .zshrc 最后加上一行 autoload -U compinit &amp;&amp; compinit 安装zsh-autosuggestions安装: brew install zsh-autosuggestions安装完之后在 .zshrc 最后加上一行:source /opt/homebrew/share/zsh-autosuggestions/zsh-autosuggestions.zsh 命令行语法高亮如果已经装好了 oh my zsh则按照下面的方法来安装: Clone this repository in oh-my-zsh’s plugins directory: git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting Activate the plugin in ~/.zshrc: plugins=( [plugins...] zsh-syntax-highlighting) 比如原来是 plugins=(git) 则改为 plugins=(git zsh-syntax-highlighting) Restart zsh (such as by opening a new instance of your terminal emulator). powerlevel10k主题的路径颜色改色改成紫色原因: 不改色的话, 默认是蓝色太亮看不清又伤眼 vim ~/.p10k.zsh 找到 POWERLEVEL9K_DIR_BACKGROUND 然后改成 typeset -g POWERLEVEL9K_DIR_BACKGROUND=128 保存 source ~/.p10k.zsh fzf安装与配置 确认已经安装好了 fd 用 brew 安装: brew install fzf, 安装完了之后他会提示你安装他的快捷键绑定的命令, 复制他的这个命令安装好他的快捷键绑定 vim ~/.zshrc 如果只是希望 fzf 每次搜索指搜当前目录下的文件和目录则在最后添加 123export FZF_DEFAULT_COMMAND=&apos;fd -HI --exclude &quot;.git&quot;&apos;122 export FZF_CTRL_T_COMMAND=&apos;fd -HI --exclude &quot;.git&quot;&apos;123 export FZF_ALT_C_COMMAND=&quot;fd -HI --exclude &quot;.git&quot; -t d . &quot; 如果希望 fzf 每次搜索指搜所有的文件和目录则在最后添加 123export FZF_DEFAULT_COMMAND=&apos;fd -HI --exclude &quot;.git&quot; . / &apos;export FZF_CTRL_T_COMMAND=&apos;fd -HI --exclude &quot;.git&quot; . / &apos;export FZF_ALT_C_COMMAND=&apos;fd -HI --exclude &quot;.git&quot; -t d . &apos; source ~/.zshrc 用 fd 找到 fzf 的 key-bindings.zsh (上次安装的时候是在/opt/homebrew/Cellar/fzf/0.32.1/shell/key-bindings.zsh), 然后在 68 行左右添加 bindkey &#39;^F&#39; fzf-file-widget 让他生效: source /opt/homebrew/Cellar/fzf/0.32.1/shell/key-bindings.zsh 现在可以先输入 cd 然后 敲击 ctrl+f 找到想要进入的目录然后回车进入目录了 也可以先输入 vim 然后 敲击 ctrl+f 找到想要进入的文件然后回车进入编辑了 tab-any-path安装与配置tab-any-path safari 相关Safari调整标签页在哪儿打开的设置terminal 输入命令: defaults write com.apple.Safari IncludeInternalDebugMenu 1 重启 Safari 之后可以看到顶部菜单栏多出一个 Debug 菜单里有 Tab Ordering 的子菜单项 safari和edge书签双向同步 在 windows 的 store 里下载iCloud应用并登录自己的Apple ID 新建TXT文件，填入内容如下： 1234Windows Registry Editor Version 5.00[HKEY_LOCAL_MACHINE\SOFTWARE\WOW6432Node\Microsoft\Windows\CurrentVersion\App Paths\chrome.exe]@=&quot;C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe&quot;&quot;Path&quot;=&quot;C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\&quot; 修改txt文件名为.reg格式，双击导入注册表 打开桌面版iCloud应用，勾选书签-&gt;Chrome, 在Edge浏览器内安装 Apple 出品的“iCloud书签”插件 在iCloud应用中打开书签同步选项, 记得点击“应用”！如果说要合并书签那就合并 如果电脑上已经装了有chrome可能会弹出来要你安装iCloud插件, 不用管, 此时因为已经点击了”应用”了, 相关设置已经保存好, 直接重启一下电脑，会自动完成同步 safari实现类似edge的smart-toc自动生成目录大纲的插件先装上 Macaque 插件(类似油猴插件, 只是这个插件更加强大可以同时支持油猴脚本和 UserScript 脚本, 可以绕过网站 CSP, 把脚本注入到开了 CSP 的网站里, 如知乎/GitHub 等), 然后去这里 装上 auto-toc 油猴脚本, 然后刷新页面即可 参考: Edge的smart-toc插件开源地址 系统完整性保护SIP注意: sip 关闭之后 iOS 程序在 mac 就不能运行了 查看 SIP 状态: 【终端】输入 csrutil status，按回车键。 ARM M1 处理器关闭 SIP 步骤: 关机 按住开机键不松手直到出现下图的画面，然后点击选项 点击继续 点击菜单栏的实用工具，再点击终端 输入csrutil disable，然后按下回车也就是 return 键 输入y，然后按下回车也就是 return 键 输入您的电脑密码，然后按下回车也就是 return 键 等待执行结果…… 出现 System Integrity Protection is off. 证明 SIP 已成功关闭。 输入 reboot 然后按下回车也就是 return 键重启电脑即可。 如果后期想再开启 SIP，只需要将上面第 5 步的 csrutil disable 换成 csrutil enable 即可。 开启 SIP: 同上，然后【终端】输入下面命令 csrutil enable 各个软件单独控制音量可以看看开源项目 background-music , 不过经测试目前(2022.08.10)还不支持最新系统 Monterey 12.5, 可以 github 搜一下它看看最新版本他支持了没 lua 相关luajit在MacOS编译与安装 openresty-luajit 的 github官网下载他的 release, 然后解压直接 make, 如果报错说要设置 export MACOSX_DEPLOYMENT_TARGET=xx.yy 的话, 那就设置一下, 比如我的 macOs 系统是 12.5, 那就 export MACOSX_DEPLOYMENT_TARGET=12.5, 然后再 make, 再 make install (make install 很重要, 不然之后的东西能编译过, 但是会提示找不到 dyld[56827]: Library not loaded: &#39;/usr/local/lib/libluajit-5.1.2.dylib&#39;) (不建议安装了)luajit-openresty安装后设置成默认lua先安装: brew install luajit-openresty然后: cd /opt/homebrew/opt/luajit-openresty/bin ln -s luajit-blabla lua (这里的 luajit-blabla 替换成当前文件夹的那个 luajit 执行文件的名字) echo &#39;export PATH=&quot;/opt/homebrew/opt/luajit-openresty/bin:$PATH&quot;&#39; &gt;&gt; ~/.zshrc sourch ~/.zshrc]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Actor模型 vs. CSP模型]]></title>
    <url>%2F2021%2F07%2F21%2Factor_vs_csp_mode%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Akka/Erlang 的 actor 模型与 Go 语言的协程 Goroutine 与通道 Channel 代表的 CSP(Communicating Sequential Processes) 模型有什么区别呢？ 首先这两者都是并发模型的解决方案，我们看看 Actor 和 Channel 这两个方案的不同： Actor 模型在 Actor 模型中，主角是 Actor，类似一种 worker，Actor 彼此之间直接发送消息，不需要经过什么中介，消息是异步发送和处理的： Actor 模型描述了一组为了避免并发编程的常见问题的公理: 所有 Actor 状态是 Actor 本地的，外部无法访问。 Actor 必须只有通过消息传递进行通信。 一个 Actor 可以响应消息: 推出新 Actor, 改变其内部状态, 或将消息发送到一个或多个其他参与者。 Actor 可能会堵塞自己, 但 Actor 不应该堵塞它运行的线程。 更多可见 Actor 模型专题 . . . Channel 模型 Channel 模型中，worker 之间不直接彼此联系，而是通过不同 channel 进行消息发布和侦听。消息的发送者和接收者之间通过 Channel 松耦合，发送者不知道自己消息被哪个接收者消费了，接收者也不知道是哪个发送者发送的消息。 Go 语言的 CSP 模型是由协程 Goroutine 与通道 Channel 实现： Go 协程 goroutine: 是一种轻量线程，它不是操作系统的线程，而是将一个操作系统线程分段使用，通过调度器实现协作式调度。是一种绿色线程，微线程，它与 Coroutine 协程也有区别，能够在发现堵塞后启动新的微线程。 通道 channel: 类似 Unix 的 Pipe，用于协程之间通讯和同步。协程之间虽然解耦，但是它们和 Channel 有着耦合。 Actor 模型和 CSP 区别Actor 模型和 CSP 区别图如下： Actor 之间直接通讯，而 CSP 是通过 Channel 通讯，在耦合度上两者是有区别的，后者更加松耦合。 同时，它们都是描述独立的流程通过消息传递进行通信。主要的区别在于：在 CSP 消息交换是同步的 (即两个流程的执行 “接触点” 的，在此他们交换消息)，而 Actor 模型是完全解耦的，可以在任意的时间将消息发送给任何未经证实的接受者。由于 Actor 享有更大的相互独立, 因为他可以根据自己的状态选择处理哪个传入消息。自主性更大些。 在 Go 语言中为了不堵塞流程，程序员必须检查不同的传入消息，以便预见确保正确的顺序。CSP 好处是 Channel 不需要缓冲消息，而 Actor 理论上需要一个无限大小的邮箱作为消息缓冲。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Misc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cpp的各个编译选项备忘整理]]></title>
    <url>%2F2021%2F06%2F11%2Fcpp_compile_options%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[编译器优化级别用 C/C++ 的肯定都知道编译器编译有各种优化级别，编译器优化级别大体如下： O0（默认选项）：不开启优化，方便功能调试 Og：方便调试的优化选项（比 O1 更保守） O1：保守的优化选项，打开了四十多个优化选项 O2：常用的发布优化选项，在 O1 的基础上额外打开了四十多个优化选项，包括自动内联等规则 Os：产生较小代码体积的优化选项（比 O2 更保守） O3：较为激进的优化选项（对错误编码容忍度最低），在 O2 的基础上额外打开了十多个优化选项 Ofast：打开可导致不符合 IEEE 浮点数等标准的性能优化选项。 具体介绍如下： . . . O0：编译器默认就是 O0，该选项下不会开启优化，方便开发者调试。 O1：致力于在不需要过多的编译时间情况下，尽量减少代码大小和尽量提高程序运行速度，它开启了下面的优化标志： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546-fauto-inc-dec -fbranch-count-reg -fcombine-stack-adjustments -fcompare-elim -fcprop-registers -fdce -fdefer-pop -fdelayed-branch -fdse -fforward-propagate -fguess-branch-probability -fif-conversion -fif-conversion2 -finline-functions-called-once -fipa-modref -fipa-profile -fipa-pure-const -fipa-reference -fipa-reference-addressable -fmerge-constants -fmove-loop-invariants -fomit-frame-pointer -freorder-blocks -fshrink-wrap -fshrink-wrap-separate -fsplit-wide-types -fssa-backprop -fssa-phiopt -ftree-bit-ccp -ftree-ccp -ftree-ch -ftree-coalesce-vars -ftree-copy-prop -ftree-dce -ftree-dominator-opts -ftree-dse -ftree-forwprop -ftree-fre -ftree-phiprop -ftree-pta -ftree-scev-cprop -ftree-sink -ftree-slsr -ftree-sra -ftree-ter -funit-at-a-time Og：如果是为了调试，该选项是比 O0 更好的选择，它会打开 O1 大部分优化标志，但是不会启用那些影响调试的标志： 12345-fbranch-count-reg -fdelayed-branch -fdse -fif-conversion -fif-conversion2 -finline-functions-called-once -fmove-loop-invariants -fssa-phiopt -ftree-bit-ccp -ftree-dse -ftree-pta -ftree-sra O2：常见的 Release 级别，该选项下几乎执行了所有支持的优化选项，它增加了编译时间，提高了程序的运行速度，又额外打开了以下优化标志： 1234567891011121314151617181920212223242526272829303132333435-falign-functions -falign-jumps -falign-labels -falign-loops -fcaller-saves -fcode-hoisting -fcrossjumping -fcse-follow-jumps -fcse-skip-blocks -fdelete-null-pointer-checks -fdevirtualize -fdevirtualize-speculatively -fexpensive-optimizations -ffinite-loops -fgcse -fgcse-lm -fhoist-adjacent-loads -finline-functions -finline-small-functions -findirect-inlining -fipa-bit-cp -fipa-cp -fipa-icf -fipa-ra -fipa-sra -fipa-vrp -fisolate-erroneous-paths-dereference -flra-remat -foptimize-sibling-calls -foptimize-strlen -fpartial-inlining -fpeephole2 -freorder-blocks-algorithm=stc -freorder-blocks-and-partition -freorder-functions -frerun-cse-after-loop -fschedule-insns -fschedule-insns2 -fsched-interblock -fsched-spec -fstore-merging -fstrict-aliasing -fthread-jumps -ftree-builtin-call-dce -ftree-pre -ftree-switch-conversion -ftree-tail-merge -ftree-vrp Os：打开了几乎所有的 O2 优化标志，除了那些经常会增加代码大小的优化标志： 123-falign-functions -falign-jumps -falign-labels -falign-loops -fprefetch-loop-arrays -freorder-blocks-algorithm=stc 它还启用了 - finline-functions 优化标志，使编译器根据代码大小而不是程序运行速度进行优化，为了减少代码大小。 O3：在 O2 的基础上又打开了以下优化标志： 12345678910111213141516-fgcse-after-reload -fipa-cp-clone-floop-interchange -floop-unroll-and-jam -fpeel-loops -fpredictive-commoning -fsplit-loops -fsplit-paths -ftree-loop-distribution -ftree-loop-vectorize -ftree-partial-pre -ftree-slp-vectorize -funswitch-loops -fvect-cost-model -fvect-cost-model=dynamic -fversion-loops-for-strides Ofast：更加激进的编译选项，它不会严格遵循标准，在 O3 的优化基础上，它又开启了一些可能导致不符合 IEEE 浮点数等标准的性能优化选项，如 - fast-math， -fallow-store-data-races 等。 tips：上述优化选项如果想要了解具体含义可以看 https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html 官方文档。 debug和release区别编译器有这么多优化级别，Debug 版本和 Release 版本其实就是优化级别的区别，Debug 称为调试版本，编译的结果通常包含有调试信息，没有做任何优化，方便开发人员进行调试，Release 称为发布版本，不会携带调试信息，同时编译器对代码进行了很多优化，使代码更小，速度更快，发布给用户使用，给用户使用以更好的体验。但 Release 模式编译比 Debug 模式花的时间也会更多。 Debug 模式下在内存分配上有所区别，在我们申请内存时，Debug 模式会多申请一部分空间，分布在内存块的前后，用于存放调试信息。 对于未初始化的变量，Debug 模式下会默认对其进行初始化，而 Release 模式则不会，所以就有个常见的问题，局部变量未初始化时，Debug 模式和 Release 模式表现有所不同。 123456789bool func() &#123; bool found; for (int i = 0; i &lt; vec.size(); ++i) &#123; if (vec[i] == 3) &#123; found = true; &#125; &#125; return found; &#125; Debug 模式下可能运行正常，但 Release 模式下可能会返回错误结果，因为 found 局部变量在 Release 模式下没有初始化。 Debug 模式以 32 字节为单位分配内存，例如当申请 24 字节内存时，Release 模式下是正常的分配 24 字节，Debug 模式会分配 32 字节，多了 8 字节，所以有些数组越界问题在 Debug 模式下可以安全运行，Release 模式下就会出问题。 assert与NDEBUG宏Debug 模式下可以使用 assert，运行过程中有异常现象会及时 crash，Release 模式下模式下不会编译 assert，遇到不期望的情况不会及时 crash，稀里糊涂继续运行，到后期可能会产生奇奇怪怪的错误，不易调试，殊不知其实在很早之前就出现了问题。编译器在 Debug 模式下定义 _DEBUG 宏，Release 模式下定义 NDEBUG 宏，预处理器就是根据对应宏来判断是否开启 assert 的。 数据溢出问题，在一个函数中，存在某些从未被使用的变量，且函数内存在数据溢出问题，在 Debug 模式下可能不会产生问题，因为不会对该变量进行优化，它在栈空间中还是占有几个字节，但是 Release 模式下可能会出问题，Release 模式下可能会优化掉此变量，栈空间相应变小，数据溢出就会导致栈内存损坏，有可能会产生奇奇怪怪的错误。例如： 12345void func() &#123; char buffer[10]; int counter; lstrcpy(buffer, &quot;abcdefghik&quot;); // 需要拷贝11字节&#125; tips：不要将 Debug 库和 Release 库混合在一起，可能会出问题，至于为啥会出问题，我也不知道，一般都是有两个目录，Debug 目录和 Release 目录。 回到那两个问题： 有时候程序在 Debug 模式下运行的好好的，Release 模式下就 crash 了，怎么办？ 看一下代码中是否有未初始化的变量，是否有数组越界问题，从这个思路入手。 有些时候程序在 Debug 模式下会崩溃，Release 模式下却正常运行，怎么办？ 可以尝试着找一找代码中的 assert，看一下是否是 assert 导致的两种模式下的差异，从这个思路入手。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-etcd与raft要点]]></title>
    <url>%2F2021%2F05%2F17%2Fself_cultivation_etcd%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[etcd etcd 参考 https://wingsxdu.com/post/database/etcd/#gsc.tab=0 raft 参考 https://www.jianshu.com/p/5aed73b288f7 重点参考 https://segmentfault.com/a/1190000022248118 etcd 是一个 Go 语言编写的分布式、高可用的强一致性键值存储系统，用于提供可靠的分布式键值(key-value)存储、配置共享和服务发现等功能。 etcd可以用于存储关键数据和实现分布式调度，它在现代化的集群运行中能够起到关键性的作用。 Raft用于保证分布式数据的一致性。动画演示Raft Raft选主过程 动画演示Raft选主前提知识: Election timeout选举周期: The election timeout is the amount of time a follower waits until becoming a candidate. heartbeat timeout心跳时间间隔 . . . 选主的具体流程如下: 假设三个节点的集群，三个节点上均运行 一个随机选举周期timer（每个 Timer 持续时间是随机的, 一般是150~300ms），Raft算法使用随机 Timer 来初始化 Leader 选举流程，第一个节点率先完成了 Timer，那它就要变成candidate，然后带着自身的数据版本信息发起vote 随后它就会向其他两个节点发送成为 Leader 的请求，其他follower节点接收到请求后会以投票回应然后第一个节点是否被选举为 Leader。 在每一任期内，最多允许一个节点被选举为leader 投票后就会重置一下选举周期timer, 重新计时 检查是否candidate的数据版本比自己要新, 如果比自己旧, 那就会无情拒绝, 如果有都变成了candidate的多个节点，follower们采取哪个candidate先来先投票的策略。 在一个任期内，一个节点只能投一票 如果超过半数的follower都认为他是合适做领导的，那么恭喜，新的leader产生了. 成为 Leader 后，该节点会以固定心跳时间间隔heartbeat timeout向其他节点发送通知确保自己仍是Leader，follower收到了心跳则会重置一下选举周期timer, 重新计时。 有些情况下当 Follower 们收不到 Leader 的通知后，比如说 Leader 节点宕机或者失去了连接，则其他节点当选举周期timer到期后就会会重复之前选举过程选举出新的 Leader。 如果多个candidate同时发起了选举: 只有follower能投票且只能投一次(投了a就不能投b了), candidate B 是不能给candidate A 投票的 如果其中一个candidate拿到了超过半数的选票也可以成为leader 如果所有candidate都没有获得大多数选票时(很可能发生这种情况, 因为在一个任期内，一个节点只能投一票, 投了A就不能再投B了)，则所有节点还是还是继续走一个随机选举周期timer的选举流程, 等待下一次某个节点的选举周期timer触发则term加1进行下一任期的选举 数据读写流程如何保证一致性的 Raft的log entry的复制流程动画 etcd写请求流程写请求的过程简要总结: 在etcd-raft实现中，所有的写请求都会由leader执行并将请求日志同步到follower节点，且若follower节点收到客户端的写请求，则是把写请求转发给leader。 待该 写请求 日志被复制到集群半数以上的节点时，该 写请求 日志会被 Leader 节点确认为己提交，Leader 会回复客户端写请求操作成功 为什么说即使完成了一次写请求流程也有可能读到过期数据? etcd又是如何解决此问题的?(见此etcd线性一致性读) 前提知识: consensus共识在实现机制上属于复制状态机(Replicated State Machine)的范畴，复制状态机是一种很有效的容错技术，基于复制日志来实现，每个 Server 存储着一份包含命令序列的日志文件，状态机会按顺序执行这些命令。因为日志中的命令和顺序都相同，因此所有节点会得到相同的数据。因此保证系统一致性就简化为保证操作日志的一致，这种复制日志的方式被大量运用，如 GSF、HDFS、ZooKeeper和 etcd 都是这种机制。 raft中的日志(log entry)并不是系统Debug日志，而是序列化后的command，这些Command复制到各个节点后，通过序列化内容的解析出命令后，在各个节点上执行并返回操作结果从而实现复制状态机 原因: etcd 就完成了一次写操作仅仅代表写请求日志操作成功完毕了，写操作成功仅仅意味着日志达成了一致（已经落盘）,并不意味着这条日志被应用到状态机(状态机 apply 日志的行为在大多数 Raft 算法的实际实现中都是异步的, raft不管具体状态机如何实现, 他只规定了日志的复制流程算法标准)，而并不能确保当前状态机也已经 apply 了日志。所以此时读取状态机并不能准确反应数据的状态，很可能会读到过期数据。 Leader 应用了这条日志也不意味着所有的 Follower 都应用了这条日志，每个节点会独立地决定应用日志的时机，这中间可能存在着一定的延迟。虽然 etcd 应用日志的过程是异步的，但是这种批处理策略能够一次批量写入多条日志，可以提升节点的 I/O 性能。 etcd 有额外的机制解决这个问题。这部分内容会在下文的etcd线性一致性读介绍到。 etcd线性一致性读etcd 两种方案来保证线性一致性读: ReadIndex 方案 ReadIndex是etcd-raft的默认方案。 虽然状态机应用日志的行为是异步的，但是已经提交的日志都满足线性一致，那么只要等待这些日志都应用到状态机中再执行查询，读请求也可以满足线性一致。 ReadIndex机制的执行流程如下： 从 leader节点读: 1. 读操作执行前记录『此时』集群的 CommittedIndex(commitIndex即为log中最后一个被提交的index值. 因为此时自己就是leader, 所以从 leader 获取到的 commited index 就作为此次读请求的 ReadIndex)，记为ReadIndex； 2. 向 Follower 发送心跳消息，如果超过法定人数的节点响应了心跳消息，那么就能保证 当前Leader 节点还是leader, 主要是防止本leader是已经隔离的了小集群里的leader，这样就确保了 Leader 节点的数据都是最新的 3. 等待状态机『至少』应用到ReadIndex，即 AppliedIndex &gt;= ReadIndex； 4. 执行读请求，将结果返回给 Client。 从 follower 节点读: Follower 先向 Leader 询问 readIndex，Leader 收到 Follower 的请求后依然要通过 上述的第2步骤广播心跳确认自己 Leader 的身份，然后返回当前的 commitIndex 作为 readIndex，Follower 拿到 readIndex 后，等待本地的 applyIndex 大于等于 readIndex 后，即可读取状态机中的数据返回。 LeaseRead 方案 etcd-raft不推荐采用此方案 基本的思路是 Leader 取一个比 Election Timeout选举周期 小的租期，在租期不会发生选举，确保 Leader 不会变，所以可以跳过 ReadIndex 的第二步，也就降低了延时。 LeaseRead 与 ReadIndex 类似，但更进一步，不仅省去了 Log，还省去了网络交互。它可以大幅提升读的吞吐也能显著降低延时。 缺陷: LeaseRead 的正确性和时间挂钩，因此时间的实现至关重要，如果漂移严重，这套机制就会有问题。LeaseRead 的正确性和时间的实现挂钩，由于不同主机的 CPU 时钟有误差，所以也有可能读取到过期的数据。 etcd存在脑裂情况吗etcd不存在脑裂情况. 众所周知 etcd 使用 Raft 协议来解决数据一致性问题。一个 Raft Group 只能有一个 Leader 存在，如果一旦发生网络分区，Leader 只会在多数派一边被选举出来，而少数派则全部处于 Follower 或 Candidate 状态，所以一个长期运行的集群是不存在脑裂问题的。etcd 官方文档也明确了这一点： The majority side becomes the available cluster and the minority side is unavailable; there is no “split-brain” in etcd. 但是有一种特殊情况，假如旧的 Leader 和集群其他节点出现了网络分区，其他节点选出了新的 Leader，但是旧 Leader 并没有感知到新的 Leader，那么此时集群可能会出现一个短暂的「双 Leader」状态。这种情况并不能称之为脑裂，原因如下： etcd网络分区时 如果leader在少数派 此时多数派会有follower选举周期timer触发则任期term增加, 并且会选出多数派新leader, 此时少数派leader会检查法定人数是否大于节点数量一半, 检查确认后则少数派集群进入是不可用状态，不支持raft请求，只支持非一致性读请求。一旦网络分区清除，少数派因为任期term较小, 则这边会自动承认来自多数这边的 leader 并同步多数派的数据状态。 如果在leader在多数派, 则一切照旧 网络分区时 etcd 也有 ReadIndex、LeaseRead 机制来解决这种状态下的数据一致性问题 新Leader会无条件提交旧Leader日志吗其实这里有 4 种情况： Leader 复制给少数节点，然后宕机 Leader 复制给多数节点，然后宕机 Leader 复制给多数节点，本地提交成功，返回客户端成功，然后宕机 场景 1-2 压根没有给客户端承诺，所以是新 Leader 不会立即 commit 前任 Leader 的日志；场景 3 承诺了客户端，无论如何日志是不允许丢的，所以新 Leader 一定会 commit 日志。 etcd可以偶数个部署吗可以, 但非常不建议. 偶数个节点的集群非但不能提升容错能力，反而会带来资源的浪费并可能使选举的时间变长。同时在奇数个集群的情况下，即使产生网络分区也能保证始终有一方占据大多数的节点，进而选举出新的 Leader 来保证集群的可用。而偶数个节点则可能会出现对半分的场景，这样任意一方都无法选举出 Leader，导致集群的不可用。 etcd架构及解析 从 etcd 的架构图中我们可以看到，etcd 主要分为四个部分。 HTTP Server： 用于处理用户发送的 API 请求以及其它 etcd 节点的同步与心跳信息请求。 Store： 用于处理 etcd 支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是 etcd 对用户提供的大多数 API 功能的具体实现。 Raft： Raft 强一致性算法的具体实现，是 etcd 的核心。 WAL： Write Ahead Log（预写式日志），是 etcd 的数据存储方式。除了在内存中存有所有数据的状态以及节点的索引以外，etcd 就通过 WAL 进行持久化存储。WAL 中，所有的数据提交前都会事先记录日志。 Snapshot 是为了防止数据过多而进行的状态快照；Entry 表示存储的具体日志内容。 通常，一个用户的请求发送过来，会经由 HTTP Server 转发给 Store 进行具体的事务处理，如果涉及到节点的修改，则交给 Raft 模块进行状态的变更、日志的记录，然后再同步给别的 etcd 节点以确认数据提交，最后进行数据的提交，再次同步。 etcd的使用场景 服务发现 负载均衡 分布式锁 分布式队列 上面说到etcd可以很容易的实现分布式锁, 锁服务有两种使用方式，一是保持独占，二是控制时序。通过控制时序，即所有想要获得锁的用户都会被安排执行，但是获得锁的顺序也是全局唯一的，同时决定了执行顺序, 就可以实现分布式队列。etcd 为此也提供了一套 API（自动创建有序键），对一个目录建值时指定为POST动作，这样 etcd 会自动在目录下生成一个当前最大的值为键，存储这个新的值（客户端编号）。同时还可以使用 API 按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可以是代表客户端的编号。 etcd概念术语 Raft： etcd所采用的保证分布式系统强一致性的算法。 Node： 一个Raft状态机实例。 Member： 一个etcd实例。它管理着一个Node，并且可以为客户端请求提供服务。 Cluster： 由多个Member构成可以协同工作的etcd集群。 Peer： 对同一个etcd集群中另外一个Member的称呼。 Client： 向etcd集群发送HTTP请求的客户端。 WAL： 预写式日志，etcd用于持久化存储的日志格式。 snapshot： etcd防止WAL文件过多而设置的快照，存储etcd数据状态。 Proxy： etcd的一种模式，为etcd集群提供反向代理服务。 Leader： Raft算法中通过竞选而产生的处理所有数据提交的节点。 Follower： 竞选失败的节点作为Raft中的从属节点，为算法提供强一致性保证。 Candidate： 当Follower超过一定时间接收不到Leader的心跳时转变为Candidate开始竞选。 Term： 某个节点成为Leader到下一次竞选时间，称为一个Term。 Index： 数据项编号。Raft中通过Term和Index来定位数据。 Entry: 表示存储的具体日志内容。 etcd数据存储etcd 的存储分为内存存储和持久化（硬盘）存储两部分，内存中的存储除了顺序化的记录下所有用户对节点数据变更的记录外，还会对用户数据进行索引、建堆等方便查询的操作。而持久化则使用预写式日志（WAL：Write Ahead Log）进行记录存储。 在 WAL 的体系中，所有的数据在提交之前都会进行日志记录。在 etcd 的持久化存储目录中，有两个子目录。一个是 WAL，存储着所有事务的变化记录；另一个则是 snapshot，用于存储某一个时刻 etcd 所有目录的数据。通过 WAL 和 snapshot 相结合的方式，etcd 可以有效的进行数据存储和节点故障恢复等操作。 WAL（Write Ahead Log）最大的作用是记录了整个数据变化的全部历程。在 etcd 中，所有数据的修改在提交前，都要先写入到 WAL 中。使用 WAL 进行数据的存储使得 etcd 拥有两个重要功能: 故障快速恢复： 当你的数据遭到破坏时，就可以通过执行所有 WAL 中记录的修改操作，快速从最原始的数据恢复到数据损坏前的状态。 数据回滚（undo）/ 重做（redo）：因为所有的修改操作都被记录在 WAL 中，需要回滚或重做，只需要方向或正向执行日志中的操作即可。 WAL的缺陷: WAL 是一种 Append Only 的日志文件，只会在文件结尾不断地添加新日志，这样做可以避免大量随机 I/O 带来的性能损失，但是随着程序的运行，节点需要处理客户端和集群中其他节点发来的大量请求，相应的 WAL 日志量也会不断增加，这会占用大量的磁盘空间。当节点宕机后，如果要恢复其状态，则需要从头读取全部的 WAL 日志文件，这显然是非常耗时的。 WAL的缺陷的解决方案: 快照, 为了解决WAL的这个缺陷，etcd 会定期创建快照，将整个节点的状态进行序列化，然后写入稳定的快照文件中，在该快照文件之前的日志记录就可以全部丢弃掉。在恢复节点状态时会先加载快照文件，使用快照数据将节点恢复到对应的状态，之后从 WAL 文件读取快照之后的数据，将节点恢复到正确的状态。 etcd 的快照有两种: 一种是用于存储某一时刻 etcd 的所有数据的数据快照， 另一种是用于集群中较慢节点追赶数据的 RPC 快照。]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的reload对于func_closure的处理踩坑]]></title>
    <url>%2F2021%2F05%2F08%2Fpython_reload_func_closure%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[带着问题学习动力是较强的, 直接上例子 . . . 引子代码如下： 1234567891011121314# coding:utf-8def make_actions(): acts = [] xxd = &#123;1: '1', 2: '2', 3: '3'&#125; for k, v in xxd.iteritems(): acts.append(lambda x: (k + int(v)) ** x) return actsfoo = make_actions()print(foo[0](2))print(foo[1](2))print(foo[2](2)) 众所周知, 结果肯定为: 123363636 因为这个闭包引用了外部的k, v嘛, 那是存在哪里呢? debug一波, 此时发现func_closure是这样存的 把 make_actions 改成123456def make_actions(): acts = [] xxd = &#123;1: '1', 2: '2', 3: '3'&#125; for k, v in xxd.iteritems(): acts.append(lambda x, key=k, val=v, : (key + int(val)) ** x) return acts 再debug断点查看一波就完事了, 会发现现在 func_defaults 里现在有东西了 基于函数替换型reload中的应用实现python热更逻辑的时候要记得处理func_closure 如类似下方的这段代码要怎么reload呢? 1234567891011121314151617181920212223242526def log(func): def _call_func(*args, **kw): print 'call ' + func.__name__ return func(*args, **kw) return _call_funcdef print_text(text): def _wrap_log(func): def _call_func(*args, **kwargs): print text return func(*args, **kwargs) return _call_func return _wrap_log@log@print_text('test_texttt')def test_log(): print "miao !"for cell in test_log.func_closure: cc = cell.cell_contentstest_log() debug断点查看一波, 发现func_closure里还有 function object 那reload需要对含有闭包的情况进行一些简单处理:python如果被更新的函数带 closure，新旧函数的 func_closure 个数不同的话，旧函数会被新函数直接替换。closure 内的函数对象也会跟着热更新，也就是说支持热更新被装饰器装饰的函数，默认值更新 2 层，有需要更多的层的项目可以改变 update_cell_depth 的值 123456789101112131415161718192021222324def update_fun( old_fun, new_fun, update_cell_depth = 2 ): old_cell_num = 0 if old_fun.func_closure: old_cell_num = len( old_fun.func_closure ) new_cell_num = 0 if new_fun.func_closure: new_cell_num = len( new_fun.func_closure ) if old_cell_num != new_cell_num: return False setattr(old_fun, 'func_code', new_fun.func_code ) setattr(old_fun, 'func_defaults', new_fun.func_defaults ) setattr(old_fun, 'func_doc', new_fun.func_doc ) setattr(old_fun, 'func_dict', new_fun.func_dict ) if not (update_cell_depth and old_cell_num ): return True for index, cell in enumerate(old_fun.func_closure): if inspect.isfunction(cell.cell_contents): update_fun(cell.cell_contents, new_fun.func_closure[index].cell_contents, update_cell_depth - 1) return True]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-Redis实现原理]]></title>
    <url>%2F2021%2F04%2F20%2Fself_cultivation_redis%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Redisredis 数据结构有哪些？分别怎么实现的？ String: 全是整数的时候用整数编码int 当有字符串的时候用简单动态字符串sds编码 HashTable: 元素比较少或者元素比较短的时候用压缩表ziplist(key1|val1|key2|val2|…这样存储), 其他时候就用字典ht Set: 元素全是整数的时候用整数集合编码(一种特殊的编码, 会使用各种规则来利用位空间, 来节省内存), 其他时候用字典ht编码(键为Set的元素, 值都为Null) List: 元素比较少或者元素比较短的时候用压缩表ziplist, 其他时候就用双端列表LinkedList编码 ZSet: 参考 http://redisbook.com/preview/object/sorted_set.html 参考 https://redisbook.readthedocs.io/en/latest/datatype/sorted_set.html 元素比较少或者元素比较短的时候用压缩表ziplist(member1|score1|member2|score2|…, 按照score从小到大排列), 其他时候就用跳跃表SkipList编码, 这个编码里包含一个字典结构和一个跳表结构, 但这两种数据结构都会通过指针来共享相同元素的成员和分值， 所以同时使用跳跃表和字典来保存集合元素不会产生任何重复成员或者分值， 也不会因此而浪费额外的内存: 字典用于快速查找, 如ZScore查询member成员的 score 值, 或者快速确定是否有某个member 跳表用于zrank/zrange等 zset各种问题为什么zset用跳表不用红黑树现在我们看看，对于这个问题，Redis的作者 @antirez 是怎么说的： There are a few reasons: They are not very memory intensive. It’s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees. A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees. They are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code. 可参考: 本博客文章跳表总结: 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。 从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。 从算法实现难度上来比较，skiplist比平衡树要简单得多。 zset是怎么支持查询排名的跳表怎么支持查询排名的 延时队列用redis怎么做用zset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者轮询zset用zrangebyscore指令获取N秒之前的数据轮询进行处理。 ZSET做排行榜时要实现分数相同时按时间顺序排序怎么实现说了一个将 score 拆成高 32 位和低 32 位，高 32 位存分数，低 32 位存时间的方法。 哈希表渐进式rehash 当以下条件中的任意一个被满足时， 程序会自动开始对哈希表执行扩展操作： 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 1 ； 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 5 ； 根据 BGSAVE 命令或 BGREWRITEAOF 命令是否正在执行， 服务器执行扩展操作所需的负载因子并不相同， 这是因为在执行 BGSAVE 命令或 BGREWRITEAOF 命令的过程中， Redis 需要创建当前服务器进程的子进程， 所以在子进程存在期间， 服务器会提高执行扩展操作所需的负载因子， 从而尽可能地避免在子进程存在期间进行哈希表扩展操作， 这可以避免不必要的内存写入操作， 最大限度地节约内存。 另一方面， 当哈希表的负载因子小于 0.1 时， 程序自动开始对哈希表执行收缩操作。 以下是哈希表渐进式 rehash 的详细步骤： 为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始。 在 rehash 进行期间， 每次对字典执行删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一。 随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成。 渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。 因为在进行渐进式 rehash 的过程中， 字典会同时使用 ht[0] 和 ht[1] 两个哈希表， 所以在渐进式 rehash 进行期间， 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找， 诸如此类。 另外， 在渐进式 rehash 执行期间， 新添加到字典的键值对一律会被保存到 ht[1] 里面， 而 ht[0] 则不再进行任何添加操作： 这一措施保证了 ht[0] 包含的键值对数量会只减不增， 并随着 rehash 操作的执行而最终变成空表。 redis 持久化有哪几种方式，怎么选？ 混合持久化 原因: 重启 Redis 时，我们很少使用 rdb 来恢复内存状态，因为会丢失大量数据。如果使用 AOF 日志重放，性能则相对 rdb 来说要慢很多，这样在 Redis 实例很大的情况下，启动的时候需要花费很长的时间。 原理: 混合持久化同样也是通过bgrewriteaof完成的，不同的是当开启混合持久化时，fork出的子进程先将共享的内存副本全量的以RDB方式写入aof文件，然后在将aof_rewrite_buf重写缓冲区的增量命令以AOF方式写入到文件，写入完成后通知主进程更新统计信息，并将新的含有RDB格式和AOF格式的AOF文件替换旧的的AOF文件。 简单的说：新的AOF文件前半段是RDB格式的全量数据后半段是AOF格式的增量数据， rdb 优势: RDB文件紧凑，全量备份，非常适合用于进行备份和灾难恢复。 生成RDB文件的时候，redis主进程会fork()一个子进程来处理所有保存工作，主进程不需要进行任何磁盘IO操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 劣势: 当进行快照持久化时，会开启一个子进程专门负责快照持久化，子进程会拥有父进程的内存数据，父进程修改内存子进程不会反应出来，所以在快照持久化期间修改的数据不会被保存，可能丢失数据。 aof 优势: AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台刷盘线程执行一次fsync操作(这种一秒刷盘一次的策略, 可能会造成追加阻塞: 当硬盘资源繁忙时，即主线程发现距离上次fsync时间超过2秒, 为了数据安全性, 主线程会阻塞直到后台刷盘线程执行fsync操作完成)，保证最多丢失1秒钟的数据。所以这也是redis重启优先加载aof的理由 AOF日志文件没有任何磁盘寻址的开销，写入性能非常高，文件不容易破损。 AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。 AOF日志文件的命令通过可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据 劣势: 对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大 AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的 bgsave流程说一下 子进程创建RDB文件, 根据父进程内存生成临时快照文件, 完成后对原有RDB文件进行原子替换. 然后子进程发送信号给父进程表示完成 aof流程说一下以及aof追加阻塞是啥 追加阻塞: AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台刷盘线程执行一次fsync操作(这种一秒刷盘一次的策略, 可能会造成追加阻塞: 当硬盘资源繁忙时，即主线程发现距离上次fsync时间超过2秒, 为了数据安全性, 主线程会阻塞直到后台刷盘线程执行fsync操作完成)，保证最多丢失1秒钟的数据。 AOF重写的实现 所谓的“重写”其实是一个有歧义的词语, AOF重写并不需要对原有AOF文件进行任何的读取，写入，分析等操作，这个功能是通过读取服务器当前的数据库状态来实现的。 如当前列表键list在数据库中的值就为[&quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;]。要使用尽量少的命令来记录list键的状态，最简单的方式不是去读取和分析现有AOF文件的内容，，而是直接读取list键在数据库中的当前值，然后用一条RPUSH list &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot;代替前面的6条命令 AOF 重写程序可以很好地完成创建一个新 AOF 文件的任务， 但是， 在执行这个程序的时候， 调用者线程会被阻塞。很明显， 作为一种辅佐性的维护手段， Redis 不希望 AOF 重写造成服务器无法处理请求， 所以 Redis 决定将 AOF 重写程序放到（后台）子进程里执行， 这样处理的最大好处是： 子进程进行 AOF 重写期间，主进程可以继续处理命令请求。 子进程带有主进程的数据副本，使用子进程而不是线程，可以在避免锁的情况下，保证数据的安全性。 不过， 使用子进程也有一个问题需要解决： 因为子进程在进行 AOF 重写期间， 主进程还需要继续处理命令， 而新的命令可能对现有的数据进行修改， 这会让当前数据库的数据和重写后的 AOF 文件中的数据不一致。为了解决这个问题， Redis 增加了一个 AOF 重写缓存， 这个缓存在 fork 出子进程之后开始启用， Redis 主进程在接到新的写命令之后， 除了会将这个写命令的协议内容追加到现有的 AOF 文件之外， 还会追加到这个重写缓存中, 换言之， 当子进程在执行 AOF 重写时， 主进程需要执行以下三个工作： 处理命令请求。 将写命令追加到现有的 AOF 文件中。 将写命令追加到 AOF 重写缓存中。 当子进程完成 AOF 重写之后， 它会向父进程发送一个完成信号， 父进程在接到完成信号之后， 会调用一个信号处理函数， 并完成以下工作： 将 AOF 重写缓存中的内容全部写入到新 AOF 文件中。 对新的 AOF 文件进行改名rename，覆盖原有的 AOF 文件。这就是aof的原子替换. 在整个 AOF 后台重写过程中， 只有最后的写入缓存和改名操作会造成主进程阻塞， 在其他时候， AOF 后台重写都不会对主进程造成阻塞， 这将 AOF 重写对性能造成的影响降到了最低。以上就是 AOF 后台重写， 也即是 BGREWRITEAOF 命令的工作原理。 如何做rdb和aof的原子替换的比如想要将temp文件原子替换origin文件, 则直接rename tmp文件到origin文件即可实现.rename通过来说, 直接修改 file system metadata, 如inode信息. 在posix标准里, rename实现是原子的, 即: rename成功, 原文件名 指向 temp 文件; 原文件内容被删除. rename失败, 原文件名 仍指向原来的文件内容. redis 主从同步是怎样的过程？ 从redis发出sync要求 主redis开始bgsave(并且一边开启指令buffer来存储bgsave过程中的写指令们记为cmd) 主redis把bgsave生成的rdb发给从redis 把cmd发送给从redis 从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令 总结:主从刚刚连接的时候，进行全量同步；全量同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。redis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。 redis key 的过期策略Redis键的过期策略，是有定期删除+惰性删除两种。 定期好理解，默认100ms就 随机 抽一些设置了过期时间的key，去检查是否过期，过期了就删了。 惰性删除，查询时再判断是否过期，过期就删除键不返回值。 内存淘汰机制当新增数据发现内存达到限制时，Redis触发内存淘汰机制。 lru lfu(least frequency used, redis 4新增) random ttl redis的LRU算法说一下 普通的LRU算法: 一般是用哈希表+双向链表来实现的: 基于 HashMap 和 双向链表实现 LRU 的整体的设计思路是，可以使用 HashMap 存储 key，这样可以做到 save 和 get key的时间都是 O(1)，而 HashMap 的 Value 指向双向链表实现的 LRU 的 Node 节点. 其核心操作的步骤是: save(key, value):首先在 HashMap 找到 Key 对应的节点，如果节点存在，更新节点的值，并把这个节点移动队头。如果不存在，需要构造新的节点，并且尝试把节点塞到队头，如果LRU空间不足，则通过 tail 淘汰掉队尾的节点，同时在 HashMap 中移除 Key。 get(key):通过 HashMap 找到 LRU 链表节点，因为根据LRU 原理，这个节点是最新访问的，所以要把节点插入到队头，然后返回缓存的值。 Redis的LRU实现: 如果按照HashMap和双向链表实现，需要额外的存储存放 next 和 prev 指针，牺牲比较大的存储空间，显然是不划算的。所以Redis采用了一个近似的做法，就是定时每隔一段时间就随机取出若干个key，然后按照访问时间排序后，淘汰掉最不经常使用的. Redis 3.0之后又改善了算法的性能，会提供一个待淘汰候选key的pool，里面默认有16个key，按照空闲时间排好序。更新时从Redis键空间随机选择N个key，分别计算它们的空闲时间 idle，key只会在pool不满或者空闲时间大于pool里最小的时，才会进入pool，然后从pool中选择空闲时间最大的key淘汰掉。 redis哨兵 Redis Sentinel是Redis的高可用实现方案：故障发现、故障自动转移、配置中心 客户端通知。 Redis Sentinel从Redis 2.8版本开始才正式生产可用，之前版本生产不可用。 尽可能在不同物理机上部署Redis Sentinel所有节点。 Redis Sentinel中的Sentinel节点个数应该为大于等于3且最好为奇数。 Redis Sentinel中的数据节点与普通数据节点没有区别。 哨兵是一个配置提供者，而不是代理。在引入哨兵之后，客户端会先连接哨兵，再获取到主节点之后，客户端会和主节点直接通信。如果发生了故障转移，哨兵会通知到客户端。所以这也需要客户端的实现对哨兵的显式支持。 Redis Sentinel通过三个定时任务实现了Sentinel节点对于主节点、从节点、其余 Sentinel节点的监控。 Redis Sentinel在对节点做失败判定时分为主观下线和客观下线。 Redis Sentinel实现读写分离高可用可以依赖Sentinel节点的消息通知，获取Redis 数据节点的状态变化。 用文字描述一下故障切换（failover）的过程: 假设主服务器宕机，哨兵1先检测到这个结果，系统并不会马上进行failover过程，仅仅是哨兵1主观的认为主服务器不可用，这个现象成为主观下线。 当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，就对这个主节点故障达成一致, 这个过程称为客观下线。 这样对于客户端而言，一切都是透明的。然后通过raft算法从哨兵中选出一个哨兵来执行故障转移 redis集群redis集群是一个由多个主从节点群组成的分布式服务器群，它具有复制、高可用和分片特性。Redis集群不需要sentinel哨兵也能完成节点移除和故障转移的功能。需要将每个节点设置成集群模式，这种集群模式没有中心节点，可水平扩展，据官方文档称可以线性扩展到上万个节点(官方推荐不超过1000个节点)。redis集群的性能和高可用性均优于之前版本的哨兵模式，且集群配置非常简单。集群模式有以下几个特点： 由多个Redis服务器组成的分布式网络服务集群； 集群之中有多个Master主节点，每一个主节点都可读可写； 节点之间会互相通信，两两相连, 采用gossip协议来通信； Redis集群无中心节点。 集群的伸缩本质是: 槽数据在节点中的移动 优点在哨兵模式中，仍然只有一个Master节点。当并发写请求较大时，哨兵模式并不能缓解写压力。 我们知道只有主节点才具有写能力，那如果在一个集群中，能够配置多个主节点，缓解写压力，redis-cluster集群模式能达到此类要求。 在Redis-Cluster集群中，可以给每一个主节点添加从节点，主节点和从节点直接遵循主从模型的特性。当用户需要处理更多读请求的时候，添加从节点开启read-only来读写分离可以扩展系统的读性能。 缺点 Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的行为。 不能用redis事务机制(不过就算不用redis集群一般也不推荐用redis的事务, 毕竟假事务无法回滚嘛, 比如multi之后那些在 EXEC 命令执行之后所产生的错误， 并没有对它们进行特别处理： 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。)。因为一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。 故障转移Redis集群的主节点内置了类似Redis Sentinel的节点故障检测和自动故障转移功能，当集群中的某个主节点下线时，集群中的其他在线主节点会注意到这一点，并对已下线的主节点进行故障转移。集群进行故障转移的方法和Redis Sentinel进行故障转移的方法基本一样(也有主观下线和客观下线)，不同的是，在集群里面，故障转移的过程是: 在集群内广播选举消息 集群中其他在线的持有槽的主节点投票到故障主节点的从节点们 被选出来的从节点变成主节点 所以集群不必另外使用Redis Sentinel。 集群分片策略常见的集群分片算法有： 一般哈希算法 一致性哈希算法 Hash Slot算法 Redis采用的是Hash Slot 一般哈希算法计算方式：hash(key)%N缺点：如果增加一个redis，映射公式变成了 hash(key)%(N+1)​ 如果一个redis宕机了，映射公式变成了 hash(key)%(N-1)​ 在以上两种情况下，几乎所有的缓存都失效了。 一致性哈希算法先构造出一个长度为2^32整数环，根据节点名称的hash值（分布在[0,2^32-1]）放到这个环上。现在要存放资源，根据资源的Key的Hash值（也是分布在[0,2^32-1]），在环上顺时针的找到离它最近的一个节点，就建立了资源和节点的映射关系。 优点：一个节点宕机时，上面的数据转移到顺时针的下一个节点中，新增一个节点时，也只需要将部分数据迁移到这个节点中，对其他节点的影响很小 缺点：由于数据在环上分布不均，可能存在某个节点存储的数据比较多，那么当他宕机的时候，会导致大量数据涌入下一个节点中，把另一个节点打挂了，然后所有节点都挂了 改进：引进了虚拟节点的概念，想象在这个环上有很多“虚拟节点”，数据的存储是沿着环的顺时针方向找一个虚拟节点，每个虚拟节点都会关联到一个真实节点 HashSlot算法Redis采用的是Hash Slot分片算法，用来计算key存储位置的。集群将整个数据库分为16384个槽位slot，所有key-value数据都存储在这些slot中的某一个上。一个slot槽位可以存放多个数据，key的槽位计算公式为：slot_number=CRC16(key)%16384，其中CRC16为16位的循环冗余校验和函数。客户端可能会挑选任意一个redis实例去发送命令，每个redis实例接收到命令，都会计算key对应的hash slot，如果在本地就在本地处理，否则返回moved给客户端，让客户端进行重定向到对应的节点执行命令(实现得好一点的smart客户端会缓存键-slot-节点的映射关系来获得性能提升). 那为什么是16384个槽呢? ps:CRC16算法产生的hash值有16bit，该算法可以产生2^16-=65536个值。换句话说，值是分布在0~65535之间。那作者在做mod运算的时候，为什么不mod65536，而选择mod16384？作者解答 在redis节点发送心跳包时需要把所有的槽放到这个心跳包里，以便让节点知道当前集群信息，16384=16k，在发送心跳包时使用char进行bitmap压缩后是2k（2 * 8 (8 bit) * 1024(1k) = 2K），也就是说使用2k的空间创建了16k的槽数。 虽然使用CRC16算法最多可以分配65535（2^16-1）个槽位，65535=65k，压缩后就是8k（8 * 8 (8 bit) * 1024(1k) = 8K），也就是说需要需要8k的心跳包，作者认为这样做不太值得；并且一般情况下一个redis集群不会有超过1000个master节点，所以16k的槽位是个比较合适的选择。 Cache和DB如何一致详细的请参考: https://segmentfault.com/a/1190000015804406本博客也有一份: Cache和DB一致性 总结: 使用cache aside pattern 对于读请求 先读 cache，再读 db 如果，cache hit，则直接返回数据 如果，cache miss，则访问 db，并将数据 set 回缓存 对于写请求 先操作数据库，再淘汰缓存（淘汰缓存，而不是更新缓存, 如果更新缓存，在并发写时，可能出现数据不一致。） Cache Aside Pattern 方案存在什么问题？ 问题1: 如果先写数据库，再淘汰缓存，在原子性被破坏时： 修改数据库成功了 淘汰缓存失败了导致，数据库与缓存的数据不一致。 如何解决问题1? 在淘汰缓存的时候，如果失败，则重试一定的次数。如果失败一定次数还不行，那就是其他原因了。比如说 redis 故障、内网出了问题。 问题2: 主从同步延迟导致的缓存和数据不一致问题 问题: 发生写请求后（不管是先操作 DB，还是先淘汰 Cache），在主从数据库同步完成之前，如果有读请求，都可能发生读 Cache Miss，读从库把旧数据存入缓存的情况。此时怎么办呢？ 解决思路: 在主从时延的时间段内，读取修改过的数据的话，强制读主，并且更新缓存，这样子缓存内的数据就是最新。在主从时延过后，这部分数据继续读从库，从而继续利用从库提高读取能力。 具体解决方案: 写请求发生的时候: 将哪个库，哪个表，哪个主键三个信息拼装一个 key 设置到 cache 里，这条记录的超时时间，设置为 “主从同步时延”, PS：key 的格式为 “db:table:PK”，假设主从延时为 1s，这个 key 的 cache 超时时间也为 1s。 当读请求发生时：这是要读哪个库，哪个表，哪个主键的数据呢，也将这三个信息拼装一个 key，到 cache 里去查询，如果， （1）cache 里有这个 key，说明 1s 内刚发生过写请求，数据库主从同步可能还没有完成，此时就应该去主库查询。并且把主库的数据 set 到缓存中，防止下一次 cache miss。 （2）cache 里没有这个 key，说明最近没有发生过写请求，此时就可以去从库查询 缓存雪崩是啥?咋处理?是指大面积的缓存失效，打崩了DB. 如果缓存挂掉，所有的请求会压到数据库，如果未提前做容量预估，可能会把数据库压垮（在缓存恢复之前，数据库可能一直都起不来），导致系统整体不可服务。又或者打个比方, 如果所有首页的Key失效时间都是12小时，中午12点刷新的，我零点有个秒杀活动大量用户涌入，假设当时每秒 6000 个请求，本来缓存在可以扛住每秒 5000 个请求，但是缓存当时所有的Key都失效了。此时 1 秒 6000 个请求全部落数据库，数据库必然扛不住. 处理方案: key随机过期 key永不过期, 比如开个单独线程去定时更新缓存 高可用, 如果Redis是集群部署，将热点数据均匀分布在不同的Redis库中也能避免全部失效的问题 隔离服务, 限流降级 缓存穿透是啥?咋处理?是指缓存和数据库中都没有的数据，而用户不断发起请求，严重会击垮数据库 我们数据库的 id 都是1开始自增上去的，如发起为id值为 -1 的数据或 id 为特别大不存在的数据。这时的用户很可能是攻击者，攻击会导致数据库压力过大，严重会击垮数据库。 处理方案: 缓存穿透我会在接口层增加校验，比如用户鉴权校验，参数做校验，不合法的参数直接代码Return，比如：id 做基础校验，id &lt;=0的直接拦截等。 布隆过滤器, 把存在的key提前存放好在布隆过滤器中, 当查询的时候快速判断出你这个Key是否在数据库中存在, 不存在则直接return 缓存击穿是啥?咋处理?是指持续的大并发的访问一个热点数据, 当这个Key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库 这个跟缓存雪崩有点像，但是又有一点不一样，缓存雪崩是因为大面积的缓存失效，打崩了DB，而缓存击穿不同的是缓存击穿是指一个Key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个Key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个完好无损的桶上凿开了一个洞。 处理方案: key永不过期, 比如开个单独线程去定时更新缓存 互斥锁, 在key失效的瞬间, 只允许一个查询操作的线程A去查询数据库并重建缓存并上互斥锁, 其他的查询操作线程全部等待线程A操作完了再从缓存里取数据]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-编码知识]]></title>
    <url>%2F2021%2F04%2F18%2Fself_cultivation_encoding%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[编码知识Base64 的原理？编码后比编码前是大了还是小了。结论: 大了. 因为Base64 编码本质上是一种将二进制数据转成文本数据的方案。对于非二进制数据，是先将其转换成二进制形式，然后每连续 6 比特（2 的 6 次方 = 64）计算其十进制值，根据该值在上面的索引表中找到对应的字符，最终得到一个文本字符串。也就是说, 每 3 个原始字符编码成 4 个字符，如果原始字符串长度不能被 3 整除，那怎么办？使用 0 值来补充原始字符串。 base64的原理Base64 编码之所以称为 Base64，是因为其使用 64 个字符来对任意数据进行编码，同理有 Base32、Base16 编码。标准 Base64 编码使用的 64 个字符为： 这 64 个字符是各种字符编码（比如 ASCII 编码）所使用字符的子集，基本，并且可打印。唯一有点特殊的是最后两个字符，因对最后两个字符的选择不同，Base64 编码又有很多变种，比如 Base64 URL 编码。 Base64 编码本质上是一种将二进制数据转成文本数据的方案。对于非二进制数据，是先将其转换成二进制形式，然后每连续 6 比特（2 的 6 次方 = 64）计算其十进制值，根据该值在上面的索引表中找到对应的字符，最终得到一个文本字符串。 假设我们要对 Hello! 进行 Base64 编码，按照 ASCII 表，其转换过程如下图所示： 可知 Hello! 的 Base64 编码结果为 SGVsbG8h ，原始字符串长度为 6 个字符，编码后长度为 8 个字符，每 3 个原始字符经 Base64 编码成 4 个字符，编码前后长度比 4/3，这个长度比很重要 - 比原始字符串长度短，则需要使用更大的编码字符集，这并不我们想要的；长度比越大，则需要传输越多的字符，传输时间越长。Base64 应用广泛的原因是在字符集大小与长度比之间取得一个较好的平衡，适用于各种场景。 是不是觉得 Base64 编码原理很简单？ 但这里需要注意一个点：Base64 编码是每 3 个原始字符编码成 4 个字符，如果原始字符串长度不能被 3 整除，那怎么办？使用 0 值来补充原始字符串。 以 Hello!! 为例，其转换过程为： 注：图表中蓝色背景的二进制 0 值是额外补充的。 Hello!! Base64 编码的结果为 SGVsbG8hIQAA 。最后 2 个零值只是为了 Base64 编码而补充的，在原始字符中并没有对应的字符，那么 Base64 编码结果中的最后两个字符 AA 实际不带有效信息，所以需要特殊处理，以免解码错误。 标准 Base64 编码通常用 = 字符来替换最后的 A，即编码结果为 SGVsbG8hIQ==。因为 = 字符并不在 Base64 编码索引表中，其意义在于结束符号，在 Base64 解码时遇到 = 时即可知道一个 Base64 编码字符串结束。 如果 Base64 编码字符串不会相互拼接再传输，那么最后的 = 也可以省略，解码时如果发现 Base64 编码字符串长度不能被 4 整除，则先补充 = 字符，再解码即可。 解码是对编码的逆向操作，但注意一点：对于最后的两个 = 字符，转换成两个 A 字符，再转成对应的两个 6 比特二进制 0 值，接着转成原始字符之前，需要将最后的两个 6 比特二进制 0 值丢弃，因为它们实际上不携带有效信息。 utf8编码和unicode字符集总结: unicode是个字符集, 只是一个符号对应表, 它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储 utf8是unicode符号具体的编码方式, 规定了该怎么存储 说到utf8，就不得不说一下unicode了。 Unicode是一个很大的集合，每一个unicode对应一个符号，不管是中文的汉字，英文字符，日文，韩文等等。现在的规模可以容纳100多万个符号。每个符号的编码都不一样，比如，U+0639表示阿拉伯字母 Ain，U+0041表示英语的大写字母A，U+4E25表示汉字“严”。具体的符号对应表，可以查询unicode.org，或者专门的汉字对应表。 需要注意的是，Unicode只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。 比如，汉字“严”的unicode是十六进制数4E25，转换成二进制数足足有15位（100111000100101），也就是说这个符号的表示至少需要2个字节。表示其他更大的符号，可能需要3个字节或者4个字节，甚至更多。 这里就有两个严重的问题，第一个问题是：如何才能区别unicode和ascii？计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？第二个问题是：我们已经知道，英文字母只用一个字节表示就够了，如果unicode统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。 它们造成的结果是： 1）出现了unicode的多种存储方式，也就是说有许多种不同的二进制格式，可以用来表示unicode。 2）unicode在很长一段时间内无法推广，直到互联网的出现。 UTF-8互联网的普及，强烈要求出现一种统一的编码方式。UTF-8就是在互联网上使用最广的一种unicode的实现方式。其他实现方式还包括UTF-16和UTF-32，不过在互联网上基本不用。重复一遍，这里的关系是，UTF-8是Unicode的实现方式之一。 UTF-8最大的一个特点，就是它是一种变长的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。 UTF-8的编码规则很简单，只有二条： 1）对于单字节的符号，字节的第一位（字节的最高位）设为0，后面7位为这个符号的unicode码。因此对于英语字母，UTF-8编码和ASCII码是相同的。 2）对于n字节的符号（n&gt;1），第一个字节的前n位都设为1，第n+1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的unicode码。 下表总结了编码规则，字母x表示可用编码的位。 Unicode符号范围 UTF-8编码方式(十六进制) | （二进制）12345—————+———————————————————————0000 0000-0000 007F | 0xxxxxxx0000 0080-0000 07FF | 110xxxxx 10xxxxxx0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 下面，还是以汉字“严”为例，演示如何实现UTF-8编码：已知“严”的unicode是4E25（100111000100101），根据上表，可以发现4E25处在第三行的范围内（0000 0800-0000 FFFF），因此“严”的UTF-8编码需要三个字节，即格式是“1110xxxx 10xxxxxx 10xxxxxx”。然后，从“严”的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。这样就得到了，“严”的UTF-8编码是“11100100 10111000 10100101”，转换成十六进制就是E4B8A5。]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rpclib源码阅读笔记之浅谈cpp func traits]]></title>
    <url>%2F2021%2F04%2F18%2Fcpp_func_traits_intro%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文出处 0. 导语大家好，我是光城，欢迎关注公众号：guangcity。在 STL 编程中，容器和算法是独立设计的，即数据结构和算法是独立设计的，连接容器和算法的桥梁就是迭代器了，迭代器使其独立设计成为可能。如下图所示： 上图给出了 STL 的目标就是要把数据和算法分开，分别对其进行设计，之后通过一种名为 iterator 的东西，把这二者再粘接到一起。 设计模式中，关于 iterator 的描述为：一种能够顺序访问容器中每个元素的方法，使用该方法不能暴露容器内部的表达方式。而类型萃取技术就是为了要解决和 iterator 有关的问题的。 它将范型算法 (find, count, find_if) 用于某个容器中, 最重要的是要给算法提供一个访问容器元素的工具，iterator 就扮演着这个重要的角色。 而在算法中我们可能会定义简单的中间变量或者设定算法的返回变量类型，这时候需要知道迭代器所指元素的类型是什么，但是由于没有 typeof 这类判断类型的函数, 我们无法直接获取，那该如何是好？本文就来具体阐述。 对于迭代器来说就是一种智能指针，因此，它也就拥有了一般指针的所有特点——能够对其进行 * 和 -&gt; 操作。但是在遍历容器的时候，不可避免的要对遍历的容器内部有所了解，所以，干脆把迭代器的开发工作交给容器的设计者好了，如此以来，所有实现细节反而得以封装起来不被使用者看到，这正是为什么每一种 STL 容器都提供有专属迭代器的缘故。 而 Traits 在bits/stl_iterator_base_types.h中： 123456789template&lt;class _Tp&gt;struct iterator_traits&lt;_Tp*&gt;&#123; typedef ptrdiff_t difference_type; typedef typename _Tp::value_type value_type; typedef typename _Tp::pointer pointer; typedef typename _Tp::reference reference; typedef typename _Tp::iterator_category iterator_category;&#125;; 看的一脸懵逼吧，没事，看完本节，入门 STL，哈哈~ 1.template 参数推导首先，在算法中运用迭代器时，很可能会用到其相应型别（associated type）（迭代器所指之物的型别）。假设算法中有必要声明一个变量，以 “迭代器所指对象的型别” 为型别，该怎么办呢？ 解决方法是：利用 function template 的参数推导机制。 例如： 如果 T 是某个指向特定对象的指针，那么在 func 中需要指针所指向对象的型别的时候，怎么办呢？这个还比较容易，模板的参数推导机制可以完成任务， 12345template &lt;class I&gt;inlinevoid func(I iter) &#123; func_impl(iter, *iter); // 传入iter和iter所指的值，class自动推导&#125; 通过模板的推导机制，我们轻而易举的或得了指针所指向的对象的类型。 12345678910template &lt;class I, class T&gt;void func_impl(I iter, T t) &#123; T tmp; // 这里就是迭代器所指物的类别 // ... 功能实现&#125;int main() &#123; int i; func(&amp;i);&#125; 但是，函数的 “template 参数推导机制” 推导的只是参数，无法推导函数的返回值类型。万一需要推导函数的传回值，就无能为力了。因此，我们引出下面的方法。 2. 声明内嵌型别迭代器所指对象的型别，称之为迭代器的 value type。 尽管在 func_impl 中我们可以把 T 作为函数的返回值，但是问题是用户需要调用的是 func。 12345678910111213template &lt;class I, class T&gt;T func_impl(I iter, T t) &#123; T tmp; // 这里就是迭代器所指物的类别 // ... 功能实现&#125;template &lt;class T&gt;(*T) func(T t) &#123; // !!!Wrong code return func_impl(t, *t); // forward the task to func_impl&#125;int main() &#123; int i =10; cout&lt;&lt;func(&amp;i)&lt;&lt;endl; // !!! Can’t pass compile&#125; 如果去编译上述代码，编译失败！ 这个问题解决起来也不难，声明内嵌型别似乎是个好主意，这样我们就可以直接获取。只要做一个 iterator，然后在定义的时候为其指向的对象类型制定一个别名，就好了，像下面这样： 12345678910111213141516171819template &lt;class T&gt;struct MyIter &#123; typedef T value_type; // 内嵌型别声明 T* ptr; MyIter(T* p = 0) : ptr(p) &#123;&#125; T&amp; operator*() const &#123; return *ptr; &#125;&#125;;template &lt;class I&gt;typename I::value_typefunc(I ite) &#123; std::cout &lt;&lt; "class version" &lt;&lt; std::endl; return *ite;&#125;int main() &#123; // ... MyIter&lt;int&gt; ite(new int(8)); cout &lt;&lt; func(ite); // 输出8&#125; 很漂亮的解决方案，看上去一切都很完美。但是，实际上还是有问题，因为 func 如果是一个泛型算法，那么它也绝对要接受一个原生指针作为迭代器，但是显然，你无法让下面的代码编译通过： 12int *p = new int(5);cout&lt;&lt;func(p)&lt;&lt;endl; // error 我们的 func 无法支持原生指针，这显然是不能接受的。此时，template partial specialization 就派上了用场。 3. 救世主 Traits前面也提到了，如果直接使用typename I::value_type，算法就无法接收原生指针，因为原生指针根本就没有 value_type 这个内嵌类型。 因此，我们还需要加入一个中间层对其进行判断，看它是不是原生指针，注意，这就是 traits 技法的妙处所在。 如果我们只使用上面的做法，也就是内嵌 value_type，那么对于没有 value_type 的指针，我们只能对其进行偏特化，这种偏特化是针对可调用函数 func 的偏特化，假如 func 有 100 万行行代码，那么就会造成极大的视觉污染。 （1）函数偏特化 函数偏特化： 12345678910111213141516171819202122232425262728293031323334template &lt;class T&gt;struct MyIter &#123; typedef T value_type; // 内嵌型别声明 T* ptr; MyIter(T* p = 0) : ptr(p) &#123;&#125; T&amp; operator*() const &#123; return *ptr; &#125;&#125;;template &lt;class I&gt;typename I::value_typefunc(I ite) &#123; std::cout &lt;&lt; "class version" &lt;&lt; std::endl; return *ite;&#125;template &lt;class I&gt;Ifunc(I* ite) &#123; std::cout &lt;&lt; "pointer version" &lt;&lt; std::endl; return *ite;&#125;template &lt;class I&gt;I func(const I* ite) &#123; std::cout &lt;&lt; "const pointer version" &lt;&lt; std::endl; return *ite;&#125;int main() &#123; // ... MyIter&lt;int&gt; ite(new int(8)); cout &lt;&lt; func(ite)&lt;&lt;endl; int *p = new int(52); cout&lt;&lt;func(p)&lt;&lt;endl; const int k = 3; cout&lt;&lt;func(&amp;k)&lt;&lt;endl;&#125; 输出： 123456class version8pointer version52const pointer version3 （2）加入中间层 在 STL 中 Traits 是什么呢？看下图： 利用一个中间层iterator_traits固定了func的形式，使得重复的代码大量减少，唯一要做的就是稍稍特化一下 iterator_tartis 使其支持 pointer 和 const pointer:) 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;template &lt;class T&gt;struct MyIter &#123; typedef T value_type; // 内嵌型别声明 T* ptr; MyIter(T* p = 0) : ptr(p) &#123;&#125; T&amp; operator*() const &#123; return *ptr; &#125;&#125;;// class typetemplate &lt;class T&gt;struct iterator_traits &#123; typedef typename T::value_type value_type;&#125;;// 偏特化1template &lt;class T&gt;struct iterator_traits&lt;T*&gt; &#123; typedef T value_type;&#125;;// 偏特化2template &lt;class T&gt;struct iterator_traits&lt;const T*&gt; &#123; typedef T value_type;&#125;;template &lt;class I&gt;typename iterator_traits&lt;I&gt;::value_type// 首先询问iterator_traits&lt;I&gt;::value_type,如果传递的I为指针,则进入特化版本,iterator_traits直接回答;如果传递进来的I为class type,就去询问T::value_type.func(I ite) &#123; std::cout &lt;&lt; "normal version" &lt;&lt; std::endl; return *ite;&#125;int main() &#123; // ... MyIter&lt;int&gt; ite(new int(8)); std::cout &lt;&lt; func(ite)&lt;&lt;std::endl; int *p = new int(52); std::cout&lt;&lt;func(p)&lt;&lt;std::endl; const int k = 3; std::cout&lt;&lt;func(&amp;k)&lt;&lt;std::endl;&#125; 上述的过程是首先询问iterator_traits&lt;I&gt;::value_type，如果传递的 I 为指针, 则进入特化版本,iterator_traits直接回答T; 如果传递进来的I为class type, 就去询问T::value_type. 上述的通俗解释为算法 (func) 问 iterator_traits(我)，但是 iterator_traits(我)发现手上是指针的时候，就由我来替它回答。如果是 class type，iterator_traits(我)就继续问(他 —T::value_type)。 总结：通过定义内嵌类型，我们获得了知晓 iterator 所指元素类型的方法，通过 traits 技法，我们将函数模板对于原生指针和自定义 iterator 的定义都统一起来，我们使用 traits 技法主要是为了解决原生指针和自定义 iterator 之间的不同所造成的代码冗余，这就是 traits 技法的妙处所在。 学习书籍： 侯捷《 STL 源码剖析》 学习文章： https://juejin.im/post/5b1a43fb51882513bf1795c6https://www.cnblogs.com/mangoyuan/p/6446046.htmlhttp://www.cppblog.com/nacci/archive/2005/11/03/911.aspx]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>UML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-Python精要]]></title>
    <url>%2F2021%2F03%2F27%2Fself_cultivation_python%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[python mro问题 怎么实现一个协程库? mock是啥: https://zhuanlan.zhihu.com/p/30380243 import流程 当Python的解释器遇到import语句或者其他上述导入语句时,它会先去查看sys.modules中是否已经有同名模块被导入了, 如果有就直接取来用;没有就去查阅sys.path里面所有已经储存的目录. sys.path这个列表初始化的时候,通常包含一些来自外部的库(external libraries)或者是来自操作系统的一些库,当然也会有一些类似于dist-package的标准库在里面.这些目录通常是被按照顺序或者是直接去搜索想要的–如果说他们当中的一个包含有期望的package或者是module,这个package或者是module将会在整个过程结束的时候被直接提取出来保存在sys.modules中(sys.modules是一个模块名:模块对象的字典结构). 当然，这个 sys.path 是可以修改的（正如上文提到的一种解决办法）。值得注意的是，如果当前目录包含有和标准库同名的模块，会直接使用当前目录的模块而不是标准模块。 当在这些个地址中实在是找不着时,它就会抛出一个ModuleNotFoundError错误. 当我们要导入一个模块（比如 foo ）时，解释器首先会根据命名查找内置模块，如果没有找到，它就会去查找 sys.path 列表中的目录，看目录中是否有 foo.py 。sys.path 的初始值来自于： 运行脚本所在的目录（如果打开的是交互式解释器则是当前目录） PYTHONPATH 环境变量（类似于 PATH 变量，也是一组目录名组成） Python 安装时的默认设置 为啥字符串join比加号连接快字符串是不可变对象，当用操作符+连接字符串的时候，每执行一次+都会申请一块新的内存，然后复制上一个+操作的结果和本次操作的右操作符到这块内存空间，因此用+连接字符串的时候会涉及好几次内存申请和复制。而join在连接字符串的时候，会先计算需要多大的内存存放结果，然后一次性申请所需内存并将字符串复制过去，这是为什么join的性能优于+的原因。所以在连接字符串数组的时候，我们应考虑优先使用join。 is和==的区别官方文档中说 is 表示的是对象标示符（object identity），而 == 表示的是相等（equality）。is 的作用是用来检查对象的标示符是否一致，也就是比较两个对象在内存中的地址是否一样，而 == 是用来检查两个对象是否相等。 我们在检查 a is b 的时候，其实相当于检查 id(a) == id(b)。而检查 a == b 的时候，实际是调用了对象 a 的 eq() 方法，a == b 相当于 a.eq(b)。 一般情况下，如果 a is b 返回True的话，即 a 和 b 指向同一块内存地址的话，a == b 也返回True，即 a 和 b 的值也相等。 元类参考: https://www.liaoxuefeng.com/wiki/1016959663602400/1017592449371072 python元类的使用场景, 比如orm框架, ORM全称“Object Relational Mapping”，即对象-关系映射，就是把关系数据库的一行映射为一个对象，也就是一个类对应一个表，这样，写代码更简单，不用直接操作SQL语句。 type()函数既可以返回一个对象的类型，又可以创建出新的类型，比如，我们可以通过type()函数创建出Hello类，而无需通过class Hello(object)...的定义： 1234567891011\&gt;&gt;&gt; def fn(self, name=&apos;world&apos;): # 先定义函数... print(&apos;Hello, %s.&apos; % name)...&gt;&gt;&gt; Hello = type(&apos;Hello&apos;, (object,), dict(hello=fn)) # 创建Hello class&gt;&gt;&gt; h = Hello()&gt;&gt;&gt; h.hello()Hello, world.&gt;&gt;&gt; print(type(Hello))&lt;class &apos;type&apos;&gt;&gt;&gt;&gt; print(type(h))&lt;class &apos;__main__.Hello&apos;&gt; 要创建一个 class 对象，type()函数依次传入 3 个参数： class 的名称； 继承的父类集合，注意 Python 支持多重继承，如果只有一个父类，别忘了 tuple 的单元素写法； class 的方法名称与函数绑定，这里我们把函数fn绑定到方法名hello上。 通过type()函数创建的类和直接写 class 是完全一样的，因为 Python 解释器遇到 class 定义时，仅仅是扫描一下 class 定义的语法，然后调用type()函数创建出 class。 正常情况下，我们都用class Xxx...来定义类，但是，type()函数也允许我们动态创建出类来，也就是说，动态语言本身支持运行期动态创建类，这和静态语言有非常大的不同，要在静态语言运行期创建类，必须构造源代码字符串再调用编译器，或者借助一些工具生成字节码实现，本质上都是动态编译，会非常复杂。 metaclass除了使用type()动态创建类以外，要控制类的创建行为，还可以使用 metaclass。metaclass，直译为元类，简单的解释就是：当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。但是如果我们想创建出类呢？那就必须根据 metaclass 创建出类，所以：先定义 metaclass，然后创建类。连接起来就是：先定义 metaclass，就可以创建类，最后创建实例。所以，metaclass 允许你创建类或者修改类。换句话说，你可以把类看成是 metaclass 创建出来的 “实例”。我们先看一个简单的例子，这个 metaclass 可以给我们自定义的 MyList 增加一个add方法：定义ListMetaclass，按照默认习惯，metaclass 的类名总是以 Metaclass 结尾，以便清楚地表示这是一个 metaclass：1234class ListMetaclass(type): def __new__(cls, name, bases, attrs): attrs\['add'\] = lambda self, value: self.append(value) return type.__new__(cls, name, bases, attrs) 有了 ListMetaclass，我们在定义类的时候还要指示使用 ListMetaclass 来定制类，传入关键字参数metaclass：12class MyList(list, metaclass=ListMetaclass): pass 当我们传入关键字参数metaclass时，魔术就生效了，它指示 Python 解释器在创建MyList时，要通过ListMetaclass.__new__()来创建，在此，我们可以修改类的定义，比如，加上新的方法，然后，返回修改后的定义。 __new__()方法接收到的参数依次是： 当前准备创建的类的对象； 类的名字； 类继承的父类集合； 类的方法集合。 测试一下MyList是否可以调用add()方法：1234\&gt;&gt;&gt; L = MyList()&gt;&gt;&gt; L.add(1)&gt;&gt; L\[1\] 而普通的list没有add()方法：12345\&gt;&gt;&gt; L2 = list()&gt;&gt;&gt; L2.add(1)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &apos;list&apos; object has no attribute &apos;add&apos; 装饰器12345def log(func): def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper 观察上面的log，因为它是一个decorator，所以接受一个函数作为参数，并返回一个函数。我们要借助Python的@语法，把decorator置于函数的定义处：123@logdef now(): print('2015-3-25') 调用now()函数，不仅会运行now()函数本身，还会在运行now()函数前打印一行日志： 123&gt;&gt;&gt; now()call now():2015-3-25 把@log放到now()函数的定义处，相当于执行了语句：now = log(now)由于log()是一个decorator，返回一个函数，所以，原来的now()函数仍然存在，只是现在同名的now变量指向了新的函数，于是调用now()将执行新函数，即在log()函数中返回的wrapper()函数。 wrapper()函数的参数定义是(args, *kw)，因此，wrapper()函数可以接受任意参数的调用。在wrapper()函数内，首先打印日志，再紧接着调用原始函数。 如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，写出来会更复杂。比如，要自定义log的文本：1234567def log(text): def decorator(func): def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator 这个3层嵌套的decorator用法如下：123@log('execute')def now(): print('2015-3-25') 执行结果如下：123&gt;&gt;&gt; now()execute now():2015-3-25 和两层嵌套的decorator相比，3层嵌套的效果是这样的：now = log(&#39;execute&#39;)(now)我们来剖析上面的语句，首先执行log(‘execute’)，返回的是decorator函数，再调用返回的函数，参数是now函数，返回值最终是wrapper函数。 python命令行参数 -u参数的使用：python命令加上-u（unbuffered）参数后会强制其标准输出也同标准错误一样不通过缓存直接打印到屏幕。 -c参数，支持执行单行命令/脚本。如: python -c &quot;import os;print(&#39;hello&#39;),print(&#39;world&#39;)&quot; python -m test_folder/test.py与python test_folder/test有什么不同桌面的test_folder文件夹下有个test.pytest.py12import sysprint(sys.path) 运行看看:1234567hulinhong@GIH-D-14531 MINGW64 ~/Desktop$ python test_folder/test.py[&apos;C:\\Users\\hulinhong\\Desktop\\test_folder&apos;, &apos;C:\\Program Files\\Python37\\python37.zip&apos;, &apos;C:\\Program Files\\Python37\\DLLs&apos;, &apos;C:\\Program Files\\Python37\\lib&apos;, &apos;C:\\Program Files\\Python37&apos;, &apos;C:\\Program Files\\Python37\\lib\\site-packages&apos;, &apos;C:\\Program Files\\Python37\\lib\\site-packages\\redis_py_cluster-2.1.0-py3.7.egg&apos;]hulinhong@GIH-D-14531 MINGW64 ~/Desktop$ python -m test_folder.test[&apos;C:\\Users\\hulinhong\\Desktop&apos;, &apos;C:\\Program Files\\Python37\\python37.zip&apos;, &apos;C:\\Program Files\\Python37\\DLLs&apos;, &apos;C:\\Program Files\\Python37\\lib&apos;, &apos;C:\\Program Files\\Python37&apos;, &apos;C:\\Program Files\\Python37\\lib\\site-packages&apos;, &apos;C:\\Program Files\\Python37\\lib\\site-packages\\redis_py_cluster-2.1.0-py3.7.egg&apos;] 细心的同学会发现，区别就是在第一个路径: python直接启动是把test.py文件所在的目录放到了sys.path属性中。 模块启动是把你输入命令的目录（也就是当前路径），放到了sys.path属性中 所以就会有下面的情况: 目录结构如下123456package/ __init__.py mod1.pypackage2/ __init__.py run.py run.py 内容如下123import sysfrom package import mod1print(sys.path) 如何才能启动run.py文件？ 直接启动（失败） 12345➜ test_import_project git:(master) ✗ python package2/run.pyTraceback (most recent call last): File &quot;package2/run.py&quot;, line 2, in &lt;module&gt; from package import mod1ImportError: No module named package 以模块方式启动（成功） 1234➜ test_import_project git:(master) ✗ python -m package2.run[&apos;C:\\Users\\hulinhong\\Desktop&apos;,&apos;/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python27.zip&apos;,...] 当需要启动的py文件引用了一个模块。你需要注意：在启动的时候需要考虑sys.path中有没有你import的模块的路径！这个时候，到底是使用直接启动，还是以模块的启动？目的就是把import的那个模块的路径放到sys.path中。你是不是明白了呢？ 官方文档参考： http://www.pythondoc.com/pythontutorial3/modules.html 导入一个叫 mod1 的模块时，解释器先在当前目录中搜索名为 mod1.py 的文件。如果没有找到的话，接着会到 sys.path 变量中给出的目录列表中查找。 sys.path 变量的初始值来自如下： 输入脚本的目录（当前目录）。 环境变量 PYTHONPATH 表示的目录列表中搜索(这和 shell 变量 PATH 具有一样的语法，即一系列目录名的列表)。 Python 默认安装路径中搜索。 实际上，解释器由 sys.path 变量指定的路径目录搜索模块，该变量初始化时默认包含了输入脚本（或者当前目录）， PYTHONPATH 和安装目录。这样就允许 Python程序了解如何修改或替换模块搜索目录。 在python程序中调用cpp的库创建的线程是否受制于GIL?首先要理解什么是GIL.Python 的多线程是真的多线程，只不过在任意时刻，它们中只有一个线程能够取得 GIL 从而被允许执行 Python 代码。其它线程要么等着，要么干别的和 Python 无关的事情（比如等待系统 I/O，或者算点什么东西）。 那如果是通过CPP扩展创建出来的线程，可以摆脱这个限制么？很简单，不访问 Python 的数据和方法，就和 GIL 没任何关系。如果需要访问 Python，还是需要先取得 GIL. GIL 是为了保护 Python 数据不被并发访问破坏，所以当你不访问 Python 的数据的时候自然就可以释放（或者不取得）GIL。反过来，如果需要访问 Python 的数据，就一定要取得 GIL 再访问。PyObject 等不是线程安全的。多线程访问任何非线程安全的数据都需要先取得对应的锁。Python 所有的 PyObject 什么的都共享一个锁，它就叫 GIL。 __new__ 与 __del__ 与 __init__先来看一个单例模式的实现12345678910111213141516171819class Demo: __isinstance = False def __new__(cls, *args, **kwargs): if not cls.__isinstance: # 如果被实例化了 cls.__isinstance = object.__new__(cls) # 否则实例化 return cls.__isinstance # 返回实例化的对象 def __init__(self, name): self.name = name print('my name is %s'%(name)) def __del__(self): print('886, %s'%(self.name))d1 = Demo('Alice')d2 = Demo('Anew')print(d1)print(d2) 打印:12345my name is Alicemy name is Anew&lt;__main__.Demo object at 0x000001446604D3C8&gt;&lt;__main__.Demo object at 0x000001446604D3C8&gt;886, Anew __new__ 是负责对当前类进行实例化，并将实例返回，并传给__init__方法，__init__方法中的self就是指代__new__传过来的对象，所以再次强调，__init__是实例化后调用的第一个方法。 __del__在对象销毁时被调用，往往用于清除数据或还原环境等操作，比如在类中的其他普通方法中实现了插入数据库的语句，当对象被销毁时我们需要将数据还原，那么这时可以在__del__方法中实现还原数据库数据的功能。__del__被成为析构方法，同样和C++中的析构方法类似。 python垃圾回收总体来说，在Python中，主要通过引用计数进行垃圾回收；通过 “标记-清除” 解决容器对象可能产生的循环引用问题；通过 “分代回收” 以空间换时间的方法提高垃圾回收效率。 引用计数 标记清除(Mark and Sweep) 分代回收 标记清除咋弄的参考: https://zhuanlan.zhihu.com/p/83251959 Python 采用了 “标记-清除”(Mark and Sweep) 算法，解决容器对象可能产生的循环引用问题。(注意，只有容器对象才会产生循环引用的情况，比如列表、字典、用户自定义类的对象、元组等。而像数字，字符串这类简单类型不会出现循环引用。作为一种优化策略，对于只包含简单类型的元组也不在标记清除算法的考虑之列) 跟其名称一样，该算法在进行垃圾回收时分成了两步，分别是： A）标记阶段，遍历所有的对象，如果是可达的（reachable），也就是还有对象引用它，那么就标记该对象为可达； B）清除阶段，再次遍历对象，如果发现某个对象没有标记为可达，则就将其回收。 如下图所示，在标记清除算法中，为了追踪容器对象，需要每个容器对象维护两个额外的指针，用来将容器对象组成一个双端链表，指针分别指向前后两个容器对象，方便插入和删除操作。python 解释器 (Cpython) 维护了两个这样的双端链表，一个链表存放着需要被扫描的容器对象，另一个链表存放着临时不可达对象。在图中，这两个链表分别被命名为”Object to Scan”和”Unreachable”。图中例子是这么一个情况：link1,link2,link3 组成了一个引用环，同时 link1 还被一个变量 A(其实这里称为名称 A 更好)引用。link4 自引用，也构成了一个引用环。从图中我们还可以看到，每一个节点除了有一个记录当前引用计数的变量 ref_count 还有一个 gc_ref 变量，这个 gc_ref 是 ref_count 的一个副本，所以初始值为 ref_count 的大小。 gc 启动的时候，会逐个遍历”Object to Scan” 链表中的容器对象，并且将当前对象所引用的所有对象的 gc_ref 减一。(扫描到 link1 的时候，由于 link1 引用了 link2, 所以会将 link2 的 gc_ref 减一，接着扫描 link2, 由于 link2 引用了 link3, 所以会将 link3 的 gc_ref 减一…..) 像这样将”Objects to Scan” 链表中的所有对象考察一遍之后，两个链表中的对象的 ref_count 和 gc_ref 的情况如下图所示。这一步操作就相当于解除了循环引用对引用计数的影响。 接着，gc 会再次扫描所有的容器对象，如果对象的 gc_ref 值为 0，那么这个对象就被标记为 GC_TENTATIVELY_UNREACHABLE，并且被移至”Unreachable” 链表中。下图中的 link3 和 link4 就是这样一种情况。 如果对象的 gc_ref 不为 0，那么这个对象就会被标记为 GC_REACHABLE。同时当 gc 发现有一个节点是可达的，那么他会递归式的将从该节点出发可以到达的所有节点标记为 GC_REACHABLE, 这就是下图中 link2 和 link3 所碰到的情形。 除了将所有可达节点标记为 GC_REACHABLE 之外，如果该节点当前在”Unreachable” 链表中的话，还需要将其移回到”Object to Scan” 链表中，下图就是 link3 移回之后的情形。 第二次遍历的所有对象都遍历完成之后，存在于”Unreachable” 链表中的对象就是真正需要被释放的对象。如上图所示，此时 link4 存在于 Unreachable 链表中，gc 随即释放之。 上面描述的垃圾回收的阶段，会暂停整个应用程序，等待标记清除结束后才会恢复应用程序的运行。 为啥标记清除回收无法回收重写了__del__方法的类对象 Circular references which are garbage are detected when the option cycle detector is enabled (it’s on by default), but can only be cleaned up if there are no Python-level __del__() methods involved. 官方文档中表明启用周期检测器时会检测到垃圾的循环引用（默认情况下它是打开的)，但只有在没有涉及 Python __del__() 方法的情况下才能清除。Python 不知道破坏彼此保持循环引用的对象的安全顺序，因此它则不会为这些方法调用析构函数。简而言之，如果定义了 __del__ 函数，那么在循环引用中Python解释器无法判断析构对象的顺序，因此就不做处理。 分代回收在循环引用对象的回收中，整个应用程序会被暂停，为了减少应用程序暂停的时间，Python 通过“分代回收”(Generational Collection)以空间换时间的方法提高垃圾回收效率。 分代回收是基于这样的一个统计事实，对于程序，存在一定比例的内存块的生存周期比较短；而剩下的内存块，生存周期会比较长，甚至会从程序开始一直持续到程序结束。生存期较短对象的比例通常在 80%～90% 之间，这种思想简单点说就是：对象存在时间越长，越可能不是垃圾，应该越少去收集。这样在执行标记-清除算法时可以有效减小遍历的对象数，从而提高垃圾回收的速度。 python gc给对象定义了三种世代(0,1,2),每一个新生对象在generation zero中，如果它在一轮gc扫描中活了下来，那么它将被移至generation one,在那里他将较少的被扫描，如果它又活过了一轮gc,它又将被移至generation two，在那里它被扫描的次数将会更少。 gc的扫描在什么时候会被触发呢?答案是当某一世代中被分配的对象与被释放的对象之差达到某一阈值的时候，就会触发gc对某一世代的扫描。值得注意的是当某一世代的扫描被触发的时候，比该世代年轻的世代也会被扫描。也就是说如果世代2的gc扫描被触发了，那么世代0,世代1也将被扫描，如果世代1的gc扫描被触发，世代0也会被扫描。 该阈值可以通过下面两个函数查看和调整: 12gc.get_threshold() # (threshold0, threshold1, threshold2).gc.set_threshold(threshold0[, threshold1[, threshold2]]) 下面对set_threshold()中的三个参数threshold0, threshold1, threshold2进行介绍。gc会记录自从上次收集以来新分配的对象数量与释放的对象数量，当两者之差超过threshold0的值时，gc的扫描就会启动，初始的时候只有世代0被检查。如果自从世代1最近一次被检查以来，世代0被检查超过threshold1次，那么对世代1的检查将被触发。相同的，如果自从世代2最近一次被检查以来，世代1被检查超过threshold2次，那么对世代2的检查将被触发。get_threshold()是获取三者的值，默认值为(700,10,10).]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ucontext实现协程]]></title>
    <url>%2F2021%2F03%2F17%2Fcoroutine_illustrated%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[干货写在前面 协程的概念就不详细介绍了,不清楚的同学可以自己google,windows和unix like系统本身就提供了协程的支持,windows下叫fiber,unix like系统下叫ucontext. 协程是一种用户态的轻量级线程。本篇主要研究协程的 C/C++ 的实现。首先我们可以看看有哪些语言已经具备协程语义： 比较重量级的有 C#、erlang、golang* 轻量级有 python、lua、javascript、ruby 还有函数式的 scala、scheme 等。 c/c++ 不直接支持协程语义，但有不少开源的协程库，如：Protothreads：一个 “蝇量级” C 语言协程库libco: 来自腾讯的开源协程库 libco 介绍，官网coroutine: 云风的一个 C 语言同步协程库, 详细信息 目前看到大概有四种实现协程的方式： 第一种：利用 glibc 的 ucontext 组件 (云风的库) 第二种：使用汇编代码来切换上下文 (实现 c 协程) 第三种：利用 C 语言语法 switch-case 的奇淫技巧来实现（Protothreads) 第四种：利用了 C 语言的 setjmp 和 longjmp（ 一种协程的 C/C++ 实现, 要求函数里面使用 static local 的变量来保存协程内部的数据） 本篇主要使用 ucontext 来实现简单的协程库。 . . . ucontext 初接触 利用 ucontext 提供的四个函数getcontext(),setcontext(),makecontext(),swapcontext()可以在一个进程中实现用户级的线程切换。 本节我们先来看 ucontext 实现的一个简单的例子： 12345678910111213#include &lt;stdio.h&gt;#include &lt;ucontext.h&gt;#include &lt;unistd.h&gt; int main(int argc, const char *argv[])&#123; ucontext_t context; getcontext(&amp;context); puts("Hello world"); sleep(1); setcontext(&amp;context); return 0;&#125; 注：示例代码来自维基百科. 保存上述代码到example.c, 执行编译命令： 1gcc example.c -o example 想想程序运行的结果会是什么样？ 12345678910cxy@ubuntu:~$ ./example Hello worldHello worldHello worldHello worldHello worldHello worldHello world^Ccxy@ubuntu:~$ 上面是程序执行的部分输出，不知道是否和你想得一样呢？我们可以看到，程序在输出第一个 “Hello world”后并没有退出程序，而是持续不断的输出”Hello world“。其实是程序通过 getcontext 先保存了一个上下文, 然后输出”Hello world”, 在通过 setcontext 恢复到 getcontext 的地方，重新执行代码，所以导致程序不断的输出”Hello world“，在我这个菜鸟的眼里，这简直就是一个神奇的跳转。 那么问题来了，ucontext 到底是什么？ ucontext 组件到底是什么 在类 System V 环境中, 在头文件 中定义了两个结构类型，mcontext_t和ucontext_t和四个函数getcontext(),setcontext(),makecontext(),swapcontext(). 利用它们可以在一个进程中实现用户级的线程切换。 mcontext_t类型与机器相关，并且不透明.ucontext_t结构体则至少拥有以下几个域: 1234567typedef struct ucontext &#123; struct ucontext *uc_link; sigset_t uc_sigmask; stack_t uc_stack; mcontext_t uc_mcontext; ... &#125; ucontext_t; 当当前上下文 (如使用 makecontext 创建的上下文）运行终止时系统会恢复uc_link指向的上下文；uc_sigmask为该上下文中的阻塞信号集合；uc_stack为该上下文中使用的栈；uc_mcontext保存的上下文的特定机器表示，包括调用线程的特定寄存器等。 下面详细介绍四个函数： 1int getcontext(ucontext_t *ucp); 初始化 ucp 结构体，将当前的上下文保存到 ucp 中 1int setcontext(const ucontext_t *ucp); 设置当前的上下文为 ucp，setcontext 的上下文 ucp 应该通过 getcontext 或者 makecontext 取得，如果调用成功则不返回。如果上下文是通过调用 getcontext() 取得, 程序会继续执行这个调用。如果上下文是通过调用 makecontext 取得, 程序会调用 makecontext 函数的第二个参数指向的函数，如果 func 函数返回, 则恢复 makecontext 第一个参数指向的上下文第一个参数指向的上下文 context_t 中指向的 uc_link. 如果 uc_link 为 NULL, 则线程退出。 1void makecontext(ucontext_t *ucp, void (*func)(), int argc, ...); makecontext 修改通过 getcontext 取得的上下文 ucp(这意味着调用 makecontext 前必须先调用 getcontext)。然后给该上下文指定一个栈空间 ucp-&gt;stack，设置后继的上下文 ucp-&gt;uc_link. 当上下文通过 setcontext 或者 swapcontext 激活后，执行 func 函数，argc 为 func 的参数个数，后面是 func 的参数序列。当 func 执行返回后，继承的上下文被激活，如果继承上下文为 NULL 时，线程退出。 1int swapcontext(ucontext_t *oucp, ucontext_t *ucp); 保存当前上下文到 oucp 结构体中，然后激活 upc 上下文。 如果执行成功，getcontext 返回 0，setcontext 和 swapcontext 不返回；如果执行失败，getcontext,setcontext,swapcontext 返回 - 1，并设置对于的 errno. 简单说来， getcontext获取当前上下文，setcontext设置当前上下文，swapcontext切换上下文，makecontext创建一个新的上下文。 小试牛刀 - 使用 ucontext 组件实现线程切换 虽然我们称协程是一个用户态的轻量级线程，但实际上多个协程同属一个线程。任意一个时刻，同一个线程不可能同时运行两个协程。如果我们将协程的调度简化为：主函数调用协程 1，运行协程 1 直到协程 1 返回主函数，主函数在调用协程 2，运行协程 2 直到协程 2 返回主函数。示意步骤如下： 12345678910执行主函数 切换：主函数 --&gt; 协程1 执行协程1 切换：协程1 --&gt; 主函数 执行主函数 切换：主函数 --&gt; 协程2 执行协程2 切换协程2 --&gt; 主函数 执行主函数 ... 这种设计的关键在于实现主函数到一个协程的切换，然后从协程返回主函数。这样无论是一个协程还是多个协程都能够完成与主函数的切换，从而实现协程的调度。 实现用户线程的过程是： 我们首先调用 getcontext 获得当前上下文 修改当前上下文 ucontext_t 来指定新的上下文，如指定栈空间及其大小，设置用户线程执行完后返回的后继上下文（即主函数的上下文）等 调用 makecontext 创建上下文，并指定用户线程中要执行的函数 切换到用户线程上下文去执行用户线程（如果设置的后继上下文为主函数，则用户线程执行完后会自动返回主函数）。 下面代码context_test函数完成了上面的要求。 12345678910111213141516171819202122232425262728293031323334#include &lt;ucontext.h&gt;#include &lt;stdio.h&gt; void func1(void * arg)&#123; puts("1"); puts("11"); puts("111"); puts("1111"); &#125;void context_test()&#123; char stack[1024*128]; ucontext_t child,main; getcontext(&amp;child); //获取当前上下文 child.uc_stack.ss_sp = stack;//指定栈空间 child.uc_stack.ss_size = sizeof(stack);//指定栈空间大小 child.uc_stack.ss_flags = 0; child.uc_link = &amp;main;//设置后继上下文 makecontext(&amp;child,(void (*)(void))func1,0);//修改上下文指向func1函数 swapcontext(&amp;main,&amp;child);//切换到child上下文，保存当前上下文到main puts("main");//如果设置了后继上下文，func1函数指向完后会返回此处&#125; int main()&#123; context_test(); return 0;&#125; 在 context_test 中，创建了一个用户线程 child, 其运行的函数为 func1. 指定后继上下文为 mainfunc1 返回后激活后继上下文，继续执行主函数。 保存上面代码到 example-switch.cpp. 运行编译命令: 1g++ example-switch.cpp -o example-switch 执行程序结果如下 1234567cxy@ubuntu:~$ ./example-switch1111111111maincxy@ubuntu:~$ 你也可以通过修改后继上下文的设置，来观察程序的行为。如修改代码 1child.uc_link = &amp;main; 为 1child.uc_link = NULL; 再重新编译执行，其执行结果为： 123456cxy@ubuntu:~$ ./example-switch1111111111cxy@ubuntu:~$ 可以发现程序没有打印 “main”，执行为 func1 后直接退出，而没有返回主函数。可见，如果要实现主函数到线程的切换并返回，指定后继上下文是非常重要的。 使用 ucontext 实现自己的协程库 掌握了上一节从主函数到协程的切换的关键，我们就可以开始考虑实现自己的协程了。定义一个协程的结构体如下： 12345678910typedef void (*Fun)(void *arg); typedef struct uthread_t&#123; ucontext_t ctx; Fun func; void *arg; enum ThreadState state; char stack[DEFAULT_STACK_SZIE];&#125;uthread_t; ctx 保存协程的上下文，stack 为协程的栈，栈大小默认为 DEFAULT_STACK_SZIE=128Kb. 你可以根据自己的需求更改栈的大小。func 为协程执行的用户函数，arg 为 func 的参数，state 表示协程的运行状态，包括 FREE,RUNNABLE,RUNING,SUSPEND, 分别表示空闲，就绪，正在执行和挂起四种状态。 在定义一个调度器的结构体 12345678910typedef std::vector&lt;uthread_t&gt; Thread_vector; typedef struct schedule_t&#123; ucontext_t main; int running_thread; Thread_vector threads; schedule_t():running_thread(-1)&#123;&#125;&#125;schedule_t; 调度器包括主函数的上下文 main, 包含当前调度器拥有的所有协程的 vector 类型的 threads，以及指向当前正在执行的协程的编号 running_thread. 如果当前没有正在执行的协程时，running_thread=-1. 接下来，在定义几个使用函数 uthread_create,uthread_yield,uthread_resume 函数已经辅助函数 schedule_finished. 就可以了。 1int uthread_create(schedule_t &amp;schedule,Fun func,void *arg); 创建一个协程，该协程的会加入到 schedule 的协程序列中，func 为其执行的函数，arg 为 func 的执行函数。返回创建的线程在 schedule 中的编号。 1void uthread_yield(schedule_t &amp;schedule); 挂起调度器 schedule 中当前正在执行的协程，切换到主函数。 1void uthread_resume(schedule_t &amp;schedule,int id); 恢复运行调度器 schedule 中编号为 id 的协程 1int schedule_finished(const schedule_t &amp;schedule); 判断 schedule 中所有的协程是否都执行完毕，是返回 1，否则返回 0. 注意：如果有协程处于挂起状态时算作未全部执行完毕，返回 0. 代码就不全贴出来了，我们来看看两个关键的函数的具体实现。首先是 uthread_resume 函数： 1234567891011121314151617181920212223242526272829303132void uthread_resume(schedule_t &amp;schedule , int id)&#123; if(id &lt; 0 || id &gt;= schedule.threads.size())&#123; return; &#125; uthread_t *t = &amp;(schedule.threads[id]); switch(t-&gt;state)&#123; case RUNNABLE: getcontext(&amp;(t-&gt;ctx)); t-&gt;ctx.uc_stack.ss_sp = t-&gt;stack; t-&gt;ctx.uc_stack.ss_size = DEFAULT_STACK_SZIE; t-&gt;ctx.uc_stack.ss_flags = 0; t-&gt;ctx.uc_link = &amp;(schedule.main); t-&gt;state = RUNNING; schedule.running_thread = id; makecontext(&amp;(t-&gt;ctx),(void (*)(void))(uthread_body),1,&amp;schedule); /* !! note : Here does not need to break */ case SUSPEND: swapcontext(&amp;(schedule.main),&amp;(t-&gt;ctx)); break; default: ; &#125;&#125; 如果指定的协程是首次运行，处于 RUNNABLE 状态，则创建一个上下文，然后切换到该上下文。如果指定的协程已经运行过，处于 SUSPEND 状态，则直接切换到该上下文即可。代码中需要注意 RUNNBALE 状态的地方不需要 break. 12345678910void uthread_yield(schedule_t &amp;schedule)&#123; if(schedule.running_thread != -1 )&#123; uthread_t *t = &amp;(schedule.threads[schedule.running_thread]); t-&gt;state = SUSPEND; schedule.running_thread = -1; swapcontext(&amp;(t-&gt;ctx),&amp;(schedule.main)); &#125;&#125; uthread_yield 挂起当前正在运行的协程。首先是将 running_thread 置为 - 1，将正在运行的协程的状态置为 SUSPEND，最后切换到主函数上下文。 更具体的代码我已经放到 github 上, 点击这里。 最后一步 - 使用我们自己的协程库 保存下面代码到 example-uthread.cpp. 123456789101112131415161718192021222324252627282930313233343536373839404142#include "uthread.h"#include &lt;stdio.h&gt; void func2(void * arg)&#123; puts("22"); puts("22"); uthread_yield(*(schedule_t *)arg); puts("22"); puts("22");&#125; void func3(void *arg)&#123; puts("3333"); puts("3333"); uthread_yield(*(schedule_t *)arg); puts("3333"); puts("3333"); &#125; void schedule_test()&#123; schedule_t s; int id1 = uthread_create(s,func3,&amp;s); int id2 = uthread_create(s,func2,&amp;s); while(!schedule_finished(s))&#123; uthread_resume(s,id2); uthread_resume(s,id1); &#125; puts("main over"); &#125;int main()&#123; schedule_test(); return 0;&#125; 执行编译命令并运行： 12g++ example-uthread.cpp -o example-uthread./example-uthread 运行结果如下： 1234567891011cxy@ubuntu:~/mythread$./example-uthread222233333333222233333333main overcxy@ubuntu:~/mythread$ 可以看到，程序协程 func2，然后切换到主函数, 在执行协程 func3，再切换到主函数，又切换到 func2, 在切换到主函数，再切换到 func3, 最后切换到主函数结束。 总结我们利用 getcontext 和 makecontext 创建上下文，设置后继的上下文到主函数，设置每个协程的栈空间。在利用 swapcontext 在主函数和协程之间进行切换。 到此，使用 ucontext 做一个自己的协程库就到此结束了。相信你也可以自己完成自己的协程库了。 最后，代码我已经放到 github 上, 点击这里。 转自: https://blog.csdn.net/qq910894904/article/details/41911175]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Coroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-分布式系统]]></title>
    <url>%2F2021%2F03%2F15%2Fself_cultivation_distributed_system%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[分布式系统分布式系统的就准备: CAP理论 BASE理论 分布式事务 分布式锁 限流 熔断 一致性选举算法 主从架构 集群架构 异地多活 负载均衡 分层架构 微服务, 服务治理 … 共识consensus 准确的翻译是共识，即多个提议者达成共识的过程，例如 Paxos，Raft 就是共识算法，paxos 是一种共识理论，分布式系统是他的场景，一致性是他的目标。 一致性（Consistency）的含义比共识（consensus）要宽泛，一致性指的是多个副本对外呈现的状态。包括顺序一致性、线性一致性、最终一致性等。而共识特指达成一致的过程，但注意，共识并不意味着实现了一致性，一些情况下他是做不到的。 一致性的类别提到分布式架构就一定绕不开 “一致性” 问题，而 “一致性” 其实又包含了数据一致性和事务一致性两种情况，本节主要讨论数据一致性（事务一致性指 ACID）。复制是导致出现数据一致性问题的唯一原因。 关于强和弱的定义，可以参考剑桥大学的 slide. Strong consistency – ensures that only consistent state can be seen: All replicas return the same value when queried for the attribute of an object * All replicas return the same value when queried for the attribute of an object. This may be achieved at a cost – high latency. Weak consistency – for when the “fast access” requirement dominates: update some replica, e.g. the closest or some designated replica the updated replica sends up date messages to all other replicas. different replicas can return different values for the queried attribute of the object the value should be returned, or “not known”, with a timestamp in the long term all updates must propagate to all replicas ……. 一致性的详细分类: 强一致性 强一致性集群中，对任何一个节点发起请求都会得到相同的回复，但可能会产生相对高的延迟 线性一致性(Linearizability consistency, 也叫原子一致性, 大多数时候我们说强一致性其实是指线性一致性) 顺序一致性(Sequential consistency, 比线性一致性稍弱, 但也算是强一致性的一种) 弱一致性 弱一致性具有更低的响应延迟，但可能会回复过期的数据, 导致各个节点拿到的数据不一致。 最终一致性(Eventual consistency) 含义: 系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型。 因果一致性(Causal consistency) 含义: 如果一系列写入按某个逻辑顺序发生，那么任何人读取这些写入时，会看见它们以正确的逻辑顺序出现。 实现: 一种方案是应用保证将问题和对应的回答写入相同的分区 读写一致性: 含义: 它可以保证，如果用户刷新页面，他们总会看到自己刚提交的任何更新。它不会对其他用户的写入做出承诺，其他用户的更新可能稍等才会看到，但它保证用户自己提交的数据能马上被自己看到。 单调读: 含义: 如果先前读取到较新的数据，后续读取不会得到更旧的数据. 实现: 实现单调读取的一种方式是确保每个用户总是从同一个节点进行读取（不同的用户可以从不同的节点读取），比如可以基于用户 ID 的哈希值来选择节点，而不是随机选择节点。 线性一致性etcd 读写都做了线性一致，即 etcd 是标准的强一致性保证。 线性一致性又被称为强一致性、严格一致性、原子一致性。是程序能实现的最高的一致性模型，也是分布式系统用户最期望的一致性。CAP 中的 C 一般就指它顺序一致性中进程只关心大家认同的顺序一样就行，不需要与全局时钟一致，线性就更严格，从这种偏序（partial order）要达到全序（total order） 要求是： 任何一次读都能读到某个数据的最近一次写的数据。 系统中的所有进程，看到的操作顺序，都与全局时钟下的顺序一致。 以下图讨论, B1 看到 x 的新值，C1 反而看到的是旧值。即对用户来说，x 的值发生了回跳。 在线性一致的系统中，如果 B1 看到的 x 值为 1，则 C1 看到的值也一定为 1。任何操作在该系统生效的时刻都对应时间轴上的一个点。如果我们把这些时刻连接起来，如下图中紫线所示，则这条线会一直沿时间轴向前，不会反向回跳。所以任何操作都需要互相比较决定，谁发生在前，谁发生在后。例如 B1 发生在 A0 前，C1 发生在 A0 后。而在前面顺序一致性模型中，我们无法比较诸如 B1 和 A0 的先后关系。 顺序一致性举例说明1下面的图满足了顺序一致，但不满足线性一致。 x 和 y 的初始值为 0 Write(x,4) 代表写入 x=4，Read(y,2) 为读取 y =2 从图上看，进程 P1，P2 的一致性并没有冲突。因为从这两个进程的角度来看，顺序应该是这样的：1Write(y,2), Read(x,0), Write(x,4), Read(y,2) 这个顺序对于两个进程内部的读写顺序都是合理的，只是这个顺序与全局时钟下看到的顺序并不一样。在全局时钟的观点来看，P2 进程对变量 X 的读操作在 P1 进程对变量 X 的写操作之后，然而 P2 读出来的却是旧的数据 0 举例说明2假设我们有个分布式 KV 系统，以下是四个进程 对其的操作顺序和结果:-- 表示持续的时间，因为一次写入或者读取，客户端从发起到响应是有时间的，发起早的客户端，不一定拿到数据就早，有可能因为网络延迟反而会更晚。情况 1：1234A: --W(x,1)----------------------B: --W(x,2)----------------------C: -R(x,1)- --R(x,2)-D: -R(x,1)- --R(x,2)-- 情况 2：1234A: --W(x,1)----------------------B: --W(x,2)----------------------C: -R(x,2)- --R(x,1)-D: -R(x,2)- --R(x,1)-- 上面情况 1 和 2 都是满足顺序一致性的，C 和 D 拿的顺序都是 1-2，或 2-1，只要 CD 的顺序一致，就是满足顺序一致性。只是从全局看来，情况 1 更真实，情况 2 就显得” 错误 “了，因为情况 2 是这样的顺序1B W(x,2) -&gt; A W(x,1) -&gt; C R(x,2) -&gt; D R(x,2) -&gt; C R(x,1) -&gt; D R(x,1) 不过一致性不保证正确性，所以这仍然是一个顺序一致。再加一种情况 3：情况 3：1234A: --W(x,1)----------------------B: --W(x,2)----------------------C: -R(x,2)- --R(x,1)-D: -R(x,1)- --R(x,2)-- 情况 3 就不属于顺序一致了，因为 C 和 D 两个进程的读取顺序不同了。 举例说明3这也是顺序一致的, 但是可能不满足产品经理要求. 从时间轴上可以看到，B0 发生在 A0 之前，读取到的 x 值为 0。B2 发生在 A0 之后，读取到的 x 值为 1。而读操作 B1，C0，C1 与写操作 A0 在时间轴上有重叠，因此他们可能读取到旧的值 0，也可能读取到新的值 1。注意，C1 发生在 B1 之后（二者在时间轴上没有重叠），但是 B1 看到 x 的新值，C1 反而看到的是旧值。即对用户来说，x 的值发生了回跳。 CAP理论一个分布式系统不可能同时满足以下三个基本需求，最多只能同时满足其中两项: 一致性（C：Consistency, CAP的C指的是强一致性） 在分布式环境下，一致性是指数据在多个副本之间能否保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。 对于一个将数据副本分布在不同分布式节点上的系统来说，如果对第一个节点的数据进行了更新操作并且更新成功后，却没有使得第二个节点上的数据得到相应的更新，于是在对第二个节点的数据进行读取操作时，获取的依然是老数据（或称为脏数据），这就是典型的分布式数据不一致的情况。在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都可以读取到其最新的值，那么这样的系统就被认为具有强一致性。 可用性（A：Availability）可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。这里的重点是 “有限时间内” 和 “返回结果”。 “有限时间内” 是指，对于用户的一个操作请求，系统必须能够在指定的时间内返回对应的处理结果，如果超过了这个时间范围，那么系统就被认为是不可用的。另外，”有限的时间内” 是指系统设计之初就设计好的运行指标，通常不同系统之间有很大的不同，无论如何，对于用户请求，系统必须存在一个合理的响应时间，否则用户便会对系统感到失望。 “返回结果” 是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确地反映出队请求的处理结果，即成功或失败，而不是一个让用户感到困惑的返回结果。 分区容错性（P：Partition tolerance）系统应该能持续提供服务，即使系统内部有消息丢失（分区）。 网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络）中，由于一些特殊的原因导致这些子网络出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。 需要注意的是，组成一个分布式系统的每个节点的加入与退出都可以看作是一个特殊的网络分区。 cap通俗而精准的解释一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。 当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。 提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。 然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。 cap的一些组合例子既然一个分布式系统无法同时满足一致性、可用性、分区容错性三个特点，所以我们就需要抛弃一个： 选择 说明 CA 放弃分区容错性，加强一致性和可用性，其实就是传统的单机数据库的选择 AP 放弃一致性（这里说的一致性是强一致性），追求分区容错性和可用性，这是很多分布式系统设计时的选择，例如很多 NoSQL 系统就是如此 CP 放弃可用性，追求一致性和分区容错性，基本不会选择，网络问题会直接让整个系统不可用, 例如 zookeeper和 etcd都是cp的 需要明确的一点是，对于一个分布式系统而言，分区容错性是一个最基本的要求。因为既然是一个分布式系统，那么分布式系统中的组件必然需要被部署到不同的节点，否则也就无所谓分布式系统了，因此必然出现子网络。而对于分布式系统而言，网络问题又是一个必定会出现的异常情况，因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。因此系统架构师往往需要把精力花在如何根据业务特点在 C（一致性）和 A（可用性）之间寻求平衡。 BASE 理论BASE 是 Basically Available（基本可用） Soft state（软状态, 即中间状态) Eventually consistent（最终一致性） 三个短语的缩写。BASE 理论是对 CAP 中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结， 是基于 CAP 定理逐步演化而来的。BASE 理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。接下来看一下 BASE 中的三要素： 基本可用 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。注意，这绝不等价于系统不可用。比如： 响应时间上的损失。正常情况下，一个在线搜索引擎需要在 0.5 秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了 1~2 秒 系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 软状态 软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 最终一致性 最终一致性强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 总的来说，BASE 理论面向的是大型高可用可扩展的分布式系统，和传统的事物 ACID 特性是相反的，它完全不同于 ACID 的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID 特性和 BASE 理论往往又会结合在一起。 服务治理服务治理主要包括: 服务注册发现 限流 监控 网关 负载均衡 日志采集 链路追踪 详细如下图: 分布式锁 主要有: etcd/zookeeper(严谨) redis(遭到质疑, 极限情况有可能有问题, 但因为性能较高且极限情况不容易发生, 也有人用) 分布式锁过期时间到了但业务没执行完怎么办注册一个定时任务，每隔一定时间就去延长锁超时时间 基于etcd的分布式锁因为 etcd 使用 Raft 算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。 保持独占即所有获取锁的用户最终只有一个可以得到。etcd 为此提供了一套实现分布式锁原子操作 CAS（CompareAndSwap）的 API。通过设置prevExist值，可以保证在多个节点同时去创建某个目录时，只有一个成功。而创建成功的用户就可以认为是获得了锁。 基于redis的分布式锁 利用setnx+expire命令 (错误的做法): setnx和expire是分开的两步操作，不具有原子性 使用Lua脚本（包含setnx和expire两条指令） 使用 set key value [EX seconds][PX milliseconds][NX|XX] 命令 (正确做法, 接下来介绍这种) Redis 在 2.6.12 版本开始，为 SET 命令增加一系列选项： 1SET key value\[EX seconds\]\[PX milliseconds\]\[NX|XX\] EX seconds: 设定过期时间，单位为秒 PX milliseconds: 设定过期时间，单位为毫秒 NX: 仅当 key 不存在时设置值 XX: 仅当 key 存在时设置值 value 必须要具有唯一性，我们可以用 UUID 来做，设置随机字符串保证唯一性，至于为什么要保证唯一性？假如 value 不是随机字符串，而是一个固定值，那么就可能存在下面的问题： 1. 客户端 1 获取锁成功2. 客户端 1 在某个操作上阻塞了太长时间3. 设置的 key 过期了，锁自动释放了4. 客户端 2 获取到了对应同一个资源的锁5. 客户端 1 从阻塞中恢复过来，因为 value 值一样，所以执行释放锁操作时就会释放掉客户端 2 持有的锁，这样就会造成问题 所以通常来说，在释放锁时，我们需要对 value 进行验证 释放锁时需要验证 value 值，也就是说我们在获取锁的时候需要设置一个 value，不能直接用 del key 这种粗暴的方式，因为直接 del key 任何客户端都可以进行解锁了，所以解锁时，我们需要判断锁是否是自己的，基于 value 值来判断, 这里使用 Lua 脚本的方式，尽量保证原子性。 致命缺陷:使用 set key value [EX seconds][PX milliseconds][NX|XX] 命令 看上去很 OK，实际上在有 Redis 主从结构的时候也会出现问题，比如说 A 客户端在 Redis 的 master 节点上拿到了锁，但是这个加锁的 key 还没有同步到 slave 节点，master 故障，发生故障转移，一个 slave 节点升级为 master 节点，B 客户端也可以获取同个 key 的锁，但客户端 A 也已经拿到锁了，这就导致多个客户端都拿到锁。 RedLock参考: https://zhuanlan.zhihu.com/p/100140241#RedLock 使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。 尝试从 N 个互相独立 Redis 实例获取锁； 计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从大多数（N / 2 + 1）实例上获取了锁，才认为获取锁成功； 如果获取锁失败，就到每个实例上释放锁 分布式锁的高并发优化先说一个超卖问题的情景, 假设订单系统部署两台机器上，不同的用户都要同时买10台iphone，分别发了一个请求给订单系统。接着每个订单系统实例都去数据库里查了一下，当前iphone库存是12台。俩大兄弟一看，乐了，12台库存大于了要买的10台数量啊！于是乎，每个订单系统实例都发送SQL到数据库里下单，然后扣减了10个库存，其中一个将库存从12台扣减为2台，另外一个将库存从2台扣减为-8台。现在完了，库存出现了负数！泪奔啊，没有20台iphone发给两个用户啊！这可如何是好。 用分布式锁如何解决库存超卖问题: 只有一个订单系统实例可以成功加分布式锁，然后只有他一个实例可以查库存、判断库存是否充足、下单扣减库存，接着释放锁。释放锁之后，另外一个订单系统实例才能加锁，接着查库存，一下发现库存只有2台了，库存不足，无法购买，下单失败。不会将库存扣减为-8的。 分布式锁的方案在高并发场景下有什么问题？ 分布式锁一旦加了之后，对同一个商品的下单请求，会导致所有客户端都必须对同一个商品的库存锁key进行加锁。比如，对iphone这个商品的下单，都必对“iphone_stock”这个锁key来加锁。这样会导致对同一个商品的下单请求，就必须串行化，一个接一个的处理,假设加锁之后，释放锁之前，查库存 -&gt; 创建订单 -&gt; 扣减库存，这个过程性能很高吧，算他全过程20毫秒，这应该不错了。那么1秒是1000毫秒，只能容纳50个对这个商品的请求依次串行完成处理。效率低下. 假如下单时，用分布式锁来防止库存超卖，但是是每秒上千订单的高并发场景，如何对分布式锁进行高并发优化来应对这个场景？解决方案: 分段加锁 其实说出来也很简单，相信很多人看过java里的ConcurrentHashMap的源码和底层原理，应该知道里面的核心思路，就是分段加锁！ 在某些情况下我们可以将锁分解技术进一步扩展为一组独立对象上的锁进行分解，这成为分段锁。其实说的简单一点就是：容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 比如：在ConcurrentHashMap中使用了一个包含16个锁的数组，每个锁保护所有散列桶的1/16，其中第N个散列桶由第（N mod 16）个锁来保护。假设使用合理的散列算法使关键字能够均匀的分部，那么这大约能使对锁的请求减少到越来的1/16。也正是这项技术使得ConcurrentHashMap支持多达16个并发的写入线程。 假如你现在iphone有1000个库存，那么你完全可以给拆成20个库存段，要是你愿意，可以在数据库的表里建20个库存字段，比如stock_01，stock_02，类似这样的，也可以在redis之类的地方放20个库存key。总之，就是把你的1000件库存给他拆开，每个库存段是50件库存，比如stock_01对应50件库存，stock_02对应50件库存。接着，每秒1000个请求过来了，好！此时其实可以是自己写一个简单的随机算法，每个请求都是随机在20个分段库存里，选择一个进行加锁。这样就好了，同时可以有最多20个下单请求一起执行，每个下单请求锁了一个库存分段，然后在业务逻辑里面，就对数据库或者是Redis中的那个分段库存进行操作即可，包括查库存 -&gt; 判断库存是否充足 -&gt; 扣减库存。 有一个坑大家一定要注意：如果某个下单请求，咔嚓加锁，然后发现这个分段库存里的库存不足了，此时咋办？这时你得自动释放锁，然后立马换下一个分段库存，再次尝试加锁后尝试处理。这个过程一定要实现。 分布式事务解决方案参考: https://www.cnblogs.com/mayundalao/p/11798502.html https://zhuanlan.zhihu.com/p/88226625 https://xiaomi-info.github.io/2020/01/02/distributed-transaction/ https://zhuanlan.zhihu.com/p/183753774 事务有两种: 刚性事务：遵循ACID原则，强一致性。 柔性事务：遵循BASE理论，最终一致性；与刚性事务不同，柔性事务允许一定时间内，不同节点的数据不一致，但要求最终一致。 二阶段提交2PC 大致的流程： 第一阶段（prepare）：事务管理器向所有本地资源管理器发起请求，询问是否是 ready 状态，所有参与者都将本事务能否成功的信息反馈发给协调者； 第二阶段 (commit/rollback)：事务管理器根据所有本地资源管理器的反馈，通知所有本地资源管理器，步调一致地在所有分支上提交或者回滚。 缺点: 同步阻塞：当参与事务者存在占用公共资源的情况，其中一个占用了资源，其他事务参与者就只能阻塞等待资源释放，处于阻塞状态。 单点故障：一旦事务管理器出现故障，整个系统不可用 数据不一致：在阶段二，如果事务管理器只发送了部分 commit 消息，此时网络发生异常，那么只有部分参与者接收到 commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。 不确定性：当协事务管理器发送 commit 之后，并且此时只有一个参与者收到了 commit，那么当该参与者与事务管理器同时宕机之后，重新选举的事务管理器无法确定该条消息是否提交成功。 实战:目前支付宝使用两阶段提交思想实现了分布式事务服务 (Distributed Transaction Service, DTS) ，它是一个分布式事务框架，用来保障在大规模分布式环境下事务的最终一致性。具体可参考支付宝官方文档：https://tech.antfin.com/docs/2/46887 TCC关于 TCC（Try-Confirm-Cancel）的概念，最早是由 Pat Helland 于 2007 年发表的一篇名为《Life beyond Distributed Transactions:an Apostate’s Opinion》的论文提出。 TCC 事务机制相比于上面介绍的 XA，解决了其几个缺点： 解决了协调者单点，由主业务方发起并完成这个业务活动。业务活动管理器也变成多点，引入集群。 同步阻塞：引入超时，超时后进行补偿，并且不会锁定整个资源，将资源转换为业务逻辑形式，粒度变小。 数据一致性，有了补偿机制之后，由业务活动管理器控制一致性 TCC(Try Confirm Cancel) Try 阶段：尝试执行，完成所有业务检查（一致性）, 预留必须业务资源（准隔离性） Confirm 阶段：确认执行真正执行业务，不作任何业务检查，只使用 Try 阶段预留的业务资源，Confirm 操作满足幂等性。要求具备幂等设计，Confirm 失败后需要进行重试。 Cancel 阶段：取消执行，释放 Try 阶段预留的业务资源 Cancel 操作满足幂等性 Cancel 阶段的异常和 Confirm 阶段异常处理方案基本上一致。 在 Try 阶段，是对业务系统进行检查及资源预览，比如订单和存储操作，需要检查库存剩余数量是否够用，并进行预留，预留操作的话就是新建一个可用库存数量字段，Try 阶段操作是对这个可用库存数量进行操作。基于 TCC 实现分布式事务，会将原来只需要一个接口就可以实现的逻辑拆分为 Try、Confirm、Cancel 三个接口，所以代码实现复杂度相对较高。 缺点:TCC 需要事务接口提供 try, confirm, cancel 三个接口，提高了编程的复杂性。依赖于业务方来配合提供这样的接口，推行难度大，所以一般不推荐使用这种方式。 实战:一般来说和钱相关的支付、交易等相关的场景，也可以用TCC，严格严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性! 本地消息表本地消息表这个方案最初是 ebay 架构师 Dan Pritchett 在 2008 年发表给 ACM 的文章。该方案中会有消息生产者与消费者两个角色，假设系统 A 是消息生产者，系统 B 是消息消费者，其大致流程如下： 当系统 A 被其他系统调用发生数据库表更操作，首先会更新数据库的业务表，其次会往相同数据库的消息表中插入一条数据，两个操作发生在同一个事务中, 如果本步骤发生操作失败, 则直接事务回滚 系统 A 的脚本定期轮询本地消息往 mq 中写入一条消息，如果消息发送失败会进行重试 系统 B 消费 mq 中的消息，并处理业务逻辑。如果本地事务处理失败，会在继续消费 mq 中的消息进行重试，如果业务上的失败，可以通知系统 A 进行回滚操作 本地消息表实现的条件： 消费者与生成者的接口都要支持幂等 生产者需要额外的创建消息表 需要提供补偿逻辑，如果消费者业务失败，需要生产者支持回滚操作 此方案的核心是将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。人工重试更多的是应用于支付场景，通过对账系统对事后问题的处理。 缺点:最大的问题就在于严重依赖于数据库的消息表来管理事务,这个会导致高并发场景无力,难以扩展,一般很少用 实战:跨行转账可通过该方案实现。用户 A 向用户 B 发起转账，首先系统会扣掉用户 A 账户中的金额，将该转账消息写入消息表中，如果事务执行失败则转账失败，如果转账成功，系统中会有定时轮询消息表，往 mq 中写入转账消息，失败重试。mq 消息会被实时消费并往用户 B 中账户增加转账金额，执行失败会不断重试。 小米海外商城用户订单数据状态变更，会将变更状态记录消息表中，脚本将订单状态消息写入 mq，最终消费 mq 给用户发送邮件、短信、push 等。 可靠消息最终一致性大致流程如下： A 系统先向 mq 发送一条 prepare 消息，如果 prepare 消息发送失败，则直接取消操作 如果消息发送成功，则执行本地事务 如果本地事务执行成功，则向 mq 发送一条 confirm 消息，如果发送失败，则发送回滚消息 B 系统定期消费 mq 中的 confirm 消息，执行本地事务，并发送 ack 消息。如果 B 系统中的本地事务失败，会一直不断重试，如果是业务失败，会向 A 系统发起回滚请求 mq 会定期轮询所有 prepared 消息调用系统 A 提供的接口查询消息的处理情况，如果该 prepare 消息本地事务处理成功，则重新发送 confirm 消息，否则直接回滚该消息 该方案与本地消息最大的不同是去掉了本地消息表，其次本地消息表依赖消息表重试写入 mq 这一步由本方案中的轮询 prepare 消息状态来重试或者回滚该消息替代。其实现条件与容错方案基本一致。 实战:目前市面上实现该方案的只有阿里的 RocketMq。 尽最大努力通知最大努力通知其实就是定期校对, 是最简单的一种柔性事务，适用于一些最终一致性时间敏感度低的业务，且被动方处理结果 不影响主动方的处理结果。 业务活动的主动方，在完成业务处理之后，向业务活动的被动方发送消息，允许消息丢失。主动方可以设置时间阶梯型通知规则，在通知失败后按规则重复通知，直到通知N次后不再通知。主动方提供校对查询接口给被动方按需校对查询，用于恢复丢失的业务消息。业务活动的被动方如果正常接收了数据，就正常返回响应，并结束事务。如果被动方没有正常接收，根据定时策略，向业务活动主动方查询，恢复丢失的业务消息 最大努力通知方案的特点: 用到的服务模式：可查询操作、幂等操作。 被动方的处理结果不影响主动方的处理结果；适用于对业务最终一致性的时间敏感度低的系统, 比如适合跨企业的系统间的操作，或者企业内部比较独立的系统间的操作，比如银行通知、商户通知等； 这个方案的大致意思就是： 系统 A 本地事务执行完之后，发送个消息到 MQ； 会有个专门消费 MQ 的服务 notify_service(即最大努力通知服务) ，这个服务会消费 MQ 并调用系统 B 的接口； 要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么 notify_service (即最大努力通知服务)就定时尝试重新调用系统 B, 反复 N 次，最后还是不行就放弃。 实战:小米海外商城目前除了支付回调外，最常用的场景是订单数据同步。例如系统 A、B 进行数据同步，当系统 A 发生订单数据变更，先将数据变更消息写入小米 notify 系统（作用等同 mq），然后 notify 系统异步处理该消息来调用系统 B 提供的接口并进行重试到最大次数。 负载均衡算法有哪些 轮询法 将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器，而不关心服务器实际的连接数和当前的系统负载。 随机法 通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。由概率统计理论可以得知，随着客户端调用服务端的次数增多，其实际效果越来越接近于平均分配调用量到后端的每一台服务器，也就是轮询的结果。 源地址哈希法 源地址哈希的思想是根据获取客户端的 IP 地址，通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。采用源地址哈希法进行负载均衡，同一 IP 地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问。 加权轮询法 不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此它们的抗压能力也不相同。给配置高、负载低的机器配置更高的权重，让其处理更多的请；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载，加权轮询能很好地处理这一问题，并将请求顺序且按照权重分配到后端。加权轮询算法的结果，就是要生成一个服务器序列。每当有请求到来时，就依次从该序列中取出下一个服务器用于处理该请求。比如针对c权重4, b权重2, a权重1的例子，加权轮询算法会生成序列{c, c, b, c, a, b, c}也有可能是{a, a, a, a, a, b, c}, 有可能不均匀, 前五个请求都会分配给服务器a。在Nginx源码中，实现了一种叫做平滑的加权轮询（smooth weighted round-robin balancing）的算法，它生成的序列更加均匀。比如前面的例子，它生成的序列为{ a, a, b, a, c, a, a}，转发给后端a的5个请求现在分散开来，不再是连续的。这样，每收到7个客户端的请求，会把其中的1个转发给后端a，把其中的2个转发给后端b，把其中的4个转发给后端c。收到的第8个请求，重新从该序列的头部开始轮询。 普通加权轮询法 平滑加权轮询法 加权随机法 与加权轮询法一样，加权随机法也根据后端机器的配置，系统的负载分配不同的权重。不同的是，它是按照权重随机请求后端服务器，而非顺序。 最小连接数法 最小连接数算法比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，它是根据后端服务器当前的连接情况，动态地选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器。 负载均衡的平滑加权轮询算法怎么实现当我们需要把一份数据发送到一个Set中的任意机器的时候，很容易想到的一个问题是，如何挑Set中的机器作为数据的接收方？显然算法需要符合以下要求： 支持加权，以便在机器故障时可以降低其权重 在加权的前提下，尽可能地把请求平摊到每台机器上 第一点很好理解，而第二点的意思是，比如说我们现在有a, b, c三个选择，权重分别是5, 1, 1，我们希望输出的结果是类似于a, a, b, a, c, a, a，而不是a, a, a, a, a, b, c。 从Github上面可以看到，Nginx以前也是使用和LVS类似的算法，并在某一次提交中修改为当前的算法，该算法大致思想如下：123456789101112131415161718192021222324252627282930313233343536Upstream: smooth weighted round-robin balancing.For edge case weights like &#123; 5, 1, 1 &#125; we now produce &#123; a, a, b, a, c, a, a &#125;sequence instead of &#123; c, b, a, a, a, a, a &#125; produced previously.Algorithm is as follows: on each peer selection we increase current_weightof each eligible peer by its weight, select peer with greatest current_weightand reduce its current_weight by total number of weight points distributedamong peers.In case of &#123; 5, 1, 1 &#125; weights this gives the following sequence ofcurrent_weight&apos;s: a b c 0 0 0 (initial state) 5 1 1 (a selected) -2 1 1 3 2 2 (a selected) -4 2 2 1 3 3 (b selected) 1 -4 3 6 -3 4 (a selected) -1 -3 4 4 -2 5 (c selected) 4 -2 -2 9 -1 -1 (a selected) 2 -1 -1 7 0 0 (a selected) 0 0 0 该算法除了有权重weight，还引入了另一个变量current_weight，在每一次遍历中会把current_weight加上weight的值，并选择current_weight最大的元素，对于被选择的元素，再把current_weight减去所有权重之和。 假设有 N 台服务器 S = {S0, S1, S2, …, Sn}，默认权重为 W = {W0, W1, W2, …, Wn}，当前权重为 CW = {CW0, CW1, CW2, …, CWn}。在该算法中有两个权重，默认权重表示服务器的原始权重，当前权重表示每次访问后重新计算的权重，当前权重的出初始值为默认权重值，当前权重值最大的服务器为 maxWeightServer，所有默认权重之和为 weightSum，服务器列表为 serverList，算法可以描述为： 找出当前权重值最大的服务器 maxWeightServer； 计算 {W0, W1, W2, …, Wn} 之和 weightSum； 将 maxWeightServer.CW = maxWeightServer.CW - weightSum； 重新计算 {S0, S1, S2, …, Sn} 的当前权重 CW，计算公式为 Sn.CW = Sn.CW + Sn.Wn 返回 maxWeightServer 服务发现是怎么实现的参考 我们可以考虑用etcd来做,服务发现要解决的也是分布式系统中最常见的问题之一，即在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发现就是想要了解集群中是否有进程在监听 udp 或 tcp 端口，并且通过名字就可以查找和连接。要解决服务发现的问题，需要有下面三大支柱，缺一不可。 一个强一致性、高可用的服务存储目录。基于 Raft 算法的 etcd 天生就是这样一个强一致性高可用的服务存储目录。 一种提供方的注册服务。提供方可以在 etcd 中注册服务，并且对注册的服务设置key TTL，定时保持服务的心跳以达到监控健康状态的效果, 比如每隔 30s 发送一次心跳设置一下这个key使代表该机器存活的节点继续存在，否则当etcd 没有检测到心跳这个key的ttl到了过期了就会把这个键值对删了 需求方可以即时更新提供方服务状态的机制.需求方通过watch机制监听自己需要用到的提供方信息的改动，提供方相关信息有变动的时候需求方就会收到消息,在接收到信息变动的时候立即从etcd获取相应最新的信息即可, 实现方式通常是这样：不同系统都在 etcd 上对同一个目录进行注册，同时设置 Watcher 观测该目录的变化（如果对子目录的变化也有需要，可以设置递归模式），当某个系统更新了 etcd 的目录，那么设置了 Watcher 的系统就会收到通知，并作出相应处理。 下面我们来看服务发现对应的具体场景。微服务协同工作架构中，服务动态添加。随着 Docker 容器的流行，多种微服务共同协作，构成一个相对功能强大的架构的案例越来越多。透明化的动态添加这些服务的需求也日益强烈。通过服务发现机制，在 etcd 中注册某个服务名字的目录，在该目录下存储可用的服务节点的 IP。在使用服务的过程中，只要从服务目录下查找可用的服务节点去使用即可。 熔断是怎么实现的什么是服务熔断呢？ 服务熔断：当下游的服务因为某种原因突然变得不可用或响应过慢，上游服务为了保证自己整体服务的可用性，不再继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。 需要说明的是熔断其实是一个框架级的处理，那么这套熔断机制的设计，基本上业内用的是断路器模式: 最开始处于closed状态，一旦检测到错误到达一定阈值，便转为open状态； 这时候会有个 reset timeout，到了这个时间了，会转移到half open状态； 尝试放行一部分请求到后端，一旦检测成功便回归到closed状态，即恢复服务； 业内目前流行的熔断器很多，例如阿里出的 Sentinel, 以及最多人使用的 Hystrix 在 Hystrix 中，对应配置如下123456//滑动窗口的大小，默认为20circuitBreaker.requestVolumeThreshold //过多长时间，熔断器再次检测是否开启，默认为5000，即5s钟circuitBreaker.sleepWindowInMilliseconds //错误率，默认50%circuitBreaker.errorThresholdPercentage 每当 20 个请求中，有 50% 失败时，熔断器就会打开，此时再调用此服务，将会直接返回失败，不再调远程服务。直到 5s 钟之后，重新检测该触发条件，判断是否把熔断器关闭，或者继续打开。这些属于框架层级的实现，我们只要实现对应接口就好！ 服务降级 降级的本质: 降级就是为了解决资源不足和访问量增加的矛盾 在有限的资源情况下，为了能抗住大量的请求，就需要对系统做出一些牺牲，有点“弃卒保帅”的意思。放弃一些功能，保证整个系统能平稳运行 降级牺牲的是: 强强一致性变成最终一致性 大多数的系统是不需要强一致性的。 强一致性就要求多种资源的占用，减少强一致性就能释放更多资源 这也是我们一般利用消息中间件来削峰填谷，变强一致性为最终一致性，也能达到效果 干掉一些次要功能 停止访问不重要的功能，从而释放出更多的资源 举例来说，比如电商网站，评论功能流量大的时候就能停掉，当然能不直接干掉就别直接，最好能简化流程或者限流最好简化功能流程。把一些功能简化掉 降级的注意点: 对业务进行仔细的梳理和分析 哪些是核心流程必须保证的，哪些是可以牺牲的 什么指标下能进行降级 吞吐量、响应时间、失败次数等达到一个阈值才进行降级处理 如何降级: 降级最简单的就是在业务代码中配置一个开关或者做成配置中心模式，直接在配置中心上更改配置，推送到相应的服务。 限流限流就是通过对并发访问进行限速。限流的实现方式: 计数器: 最简单的实现方式 ，维护一个计数器，来一个请求计数加一，达到阈值时，直接拒绝请求。 一般实践中用 ngnix + lua + redis 这种方式，redis 存计数值 漏斗模式: 流量就像进入漏斗中的水一样，而出去的水和我们系统处理的请求一样，当流量大于漏斗的流出速度，就会出现积水，水多了会溢出, 漏斗很多是用一个队列实现的，当流量过多时，队列会出现积压，队列满了，则开始拒绝请求。 令牌桶: 看图例，令牌通和漏斗模式很像，主要的区别是增加了一个中间人，这个中间人按照一定的速率放入一些token，然后，处理请求时，需要先拿到token才能处理，如果桶里没有token可以获取，则不进行处理。 熔断-降级-限流三者的关系 熔断强调的是服务之间的调用能实现自我恢复的状态； 限流是从系统的流量入口考虑，从进入的流量上进行限制，达到保护系统的作用； 降级，是从系统内部的平级服务或者业务的维度考虑，流量大了，可以干掉一些，保护其他正常使用； 熔断是降级方式的一种；降级又是限流的一种方式；三者都是为了通过一定的方式去保护流量过大时，保护系统的手段。 id生成器如何实现全局递增参考 介绍一下美团在用的工业级 Leaf-snowflake 方案。 41-bit的时间是毫秒级时间, 可以表示(1L&lt;&lt;41)/(1000L*3600*24*365)=69年的时间， 10-bit机器可以分别表示1024台机器。如果我们对IDC划分有需求，还可以将10-bit分5-bit给IDC，分5-bit给工作机器。这样就可以表示32个IDC，每个IDC下可以有32台机器，可以根据自身需求定义。 12个自增序列号可以表示2^12个ID，理论上snowflake方案的QPS约为409.6w/s， 这种分配方式可以保证在任何一个IDC的任何一台机器在任意毫秒内生成的ID都是不同的 优点： 毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。 不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。 可以根据自身业务特性分配bit位，非常灵活。 缺点：强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。 解决时钟回拨问题解决方案: 由于强依赖时钟，对时间的要求比较敏感，在机器工作时 NTP 同步也会造成秒级别的回退，建议可以直接关闭 NTP 同步。 在时钟回拨的时候直接不提供服务直接返回 ERROR_CODE，等时钟追上即可 偏差在5秒之内, 比如就等待2倍的偏差时间(比如比上次的还小3秒, 那我就等6秒)。然后做一层重试, 如果还是小于上次的时间, 就上报报警系统 更或者是发现有时钟回拨之后自动摘除本身节点并报警 代码如下：123456789101112131415161718192021//发生了回拨，此刻时间小于上次发号时间if (timestamp &lt; lastTimestamp) &#123; long offset = lastTimestamp - timestamp; if (offset &lt;= 5) &#123; try &#123; //时间偏差大小小于5ms，则等待两倍时间 wait(offset &lt;&lt; 1);//wait timestamp = timeGen(); if (timestamp &lt; lastTimestamp) &#123; //还是小于，抛异常并上报 throwClockBackwardsEx(timestamp); &#125; &#125; catch (InterruptedException e) &#123; throw e; &#125; &#125; else &#123; //throw throwClockBackwardsEx(timestamp); &#125;&#125; //分配ID 从上线情况来看，在 2017 年闰秒出现那一次出现过部分机器回拨，由于 Leaf-snowflake 的策略保证，成功避免了对业务造成的影响。Leaf 在美团点评公司内部服务包含金融、支付交易、餐饮、外卖、酒店旅游、猫眼电影等众多业务线。目前 Leaf 的性能在 4C8G 的机器上 QPS 能压测到近 5w/s，TP999 1ms，已经能够满足大部分的业务的需求。每天提供亿数量级的调用量，作为公司内部公共的基础技术设施，必须保证高 SLA 和高性能的服务，我们目前还仅仅达到了及格线，还有很多提高的空间。]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-CPP要点]]></title>
    <url>%2F2021%2F03%2F13%2Fself_cultivation_cpp%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[C++参考: 看之前一个哥们总结的c++要点 https://interview.huihut.com/ new 和 delete 为什么要配对用: 12345class A&#123;//...&#125;;A *pa = new A();A *pas = new A[NUM](); delete []pas; //详细流程: delete[] pas 用来释放pas指向的内存！！还逐一调用数组中每个对象的destructor！！ delete []pa; //会发生什么, 答案是调用未知次数的A的析构函数. 因为delete[]会去通过pa+offset找一个基于pa的偏移量找一个内存里的数据, 他假定这个内存里放了要调用析构的次数n这个数据, 而这个内存里到底是多少是未知的. delete pas; //哪些指针会变成野指针, 答案是pas和A[0]中的指针会变成野指针. 因为只有这两个指针指向的内存被释放了, 也就是说, 仅释放了pas指针指向的这个数组的全部内存空间, 以及只调用了a[0]对象的析构函数 cqq vec set map list vector和string的内存分配与使用注意点 stl关联容器的特性 map的[]和insert的区别? insert 含义是：如果key存在，则插入失败，如果key不存在，就创建这个key－value。实例: map.insert((key, value)) 利用下标操作的含义是：如果这个key存在，就更新value；如果key不存在，就创建这个key－value对 实例：map[key] = value vector的resize和reserve的区别? 总结: resize既分配了空间，也创建了对象，可以通过下标访问。当new_size大于原size, 则resize既修改capacity大小，也修改size大小。否则只修改size大小. reserve只分配了空间, 也就是说它只修改capacity大小，不修改size大小, 若 new_cap 小于等于当前的 capacity(), 它啥也不干. resize: 重设容器大小以容纳 count 个元素。 若当前大小大于 count ，则减小容器为其首 count 个元素。 若当前大小小于 count: 则后附额外的默认插入的元素 则后附额外的 value 的副本 reserve: 增加 vector 的容量到大于或等于 new_cap 的值。若 new_cap 大于当前的 capacity() ，则分配新存储，否则该方法不做任何事。reserve() 不更改 vector 的 size 。若 new_cap 大于 capacity() ，则所有迭代器，包含尾后迭代器和所有到元素的引用都被非法化。否则，没有迭代器或引用被非法化。 字节对齐 对象模型之内存对齐基础 定位new 12345678910111213141516171819202122#include &lt;iostream&gt;using namespace std;int main() &#123; char buffer[512]; //chunk of memory内存池 int *p2, *p3; //定位new: p2 = new (buffer) int[10]; p2[0] = 99; p2[1] = 88; cout &lt;&lt; "buffer = " &lt;&lt;(void *)buffer &lt;&lt; endl; //内存池地址 cout &lt;&lt; "p2 = " &lt;&lt; p2 &lt;&lt; endl; //定位new指向的地址 cout &lt;&lt; "p2[0] = " &lt;&lt; p2[0] &lt;&lt; endl; p3 = new (buffer) int[2]; p3[0] = 1; p3[1] = 2; cout &lt;&lt; "p3 = " &lt;&lt; p3 &lt;&lt; endl; cout &lt;&lt; "p2[0] = " &lt;&lt; p2[0] &lt;&lt; endl; cout &lt;&lt; "p2[1] = " &lt;&lt; p2[1] &lt;&lt; endl; cout &lt;&lt; "p2[2] = " &lt;&lt; p2[2] &lt;&lt; endl; cout &lt;&lt; "p2[3] = " &lt;&lt; p2[3] &lt;&lt; endl; return 0;&#125; 结果发现p3和p2还有buffer都是使用同样的内存地址，符合指定地址的内存块，而且p3在指定位置覆盖了p2的前两处的值。 c++一个空类会生成什么 (答: 默认构造/析构(非虚)/赋值运算符/默认拷贝/取地址/const取地址) 虚函数（virtual）可以是内联函数（inline）吗？ 虚函数可以是内联函数，内联是可以修饰虚函数的，但是当虚函数表现多态性的时候不能内联。 内联是在编译器建议编译器内联，而虚函数的多态性在运行期，编译器无法知道运行期调用哪个代码，因此虚函数表现为多态性时（运行期）不可以内联。 inline virtual 唯一可以内联的时候是：编译器知道所调用的对象是哪个类（如 Base::who()），这只有在编译器具有实际对象而不是对象的指针或引用时才会发生。虚函数内联使用:12345678910// 此处的虚函数 who()，是通过类（Base）的具体对象（b）来调用的，// 编译期间就能确定了，所以它可以是内联的，// 但最终是否内联取决于编译器。 Base b;b.who();// 此处的虚函数是通过指针调用的，呈现多态性，// 需要在运行时期间才能确定，所以不能为内联。 Base *ptr = new Derived();ptr-&gt;who(); 虚函数指针、虚函数表 虚函数指针：在含有虚函数类的对象中，指向虚函数表，在运行时确定。 虚函数表：在程序内存的只读数据段（.rodata section，见：CPP目标文件内存布局），存放虚函数指针，如果派生类实现了基类的某个虚函数，则在虚表中覆盖原本基类的那个虚函数指针，在编译时根据类的声明创建。 virtual修饰符: 如果一个类是局部变量则该类数据存储在栈区，如果一个类是通过new/malloc动态申请的，则该类数据存储在堆区。 如果该类是virutal继承而来的子类，则该类的虚函数表指针和该类其他成员一起存储。虚函数表指针指向只读数据段中的类虚函数表，虚函数表中存放着一个个函数指针，函数指针指向代码段中的具体函数。 内存泄漏的工具 vargrid..? 还有啥工具 了解ASAN查找内存越界问题 cpp找找冰川, 大梦龙图的面试题，网上常用题 gdb怎么切换线程 C++ 的动态多态怎么实现的？ C++ 的构造函数可以是虚函数吗？ 无锁队列原理是否一定比有锁快?(不一定, 如果临界区小因为有上下文切换则mutex慢, 再来看lockfree的spin，一般都遵循一个固定的格式：先把一个不变的值X存到某个局部变量A里，然后做一些计算，计算/生成一个新的对象，然后做一个CAS操作，判断A和X还是不是相等的，如果是，那么这次CAS就算成功了，否则再来一遍。如果上面这个loop里面“计算/生成一个新的对象”非常耗时并且contention很严重，那么lockfree性能有时会比mutex差。另外lockfree不断地spin引起的CPU同步cacheline的开销也比mutex版本的大。关于ABA问题) 编译过程 预处理(Preprocessing): 做一些类似于将所有的#define删除，并且展开所有的宏定义的操作, 然后生成hello.i 编译(Compilation): 编译过程就是把预处理完的文件进行一系列的词法分析，语法分析，语义分析及优化后生成相应的汇编代码。得到hello.a 汇编(Assembly): 汇编器是将汇编代码转变成机器可以执行的命令，每一个汇编语句几乎都对应一条机器指令。汇编相对于编译过程比较简单，根据汇编指令和机器指令的对照表一一翻译即可。得到hello.o 链接(Linking): 通过调用链接器ld来链接程序运行需要的一大堆目标文件，以及所依赖的其它库文件，最后生成可执行文件 静态链接: 指在编译阶段直接把静态库加入到可执行文件中去，这样可执行文件会比较大 动态链接: 指链接阶段仅仅只加入一些描述信息，而程序执行时再从系统中把相应动态库加载到内存中去。 目标文件编译器编译源代码后生成的文件叫做目标文件。目标文件从结构上讲，它是已经编译后的可执行文件格式，只是还没有经过链接的过程，其中可能有些符号或有些地址还没有被调整。 可执行文件（Windows 的 .exe 和 Linux 的 ELF）、动态链接库（Windows 的 .dll 和 Linux 的 .so）、静态链接库（Windows 的 .lib 和 Linux 的 .a）都是按照可执行文件格式存储（Windows 按照 PE-COFF，Linux 按照 ELF） 目标文件格式: Windows 的 PE（Portable Executable），或称为 PE-COFF，.obj 格式 Linux 的 ELF（Executable Linkable Format），.o 格式 Intel/Microsoft 的 OMF（Object Module Format） Unix 的 a.out 格式 MS-DOS 的 .COM 格式 PE 和 ELF 都是 COFF（Common File Format）的变种 CPP目标文件内存布局段功能File Header文件头，描述整个文件的文件属性（包括文件是否可执行、是静态链接或动态连接及入口地址、目标硬件、目标操作系统等）.text section代码段，执行语句编译成的机器代码.data section数据段，已初始化的全局变量和局部静态变量.bss sectionBSS 段（Block Started by Symbol），未初始化的全局变量和局部静态变量（因为默认值为 0，所以只是在此预留位置，不占空间）.rodata section只读数据段，存放只读数据，一般是程序里面的只读变量（如 const 修饰的变量）和字符串常量.comment section注释信息段，存放编译器版本信息.note.GNU-stack section堆栈提示段]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-Linux进程管理]]></title>
    <url>%2F2021%2F03%2F08%2Fself_cultivation_linux_process%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Linux进程管理读者-写者问题定义： 允许多个进程同时对数据进行读操作，但是不允许读和写以及写和写操作同时发生。 解决方案： 读者优先 读进程只要看到有其他读进程正在访问文件，就可以继续作读访问；写进程必须等待所有读进程都不访问时才能写文件，即使写进程可能比一些读进程更早提出申请。 读者写者公平竞争，老实排队 因为读者优先的方案如果在读访问非常频繁的场合，有可能造成写进程一直无法访问文件的局面….为了避免这种情况的产生，读者写者请求都老实排队， 排到谁就执行谁， 不准读者插队 写者优先 如果有写者申请写文件，那么在申请之前已经开始读取文件的可以继续读取，但是如果再有读者申请读取文件，则不能够读取，只有在所有的写者写完之后才可以读取 哲学家就餐问题 5 个沉默寡言的哲学家围坐在圆桌前，每人面前一盘意面。叉子放在哲学家之间的桌面上。（5 个哲学家，5 根叉子） 所有的哲学家都只会在思考和进餐两种行为间交替。哲学家只有同时拿到左边和右边的叉子才能吃到面，而同一根叉子在同一时间只能被一个哲学家使用。每个哲学家吃完面后都需要把叉子放回桌面以供其他哲学家吃面。只要条件允许，哲学家可以拿起左边或者右边的叉子，但在没有同时拿到左右叉子时不能进食。 设计一个进餐规则（并行算法）使得每个哲学家都不会挨饿；也就是说，在没有人知道别人什么时候想吃东西或思考的情况下，每个哲学家都可以在吃饭和思考之间一直交替下去。 显而易见，如果不小心处理会有死锁现象， 比如：当每个科学家都同时拿起了左边的筷子时候死锁发生了，都想拿自己右边的筷子，但是科学家每个人左手都不松手。导致都吃不了饭 参考解决方案： 规定奇数号科学家先拿左边的筷子，然后拿右边的筷子。偶数号科学家先拿右边的筷子，然后那左边的筷子。导致0，1科学家竞争1号筷子，2，3科学家竞争3号筷子。四号科学家无人竞争。最后总有一个科学家能获得两只筷子。 仅当科学家左右两只筷子都能用的时候，才允许他进餐，代码里的用trylock来实现 至多允许四个哲学家同时去拿左边的筷子，最终保证至少有一个科学家能进餐，并且用完之后释放筷子，从而使更多的哲学家能够拿到筷子。 活锁在某种情形下，轮询（忙等待）可用于进入临界区或存取资源。采用这一策略的主要原因是，相比所做的工作而言，互斥的时间很短而挂起等待的时间开销很大。考虑一个原语，通过该原语，调用进程测试一个互斥信号量，然后或者得到该信号量或者返回失败信息。 现在假设有一对进程使用两种资源。每个进程需要两种资源，它们利用轮询原语enter_region去尝试取得必要的锁，如果尝试失败，则该进程继续尝试。如果进程A先运行并得到资源1，然后进程2运行并得到资源2，以后不管哪一个进程运行，都不会有任何进展，但是哪一个进程也没有被阻塞。结果是两个进程总是一再消耗完分配给它们的CPU配额，但是没有进展也没有阻塞。因此，没有出现死锁现象（因为没有进程阻塞），但是从现象上看好像死锁发生了，这就是活锁（livelock）。 死锁参考 必要条件 (口诀互占不还？233): 互斥：每个资源要么已经分配给了一个进程，要么就是可用的。 占有和等待：已经得到了某个资源的进程可以再请求新的资源。 不可抢占：已经分配给一个进程的资源不能强制性地被抢占，它只能被占有它的进程显式地释放。 环路等待：有两个或者两个以上的进程组成一条环路，该环路中的每个进程都在等待下一个进程所占有的资源。 死锁处理方法大纲主要有以下四种方法： 鸵鸟策略 死锁检测与死锁恢复 死锁预防 死锁避免 鸵鸟策略把头埋在沙子里，假装根本没发生问题。因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任务措施的方案会获得更高的性能。当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。大多数操作系统，包括 Unix，Linux 和 Windows，处理死锁问题的办法仅仅是忽略它。 死锁检测与死锁恢复不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。 每种类型一个资源的死锁检测 上图为资源分配图，其中方框表示资源，圆圈表示进程。资源指向进程表示该资源已经分配给该进程，进程指向资源表示进程请求获取该资源。 图 a 可以抽取出环，如图 b，它满足了环路等待条件，因此会发生死锁。 每种类型一个资源的死锁检测算法是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。（当然也可以用拓扑排序思路来检测哈） 每种类型多个资源的死锁检测 上图中，有三个进程四个资源，每个数据代表的含义如下： E 向量：资源总量 A 向量：资源剩余量 C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量 R 矩阵：每个进程请求的资源数量 进程 P1 和 P2 所请求的资源都得不到满足，只有进程 P3 可以，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P2 可以执行，执行后释放 P2 拥有的资源，A = (4 2 2 1) 。P1 也可以执行。所有进程都可以顺利执行，没有死锁。 算法总结如下： 每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。 寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。 如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。 如果没有这样一个进程，算法终止。 死锁恢复 利用抢占恢复 利用回滚恢复 通过杀死进程恢复 死锁预防在程序运行之前预防发生死锁。 破坏互斥条件 例如假脱机打印机技术允许若干个进程同时输出，唯一真正请求物理打印机的进程是打印机守护进程。 破坏占有和等待条件 一种实现方式是规定所有进程在开始执行前请求所需要的全部资源。 破坏不可抢占条件 破坏环路等待 给资源统一编号，进程只能按编号顺序来请求资源。 死锁避免在程序运行时避免发生死锁。避免死锁的主要算法是基于一个安全状态的概念。在描述算法前，我们先讨论有关安全的概念。 安全状态的检测 图 a 的第二列 Has 表示已拥有的资源数，第三列 Max 表示总共需要的资源数，Free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。 安全状态的定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的。 安全状态的检测与死锁的检测类似，因为安全状态必须要求不能发生死锁。下面的银行家算法与死锁检测算法非常类似，可以结合着做参考对比。 单个资源的银行家算法Dijkstra（1965）提出了一种能够避免死锁的调度算法，称为银行家算法（banker’s algorithm），一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。 客户们各自做自己的生意，在某些时刻需要贷款（相当于请求资源）。在某一时刻，具体情况如图b所示。这个状态是安全的，由于保留着2个单位，银行家能够拖延除了C以外的其他请求。因而可以让C先完成，然后释放C所占的4个单位资源。有了这4个单位资源，银行家就可以给D或B分配所需的贷款单位，以此类推。 考虑假如向B提供了另一个他所请求的贷款单位，如图b所示，那么我们就有如图c所示的状态，该状态是不安全的。如果忽然所有的客户都请求最大的限额，而银行家无法满足其中任何一个的要求，那么就会产生死锁。不安全状态并不一定引起死锁，由于客户不一定需要其最大贷款额度，但银行家不敢抱这种侥幸心理。 银行家算法就是对每一个请求进行检查，检查如果满足这一请求是否会达到安全状态。若是，那么就满足该请求；若否，那么就推迟对这一请求的满足。为了看状态是否安全，银行家看他是否有足够的资源满足某一个客户。如果可以，那么这笔投资认为是能够收回的，并且接着检查最接近最大限额的一个客户，以此类推。如果所有投资最终都被收回，那么该状态是安全的，最初的请求可以批准。 上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。 多个资源的银行家算法 可以把银行家算法进行推广以处理多个资源 上图中有五个进程，四个资源。左边的图表示已经分配的资源，右边的图表示还需要分配的资源。最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。 检查一个状态是否安全的算法如下： 查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。 假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。 重复以上两步，直到所有进程都标记为终止，则状态时安全的。 如果一个状态不是安全的，需要拒绝进入这个状态。 linux进程调度参考 https://juejin.im/post/6844903568613310477 在Linux中，线程和进程一视同仁，所以讲到进程调度，也包含了线程调度。 调度分两种: 非抢占式多任务 除非任务自己结束，否则将会一直执行。 抢占式多任务（Linux用的是这种) 这种情况下，由调度程序来决定什么时候停止一个进程的运行，这个强制的挂起动作即为抢占。采用抢占式多任务的基础是使用时间片轮转机制来为每个进程分配可以运行的时间单位。 Linux有两种不同的进程优先级范围: 使用nice值：越大的nice值意味着更低的优先级。 (-19 ~ 20之间) 实时优先级：可配置(通过实时调度API)，越高意味着进程优先级越高。 任何实时的进程优先级都高于普通的进程，因此上面的两种优先级范围处于互不相交的范畴。 时间片：Linux中并不是以固定的时间值(如10ms)来分配时间片的，而是将处理器的使用比作为“时间片”划分给进程。这样，进程所获得的实际CPU时间就和系统的负载密切相关。 Linux内核有两个调度类： CFS(完全公平调度器Completely Fair Scheduler) 实时调度类。 公平调度CFS举个例子来区分Unix调度和CFS, 有两个运行的优先级相同的进程: 在Unix中可能是每个各执行5ms，执行期间完全占用处理器，但在“理想情况”下，应该是，能够在10ms内同时运行两个进程，每个占用处理器一半的能力。 CFS的做法是：CFS 调度程序并不采用严格规则来为一个优先级分配某个长度的时间片, 在所有可运行进程的总数上计算出一个进程应该运行的时间，nice值不再作为时间片分配的标准，而是用于处理计算获得的处理器使用权重。 现在我们来看一个简单的例子，假设我们的系统只有两个进程在运行，一个是文本编辑器（I/O消耗型），另一个是视频解码器（处理器消耗型）。理想的情况下，文本编辑器应该得到更多的处理器时间，至少当它需要处理器时，处理器应该立刻被分配给它（这样才能完成用户的交互），这也就意味着当文本编辑器被唤醒的时候，它应该抢占视频解码程序。按照普通的情况，OS应该分配给文本编辑器更大的优先级和更多的时间片，但在Linux中，这两个进程都是普通进程，他们具有相同的nice值，因此它们将得到相同的处理器使用比（50%）。但实际的运行过程中会发生什么呢？CFS将能够注意到，文本编辑器使用的处理器时间比分配给它的要少得多（因为大多时间在等待I/O），这种情况下，要实现所有进程“公平”地分享处理器，就会让文本编辑器在需要运行时立刻抢占视频解码器（每次都是如此）。 实时调度Linux还实现了 POS1X实时调度扩展。这些扩展允许应用程序精确地控制如何分配CPU给进程。运作在两个实时调度策略 SCHED RR （循环） SCHED FIFO （先入先出） 下的进程的优先级总是高于运作在非实时策略下的进程。实时进程优先级的取值范围为1 （低）〜99（高）。只有进程处于可运行状态，那么优先级更高的进程就会完全将优先级低的进程排除在CPU之外。运作在SCHED_FIFO策略下的进程会互斥地访问CPU直到它执行终止或自动释放CPU或被进入可运行状态的优先级更高的进程抢占。类似的规则同样适用于SCHED RR策略,但在该策略下，如果存在多个进程运行于同样的优先级下，那么CPU就会以循环的方式被这些进程共享。 实时调度采用 SCHED_FIFO 或 SCHED_RR 实时策略来调度的任何任务，与普通（非实时的）任务相比，具有更高的优先级。 Linux 采用两个单独的优先级范围，一个用于实时任务，另一个用于正常任务。实时任务分配的静态优先级为 0〜99，而正常任务分配的优先级为 100〜139。 这两个值域合并成为一个全局的优先级方案，其中较低数值表明较高的优先级。正常任务，根据它们的nice值，分配一个优先级；这里 -20 的nice值映射到优先级 100，而 +19 的nice值映射到 139。下图显示了这个方案。 linux轻量级进程LWP对于Linux操作系统而言，它对Thread的实现方式比较特殊。在Linux内核中，其实是没有线程的概念的，它把所有的线程当做标准的进程来实现，也就是说Linux内核，并没有为线程提供任何特殊的调度语义，也没有为线程实现特定的数据结构。取而代之的是，线程只是一个与其他进程共享某些资源的进程。每一个线程拥有一个唯一的task_struct结构，Linux内核它仅仅把线程当做一个正常的进程，或者说是轻量级进程，LWP(Lightweight processes)。 Linux线程与进程的区别，主要体现在资源共享、调度、性能几个方面，首先看一下资源共享方面。上面也提到，线程其实是共享了某一个进程的资源，这些资源包括： 内存地址空间 进程基础信息 大部分数据 打开的文件 信号处理 当前工作目录 用户和用户组属性 … 哪些是线程独自拥有的呢？ 线程ID 一系列的寄存器 栈的局部变量和返回地址 错误码 errno 信号掩码 优先级 … 这里说一个黑科技，线程拥有独立的调用栈，除了栈之外共享了其他所有的段segment。但是由于线程间共享了内存，也就是说一个线程，理论上是可以访问到其他线程的调用栈的，可以用一个指针变量，去访问其他线程的局部栈帧，以访问其他线程的局部变量。 LWP如何创建出来那么Linux中线程是如何创建出来的呢？上面也提到，在Linux中线程是一种资源共享的方式，可以在创建进程的时候，指定某些资源是从其他进程共享的，从而在概念上创建了一个线程。在Linux中，可以通过clone系统调用来创建一个进程，它的函数签名如下：12#include &lt;sched.h&gt;int clone(int (*fn)(void *), void *child_stack, int flags, void *arg, ...); 我们在使用clone创建进程的过程中，可以指明相应的参数，来决定共享某些资源，比如:1clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0); 这个clone系统调用的行为类似于fork，不过新创建出来的进程，它的内存地址、文件系统资源、打开的文件描述符和信号处理器，都是共享父进程的。换句话说，这个新创建出来的进程，也被叫做Linux Thread。从这个例子中，也可以看出Linux中，线程其实是进程实现资源共享的一种方式。 在内核中，clone调用经过参数传递和解释后会调用do_fork，这个核内函数同时也是fork、vfork系统调用的最终实现：1int do_fork(unsigned long clone_flags, unsigned long stack_start, struct pt_regs* regs,unsigned long stack_size); 在do_fork中，不同的clone_flags将导致不同的行为（共享不同的资源），下面列举几个flag的作用。 CLONE_VM 如果do_fork时指定了CLONE_VM开关，创建的轻量级进程的内存空间将会和父进程指向同一个地址，即创建的轻量级进程将与父进程共享内存地址空间。 CLONE_FS 如果do_fork时指定了CLONE_FS开关，对于轻量级进程则会与父进程共享相同的所在文件系统的根目录和当前目录信息。也就是说，轻量级进程没有独立的文件系统相关的信息，进程中任何一个线程改变当前目录、根目录等信息都将直接影响到其他线程。 CLONE_FILES 如果do_fork时指定了CLONE_FILES开关，创建的轻量级进程与父进程将会共享已经打开的文件。这一共享使得任何线程都能访问进程所维护的打开文件，对它们的操作会直接反映到进程中的其他线程。 CLONE_SIGHAND 如果do_fork时指定了CLONE_FILES开关，轻量级进程与父进程将会共享对信号的处理方式。也就是说，子进程与父进程的信号处理方式完全相同，而且可以相互更改。 尽管linux支持轻量级进程，但并不能说它就支持内核线程，因为linux的”线程”和”进程”实际上处于一个调度层次，共享一个进程标识符空间，这种限制使得不可能在linux上实现完全意义上的POSIX线程机制，因此众多的linux线程库实现尝试都只能尽可能实现POSIX的绝大部分语义，并在功能上尽可能逼近。 多核CPU是否能同时执行多个进程？多核的作用就是每个CPU可以调度不同的任务“并行”执行。注意，这里说的是“并行”，而不是“并发”，所以问题的回答是“能”。 第二个问题，“同时最多执行几个进程“?这里你想描述的“同时”的意思，是某一个特定时刻吗？如果是，很明显，在某一特定时刻，每个核只能调度一个任务执行，所以有多少个核最多就可以调度多少个进程（或者说成线程比较准确些）。但在一段时间之内，每个核可以“并发”调度多个任务执行。如何“并发”，这就是由不同操作系统的进程调度策略规定的了，比如常见Linux的CFS调度算法和Windows的抢占式调度算法。 创建守护进程的步骤(两fork一set, u工文dev)最关键的三步骤: 调用fork，然后使父进程exit。虽然子进程继承了父进程的进程组ID，但获得了一个新的进程ID，这就保证了子进程不是一个进程组的组长进程。这是下面将要进行的setsid调用的先决条件。 调用setsid创建一个新会话。使调用进程：(a)成为新会话的首进程，(b)成为一个新进程组的组长进程．(c)没有控制终端。也可概括为 : 开启一个新会话并释放它与控制终端之间的所有关联关系 再次fork并杀掉首进程.这样就确保了子进程不是一个会话首进程， 根据linux中获取终端的规则（只有会话首进程才能请求一个控制终端）， 这样进程永远不会重新请求一个控制终端 1234567 会 话 / | \ / | \ / | \ 前台进程组 后台进程组1 后台进程组2 ... / | \ / | \ / | \进程1 进程2 ... 进程3 进程4 ... ... 进程组进程组就是一系列相互关联的进程集合，系统中的每一个进程也必须从属于某一个进程组；每个进程组中都会有一个唯一的 ID(process group id)，简称 PGID；PGID 一般等同于进程组的创建进程的 Process ID，而这个进进程一般也会被称为进程组先导 (process group leader) 会话会话（session）是一个若干进程组的集合，同样的，系统中每一个进程组也都必须从属于某一个会话；一个会话只拥有最多一个控制终端（也可以没有），该终端为会话中所有进程组中的进程所共用。一个会话中前台进程组只会有一个，只有其中的进程才可以和控制终端进行交互；除了前台进程组外的进程组，都是后台进程组；和进程组先导类似，会话中也有会话先导 (session leader) 的概念，用来表示建立起到控制终端连接的进程。在拥有控制终端的会话中，session leader 也被称为控制进程(controlling process)，一般来说控制进程也就是登入系统的 shell 进程(login shell)； 杀死进程组或会话中的所有进程我们可以使用该 PGID，通过 kill 命令向整个进程组发送信号： kill -SIGTERM -- -19701 我们用一个负数 -19701 向进程组发送信号。如果我们传递的是一个正数，这个数将被视为进程 ID 用于终止进程。如果我们传递的是一个负数，它被视为 PGID，用于终止整个进程组。负数来自系统调用的直接定义。 杀死会话中的所有进程与之完全不同。有些系统没有会话 ID 的概念。即使是具有会话 ID 的系统，例如 Linux，也没有提供系统调用来终止会话中的所有进程。你需要遍历 /proc 输出的进程树，收集所有的 SID，然后一一终止进程。Pgrep 实现了遍历、收集并通过会话 ID 杀死进程的算法。使用以下命令： pkill -s &lt;SID&gt; SIGHUPSIGHUP 会在以下 3 种情况下被发送给相应的进程： 终端关闭时，该信号被发送到 session 首进程以及作为 job 提交的进程（即用 &amp; 符号提交的进程）； session 首进程退出时，该信号被发送到该 session 中的前台进程组中的每一个进程； 若父进程退出导致进程组成为孤儿进程组，且该进程组中有进程处于停止状态（收到 SIGSTOP 或 SIGTSTP 信号），该信号会被发送到该进程组中的每一个进程。 例如：在我们登录 Linux 时，系统会分配给登录用户一个终端 (Session)。在这个终端运行的所有程序，包括前台进程组和后台进程组，一般都属于这个 Session。当用户退出 Linux 登录时，前台进程组和后台有对终端输出的进程将会收到 SIGHUP 信号。这个信号的默认操作为终止进程，因此前台进程组和后台有终端输出的进程就会中止。此外，对于与终端脱离关系的守护进程，正常情况下是永远都收不到这个信号的, 所以可以人为的发SIGHUP信号给她用于通知它做一些想要的自定义的操作, 比较常见的如重新读取配置文件操作。 比如 xinetd 超级服务程序。 SIGCHLD与僵死进程SIGCHLD信号,子进程结束时, 父进程会收到这个信号。如果父进程没有处理这个信号，也没有等待(waitpid)子进程，子进程虽然终止，但是还会在内核进程表中占有表项，这时的子进程称为僵尸进程。这种情 况我们应该捕捉它，或者wait它派生的子进程，或者父进程先终止，这时子进程的终止自动由init进程 来接管 SIGPIPE在网络编程中，SIGPIPE 这个信号是很常见的。当往一个写端关闭的管道或 socket 连接中连续写入数据时会引发 SIGPIPE 信号, 引发 SIGPIPE 信号的写操作将设置 errno 为 EPIPE。在 TCP 通信中，当通信的双方中的一方 close 一个连接时，若另一方接着发数据，根据 TCP 协议的规定，会收到一个 RST 响应报文，若再往这个服务器发送数据时，系统会发出一个 SIGPIPE 信号给进程，告诉进程这个连接已经断开了，不能再写入数据。 因为 SIGPIPE 信号的默认行为是结束进程，而我们绝对不希望因为写操作的错误而导致程序退出，尤其是作为服务器程序来说就更恶劣了。所以我们应该对这种信号加以处理，在这里，介绍处理 SIGPIPE 信号的方式： 一般给 SIGPIPE 设置 SIG_IGN 信号处理函数，忽略该信号: signal(SIGPIPE, SIG_IGN); 前文说过，引发 SIGPIPE 信号的写操作将设置 errno 为 EPIPE,。所以，第二次往关闭的 socket 中写入数据时, 会返回 - 1, 同时 errno 置为 EPIPE. 这样，便能知道对端已经关闭，然后进行相应处理，而不会导致整个进程退出. 内核态与用户态的区别 内核态：cpu可以访问内存的所有数据，包括外围设备，例如硬盘，网卡，cpu也可以将自己从一个程序切换到另一个程序。 用户态：只能受限的访问内存，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取。 从用户态到内核态切换可以通过三种方式： 系统调用: 其实系统调用本身就是中断，但是软件中断，跟硬中断不同。 异常：如果当前进程运行在用户态，如果这个时候发生了异常事件，就会触发切换。例如：缺页异常。 外设中断：当外设完成用户的请求时，会向CPU发送中断信号。]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-Linux内存管理]]></title>
    <url>%2F2021%2F03%2F08%2Fself_cultivation_linux_memory_manage%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Linux内存管理为什么需要虚拟内存虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。 为了更好的管理内存，操作系统将内存抽象成地址空间。每个程序拥有自己的地址空间，这个地址空间被分割成多个块，每一块称为一页。这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内存中。当程序引用到不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令。 从上面的描述中可以看出，虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存，也就是说一个程序不需要全部调入内存就可以运行，这使得有限的内存运行大程序成为可能。例如有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。 MMU工作原理内存管理单元（MMU）管理着地址空间和物理内存的转换，其中的页表（Page table）存储着页（程序地址空间）和页框（物理内存空间）的映射表。 一个虚拟地址分成两个部分: 一部分存储页面号， 一部分存储偏移量。 上图的页表存放着 16 个页，这 16 个页需要用 4 个比特位来进行索引定位。例如对于虚拟地址（0010 000000000100），前 4 位是存储页面号 2，读取表项内容为（110 1），页表项最后一位表示是否存在于内存中，1 表示存在。后 12 位存储偏移量。这个页对应的页框的地址为 （110 000000000100）。 主机字节序主机字节序又叫 CPU 字节序，其不是由操作系统决定的，而是由 CPU 指令集架构决定的。主机字节序分为两种： 记忆技巧: 低序地址存了高序字节就叫大端, 反之就小端 大端字节序（Big Endian）：高序字节存储在低位地址，低序字节存储在高位地址, 目前主要是ARM/PowerPC在用 小端字节序（Little Endian）：低序字节存储在低位地址, 高序字节存储在高位地址，目前主要是Intel在用 存储方式:32 位整数 0x12345678 是从起始位置为 0x00 的地址开始存放，则： 内存地址 0x00 0x01 0x02 0x03 大端 12 34 56 78 小端 78 56 34 12 网络字节序网络字节顺序是 TCP/IP 中规定好的一种数据表示格式，它与具体的 CPU 类型、操作系统等无关，从而可以保证数据在不同主机之间传输时能够被正确解释。 网络字节顺序采用：大端（Big Endian）排列方式。 Linux虚拟地址空间如何分布Linux 使用虚拟地址空间，大大增加了进程的寻址空间，由低地址到高地址(下图中从下到上即为从低到高)分别为(口诀: 文初堆栈)： 文本段(只读段)：该部分空间只能读，不可写；(包括：代码段、rodata 段(C常量字符串和#define定义的常量) ) 数据段(初始化数据段与未初始化数据段)：保存初始化了的与未初始化的全局变量、静态变量的空间； 堆 ：就是平时所说的动态内存， malloc/new 大部分都来源于此。其中堆顶的位置可通过函数 brk 和 sbrk 进行动态调整。 文件映射区域 ：如动态库、共享内存等映射物理空间的内存，一般是 mmap 函数所分配的虚拟地址空间。 栈：用于维护函数调用的上下文空间，一般为 8M ，可通过 ulimit –s 查看。 内核虚拟空间：用户代码不可见的内存区域，由内核管理(页表就存放在内核虚拟空间)。上图是 32 位系统典型的虚拟地址空间分布(来自《深入理解计算机系统》)。 brk函数先了解：brk()和sbrk()函数12int brk( const void *addr )void* sbrk ( intptr_t incr ); 这两个函数的作用主要是扩展heap的上界brk。第一个函数的参数为设置的新的brk上界地址，如果成功返回0，失败返回-1。第二个函数的参数为需要申请的内存的大小，然后返回heap新的上界brk地址。如果sbrk的参数为0，则返回的为原来的brk地址。 mmap虚拟内存系统通过将虚拟内存分割为称作虚拟页 (Virtual Page，VP) 大小固定的块，一般情况下，每个虚拟页的大小默认是 4096 字节。同样的，物理内存也被分割为物理页(Physical Page，PP)，也为 4096 字节。 在 LINUX 中我们可以使用 mmap 用来在进程虚拟内存地址空间中分配地址空间，创建和物理内存的映射关系。 映射关系可以分为两种 文件映射 磁盘文件映射进程的虚拟地址空间，使用文件内容初始化物理内存。 匿名映射 初始化全为 0 的内存空间。 而对于映射关系是否共享又分为 私有映射 (MAP_PRIVATE) 多进程间数据共享，修改不反应到磁盘实际文件，是一个 copy-on-write（写时复制）的映射方式。 共享映射 (MAP_SHARED) 多进程间数据共享，修改反应到磁盘实际文件中。 因此总结起来有 4 种组合 私有文件映射 多个进程使用同样的物理内存页进行初始化，但是各个进程对内存文件的修改不会共享，也不会反应到物理文件中 私有匿名映射 mmap 会创建一个新的映射，各个进程不共享，这种使用主要用于分配内存 (malloc 分配大内存会调用 mmap)。 例如开辟新进程时，会为每个进程分配虚拟的地址空间，这些虚拟地址映射的物理内存空间各个进程间读的时候共享，写的时候会 copy-on-write。 共享文件映射 多个进程通过虚拟内存技术共享同样的物理内存空间，对内存文件 的修改会反应到实际物理文件中，他也是进程间通信 (IPC) 的一种机制。 共享匿名映射 这种机制在进行 fork 的时候不会采用写时复制，父子进程完全共享同样的物理内存页，这也就实现了父子进程通信 (IPC). 这里值得注意的是，mmap 只是在虚拟内存分配了地址空间，只有在第一次访问虚拟内存的时候才分配物理内存。在 mmap 之后，并没有在将文件内容加载到物理页上，只上在虚拟内存中分配了地址空间。当进程在访问这段地址时，通过查找页表，发现虚拟内存对应的页没有在物理内存中缓存，则产生 “缺页”，由内核的缺页异常处理程序处理，将文件对应内容，以页为单位 (4096) 加载到物理内存，注意是只加载缺页，但也会受操作系统一些调度策略影响，加载的比所需的多。 12void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);int munmap(void *addr, size_t length); 这里要注意的是fd参数，fd为映射的文件描述符，如果是匿名映射，可以设为-1； mmap函数第一种用法是映射磁盘文件到内存中；而malloc使用的是mmap函数的第二种用法，即匿名映射，匿名映射不映射磁盘文件，而是向映射区申请一块内存。 munmap函数是用于释放内存，第一个参数为内存首地址，第二个参数为内存的长度。接下来看下mmap函数的参数。 由于brk/sbrk/mmap属于系统调用，如果每次申请内存，都调用这三个函数中的一个，那么每次都要产生系统调用开销（即cpu从用户态切换到内核态的上下文切换，这里要保存用户态数据，等会还要切换回用户态），这是非常影响性能的；其次，这样申请的内存容易产生碎片，因为堆是从低地址到高地址，如果低地址的内存没有被释放，高地址的内存就不能被回收。 malloc和free原理malloc: 当申请小内存的时，malloc使用sbrk分配内存 当申请大内存时，使用mmap函数申请内存 但是这只是分配了虚拟内存，还没有映射到物理内存，当访问申请的内存时，才会因为缺页异常，内核分配物理内存。 将所有空闲内存块连成链表，每个节点记录空闲内存块的地址、大小等信息 分配内存时，找到大小合适的块，切成两份，一分给用户，一份放回空闲链表 free时，直接把内存块返回链表 解决外部碎片：将能够合并的内存块进行合并 malloc函数的实质体现在：它有一个将可用的内存块连接为一个长长的列表的所谓空闲链表。调用malloc函数时，它沿连接表寻找一个大到足以满足用户请求所需要的内存块。然后，将该内存块一分为二（一块的大小与用户请求的大小相等，另一块的大小就是剩下的字节）。接下来，将分配给用户的那块内存传给用户，并将剩下的那块（如果有的话）返回到连接表上。 这里注意，malloc找到的内存块大小一定是会大于等于我们需要的内存大小，下面会提到如果所有的内存块都比要求的小会怎么办？ 调用free函数时，它将用户释放的内存块连接到空闲链上。到最后，空闲链会被切成很多的小内存片段，如果这时用户申请一个大的内存片段，那么空闲链上可能没有可以满足用户要求的片段了。于是，malloc函数请求延时，并开始在空闲链上翻箱倒柜地检查各内存片段，对它们进行整理，将相邻的小空闲块合并成较大的内存块。 在对内存块进行了 free 调用之后，我们需要做的是诸如将它们标记为未被使用的等事情，并且，在调用 malloc 时，我们要能够定位未被使用的内存块。因此， malloc返回的每块内存的起始处首先要有这个结构： 这就解释了，为什么在程序中free之后，但是堆的内存还是没有释放。 内存控制块结构定义1234struct mem_control_block &#123; int is_available; int size;&#125;; 现在，您可能会认为当程序调用 malloc 时这会引发问题 —— 它们如何知道这个结构？答案是它们不必知道；在返回指针之前，我们会将其移动到这个结构之后，把它隐藏起来。这使得返回的指针指向没有用于任何其他用途的内存。那样，从调用程序的角度来看，它们所得到的全部是空闲的、开放的内存。然后，当通过 free() 将该指针传递回来时，我们只需要倒退几个内存字节就可以再次找到这个结构。 关于 malloc 获得虚存空间的实现，与 glibc 的版本有关，但大体逻辑是： 若分配内存小于 128k ，调用 sbrk() ，将堆顶指针向高地址移动，获得新的虚存空间。 若分配内存大于 128k ，调用 mmap() ，在文件映射区域中分配匿名虚存空间。 接着： VSZ为虚拟内存 RSS为物理内存 VSZ 并不是每次 malloc 后都增长，是与上一节说的堆顶没发生变化有关，因为可重用堆顶内剩余的空间，这样的 malloc 是很轻量快速的。 但如果 VSZ 发生变化，基本与分配内存量相当，因为 VSZ 是计算虚拟地址空间总大小。 RSS 的增量很少，是因为 malloc 分配的内存并不就马上分配实际存储空间，只有第一次使用，如第一次 memset 后才会分配。 由于每个物理内存页面大小是 4k ，不管 memset 其中的 1k 还是 5k 、 7k ，实际占用物理内存总是 4k 的倍数。所以 RSS 的增量总是 4k 的倍数。 因此，不是 malloc 后就马上占用实际内存，而是第一次使用时发现虚存对应的物理页面未分配，产生缺页中断，才真正分配物理页面，同时更新进程页面的映射关系。这也是 Linux 虚拟内存管理的核心概念之一。 vmalloc和kmalloc和malloc的区别 kmalloc和vmalloc是分配的是内核的内存,malloc分配的是用户的内存 kmalloc保证分配的内存在物理上是连续的,vmalloc保证的是在虚拟地址空间上的连续,malloc不保证任何东西(这点是自己猜测的,不一定正确) kmalloc能分配的大小有限,vmalloc和malloc能分配的大小相对较大 内存只有在要被DMA访问的时候才需要物理上连续 vmalloc比kmalloc要慢 对于提供了MMU（存储管理器，辅助操作系统进行内存管理，提供虚实地址转换等硬件支持）的处理器而言，Linux提供了复杂的存储管理系统，使得进程所能访问的内存达到4GB。 进程的4GB内存空间被人为的分为两个部分–用户空间与内核空间。用户空间地址分布从0到3GB(PAGE_OFFSET，在0x86中它等于0xC0000000)，3GB到4GB为内核空间。 内核空间中，从3G到vmalloc_start这段地址是物理内存映射区域（该区域中包含了内核镜像、物理页框表mem_map等等），比如我们使用 的 VMware虚拟系统内存是160M，那么3G～3G+160M这片内存就应该映射物理内存。在物理内存映射区之后，就是vmalloc区域。对于 160M的系统而言，vmalloc_start位置应在3G+160M附近（在物理内存映射区与vmalloc_start期间还存在一个8M的gap 来防止跃界），vmalloc_end的位置接近4G(最后位置系统会保留一片128k大小的区域用于专用页面映射) 一般情况下，只有硬件设备才需要物理地址连续的内存，因为硬件设备往往存在于MMU之外，根本不了解虚拟地址；但为了性能上的考虑，内核中一般使用kmalloc(),而只有在需要获得大块内存时才使用vmalloc，例如当模块被动态加载到内核当中时，就把模块装载到由vmalloc（）分配的内存上。 kmalloc: kmalloc申请的是较小的连续的物理内存，内存物理地址上连续，虚拟地址上也是连续的，使用的是内存分配器slab的一小片。申请的内存位于物理内存的映射区域。其真正的物理地址只相差一个固定的偏移。而且不对获得空间清零。可以查看slab分配器 kzalloc: 用kzalloc申请内存的时候， 效果等同于先是用 kmalloc() 申请空间 , 然后用 memset() 来初始化 ,所有申请的元素都被初始化为 0. vmalloc: vmalloc用于申请较大的内存空间，虚拟内存是连续。申请的内存的则位于vmalloc_start～vmalloc_end之间，与物理地址没有简单的转换关系，虽然在逻辑上它们也是连续的，但是在物理上它们不要求连续。 malloc: malloc分配的是用户的内存。除非被阻塞否则他执行的速度非常快，而且不对获得空间清零。 Buddy（伙伴）分配算法参考: https://zhuanlan.zhihu.com/p/149581303 伙伴系统用于管理物理页，主要目的在于维护可用的连续物理空间，避免外部碎片。所有关于内存分配的操作都会与其打交道，buddy是物理内存的管理的门户 Linux 内核引入了伙伴系统算法（Buddy system），什么意思呢？就是把相同大小的页框块用链表串起来，页框块就像手拉手的好伙伴，也是这个算法名字的由来。 具体的，所有的空闲页框分组为11个块链表，每个块链表分别包含大小为1，2，4，8，16，32，64，128，256，512和1024个连续页框的页框块。最大可以申请1024个连续页框，对应4MB大小的连续内存。 伙伴系统:因为任何正整数都可以由 2^n 的和组成，所以总能找到合适大小的内存块分配出去，减少了外部碎片产生 。 分配实例:比如：我需要申请4个页框，但是长度为4个连续页框块链表没有空闲的页框块，伙伴系统会从连续8个页框块的链表获取一个，并将其拆分为两个连续4个页框块，取其中一个，另外一个放入连续4个页框块的空闲链表中。释放的时候会检查，释放的这几个页框前后的页框是否空闲，能否组成下一级长度的块。 Slab分配器伙伴系统和slab不是二选一的关系，slab 内存分配器是对伙伴分配算法的补充 slab的目的在于避免内部碎片。从buddy系统获取的内存至少是一个页，也就是4K，如果仅仅需要8字节的内存，显然巨大的内部碎片无法容忍。 slab从buddy系统申请空间，将较大的连续内存拆分成一系列较小的内存块。 用户申请空间时从slab中获取大小最相近的小块内存，这样可以有效减少内部碎片。在slab最大的块为8K，slab中所有块在物理上也是连续的。 上面说的用于内存分配的slab是通用的slab，主要用于支持kmalloc分配内存。 slab还有一个作用就是用作对象池，针对经常分配和回收的对象比如task_struct，可以分配一个slab对象池对其优化。这种slab是独立于通用的内存分配slab的，在内核中有很多这样的针对特定对象的slab。 在内核中想要分配一段连续的内存，首先向slab系统申请，如果不满足（超过两个页面，也就是8K），直接向buddy系统申请。如果还不满足（超过4M，也就是1024个页面），将无法获取到连续的物理地址。可以通过vmalloc获取虚拟地址空间连续，但物理地址不连续的更大的内存空间。 malloc是用户态使用的内存分配接口，最终还是向buddy申请内存，因为buddy系统是管理物理内存的门户。申请到大块内存后，再像slab一样对其进行细分维护，根据用户需要返回相应内存的指针。 fork内存语义 共享代码段, 子指向父 : 父子进程共享同一代码段, 子进程的页表项指向父进程相同的物理内存页(即数据段/堆段/栈段的各页) 写时复制(copy-on-write) : 内核会捕获所有父进程或子进程针对这些页面(即数据段/堆段/栈段的各页)的修改企图, 并为将要修改的页面创建拷贝, 将新的页面拷贝分配给遭内核捕获的进程, 从此父/子进程可以分别修改各自的页拷贝, 不再相互影响. 虽然fork创建的子进程不需要拷贝父进程的物理内存空间, 但是会复制父进程的空间内存页表. 例如对于10GB的redis进程, 需要复制约20MB的内存页表, 因为此fork操作耗时跟进程总内存量息息相关 零拷贝参考 https://juejin.im/post/6844903949359644680 “先从简单开始，实现下这个场景：从一个文件中读出数据并将数据传到另一台服务器上？”大概伪代码如下:12File.read(file, buf, len);Socket.send(socket, buf, len); 可以看出, 这样效率是很低的. 下图分别对应传统 I/O 操作的数据读写流程，整个过程涉及 2 次 CPU 拷贝、2 次 DMA 拷贝总共 4 次拷贝，以及 4 次上下文切换，下面简单地阐述一下相关的概念。 上下文切换：当用户程序向内核发起系统调用时，CPU 将用户进程从用户态切换到内核态；当系统调用返回时，CPU 将用户进程从内核态切换回用户态。 CPU拷贝：由 CPU 直接处理数据的传送，数据拷贝时会一直占用 CPU 的资源。 DMA拷贝：由 CPU 向DMA磁盘控制器下达指令，让 DMA 控制器来处理数据的传送，数据传送完毕再把信息反馈给 CPU，从而减轻了 CPU 资源的占有率。 传统读操作当应用程序执行 read 系统调用读取一块数据的时候，如果这块数据已经存在于用户进程的页内存中，就直接从内存中读取数据；如果数据不存在，则先将数据从磁盘加载数据到内核空间的读缓存（read buffer）中，再从读缓存拷贝到用户进程的页内存中。1read(file_fd, tmp_buf, len); 复制代码基于传统的 I/O 读取方式，read 系统调用会触发 2 次上下文切换，1 次 DMA 拷贝和 1 次 CPU 拷贝，发起数据读取的流程如下： 用户进程通过 read() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。 CPU利用DMA控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。 CPU将读缓冲区（read buffer）中的数据拷贝到用户空间（user space）的用户缓冲区（user buffer）。 上下文从内核态（kernel space）切换回用户态（user space），read 调用执行返回。 传统写操作当应用程序准备好数据，执行 write 系统调用发送网络数据时，先将数据从用户空间的页缓存拷贝到内核空间的网络缓冲区（socket buffer）中，然后再将写缓存中的数据拷贝到网卡设备完成数据发送。1write(socket_fd, tmp_buf, len); 复制代码基于传统的 I/O 写入方式，write() 系统调用会触发 2 次上下文切换，1 次 CPU 拷贝和 1 次 DMA 拷贝，用户程序发送网络数据的流程如下： 用户进程通过 write() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。 CPU 将用户缓冲区（user buffer）中的数据拷贝到内核空间（kernel space）的网络缓冲区（socket buffer）。 CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。 上下文从内核态（kernel space）切换回用户态（user space），write 系统调用执行返回。 sendfilesendfile 系统调用在 Linux 内核版本 2.1 中被引入，目的是简化通过网络在两个通道之间进行的数据传输过程。sendfile 系统调用的引入，不仅减少了 CPU 拷贝的次数，还减少了上下文切换的次数，它的伪代码如下：1sendfile(socket_fd, file_fd, len); 复制代码通过 sendfile 系统调用，数据可以直接在内核空间内部进行 I/O 传输，从而省去了数据在用户空间和内核空间之间的来回拷贝。与 mmap 内存映射方式不同的是， sendfile 调用中 I/O 数据对用户空间是完全不可见的。也就是说，这是一次完全意义上的数据传输过程。 基于 sendfile 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝，用户程序读写数据的流程如下： 用户进程通过 sendfile() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。 CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。 CPU 将读缓冲区（read buffer）中的数据拷贝到的网络缓冲区（socket buffer）。 CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。 上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。 相比较于 mmap 内存映射的方式，sendfile 少了 2 次上下文切换，但是仍然有 1 次 CPU 拷贝操作。sendfile 存在的问题是用户程序不能对数据进行修改，而只是单纯地完成了一次数据传输过程。 “这样确实改善了很多，但还没达到零拷贝的要求（还有一次cpu参与的拷贝），还有其它黑技术？”“对的，如果底层网络接口卡支持收集(gather)操作的话，就可以进一步的优化。”“怎么说？”“继续看下一小节” sendfile + DMA gather copyLinux 2.4 版本的内核对 sendfile 系统调用进行修改，如果底层网络接口卡支持收集(gather)操作的话, 为 DMA 拷贝引入了 gather 操作。它将内核空间（kernel space）的读缓冲区（read buffer）中对应的数据描述信息（内存地址、地址偏移量）记录到相应的网络缓冲区（ socket buffer）中，由 DMA 根据内存地址、地址偏移量将数据批量地从读缓冲区（read buffer）拷贝到网卡设备中，这样就省去了内核空间中仅剩的 1 次 CPU 拷贝操作，sendfile 的伪代码如下：1sendfile(socket_fd, file_fd, len); 复制代码在硬件的支持下，sendfile 拷贝方式不再从内核缓冲区的数据拷贝到 socket 缓冲区，取而代之的仅仅是缓冲区文件描述符和数据长度的拷贝，这样 DMA 引擎直接利用 gather 操作将页缓存中数据打包发送到网络中即可，本质就是和虚拟内存映射的思路类似。 基于 sendfile + DMA gather copy 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换、0 次 CPU 拷贝以及 2 次 DMA 拷贝，用户程序读写数据的流程如下： 用户进程通过 sendfile() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。 CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。 CPU 把读缓冲区（read buffer）的文件描述符（file descriptor）和数据长度拷贝到网络缓冲区（socket buffer）。 基于已拷贝的文件描述符（file descriptor）和数据长度，CPU 利用 DMA 控制器的 gather/scatter 操作直接批量地将数据从内核的读缓冲区（read buffer）拷贝到网卡进行数据传输。 上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。 sendfile + DMA gather copy 拷贝方式同样存在用户程序不能对数据进行修改的问题，而且本身需要硬件的支持，它只适用于将数据从文件拷贝到 socket 套接字上的传输过程。]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QUIC原理与KCP会话握手借鉴]]></title>
    <url>%2F2021%2F02%2F26%2Fquic_intro%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[参考: https://zhuanlan.zhihu.com/p/142794794 https://zhuanlan.zhihu.com/p/32553477 本文主要介绍 QUIC 协议产生的背景和核心特性。 写在前面如果你的 App，在不需要任何修改的情况下就能提升 15% 以上的访问速度。特别是弱网络的时候能够提升 20% 以上的访问速度。 如果你的 App，在频繁切换 4G 和 WIFI 网络的情况下，不会断线，不需要重连，用户无任何感知。如果你的 App，既需要 TLS 的安全，也想实现 HTTP2 多路复用的强大。 如果你刚刚才听说 HTTP2 是下一代互联网协议，如果你刚刚才关注到 TLS1.3 是一个革命性具有里程碑意义的协议，但是这两个协议却一直在被另一个更新兴的协议所影响和挑战。 如果这个新兴的协议，它的名字就叫做 “快”，并且正在标准化为新一代的互联网传输协议。 你愿意花一点点时间了解这个协议吗？你愿意投入精力去研究这个协议吗？你愿意全力推动业务来使用这个协议吗？ QUIC 概述Quic 全称 quick udp internet connection [1]，“快速 UDP 互联网连接”，（和英文 quick 谐音，简称 “快”）是由 google 提出的使用 udp 进行多路并发传输的协议。 Quic 相比现在广泛应用的 http2+tcp+tls 协议有如下优势 [2]： 减少了 TCP 三次握手及 TLS 握手时间。 改进的拥塞控制。 避免队头阻塞的多路复用。 连接迁移。 前向冗余纠错。 TCP的缺陷从上个世纪 90 年代互联网开始兴起一直到现在，大部分的互联网流量传输只使用了几个网络协议。使用 IPv4 进行路由，使用 TCP 进行连接层面的流量控制，使用 SSL/TLS 协议实现传输安全，使用 DNS 进行域名解析，使用 HTTP 进行应用数据的传输。 而且近三十年来，这几个协议的发展都非常缓慢。TCP 主要是拥塞控制算法的改进，SSL/TLS 基本上停留在原地，几个小版本的改动主要是密码套件的升级，TLS1.3[3] 是一个飞跃式的变化，但截止到今天，还没有正式发布。IPv4 虽然有一个大的进步，实现了 IPv6，DNS 也增加了一个安全的 DNSSEC，但和 IPv6 一样，部署进度较慢。 随着移动互联网快速发展以及物联网的逐步兴起，网络交互的场景越来越丰富，网络传输的内容也越来越庞大，用户对网络传输效率和 WEB 响应速度的要求也越来越高。 一方面是历史悠久使用广泛的古老协议，另外一方面用户的使用场景对传输性能的要求又越来越高。如下几个由来已久的问题和矛盾就变得越来越突出。 协议历史悠久导致中间设备僵化。 依赖于操作系统的实现导致协议本身僵化。 建立连接的握手延迟大。 队头阻塞。 这里分小节简单说明一下： 中间设备的僵化可能是 TCP 协议使用得太久，也非常可靠。所以我们很多中间设备，包括防火墙、NAT 网关，整流器等出现了一些约定俗成的动作。 比如有些防火墙只允许通过 80 和 443，不放通其他端口。NAT 网关在转换网络地址时重写传输层的头部，有可能导致双方无法使用新的传输格式。整流器和中间代理有时候出于安全的需要，会删除一些它们不认识的选项字段。 TCP 协议本来是支持端口、选项及特性的增加和修改。但是由于 TCP 协议和知名端口及选项使用的历史太悠久，中间设备已经依赖于这些潜规则，所以对这些内容的修改很容易遭到中间环节的干扰而失败。 而这些干扰，也导致很多在 TCP 协议上的优化变得小心谨慎，步履维艰。 依赖于操作系统的实现导致协议僵化TCP 是由操作系统在内核西方栈层面实现的，应用程序只能使用，不能直接修改。虽然应用程序的更新迭代非常快速和简单。但是 TCP 的迭代却非常缓慢，原因就是操作系统升级很麻烦。 现在移动终端更加流行，但是移动端部分用户的操作系统升级依然可能滞后数年时间。PC 端的系统升级滞后得更加严重，windows xp 现在还有大量用户在使用，尽管它已经存在快 20 年。 服务端系统不依赖用户升级，但是由于操作系统升级涉及到底层软件和运行库的更新，所以也比较保守和缓慢。 这也就意味着即使 TCP 有比较好的特性更新，也很难快速推广。比如 TCP Fast Open。它虽然 2013 年就被提出了，但是 Windows 很多系统版本依然不支持它。 建立连接的握手延迟大不管是 HTTP1.0/1.1 还是 HTTPS，HTTP2，都使用了 TCP 进行传输。HTTPS 和 HTTP2 还需要使用 TLS 协议来进行安全传输。这就出现了两个握手延迟： 1.TCP 三次握手导致的 TCP 连接建立的延迟。 2.TLS 完全握手需要至少 2 个 RTT 才能建立，简化握手需要 1 个 RTT 的握手延迟。 对于很多短连接场景，这样的握手延迟影响很大，且无法消除。 队头阻塞队头阻塞主要是 TCP 协议的可靠性机制引入的。TCP 使用序列号来标识数据的顺序，数据必须按照顺序处理，如果前面的数据丢失，后面的数据就算到达了也不会通知应用层来处理。 另外 TLS 协议层面也有一个队头阻塞，因为 TLS 协议都是按照 record 来处理数据的，如果一个 record 中丢失了数据，也会导致整个 record 无法正确处理。 概括来讲，TCP 和 TLS1.2 之前的协议存在着结构性的问题，如果继续在现有的 TCP、TLS 协议之上实现一个全新的应用层协议，依赖于操作系统、中间设备还有用户的支持。部署成本非常高，阻力非常大。 所以 QUIC 协议选择了 UDP，因为 UDP 本身没有连接的概念，不需要三次握手，优化了连接建立的握手延迟，同时在应用程序层面实现了 TCP 的可靠性，TLS 的安全性和 HTTP2 的并发性，只需要用户端和服务端的应用程序支持 QUIC 协议，完全避开了操作系统和中间设备的限制。 0RTT建立连接现如今，高速且安全的网络接入服务已经成为人们的必须。传统 TCP+TLS 构建的安全互联服务，升级与补丁更新时有提出（如 TCP Fastopen，新的 TLS 1.3），但是由于基础设施僵化，升级与应用困难。为解决这个问题，Google 另辟蹊径在 UDP 的基础上实现了带加密的更好的 TCP–QUIC（Quick UDP Internet Connection), 一种基于 UDP 的低时延的互联网传输层协议。近期成立了 Working Group 也将 QUIC 作为制定 HTTP 3.0 的标准的基础, 说明 QUIC 的应用前景美好。本文单独就网络传输的建连问题展开了分析, 浅析了建连时间对传输的影响, 以及 QUIC 的 0-RTT 建连是如何解决建连耗时长的问题的。在此基础上, 结合 QUIC 的源码, 浅析了 QUIC 的基本实现, 并描述一种可供参考的分布式环境下的 0-RTT 的落地实践方案。 以一次简单的 HTTPS 请求（example.com）为例（假设访问 example.com 时返回的内容较小，server 端可以在一个数据包里返回响应），为获取请求资源。需要经过以下 4 个步骤： DNS 查询 example.com 获取 IP。DNS 服务一般默认是由你的 ISP 提供，ISP 通常都会有缓存的，这部分时间能适当减少； TCP 握手，我们熟悉的 TCP 三次握手需要需要 1 个 RTT； TLS 握手，以目前应用最广泛的 TLS 1.2 而言，需要 2 个 RTT。对于非首次建连, 可以选择启用会话重用 (Session Resumption)，则可缩小握手时间到 1 个 RTT； HTTP 业务数据交互，假设 example.com 的数据在一次交互就能取回来。那么业务数据的交互需要 1 个 RTT； 经过上面的过程分析可知，要完成一次简短的 HTTPS 业务数据交互，需要经历： 新连接：4RTT + DNS。 会话重用：3RTT + DNS 尤其对于小数据量的交互而言，抛开 DNS 查询时间, 建连时间占剩下的总时间的 2/3 至 3/4 不等，影响不可小觑。加之如果用户网络不好，RTT 延时大的话，建连时间可能耗费数百毫秒至数秒不等，这将极大的影响用户体验。究其原因一方面是 TCP 和 TLS 分层设计导致的：分层的设计需要每个逻辑层次分别建立自己的连接状态。另一方面是 TLS 的握手阶段复杂的密钥协商机制导致的。要降低建连耗时，需要从这两方面着手。 针对 TLS 的握手阶段复杂的密钥协商机问题, TLS 1.3 精简了握手交互过程, 实现了 1-RTT 握手。在会话重用类似的理念的基础上, 对非首次握手会话, 可以进一步实现 0-RTT 握手 (在刚开始 TLS 密钥协商的时候，就能附送一部分经过加密的数据传递给对方)。由于 TLS 是建立在 TCP 之上的, 0-RTT 没有计算 TCP 层的握手开销, 因而对用户来说, 发送数据之前还是要经历 TCP 层的 1RTT 握手, 因而不是真正的 0-RTT 握手。 TLS 1.3 为实现 0-RTT，需要双方在刚开始建立连接的时候就已经持有一个对称密钥，这个密钥在 TLS 1.3 中称为 PSK（Pre-Shared-Key）。PSK 是 TLS 1.2 中的会话重用 (Session Resumption) 机制的一个升级，TLS 1.3 握手结束后，服务器可以发送一个 NST（New-Session-Ticket）的报文给客户端，该报文中记录 PSK 的值、名字和有效期等信息，双方下一次建立连接可以使用该 PSK 值作为初始密钥材料。因为 PSK 是从以前建立的安全信道中获得的，只要证明了双方都持有相同的 PSK，不再需要证书认证，就可以证明双方的身份(因此 PSK 也是一种身份认证机制)。TLS 1.3 新加了 Early Data 类型的报文, 用于在 0-RTT 的握手阶段传递应用层数据, 实现了握手的同时就能附带加密的应用层数据从而实现 0-RTT。 TLS 1.3 的 0-RTT 特性不能防止重放攻击, 需要业务在使用时评估是否有重放攻击风险。如有相关风险的话, 可能需要酌情考虑禁用 0-RTT 特性。 下图对比了 TLS 各版本与场景下的延时对比: TLS 各版本与场景下的耗时 从对比我们可以看到, 即使用上了 TLS 1.3, 精简了握手过程, 最快能做到 0-RTT 握手 (首次是 1-RTT), 但是对用户感知而言, 还要加上 1RTT 的 TCP 握手开销。 Google 有提出 Fastopen 的方案来使得 TCP 非首次握手就能附带用户数据, 但是由于 TCP 实现僵化, 无法升级应用, 相关 RFC 到现今都是 experimental 状态。这种分层设计带来的延时, 有没有办法进一步降低呢? QUIC 通过合并加密与连接管理解决了这个问题, 我们来看看其是如何实现真正意义上的 0-RTT 的握手, 让与 server 进行第一个数据包的交互就能带上用户数据。 QUIC 为规避 TCP 协议僵化的问题，将 QUIC 协议建立在了 UDP 之上。考虑到安全性是网络的必备选项，加密在 QUIC 里强制的。传输方面参考 TCP 并充分优化了 TCP 多年发现的缺陷和不足, 实现了一套端到端的可靠加密传输。通过将加密和连接管理两层合二为一，消除了当前 TCP+TLS 的分层设计传输引入的时延。 同 TLS 的握手一样, QUIC 的加密握手的核心在于协商出一个加密会话数据的对称密钥。QUIC 的握手使用了 DH 密钥协商算法来协商一个对称密钥。DH 密钥协商算法简单来讲, 需要通信双方各自生成自己的非对称公私钥对, 双发各自保留自己的私钥, 将公钥发给对方, 利用对方的公钥和自己的私钥可以运算出同一个对称密钥。详细的原理这里不展开叙述, 有专业的密码学书籍对其原理有详细的论述, 网上也有很多好的教程对其由深入浅出的总结, 如这一篇。 如上所述, DH 密钥协商需要通行双方各自生成自己的非对称公私钥对。server 端与客户端的关系是 1 对 N 的关系, 明显 server 端生成一份公私钥对, 让 N 个客户端公用, 能明显减少生成开销, 降低管理的成本。server 端的这份公私钥对就是专门用于握手使用的, 客户端一经获取, 就可以缓存下来后续建连时继续使用, 这个就是达成 0-RTT 握手的关键, 因此 server 生成的这份公钥称为 0-RTT 握手公钥。真正的握手过程是这样 (简化了实现细节): server 端在握手开始前，server 端需要首先生成 (或加载使用上次保存下来的) 握手公私钥对, 该份公私钥对是所有客户端共享的。 client 端首次握手时, client 对 server 一无所知, 需要 1 个 RTT 来询问 server 端的握手公钥 (实际的握手交互还会发送诸如版本等其他数据) 并缓存下来。本步骤只在首次建连时发生(0-RTT 握手公钥的过期也会导致需要重走这一步), 但这种情况很少发生, 影响很小(也没办法避免)。 client 收到 server 端返回这份握手公钥后，生成自己的临时公私钥对后, 计算出共享的对称密钥后, 加密好数据, 并连同 client 的公钥一并发给 server 端。照 DH 密钥协商的原理, 此处已经可以协商出每条会话不一样的会话密钥了 (因为每个 client 生成的公私钥是不同的), 是不是拿这个来加密会话数据就行了呢? 真实的情况不是这样的! server 端会再次生成一份临时的公私钥对，使用这份临时的私钥与客户端的公钥运算出最终的会话对称密钥。接下来 server 会拿这个最终的会话密钥加密应用层数据, 连同这份临时的 server 端公钥一并发给 client 端, client 端收到后可以按照 DH 的原理依瓢画葫会恢复出最终的会话对称密钥。后续所有的数据都是用最终的会话对称密钥进行加密。server 侧这个动作是不是多此一举呢? 不是的, 这么做的目的是为了获取所谓的前向安全特性: 因为 server 端的后面生成的这份公私钥是临时生成的, 不会保存下来，也就杜绝了密钥泄漏导致会话数据被恶意收集后的被解密掉的风险。 首次握手要多一个 RTT 询问 server 端的 0-RTT 握手公钥, 此询问过程不携带任何应用层数据, 因此是 1-RTT 握手。在首次握手完成后, client 端可以缓存下第一次询问获知的 server 端的公钥信息, 后续的连接过程可以跳过询问，直接使用缓存的 server 端的公钥。公钥信息在 QUIC 的实现里是保存在握手数据包里的 SCFG 中的 (Server Config), 获取 0-RTT 握手公钥就是要获取 SCFG, 后续均以获取 SCFG 代替。详细的握手过程, 可以参见 dog250 博客文章的详细叙述。 从上面的分析可知，0-RTT 握手的首次交互，server 端使用的是保存下来的握手密钥，因而没有无法做到前向安全，不能防止重放攻击, 需要业务在使用时评估是否有重放攻击风险。 QUIC 0-RTT 握手 上文简述了 QUIC 握手的密钥协商过程，首次需要询问一次 Server 以获取 SCFG, 从而获取存储于其中的 0-RTT 握手公钥。这一交互过程中 client 和 server 在 QUIC 的握手协议发送的报文中分别叫 Inchoate Client hello message (inchoate CHLO) 和 Rejection message (REJ)。因而包含 server 端的握手公钥的 SCFG 是在 REJ 报文中发给客户端的。有了 SCFG，接下来就能发起 0-RTT 握手。这一过程中 client 和 server 端在 QUIC 的握手协议发送的报文中分别叫 Full Client Hello message (full CHLO) 和 Server Hello message (SHLO)。下图摘自 Google QUIC 握手协议的官方文档，详细叙述了握手过程 client 侧的处理流程： QUIC 握手客户端侧处理流程 0RTT的重放攻击风险0-RTT连接恢复并非那么简单，它会带来许多意外风险及警告，这正是为什么有些默认程序不启用0-RTT连接恢复的原因。用户必须提前考虑所涉及的风险，并做出决定是否使用此功能。可以在应用程序层面使用预防措施来减轻这种情况。 首先，0-RTT连接恢复是不提供forwardsecrecy的，针对连接的secret parameters的妥协将微不足道地允许恢复新连接0-RTT阶段期间，发送applicationdata 进行特定的妥协。 在0-RTT阶段之后、或握手之后发送的数据，仍然是安全的。这是因为TLS1.3 (及QUIC)还是会对handshakecompletion之后发送的数据执行normal key exchange algorithm (这是forwardsecret) 。 值得注意的是，在0-RTT期间发送的applicationdata 可以被路径上的on-path attacker 捕获，然后多次重播到同一个服务器。这个在大部分情况下也许不是问题，因为attacker无法解密数据，0-RTT连接恢复还是非常有用的。在其他的情况下，这可能会引起出乎意料的风险。 举个比例，假设一家银行允许authenticated user (e.g using HTTPcookie, 或其他HTTP身份验证机制)通过specificAPI endpoint发出HTTP请求将资金从其账户发送到另一个用户。 如果attacker能够在使用0-RTT连接恢复时捕获该请求，他们将无法看到plaintext并且获取用户的凭据，原因是因为他们不知道用于加密数据的secretkey；但是他们仍然有可能一直散发重复性地请求，耗尽该用户的银行账户里的钱。 改进的拥塞控制TCP 的拥塞控制实际上包含了四个算法：慢启动，拥塞避免，快速重传，快速恢复 [22]。 QUIC 协议当前默认使用了 TCP 协议的 Cubic 拥塞控制算法 [6]，同时也支持 CubicBytes, Reno, RenoBytes, BBR, PCC 等拥塞控制算法。 从拥塞算法本身来看，QUIC 只是按照 TCP 协议重新实现了一遍，那么 QUIC 协议到底改进在哪些方面呢？主要有如下几点： 可插拔什么叫可插拔呢？就是能够非常灵活地生效，变更和停止。体现在如下方面： 应用程序层面就能实现不同的拥塞控制算法，不需要操作系统，不需要内核支持。这是一个飞跃，因为传统的 TCP 拥塞控制，必须要端到端的网络协议栈支持，才能实现控制效果。而内核和操作系统的部署成本非常高，升级周期很长，这在产品快速迭代，网络爆炸式增长的今天，显然有点满足不了需求。 即使是单个应用程序的不同连接也能支持配置不同的拥塞控制。就算是一台服务器，接入的用户网络环境也千差万别，结合大数据及人工智能处理，我们能为各个用户提供不同的但又更加精准更加有效的拥塞控制。比如 BBR 适合，Cubic 适合。 应用程序不需要停机和升级就能实现拥塞控制的变更，我们在服务端只需要修改一下配置，reload 一下，完全不需要停止服务就能实现拥塞控制的切换。 STGW 在配置层面进行了优化，我们可以针对不同业务，不同网络制式，甚至不同的 RTT，使用不同的拥塞控制算法。 单调递增的 Packet NumberTCP 为了保证可靠性，使用了基于字节序号的 Sequence Number 及 Ack 来确认消息的有序到达。 QUIC 同样是一个可靠的协议，它使用 Packet Number 代替了 TCP 的 sequence number，并且每个 Packet Number 都严格递增，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值。而 TCP 呢，重传 segment 的 sequence number 和原始的 segment 的 Sequence Number 保持不变，也正是由于这个特性，引入了 Tcp 重传的歧义问题。 如上图所示，超时事件 RTO 发生后，客户端发起重传，然后接收到了 Ack 数据。由于序列号一样，这个 Ack 数据到底是原始请求的响应还是重传请求的响应呢？不好判断。 如果算成原始请求的响应，但实际上是重传请求的响应（上图左），会导致采样 RTT 变大。如果算成重传请求的响应，但实际上是原始请求的响应，又很容易导致采样 RTT 过小。 由于 Quic 重传的 Packet 和原始 Packet 的 Pakcet Number 是严格递增的，所以很容易就解决了这个问题。 如上图所示，RTO 发生后，根据重传的 Packet Number 就能确定精确的 RTT 计算。如果 Ack 的 Packet Number 是 N+M，就根据重传请求计算采样 RTT。如果 Ack 的 Pakcet Number 是 N，就根据原始请求的时间计算采样 RTT，没有歧义性。 但是单纯依靠严格递增的 Packet Number 肯定是无法保证数据的顺序性和可靠性。QUIC 又引入了一个 Stream Offset 的概念。 即一个 Stream 可以经过多个 Packet 传输，Packet Number 严格递增，没有依赖。但是 Packet 里的 Payload 如果是 Stream 的话，就需要依靠 Stream 的 Offset 来保证应用数据的顺序。如错误! 未找到引用源。所示，发送端先后发送了 Pakcet N 和 Pakcet N+1，Stream 的 Offset 分别是 x 和 x+y。 假设 Packet N 丢失了，发起重传，重传的 Packet Number 是 N+2，但是它的 Stream 的 Offset 依然是 x，这样就算 Packet N + 2 是后到的，依然可以将 Stream x 和 Stream x+y 按照顺序组织起来，交给应用程序处理。 不允许 Reneging什么叫 Reneging 呢？就是接收方丢弃已经接收并且上报给 SACK 选项的内容 [8]。TCP 协议不鼓励这种行为，但是协议层面允许这样的行为。主要是考虑到服务器资源有限，比如 Buffer 溢出，内存不够等情况。 Reneging 对数据重传会产生很大的干扰。因为 Sack 都已经表明接收到了，但是接收端事实上丢弃了该数据。 QUIC 在协议层面禁止 Reneging，一个 Packet 只要被 Ack，就认为它一定被正确接收，减少了这种干扰。 更多的 Ack 块TCP 的 Sack 选项能够告诉发送方已经接收到的连续 Segment 的范围，方便发送方进行选择性重传。 由于 TCP 头部最大只有 60 个字节，标准头部占用了 20 字节，所以 Tcp Option 最大长度只有 40 字节，再加上 Tcp Timestamp option 占用了 10 个字节 [25]，所以留给 Sack 选项的只有 30 个字节。 每一个 Sack Block 的长度是 8 个，加上 Sack Option 头部 2 个字节，也就意味着 Tcp Sack Option 最大只能提供 3 个 Block。 但是 Quic Ack Frame 可以同时提供 256 个 Ack Block，在丢包率比较高的网络下，更多的 Sack Block 可以提升网络的恢复速度，减少重传量。 Ack Delay 时间Tcp 的 Timestamp 选项存在一个问题 [25]，它只是回显了发送方的时间戳，但是没有计算接收端接收到 segment 到发送 Ack 该 segment 的时间。这个时间可以简称为 Ack Delay。 这样就会导致 RTT 计算误差。如下图： 可以认为 TCP 的 RTT 计算：RTT = timestamp2 - timestamp1 而 Quic 计算如下：RTT = timestamp2 - timestamp1 - AckDelay 当然 RTT 的具体计算没有这么简单，需要采样，参考历史数值进行平滑计算，参考如下公式12SRTT = SRTT + α(RTT - SRTT)RTO = μ * SRTT + δ*DevRTT 基于 stream 和 connecton 级别的流量控制QUIC 的流量控制 [22] 类似 HTTP2，即在 Connection 和 Strea6 级别提供了两种流量控制。为什么需要两类流量控制呢？主要是因为 QUIC 支持多路复用。 Stream 可以认为就是一条 HTTP 请求。 Connection 可以类比一条 TCP 连接。多路复用意味着在一7 Connetion 上会同时存在多条 Stream。既需要对单个 Stream 进行控制，又需要针对所有 Stream 进行总体控制。 QUIC 实现流量控制的原理比较简单： 通过 window_update 帧告诉对端自己可以接收的字节数，这样发送方就不会发送超过这个数量的数据。 通过 BlockFrame 告诉对端由于流量控制被阻塞了，无法发送数据。 QUIC 的流量控制和 TCP 有点区别，TCP 为了保证可靠性，窗口左边沿向右滑动时的长度取决于已经确认的字节数。如果中间出现丢包，就算接收到了更大序号的 Segment，窗口也无法超过这个序列号。 但 QUIC 不同，就算此前有些 packet 没有接收到，它的滑动只取决于接收到的最大偏移字节数。 针对 Stream：可用窗口 = 最大窗口数 - 接收到的最大偏移数 针对 Connection：可用窗口 = stream1可用窗口 + stream2可用窗口 + ... + streamN可用窗口 同样地，STGW 也在连接和 Stream 级别设置了不同的窗口数。 最重要的是，我们可以在内存不足或者上游处理性能出现问题时，通过流量控制来限制传输速率，保障服务可用性。 没有队头阻塞的多路复用QUIC 的多路复用和 HTTP2 类似。在一条 QUIC 连接上可以并发发送多个 HTTP 请求 (stream)。但是 QUIC 的多路复用相比 HTTP2 有一个很大的优势。 QUIC 一个连接上的多个 stream 之间没有依赖。这样假如 stream2 丢了一个 udp packet，也只会影响 stream2 的处理。不会影响 stream2 之前及之后的 stream 的处理。 这也就在很大程度上缓解甚至消除了队头阻塞的影响。 多路复用是 HTTP2 最强大的特性 [7]，能够将多条请求在一条 TCP 连接上同时发出去。但也恶化了 TCP 的一个问题，队头阻塞 [11]，如下图示： HTTP2 在一个 TCP 连接上同时发送 4 个 Stream。其中 Stream1 已经正确到达，并被应用层读取。但是 Stream2 的第三个 tcp segment 丢失了，TCP 为了保证数据的可靠性，需要发送端重传第 3 个 segment 才能通知应用层读取接下去的数据，虽然这个时候 Stream3 和 Stream4 的全部数据已经到达了接收端，但都被阻塞住了。 不仅如此，由于 HTTP2 强制使用 TLS，还存在一个 TLS 协议层面的队头阻塞 [12]。 Record 是 TLS 协议处理的最小单位，最大不能超过 16K，一些服务器比如 Nginx 默认的大小就是 16K。由于一个 record 必须经过数据一致性校验才能进行加解密，所以一个 16K 的 record，就算丢了一个字节，也会导致已经接收到的 15.99K 数据无法处理，因为它不完整。 那 QUIC 多路复用为什么能避免上述问题呢？ QUIC 最基本的传输单元是 Packet，不会超过 MTU 的大小，整个加密和认证过程都是基于 Packet 的，不会跨越多个 Packet。这样就能避免 TLS 协议存在的队头阻塞。 Stream 之间相互独立，比如 Stream2 丢了一个 Pakcet，不会影响 Stream3 和 Stream4。不存在 TCP 队头阻塞。 当然，并不是所有的 QUIC 数据都不会受到队头阻塞的影响，比如 QUIC 当前也是使用 Hpack 压缩算法 [10]，由于算法的限制，丢失一个头部数据时，可能遇到队头阻塞。 总体来说，QUIC 在传输大量数据时，比如视频，受到队头阻塞的影响很小。 加密认证的报文TCP 协议头部没有经过任何加密和认证，所以在传输过程中很容易被中间网络设备篡改，注入和窃听。比如修改序列号、滑动窗口。这些行为有可能是出于性能优化，也有可能是主动攻击。 但是 QUIC 的 packet 可以说是武装到了牙齿。除了个别报文比如 PUBLIC_RESET 和 CHLO，所有报文头部都是经过认证的，报文 Body 都是经过加密的。 这样只要对 QUIC 报文任何修改，接收端都能够及时发现，有效地降低了安全风险。 如下图所示，红色部分是 Stream Frame 的报文头部，有认证。绿色部分是报文内容，全部经过加密。 连接迁移一条 TCP 连接 [17] 是由四元组标识的（源 IP，源端口，目的 IP，目的端口）。什么叫连接迁移呢？就是当其中任何一个元素发生变化时，这条连接依然维持着，能够保持业务逻辑不中断。当然这里面主要关注的是客户端的变化，因为客户端不可控并且网络环境经常发生变化，而服务端的 IP 和端口一般都是固定的。 比如大家使用手机在 WIFI 和 4G 移动网络切换时，客户端的 IP 肯定会发生变化，需要重新建立和服务端的 TCP 连接。 又比如大家使用公共 NAT 出口时，有些连接竞争时需要重新绑定端口，导致客户端的端口发生变化，同样需要重新建立 TCP 连接。 针对 TCP 的连接变化，MPTCP[5] 其实已经有了解决方案，但是由于 MPTCP 需要操作系统及网络协议栈支持，部署阻力非常大，目前并不适用。 所以从 TCP 连接的角度来讲，这个问题是无解的。 那 QUIC 是如何做到连接迁移呢？很简单，任何一条 QUIC 连接不再以 IP 及端口四元组标识，而是以一个 64 位的随机数作为 ID 来标识，这样就算 IP 或者端口发生变化时，只要 ID 不变，这条连接依然维持着，上层业务逻辑感知不到变化，不会中断，也就不需要重连。 由于这个 ID 是客户端随机产生的，并且长度有 64 位，所以冲突概率非常低。 其他亮点此外，QUIC 还能实现前向冗余纠错，在重要的包比如握手消息发生丢失时，能够根据冗余信息还原出握手消息。 QUIC 还能实现证书压缩，减少证书传输量，针对包头进行验证等。 限于篇幅，本文不再详细介绍，有兴趣的可以参考文档 [23] 和文档 [4] 和文档 [26]。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>QUIC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-TCP详解]]></title>
    <url>%2F2021%2F02%2F16%2Fself_cultivation_tcp%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[TCP包头长度 20个字节 三次握手 如果第三次握手的ack丢失了咋办当客户端收到服务端的SYNACK应答后，其状态变为ESTABLISHED，并会发送ACK包给服务端，准备发送数据了。如果此时ACK在网络中丢失（如上图所示），过了超时计时器后，那么服务端会重新发送SYNACK包，重传次数根据/proc/sys/net/ipv4/tcp_synack_retries来指定，默认是5次。如果重传指定次数到了后，仍然未收到ACK应答，那么一段时间后，Server自动关闭这个连接。 问题就在这里，客户端已经认为连接建立，而服务端则可能处在SYN-RCVD或者CLOSED，接下来我们需要考虑这两种情况下服务端的应答： 服务端处于CLOSED，当接收到连接已经关闭的请求时，服务端会返回RST 报文，客户端接收到后就会关闭连接，如果需要的话则会重连，那么那就是另一个三次握手了。 服务端处于SYN-RCVD，此时如果接收到正常的ACK 报文，那么很好，连接恢复，继续传输数据；如果接收到写入数据等请求呢？注意了，此时写入数据等请求也是带着ACK 报文的，实际上也能恢复连接，使服务器恢复到ESTABLISHED状态，继续传输数据。 SYN-Flood与SYN-Cookie所谓SYN-Flood(SYN 洪泛攻击)，就是利用SYNACK 报文的时候，服务器会为客户端请求分配缓存，那么黑客（攻击者），就可以使用一批虚假的ip向服务器大量地发建立TCP 连接的请求，服务器为这些虚假ip分配了缓存后，处在SYN_RCVD状态，存放在半连接队列中；另外，服务器发送的请求又不可能得到回复（ip都是假的，能回复就有鬼了），只能不断地重发请求，直到达到设定的时间/次数后，才会关闭。 服务器不断为这些半开连接分配资源，导致服务器的连接资源被消耗殆尽，不过所幸，我们可以使用SYN Cookie进行稍微的防御一下。 所谓的SYN Cookie防御系统，与前面接收到SYN 报文就分配缓存不同，此时暂不分配资源；同时利用SYN 报文的源和目的地IP和端口，以及服务器存储的一个秘密数，使用它们进行散列，得到server_isn作为服务端的初始 TCP 序号，也就是所谓的SYN cookie, 然后将SYNACK 报文中发送给客户端，接下来就是对ACK 报文进行判断，如果其返回的ack里的确认号正好等于server_isn + 1，说明这是一个合法的ACK，那么服务器才会为其生成一个具有套接字的全开的连接。(有点类似于JWT那一套机制哈) 缺点: 增加了密码学运算, 增大了cpu消耗 因为没有保存半连接状态, 所以无法存储一些比如大窗口/sack等信息 四次挥手 timewait的意义 2msl之后网络中的数据分节全部消失, 防止影响到复用了原端口ip的新连接 如果b没收到最后一个ack, b就会重发fin, a如果不维护一个timewait却收到了一个fin会感觉莫名其妙然后响应一个rst, 然后b就会解释为一个错误 timewait和closewait太多咋办 timewait太多咋办? net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭； net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。 net.ipv4.tcp_fin_timeout这个时间可以减少在异常情况下服务器从FIN-WAIT-2转到TIME_WAIT的时间。 closewait太多咋办? 解决方案只有: 查代码. 因为如果一直保持在CLOSE_WAIT状态，那么只有一种情况，就是在对方关闭连接之后服务器程序自己没有进一步发出fin信号。换句话说，就是在对方连接关闭之后，程序里没有检测到，或者由于什么逻辑bug导致服务端没有主动发起close, 或者程序压根就忘记了这个时候需要关闭连接，于是这个资源就一直被程序占着。 tcp拥塞控制 快速重传: 报文段1成功接收并被确认ACK 2，接收端的期待序号为2，当报文段2丢失，报文段3失序到来，与接收端的期望不匹配，接收端重复发送冗余ACK 2。这样，如果在超时重传定时器溢出之前，接收到连续的三个重复冗余ACK（其实是收到4个同样的ACK，第一个是正常的，后三个才是冗余的），发送端便知晓哪个报文段在传输过程中丢失了，于是重发该报文段，不需要等待超时重传定时器溢出，大大提高了效率。这便是快速重传机制。 快速恢复 慢启动 拥塞避免 tcp滑动窗口 每个TCP连接的两端都维护一组窗口：发送窗口结构（send window structure）和接收窗口结构（receive window structure）。TCP以字节为单位维护其窗口结构。TCP头部中的窗口大小字段相对ACK号有一个字节的偏移量。发送端计算其可用窗口，即它可以立即发送的数据量。可用窗口（允许发送但还未发送）计算值为提供窗口（即由接收端通告的窗口）大小减去在传（已发送但未得到确认）的数据量。图中P1、P2、P3分别记录了窗口的左边界、下次发送的序列号、右边界。 如上图所示， 随着发送端接收到返回的数据ACK，滑动窗口也随之右移。发送端根据接收端返回的ACK可以得到两个重要的信息：一是接收端期望收到的下一个字节序号；二是当前的窗口大小（再结合发送端已有的其他信息可以得出还能发送多少字节数据）。 需要注意的是：发送窗口的左边界只能右移，因为它控制的是已发送并受到确认的数据，具有累积性，不能返回；右边界可以右移也可以左移（能左移的右边界会带来一些缺陷，下文会讲到）。 接收端也维护一个窗口结构，但比发送窗口简单（只有左边界和右边界）。该窗口结构记录了已接收并确认的数据，以及它能够接收的最大序列号，该窗口能保证接收数据的正确性（避免存储重复的已接收和确认的数据，以及避免存储不应接收的数据）。由于TCP的累积ACK特性，只有当到达数据序列号等于左边界时，窗口才能向前滑动。 零窗口与TCP持续计时器 Zero Window 上图，我们可以看到一个处理缓慢的Server（接收端）是怎么把Client（发送端）的TCP Sliding Window给降成0的。此时，你一定会问，如果Window变成0了，TCP会怎么样？是不是发送端就不发数据了？是的，发送端就不发数据了，你可以想像成“Window Closed”，那你一定还会问，如果发送端不发数据了，接收方一会儿Window size 可用了，怎么通知发送端呢？ 解决这个问题，TCP使用了Zero Window Probe技术，缩写为ZWP，也就是说，发送端在窗口变成0后，会发ZWP的包给接收方，让接收方来ack他的Window尺寸，一般这个值会设置成3次，第次大约30-60秒（不同的实现可能会不一样）。如果3次过后还是0的话，有的TCP实现就会发RST把链接断了。 ACK延迟确认机制接收方在收到数据后，并不会立即回复ACK,而是延迟一定时间。一般ACK延迟发送的时间为200ms，但这个200ms并非收到数据后需要延迟的时间。系统有一个固定的定时器每隔200ms会来检查是否需要发送ACK包。这样做有两个目的。 这样做的目的是ACK是可以合并的，也就是指如果连续收到两个TCP包，并不一定需要ACK两次，只要回复最终的ACK就可以了，可以降低网络流量。 如果接收方有数据要发送，那么就会在发送数据的TCP数据包里，带上ACK信息。这样做，可以避免大量的ACK以一个单独的TCP包发送，减少了网络流量。]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-Linux高低精度定时器实现原理]]></title>
    <url>%2F2021%2F02%2F06%2Fself_cultivation_linux_timer%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Linux定时器实现原理时间轮定时器-低分辨率实现Linux 2.6.16之前，内核只支持低精度时钟，内核定时器的工作方式： 系统启动后，会读取时钟源设备(RTC,HPET，PIT…)，初始化当前系统时间。 内核会根据HZ(系统定时器频率，节拍率)参数值，设置时钟事件设备，启动tick(节拍)中断。HZ表示1秒种产生多少个时钟硬件中断，tick就表示连续两个中断的间隔时间。 设置时钟事件设备后，时钟事件设备会定时产生一个tick中断，触发时钟中断处理函数，更新系统时钟,并检测timer wheel，进行超时事件的处理。 在上面工作方式下，Linux 2.6.16 之前，内核软件定时器采用timer wheel多级时间轮的实现机制，维护操作系统的所有定时事件。timer wheel的触发是基于系统tick周期性中断。所以说这之前，linux只能支持ms级别的时钟，随着时钟源硬件设备的精度提高和软件高精度计时的需求，有了高精度时钟的内核设计。 所谓低分辨率定时器，是指这种定时器的计时单位基于jiffies值的计数，也就是说，它的精度只有1HZ，假如你的内核配置的HZ是1000，那意味着系统中的低分辨率定时器的精度就是1ms。早期的内核版本中，内核并不支持高精度定时器，理所当然只能使用这种低分辨率定时器, 后来随着时钟源硬件设备的精度提高和软件高精度计时的需求，才有了高精度时钟的内核设计 . . . 时间轮算法思想多级时间轮, 插入/删除/execute复杂度都O(1) 算法思想: 把定时器分为 5 个桶，每桶的粒度分别表示为：1 jiffies，256 jiffies，256*64 jiffies，256*64*64 jiffies，256*64*64*64 jiffies，每桶bucket中的slot的数量分别为：256，64，64，64，64，能表示的范围为 2^32 这好几个bucket, 其中一个bucket叫near是差不多要触发的定时器范围是[0, 0x100), 和几个定时时长比较久的bucket: [0x100, 0x4000)以及[0x4000, 0x100000)以及[0x100000, 0x4000000) tick: 每次tick都检查jiffies是否已经又经过一轮 TVR_MASK(255) 了, 经过了一轮index就又等于0, 然后就去后面的bucket[0][INDEX(0)]里去拿定时器迁移到near里(这个INDEX(0)宏其实是拿到jiffies_的第9到14位的值), 如果INDEX(0)也等于0, 则说明bucket[0]也轮转迁移了一圈了, 接着就需要去bucket[1]里拿定时器迁移到bucket[0]里, 后面INDEX(1)和INDEX(2)对应的bucket调整都以此类推, 这就跟水表一样, 小表转一圈需要调整中表, 中表转一圈则要调整大表差不多 为啥可以直接把这个bucket[0][INDEX(0)]里的定时器直接迁移到near里呢? 因为在插入的时候就是这么哈希的, 举个比较简单的不准确但是可以说明原理的例子, 假如 near里是存最近60秒过期的定时器, bucket[0][0]存的是60到120过期的, bucket[0][1]的是120到180过期的, 则jiffies等于60的时候就要把bucket[0][0]迁移到near里, jiffies等于120的时候bucket[0][1]迁移到near里… 类似于linux的时间轮实现: 假设curr_time=0x12345678，那么下一个检查的时刻为0x12345679。如果tv1.bucket[0x79]上链表非空，则下一个检查时刻tv1.bucket[0x79]上的定时器节点超时。如果curr_time到了0x12345700，低8位为空，说明有进位产生，这时移出9～14位对应的定时器链表(即正好对应着tv2轮)，把tv2.bucket[此时9-14位的值]所对应的timer链表迁移到tv1来，这就完成了一次进位迁移操作。同样地，当curr_time的第9-14位为0时，这表明tv2轮对tv3轮有进位发生，将curr_time第14-19位的值作为下标，移出tv3中对应的定时器链表，然后将它们迁移到tv2去。tv4,tv5依次类推。之所以能够根据curr_time来检查超时链，是因为tv1~tv5轮的度量范围正好依次覆盖了整型的32位：tv1(1-8位)，tv2(9-14位)，tv3(15-20位)，tv4(21-26位)，tv5(27-32位)；而curr_time计数的递增中，低位向高位的进位正是低级时间轮转圈带动高级时间轮走动的过程。 插入: 有好几个bucket, 然后用类似于取模哈希的思想先判断还有多久过期的区间, 然后根据过期时间expire取他相应的位放入相应的桶里的某个slot的定时器链表TimerList里即可, 参考下方代码 excute: near_ 里面的定时器因为都已经在 addTimerNode 根据expire哈希安插好了, 所以这里 jiffies_ &amp; TVR_MASK 出来的index是几, 那就直接从near_里取出来执行就完事了,见下方代码 删除: 因为插入的时候还专门另外有个哈希表来保存定时器id和定时器的映射关系, 所有删除的时候就直接根据传入的定时器id来找到定时器本身然后把他标记为已删除, 然后在excute的时候会找到near_[index]这个定时器链表TimerList移除 删除: 惰性删除, 只是标记相关node为被canceled, 然后excute的时候再freeNode tickless: 不嫌麻烦还可以每次从 timer 集合里面选择最先要超时的事件，计算还有多长时间就会超时，作为 select wait 的值，每次都不一样，每次都基本精确，同时不会占用多余 cpu，这叫 tickless，Linux 的 3.x以上版本也支持 tickless 的模式来驱动各种系统级时钟，号称更省电更精确，不过需要你手动打开，FreeBSD 9 以后也引入了 tickless。 时间轮有什么缺点虽然大部分时间里，时间轮可以实现O(1)时间复杂度，但是当有进位发生时，不可预测的O(N)定时器级联迁移时间，这对于低分辨率定时器来说问题不大，可是它大大地影响了定时器的精度； 时间轮核心代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697void WheelTimer::addTimerNode(TimerNode* node)&#123; int64_t expires = node-&gt;expire; uint64_t idx = (uint64_t)(expires - jiffies_); TimerList* list = nullptr; if (idx &lt; TVR_SIZE) // [0, 0x100) &#123; int i = expires &amp; TVR_MASK; // 因为只关心后8位(即TVR_BITS=8) list = &amp;near_[i]; &#125; else if(idx &lt; (1 &lt;&lt; (TVR_BITS + TVN_BITS))) // [0x100, 0x4000) &#123; // 因为不关心后8位(即TVR_BITS=8)的数, 所以直接 expires &gt;&gt; TVR_BITS 了 // 又因为 TimerList buckets_[WHEEL_BUCKETS][TVN_SIZE] 的第二维为 TVN_SIZE, 所以要 &amp; TVN_MASK int i = (expires &gt;&gt; TVR_BITS) &amp; TVN_MASK; list = &amp;buckets_[0][i]; &#125; else if(idx &lt; (1 &lt;&lt; (TVR_BITS + 2 * TVN_BITS))) // [0x4000, 0x100000) &#123; ...&#125;// #define INDEX(N) ( ( jiffies_ &gt;&gt; (8 + (N) * 6) ) &amp; 1111 11)#define INDEX(N) ((jiffies_ &gt;&gt; (TVR_BITS + (N) * TVN_BITS)) &amp; TVN_MASK)// cascades all vectors and executes all expired timerint WheelTimer::tick()&#123; int fired = 0; // 每次tick都检查是否已经又经过一轮 TVR_MASK(255) 了, // 经过了一轮index就又等于0, 然后就去后面的bucket里找是否有需要调整到near的定时器 // 就跟水表一样, 小表转一圈需要调整中表, 中表转一圈则要调整大表 int index = jiffies_ &amp; TVR_MASK; if(index == 0) // cascade timers &#123; if(cascade(0, INDEX(0)) &amp;&amp; cascade(1, INDEX(1)) &amp;&amp; cascade(2, INDEX(2)) ) cascade(3, INDEX(3)); &#125; jiffies_++; fired += execute(index); return fired;&#125;int WheelTimer::execute()&#123; int fired = 0; // near 里面的定时器因为都已经在 addTimerNode 根据expire里哈希安插好了, // 所以这里 jiffies_ &amp; TVR_MASK 出来的index是几, 那就直接从near_里取出来执行就完事了 int index = jiffies_ &amp; TVR_MASK; TimerList expired; near_[index].swap(expired); // swap list for (auto node : expired) &#123; if (!node-&gt;canceled &amp;&amp; node-&gt;cb) &#123; //printf("wheel node %d triggered at %lld of jiffies %lld\n", node-&gt;id, current_, jiffies_); node-&gt;cb(); size_--; fired++; &#125; ref_.erase(node-&gt;id); freeNode(node); &#125; return fired; // cascade all the timers at bucket of index up one level bool WheelTimer::cascade(int bucket, int index)&#123; // swap list TimerList list; buckets_[bucket][index].swap(list); for(auto&amp; node : list)&#123; if(node-&gt;id &gt; 0)&#123; addTimerNode(node); // 把各个定时器往前推, 比如条件达成就挪到this-&gt;near_里去 &#125; &#125; // 如INDEX(N), 当N=0, 因为进入本函数之前, jiffies_ &amp; TVR_MASK 是为 0 的, // 说明 jiffies_ 8位以前的高位绝对有不为0的位, // jiffies右移8位然后跟TVN_MASK(即63, 即二进制111111, 六位)做且操作之后的结果 index == 0 , // 则说明jiffies大于N=0的这个bucket区间了, 还需要调整下一个区间(即 N+1 这个bucket区间), // 就跟水表一样, 小表转一圈需要调整中表, 中表转一圈则要调整大表 return index == 0; &#125; // Do lazy cancellation, so we can effectively use vector as container of timer nodes bool WheelTimer::Cancel(int id) &#123; TimerNode* node = ref_[id]; if (node != nullptr) &#123; node-&gt;canceled = true; size_--; return true; &#125; return false; &#125; 红黑树定时器-高精度实现而随着内核的不断演进，大牛们已经对这种低分辨率定时器的精度不再满足，而且，硬件也在不断地发展，系统中的定时器硬件的精度也越来越高，这也给高分辨率定时器的出现创造了条件。内核从2.6.16开始加入了高精度定时器架构。它可以为我们提供纳秒级的定时精度，以满足对精确时间有迫切需求的应用程序或内核驱动，例如多媒体应用，音频设备的驱动程序等等。 当前内核同时存在新旧timer wheel 和hrtimer两套timer的实现，内核启动后会进行从低精度模式到高精度时钟模式的切换. 与时间轮的区别Linux 2.6.16 ，内核支持了高精度的时钟，内核采用新的定时器hrtimer，其实现逻辑和Linux 2.6.16 之前定时器逻辑区别： hrtimer采用红黑树进行高精度定时器的管理，而不是时间轮； 高精度时钟定时器不在依赖系统的tick中断，而是基于时钟硬件的事件触发。 旧内核的定时器实现依赖于系统定时器硬件定期的tick，基于该tick，内核会扫描timer wheel处理超时事件，会更新jiffies，wall time(墙上时间，现实时间)，process的使用时间等等工作。 新的内核不再会直接支持周期性的tick，新内核定时器框架采用了基于高精度时钟硬件的下次中断触发，而不是以前的周期性触发。新内核实现了hrtimer(high resolution timer)于事件触发。 hrtimer的工作原理我们知道，低分辨率定时器使用5个链表数组来组织timer_list结构，形成了著名的时间轮概念，对于高分辨率定时器，我们期望组织它们的数据结构至少具备以下条件： 稳定而且快速的查找能力； 快速地插入和删除定时器的能力； 排序功能； 内核的开发者考察了多种数据结构，例如基数树、哈希表等等，最终他们选择了红黑树（rbtree）来组织hrtimer，红黑树已经以库的形式存在于内核中，并被成功地使用在内存管理子系统和文件系统中，随着系统的运行，hrtimer不停地被创建和销毁，新的hrtimer按顺序被插入到红黑树中，树的最左边的节点就是最快到期的定时器，内核用一个hrtimer结构来表示一个高精度定时器 通过将高精度时钟硬件的下次中断触发时间设置为红黑树中最早到期的Timer 的时间，时钟到期后从红黑树中得到下一个 Timer 的到期时间，并设置硬件，如此循环反复。 如何在高精度模式下模拟tick当系统切换到高精度模式后，tick_device被高精度定时器系统接管，不再定期地产生tick事件，我们知道，到目前的版本为止（V3.4），内核还没有彻底废除jiffies机制，系统还是依赖定期到来的tick事件，供进程调度系统和时间更新等操作，大量存在的低精度定时器也仍然依赖于jiffies的计数，所以，尽管tick_device被接管，高精度定时器系统还是要想办法继续提供定期的tick事件。为了达到这一目的，内核使用了一个取巧的办法：既然高精度模式已经启用，可以定义一个hrtimer，把它的到期时间设定为一个jiffy的时间，当这个hrtimer到期时，在这个hrtimer的到期回调函数中，进行和原来的tick_device同样的操作，然后把该hrtimer的到期时间顺延一个jiffy周期，如此反复循环，完美地模拟了原有tick_device的功能。]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-MySQL原理]]></title>
    <url>%2F2021%2F01%2F24%2Fself_cultivation_mysql%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[MySQL参考网址 https://chenjiabing666.github.io/2020/04/20/Mysql%E6%9C%80%E5%85%A8%E9%9D%A2%E8%AF%95%E6%8C%87%E5%8D%97/ https://blog.csdn.net/qq_41011723/article/details/105953813 https://blog.csdn.net/qq_41011723/article/details/106028153 MySQL、MongoDB、Redis 数据库之间的区别 mysql一条语句的执行过程速记: 连/分/优/执/存 char和varchar的区别是什么 char(n) ：固定长度类型，比如订阅 char(10)，当你输入”abc”三个字符的时候，它们占的空间还是 10 个字节，其他 7 个是空字节。 chat 优点：效率高；缺点：占用空间；适用场景：存储密码的 md5 值，固定长度的，使用 char 非常合适。 varchar(n) ：可变长度，存储的值是每个值占用的字节再加上一个用来记录其长度的字节的长度。 所以，从空间上考虑 varcahr 比较合适；从效率上考虑 char 比较合适，二者使用需要权衡。 redo log与binlog与undo log的区别参考 https://www.cnblogs.com/Java3y/p/12453755.html , 写的非常好也可参考 https://www.jianshu.com/p/68d5557c65be redo logredo log 存在于InnoDB 引擎中，InnoDB引擎是以插件形式引入Mysql的，redo log的引入主要是为了实现Mysql的crash-safe能力。实际上Mysql的基本存储结构是页(记录都存在页里边)，所以MySQL是先把这条记录所在的页找到，然后把该页加载到内存中，将对应记录进行修改。现在就可能存在一个问题：如果在内存中把数据改了，还没来得及落磁盘，而此时的数据库挂了怎么办？显然这次更改就丢了。 如果每个请求都需要将数据立马落磁盘之后，那速度会很慢，MySQL可能也顶不住。所以MySQL是怎么做的呢？MySQL引入了redo log，内存写完了，然后会写一份redo log，这份redo log记载着这次在某个页上做了什么修改.其实写redo log的时候，也会有buffer，是先写buffer，再真正落到磁盘中的。至于从buffer什么时候落磁盘，会有配置供我们配置。 写redo log也是需要写磁盘的，但它的好处就是顺序IO（我们都知道顺序IO比随机IO快非常多）。 所以，redo log的存在为了：当我们修改的时候，写完内存了，但数据还没真正写到磁盘的时候。此时我们的数据库挂了，我们可以根据redo log来对数据进行恢复。因为redo log是顺序IO，所以写入的速度很快，并且redo log记载的是物理变化（xxxx页做了xxx修改），文件的体积很小，恢复速度很快 binlogbinlog记录了数据库表结构和表数据变更，比如update/delete/insert/truncate/create。它不会记录select（因为这没有对表没有进行变更）binlog我们可以简单理解为：存储着每条变更的SQL语句 undo logundo log主要有两个作用： 回滚 多版本控制(MVCC) 在数据修改的时候，不仅记录了redo log，还记录undo log，如果因为某些原因导致事务失败或回滚了，可以用undo log进行回滚undo log主要存储的也是逻辑日志，比如我们要insert一条数据了，那undo log会记录的一条对应的delete日志。我们要update一条记录时，它会记录一条对应相反的update记录。 这也应该容易理解，毕竟回滚嘛，跟需要修改的操作相反就好，这样就能达到回滚的目的。因为支持回滚操作，所以我们就能保证：“一个事务包含多个操作，这些操作要么全部执行，要么全都不执行”。【原子性】 因为undo log存储着修改之前的数据，相当于一个前版本，MVCC实现的是读写不阻塞，读的时候只要返回前一个版本的数据就行了。 undolog和binlog和redolog不同之处总结 参考 https://www.jianshu.com/p/68d5557c65be redo log: 只存在于innodb引擎中 物理格式的日志，记录的是物理数据页面的修改的信息（数据库中每个页的修改），面向的是表空间、数据文件、数据页、偏移量等。 undo log 逻辑格式的日志，在执行undo的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，与redo log不同。 binlog 逻辑格式的日志，可以简单认为就是执行过的事务中的sql语句。 但又不完全是sql语句这么简单，而是包括了执行的sql语句（增删改）反向的信息。比如delete操作的话，就对应着delete本身和其反向的insert/update操作的话，就对应着update执行前后的版本的信息；insert操作则对应着delete和insert本身的信息。 因此可以基于binlog做到闪回功能。 binlog可以作为恢复数据使用，主从复制搭建，redo log作为异常宕机或者介质故障后的数据恢复使用。 redo log是在InnoDB存储引擎层产生，而binlog是MySQL数据库的上层产生的，并且binlog日志不仅仅针对INNODB存储引擎，MySQL数据库中的任何存储引擎对于数据库的更改都会产生binlog日志。 两种日志记录的内容形式不同。MySQL的binlog是逻辑日志，可以简单认为记录的就是sql语句。而innodb存储引擎层面的redo日志是物理日志, 是数据页面的修改之后的物理记录。 关于事务提交时，redo log和binlog的写入顺序，为了保证主从复制时候的主从一致（当然也包括使用binlog进行基于时间点还原的情况），是要严格一致的，MySQL通过两阶段提交过程来完成事务的一致性的，也即redo log和binlog的一致性的，理论上是先写redo log，再写binlog，两个日志都提交成功（刷入磁盘），事务才算真正的完成。因此redo日志的写盘，并不一定是随着事务的提交才写入redo日志文件的，而是随着事务的开始，逐步开始的。那么当我执行一条 update 语句时，redo log 和 binlog 是在什么时候被写入的呢？这就有了我们常说的「两阶段提交」： 写入：redo log（prepare） 写入：binlog 写入：redo log（commit） 两种日志与记录写入磁盘的时间点不同，binlog日志只在事务提交完成后进行一次写入。而innodb存储引擎的redo日志在事务进行中不断地被写入，并日志不是随事务提交的顺序进行写入的。 binlog日志仅在事务提交时记录，并且对于每一个事务，仅在事务提交时记录，并且对于每一个事务，仅包含对应事务的一个日志。而对于innodb存储引擎的redo日志，由于其记录是物理操作日志，因此每个事务对应多个日志条目，并且事务的redo日志写入是并发的，并非在事务提交时写入，其在文件中记录的顺序并非是事务开始的顺序。 binlog不是循环使用，在写满或者重启之后，会生成新的binlog文件，redo log是循环使用。 binlog 日志是 master 推的还是 salve 来拉的？slave来拉的, 因为每一个slave都是完全独立的个体，所以slave完全依据自己的节奏去处理同步， 二阶段提交redo log 保证的是数据库的 crash-safe 能力。采用的策略就是常说的“两阶段提交”。 一条update的SQL语句是按照这样的流程来执行的：将数据页加载到内存 → 修改数据 → 更新数据 → 写redo log（状态为prepare） → 写binlog → 提交事务(数据写入成功后将redo log状态改为commit) 只有当两个日志都提交成功（刷入磁盘），事务才算真正的完成。一旦发生系统故障（不管是宕机、断电、重启等等），都可以配套使用 redo log 与 binlog 做数据修复。 两阶段提交机制的必要性 binlog 存在于Mysql Server层中，主要用于数据恢复；当数据被误删时，可以通过上一次的全量备份数据加上某段时间的binlog将数据恢复到指定的某个时间点的数据。 redo log 存在于InnoDB 引擎中，InnoDB引擎是以插件形式引入Mysql的，redo log的引入主要是为了实现Mysql的crash-safe能力。 假设redo log和binlog分别提交，可能会造成用日志恢复出来的数据和原来数据不一致的情况。 假设先写redo log再写binlog，即redo log没有prepare阶段，写完直接置为commit状态，然后再写binlog。那么如果写完redo log后Mysql宕机了，重启后系统自动用redo log 恢复出来的数据就会比binlog记录的数据多出一些数据，这就会造成磁盘上数据库数据页和binlog的不一致，下次需要用到binlog恢复误删的数据时，就会发现恢复后的数据和原来的数据不一致。 假设先写binlog再写redolog。如果写完binlog后Mysql宕机了，那么binlog上的记录就会比磁盘上数据页的记录多出一些数据出来，下次用binlog恢复数据，就会发现恢复后的数据和原来的数据不一致。 由此可见，redo log和binlog的两阶段提交是非常必要的。 索引 聚集索引(也叫聚簇索引)是啥 聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据 非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因。 外键是啥: 比如在students表中，通过class_id的字段，可以把数据与另一张表关联起来，这种列称为外键(一般不用外键, 因为会降低数据库性能) mysql 索引在什么情况下会失效 https://database.51cto.com/art/201912/607742.htm 查询条件包含or，可能导致索引失效 如何字段类型是字符串，where时一定用引号括起来，否则索引失效 like通配符可能导致索引失效。 联合索引，查询时的条件列不是联合索引中的第一个列，索引失效。 在索引列上使用mysql的内置函数，索引失效 对索引列运算（如，+、-、*、/），索引失效。 索引字段上使用（！= 或者 &lt; &gt;，not in）时，可能会导致索引失效。 索引字段上使用is null， is not null，可能导致索引失效。 左连接查询或者右连接查询查询关联的字段编码格式不一样，可能导致索引失效。 mysql估计使用全表扫描要比使用索引快,则不使用索引。 mysql 的索引模型:在MySQL中使用较多的索引有Hash索引，B+树索引等，而我们经常使用的InnoDB存储引擎的默认索引实现为：B+树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 为什么说B类树更适合数据库索引为什么说B类树更适合数据库索引 mysql全文索引 pending_finmysql 有那些存储引擎，有哪些区别 innodb是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ Next-Key Locking 防止幻影读。主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。 MyISAM类型不支持事务处理等高级处理，而InnoDB类型支持。 MyISAM类型的表强调的是性能，其执行速度比InnoDB类型更快，但是不提供事务支持，而InnoDB提供事务支持以及外部键等高级数据库功能。 现在一般都是选用InnoDB了，InnoDB支持行锁, 而MyISAM的全表锁，myisam的读写串行问题，并发效率锁表，效率低，MyISAM对于读写密集型应用一般是不会去选用的 memory引擎一般用于临时表, 使用表级锁，没有事务机制, 虽然内存访问快，但如果频繁的读写，表级锁会成为瓶颈, 且内存昂贵..满了就亏了 InnoDB是聚集索引，使用B+Tree作为索引结构，数据文件是和（主键）索引绑在一起的（表数据文件本身就是按B+Tree组织的一个索引结构），必须要有主键，通过主键索引效率很高。MyISAM是非聚集索引，也是使用B+Tree作为索引结构，索引和数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 综上所述, 如果表的读操作远远多于写操作时，并且不需要事务的支持的，可以将 MyIASM 作为数据库引擎的首选 mysql 主从同步分哪几个过程复制的基本过程如下： 从节点上的I/O 线程连接主节点，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容； 主节点接收到来自从节点的I/O请求后，通过负责复制的I/O线程根据请求信息读取指定日志指定位置之后的日志信息，返回给从节点。返回信息中除了日志所包含的信息之外，还包括本次返回的信息的bin-log file 的以及bin-log position；从节点的I/O线程接收到内容后，将接收到的日志内容更新到本机的relay log中，并将读取到的binary log文件名和位置保存到master-info 文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log 的哪个位置开始往后的日志内容，请发给我”； Slave 的 SQL线程检测到relay-log 中新增加了内容后，会将relay-log的内容解析成在主节点上实际执行过的操作，并在本数据库中执行。 主从同步延迟与同步数据丢失问题主库将变更写binlog日志，然后从库连接到主库之后，从库有一个IO线程，将主库的binlog日志拷贝到自己本地，写入一个中继日志中。接着从库中有一个SQL线程会从中继日志读取binlog，然后执行binlog日志中的内容，也就是在自己本地再次执行一遍SQL，这样就可以保证自己跟主库的数据是一样的。 这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行SQL的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。所以mysql实际上在这一块有两个机制: 一个是半同步复制，用来解决主库数据丢失问题 一个是并行复制，用来解决主从同步延时问题(实在解决不了只能强制读主库)。 半同步复制（Semisynchronous replication） 逻辑上: 是介于全同步复制与全异步复制之间的一种，主库只需要等待至少一个从库节点收到并且 Flush Binlog 到 Relay Log 文件即可，主库不需要等待所有从库给主库反馈。同时，这里只是一个收到的反馈，而不是已经完全完成并且提交的反馈，如此，节省了很多时间。 技术上: 介于异步复制和全同步复制之间，主库在执行完客户端提交的事务后不是立刻返回给客户端，而是等待至少一个从库接收到并写到relay log中才返回给客户端。相对于异步复制，半同步复制提高了数据的安全性，同时它也造成了一定程度的延迟，这个延迟最少是一个TCP/IP往返的时间。所以，半同步复制最好在低延时的网络中使用。 库并行复制所谓并行复制，指的是从库开启多个线程，并行读取relay log中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 异步复制（Asynchronous replication） 逻辑上: MySQL默认的复制即是异步的，主库在执行完客户端提交的事务后会立即将结果返给给客户端，并不关心从库是否已经接收并处理，这样就会有一个问题，主如果crash掉了，此时主上已经提交的事务可能并没有传到从库上，如果此时，强行将从提升为主，可能导致新主上的数据不完整。 技术上: 主库将事务 Binlog 事件写入到 Binlog 文件中，此时主库只会通知一下 Dump 线程发送这些新的 Binlog，然后主库就会继续处理提交操作，而此时不会保证这些 Binlog 传到任何一个从库节点上。 全同步复制（Fully synchronous replication） 逻辑上: 指当主库执行完一个事务，所有的从库都执行了该事务才返回给客户端。因为需要等待所有从库执行完该事务才能返回，所以全同步复制的性能必然会收到严重的影响。 技术上: 当主库提交事务之后，所有的从库节点必须收到、APPLY并且提交这些事务，然后主库线程才能继续做后续操作。但缺点是，主库完成一个事务的时间会被拉长，性能降低。 乐观锁与悲观锁的区别？ 悲观锁：认为数据随时会被修改，因此每次读取数据之前都会上锁，防止其它事务读取或修改数据；应用于数据更新比较频繁的场景； 乐观锁：操作数据时不会上锁，但是更新时会判断在此期间有没有别的事务更新这个数据，若被更新过，则失败重试；适用于读多写少的场景。 乐观锁怎么实现: 加版本号 cas 实现事务采取了哪些技术以及思想？ ★ a原子性：使用 undo log ，从而达到回滚 ★ d持久性：使用 redo log，从而达到故障后恢复 ★ i隔离性：使用锁以及MVCC,运用的优化思想有读写分离，读读并行，读写并行 ★ c一致性：通过回滚，以及恢复，和在并发环境下的隔离做到一致性。 mysql四个事务隔离级别四个隔离级别的区别以及每个级别可能产生的问题以及实现原理MySQL 的事务隔离是在 MySQL. ini 配置文件里添加的，在文件的最后添加：transaction-isolation = REPEATABLE-READ可用的配置值：READ-UNCOMMITTED、READ-COMMITTED、REPEATABLE-READ、SERIALIZABLE。 MySQL的事务隔离级别一共有四个，分别是 读未提交 读已提交 可重复读 可串行化 MySQL的隔离级别的作用就是让事务之间互相隔离，互不影响，这样可以保证事务的一致性。 隔离级别比较：可串行化&gt;可重复读&gt;读已提交&gt;读未提交 隔离级别对性能的影响比较：可串行化&gt;可重复读&gt;读已提交&gt;读未提交 由此看出，隔离级别越高，所需要消耗的MySQL性能越大（如事务并发严重性），为了平衡二者，一般建议设置的隔离级别为可重复读，MySQL默认的隔离级别也是可重复读。 事务并发可能出现的情况 脏读（Dirty Read） 一个事务读到了另一个未提交事务修改过的数据 会话B开启一个事务，把id=1的name为武汉市修改成温州市，此时另外一个会话A也开启一个事务，读取id=1的name，此时的查询结果为温州市，会话B的事务最后回滚了刚才修改的记录，这样会话A读到的数据是不存在的，这个现象就是脏读。（脏读只在读未提交隔离级别才会出现） 不可重复读（Non-Repeatable Read） 一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值。（不可重复读在读未提交和读已提交隔离级别都可能会出现） 会话A开启一个事务，查询id=1的结果，此时查询的结果name为武汉市。接着会话B把id=1的name修改为温州市（隐式事务，因为此时的autocommit为1，每条SQL语句执行完自动提交），此时会话A的事务再一次查询id=1的结果，读取的结果name为温州市。会话B再此修改id=1的name为杭州市，会话A的事务再次查询id=1，结果name的值为杭州市，这种现象就是不可重复读。 幻读（Phantom） 一个事务先根据某些条件查询出一些记录，之后另一个事务又向表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能把另一个事务插入的记录也读出来。（幻读在读未提交、读已提交、可重复读隔离级别都可能会出现） 会话A开启一个事务，查询id&gt;0的记录，此时会查到name=武汉市的记录。接着会话B插入一条name=温州市的数据（隐式事务，因为此时的autocommit为1，每条SQL语句执行完自动提交），这时会话A的事务再以刚才的查询条件（id&gt;0）再一次查询，此时会出现两条记录（name为武汉市和温州市的记录），这种现象就是幻读。 各个隔离级别的详细说明 读未提交（READ UNCOMMITTED） 在读未提交隔离级别下，事务A可以读取到事务B修改过但未提交的数据。 可能发生脏读、不可重复读和幻读问题，一般很少使用此隔离级别。 读已提交（READ COMMITTED） 在读已提交隔离级别下，事务B只能在事务A修改过并且已提交后才能读取到事务B修改的数据。 读已提交隔离级别解决了脏读的问题，但可能发生不可重复读和幻读问题，一般很少使用此隔离级别。 可重复读（REPEATABLE READ） 在可重复读隔离级别下，事务B只能在事务A修改过数据并提交后，自己也提交事务后，才能读取到事务B修改的数据。 可重复读隔离级别解决了脏读和不可重复读的问题，但可能发生幻读问题。 提问：为什么上了写锁（写操作），别的事务还可以读操作？因为InnoDB有MVCC机制（多版本并发控制），可以使用快照读，而不会被阻塞。 可串行化（SERIALIZABLE） 各种问题（脏读、不可重复读、幻读）都不会发生，通过加锁实现（读锁和写锁）。 mvcc是啥mvcc必看文章: mysql mvcc实现原理 https://chenjiayang.me/2019/06/22/mysql-innodb-mvcc/ MVCC (Multiversion Concurrency Control) 中文全程叫多版本并发控制，是现代数据库（包括 MySQL、Oracle、PostgreSQL 等）引擎实现中常用的处理读写冲突的手段，目的在于提高数据库高并发场景下的吞吐性能。MVCC 的每一个写操作都会创建一个新版本的数据，读操作会从有限多个版本的数据中挑选一个最合适（要么是最新版本，要么是指定版本）的结果直接返回 。通过这种方式，我们就不需要关注读写操作之间的数据冲突 每条记录在更新的时候都会同时记录一条回滚操作（回滚操作日志undo log）。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。即通过回滚（rollback操作），可以回到前一个状态的值。InnoDB 为了解决这个问题，设计了 ReadView（可读视图）的概念. 如此一来不同的事务在并发过程中，SELECT 操作可以不加锁而是通过 MVCC 机制读取指定的版本历史记录，并通过一些手段保证保证读取的记录值符合事务所处的隔离级别，从而解决并发场景下的读写冲突。 mysql把每个操作都定义成一个事务，每开启一个事务，系统的事务版本号自动递增。每行记录都有两个隐藏列：创建版本号和删除版本号 ReadView 系统版本号 SYS_ID：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。 事务版本号 TRX_ID ：事务开始时的系统版本号。 MVCC 维护了一个 ReadView 结构，主要包含了当前系统未提交的事务列表 TRX_IDs {TRX_ID_1, TRX_ID_2, …}，还有该列表的最小值 TRX_ID_MIN 和 TRX_ID_MAX。 在进行 SELECT 操作时，根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系，从而判断数据行快照是否可以使用： TRX_ID &lt; TRX_ID_MIN，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此可以使用。 TRX_ID &gt; TRX_ID_MAX，表示该数据行快照是在事务启动之后被更改的，因此不可使用。 TRX_ID_MIN &lt;= TRX_ID &lt;= TRX_ID_MAX，需要根据隔离级别再进行判断： 提交读：如果 TRX_ID 在 TRX_IDs 列表中，表示该数据行快照对应的事务还未提交，则该快照不可使用。否则表示已经提交，可以使用。 可重复读：都不可以使用。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题。 在数据行快照不可使用的情况下，需要沿着 Undo Log 的回滚指针 ROLL_PTR 找到下一个快照，再进行上面的判断。 mysql在可重复读RR的隔离级别下如何避免幻读的参考: next-key锁: mysql 排它锁之行锁、间隙锁、后码锁 mvcc: mysql mvcc实现原理 https://blog.csdn.net/DILIGENT203/article/details/100751755 知识点: Record Lock：行锁, 锁直接加在索引记录上面，锁住的是key。当需要对表中的某条数据进行写操作（insert、update、delete、select for update）时，需要先获取记录的排他锁（X锁），这个就称为行锁。 Gap Lock：间隙锁，锁定索引记录间隙，确保索引记录的间隙不变。间隙锁是针对事务隔离级别为可重复读或以上级别而设计的。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。例如当一个事务执行语句SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE;，则其它事务就不能在 t.c 中插入 15。 Next-Key Lock = Record Lock + Gap Lock ， 它是 Record Locks 和 Gap Locks 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个前开后闭区间，例如一个索引包含以下值：10, 11, 13, and 20，那么就需要锁定以下区间： 12345(-∞, 10](10, 11](11, 13](13, 20](20, +∞) 默认情况下，InnoDB工作在可重复读隔离级别下，并且会以Next-Key Lock的方式对数据行进行加锁，这样可以有效防止幻读的发生。Next-Key Lock是行锁和间隙锁的组合，当InnoDB扫描索引记录的时候，会首先对索引记录加上Record Lock，再对索引记录两边的间隙加上间隙锁（Gap Lock）。加上间隙锁之后，其他事务就不能在这个间隙修改或者插入记录。 快照读：简单的select操作，属于快照读，不加锁。(当然，也有例外，下面会分析) select * from table where ?; 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。 select * from table where ? lock in share mode; select * from table where ? for update; insert into table values (…); update table set ? where ?; delete from table where ?; 在MySQL中select称为快照读，不需要锁，而insert、update、delete、select for update则称为当前读，需要给数据加锁，幻读中的“读”即是针对当前读。RR事务隔离级别允许存在幻读，但InnoDB RR级别却通过Gap锁避免了幻读 mysql如何实现避免幻读: 在快照读读情况下，mysql通过mvcc来避免幻读。 在当前读读情况下，mysql通过next-key lock来避免幻读。 正确理解InnoDB引擎RR隔离级别解决了幻读这件事Mysql官方给出的幻读解释是：只要在一个事务中，第二次select多出了row就算幻读。 先看问题:a事务先select，b事务insert确实会加一个gap锁，但是如果b事务commit，这个gap锁就会释放（释放后a事务可以随意dml操作），a事务再select出来的结果在MVCC下还和第一次select一样，接着a事务不加条件地update，这个update会作用在所有行上（包括b事务新加的），a事务再次select就会出现b事务中的新行，并且这个新行已经被update修改了，实测在RR级别下确实如此。 如果这样理解的话，Mysql的RR级别确实防不住幻读, 但是我们不能向上面这样理解, 我们得如下理解: select * from t where a=1;属于快照读 select * from t where a=1 lock in share mode;属于当前读 不能把快照读和当前读得到的结果不一样这种情况认为是幻读，这是两种不同的使用。所以mysql的rr级别是解决了幻读的。 如上面问题所说，T1 select 之后 update，会将 T2 中 insert 的数据一起更新，那么认为多出来一行，所以防不住幻读。看着说法无懈可击，但是其实是错误的，InnoDB 中设置了快照读和当前读两种模式，如果只有快照读，那么自然没有幻读问题，但是如果将语句提升到当前读，那么 T1 在 select 的时候需要用如下语法： select * from t for update (lock in share mode) 进入当前读，那么自然没有 T2 可以插入数据这一回事儿了。]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-HTTP深入浅出]]></title>
    <url>%2F2021%2F01%2F12%2Fself_cultivation_http%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[HTTP与HTTPSHTTP 协议考察 HTTP 协议的返回码、HTTP 的方法等。需要特别指出的是 HTTPS 加密的详细过程要非常透彻，不然容易产生一种感觉好像都清楚了，但是一问就有点说不清楚。 下面实例是一点典型的使用GET来传递数据的实例,客户端请求：1234GET /hello.txt HTTP/1.1User-Agent: curl/7.16.3 libcurl/7.16.3 OpenSSL/0.9.7l zlib/1.2.3Host: www.example.comAccept-Language: en, mi 服务端响应:123456789HTTP/1.1 200 OKDate: Mon, 27 Jul 2009 12:28:53 GMTServer: ApacheLast-Modified: Wed, 22 Jul 2009 19:15:56 GMTETag: &quot;34aa387-d-1568eb00&quot;Accept-Ranges: bytesContent-Length: 51Vary: Accept-EncodingContent-Type: text/plain 输出结果：1Hello World! My payload includes a trailing CRLF. 客户端请求消息客户端发送一个HTTP请求到服务器的请求消息包括以下格式: 请求行（request line） 请求头部（header） 空行 请求数据 由四个部分组成，下图给出了请求报文的一般格式。 服务器响应消息HTTP响应也由四个部分组成，分别是: 状态行 消息报头 空行 响应正文 httpsHTTPS 协议（HyperText Transfer Protocol over Secure Socket Layer）：一般理解为HTTP+SSL/TLS，通过 SSL证书来验证服务器的身份，并为浏览器和服务器之间的通信进行加密。 那么SSL/TLS又是什么？ SSL（Secure Socket Layer，安全套接字层）：1994年为 Netscape 所研发，SSL 协议位于 TCP/IP 协议与各种应用层协议之间，为数据通讯提供安全支持。 TLS（Transport Layer Security，传输层安全）：其前身是 SSL，它最初的几个版本（SSL 1.0、SSL 2.0、SSL 3.0）由网景公司开发，1999年从 3.1 开始被 IETF 标准化并改名，发展至今已经有 TLS 1.0、TLS 1.1、TLS 1.2 三个版本。SSL3.0和TLS1.0由于存在安全漏洞，已经很少被使用到。TLS 1.3 改动会比较大，目前还在草案阶段，目前使用最广泛的是TLS 1.1、TLS 1.2。 https 不是一种新的协议，只是 http 的通信接口部分使用了 ssl 和 tsl 协议替代，加入了加密、证书、完整性保护的功能，下面解释一下加密和证书，如下图所示 对称加密也叫共享密钥加密, 加密和解密公用一套秘钥，这样就会产生问题，已共享秘钥加密方式必须将秘钥传送给对方，但如果通信被监听，那么秘钥可能会被泄漏产生危险。常见对称加密算法有des, aes 非对称加密也叫公开秘钥加密, 使用一种非对称加密的算法，使用一对非对称的秘钥，一把叫做公有秘钥，一把叫做私有秘钥，在加密的时候，通信的一方使用公有秘钥进行加密，通信的另一方使用私有秘钥进行解密，利用这种方式不需要发送私有秘钥，也就不存在泄漏的风险了。常见非对称加密算法有rsa https 加密方式因为公开秘钥加密的方式比共享秘钥加密的方式钥消耗 cpu 资源，https 采取了混合加密的方式，来结合两者的优点。 在秘钥交换阶段使用公开加密的方式，之后建立连接后使用共享秘钥加密方式进行加密，如下图。 为什么要使用证书因为公开加密还存在一些问题就是无法证明公开秘钥的正确性(有可能被黑客中间替换成了黑客自己的公钥, 然后黑客伪装成服务器/客户端做中间转发)，为了解决这个问题，https 采取了有数字证实认证机构和其相关机构颁发的公开秘钥证书，通信过程如下图所示。 解释一下上图的步骤： 服务器将自己的公开秘钥传到数字证书认证机构 数字证书认证机构使用自己的秘钥来对传来的服务器公钥进行加密，并颁发数字证书 服务器将传回的公钥证书发送给客户端，客户端使用数字机构颁发的公开秘钥来验证证书的有效性，以及公开秘钥的真实性 证书签名是先将证书信息（证书机构名称、有效期、拥有者、拥有者公钥）进行hash，再用CA的私有密钥对hash值加密而生成的。 所以拦截者虽然可以拦截并篡改证书信息（主要是拥有者和拥有者的公钥），但是由于拦截者没有CA的私钥，所以无法生成正确的签名，从而导致客户端拿到签名后，用CA公有密钥对证书签名解密后值与用证书计算出来的实际hash值不一样，从而得不到客户端信任。(其实这个ca公钥和私钥也就是非对称加密的思想了) 客户端使用服务器的公开秘钥进行消息加密，后发送给服务器。 服务器使用私有秘钥进行解密。 浏览器在安装的时候会内置可信的数字证书机构的公开秘钥，如下图所示。 这就是为什么我们使用自己生成的证书的时候会产生安全警告的原因。 再附一张 https 的具体通信步骤和图解。 cookie服务器发送的响应报文包含 Set-Cookie 首部字段，客户端得到响应报文后把 Cookie 内容保存到浏览器中。1234HTTP/1.0 200 OKContent-type: text/htmlSet-Cookie: yummy_cookie=chocoSet-Cookie: tasty_cookie=strawberry 客户端之后对同一个服务器发送请求时，会从浏览器中取出 Cookie 信息并通过 Cookie 请求首部字段发送给服务器。123GET /sample_page.html HTTP/1.1Host: www.example.orgCookie: yummy_cookie=choco; tasty_cookie=strawberry Domain 标识指定了哪些主机可以接受 Cookie。如果不指定，默认为当前文档的主机（不包含子域名）。如果指定了 Domain，则一般包含子域名。例如，如果设置 Domain=mozilla.org，则 Cookie 也包含在子域名中（如 developer.mozilla.org）。 Path 标识指定了主机下的哪些路径可以接受 Cookie（该 URL 路径必须存在于请求 URL 中）。以字符 %x2F (“/“) 作为路径分隔符，子路径也会被匹配。例如，设置 Path=/docs，则以下地址都会匹配： /docs /docs/Web/ /docs/Web/HTTP session如何保存较好 一个用户的 Session 信息如果存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器，由于服务器没有用户的 Session 信息，那么该用户就需要重新进行登录等操作..有什么好的解决方案呢?Session Server使用一个单独的服务器存储 Session 数据，可以使用传统的 MySQL，也使用 Redis 或者 Memcached 这种内存型数据库。 优点： 为了使得大型网站具有伸缩性，集群中的应用服务器通常需要保持无状态，那么应用服务器不能存储用户的会话信息。Session Server 将用户的会话信息单独进行存储，从而保证了应用服务器的无状态。 缺点： 需要去实现存取 Session 的代码 cookie和session和token的区别 由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，这个机制就是Session.典型的场景比如购物车，当你点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用户创建了特定的Session，用用于标识这个用户，并且跟踪用户，这样才知道购物车里面有几本书。这个Session是保存在服务端的，有一个唯一标识。在服务端保存Session的方法很多，内存、数据库、文件都有。集群的时候也要考虑Session的转移，在大型的网站，一般会有专门的Session服务器集群，用来保存用户会话，这个时候 Session 信息都是放在内存的，使用一些缓存服务比如Memcached之类的来放 Session。 思考一下服务端如何识别特定的客户？这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端，需要在 Cookie 里面记录一个Session ID，以后每次请求把这个会话ID发送到服务器，我就知道你是谁了。有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。 Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。所以，总结一下：Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中；Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。 为什么需要token来替代session机制? 因为session的存储对服务器说是一个巨大的开销， 严重的限制了服务器扩展能力， 比如说我用两个机器组成了一个集群， 小 F 通过机器 A 登录了系统， 那 session id 会保存在机器 A 上， 假设小 F 的下一次请求被转发到机器 B 怎么办？ 机器 B 可没有小 F 的 session id 啊。有时候会采用一点小伎俩： session sticky ， 就是让小 F 的请求一直粘连在机器 A 上， 但是这也不管用， 要是机器 A 挂掉了， 还得转到机器 B 去。 接下来我们介绍事实上的token标准JWT JWTsessionId 的方式本质是把用户状态信息维护在 server 端，token 的方式就是把用户的状态信息加密成一串 token 传给前端，然后每次发请求时把 token 带上，传回给服务器端；服务器端收到请求之后，解析 token 并且验证相关信息(用jwt的header里的加密方式然后根据自己的不公开的密钥把jwt中的payload用加密一下得到一个签名 s, 然后用s对比看看是不是跟jwt里的signature相等, 相等则说明token对了)； 备注: 对于数据校验，专门的消息认证码生成算法, HMAC - 一种使用单向散列函数构造消息认证码的方法，其过程是不可逆的、唯一确定的，并且使用密钥来生成认证码，其目的是防止数据在传输过程中被篡改或伪造。将原始数据与认证码一起传输，数据接收端将原始数据使用相同密钥和相同算法再次生成认证码，与原有认证码进行比对，校验数据的合法性。 所以跟第一种登录方式最本质的区别是：通过解析 token 的计算时间换取了 session 的存储空间 业界通用的加密方式是 jwt, jwt 的具体格式如图：简单的介绍一下 jwt，它主要由 3 部分组成：1234567891011121314151617181920header 头部&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;&#125;payload 负载&#123; &quot;sub&quot;: &quot;1234567890&quot;, &quot;name&quot;: &quot;John Doe&quot;, &quot;iat&quot;: 1516239022, &quot;exp&quot;: 1555341649998&#125;signature 签名&#123; HMACSHA256( base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload), your-256-bit-secret ) secret base64 encoded&#125; header header 里面描述加密算法和 token 的类型，类型一般都是 JWT； payload 里面放的是用户的信息，也就是第一种登录方式中需要维护在服务器端 session 中的信息； signature 是对前两部分的签名，也可以理解为加密；实现需要一个密钥（secret），这个 secret 只有服务器才知道，然后使用 header 里面的算法按照如下方法来加密： 1234HMACSHA256(base64UrlEncode(header) + "." +base64UrlEncode(payload),secret) 总之，最后的 jwt = base64url(header) + &quot;.&quot; + base64url(payload) + &quot;.&quot; + signaturejwt 可以放在 response 中返回，也可以放在 cookie 中返回，这都是具体的返回方式，并不重要。客户端发起请求时，官方推荐放在 HTTP header 中：1Authorization: Bearer &lt;token&gt; 这样子确实也可以解决 cookie 跨域(比如移动平台上对cookie支持不好)的问题，不过具体放在哪儿还是根据业务场景来定，并没有一定之规。 jwt过期了如何刷新前面讲的 Token，都是 Access Token，也就是访问资源接口时所需要的 Token，还有另外一种 Token，Refresh Token，通常情况下，Refresh Token 的有效期会比较长，而 Access Token 的有效期比较短，当 Access Token 由于过期而失效时，使用 Refresh Token 就可以获取到新的 Access Token，如果 Refresh Token 也失效了，用户就只能重新登录了。 在 JWT 的实践中，引入 Refresh Token，将会话管理流程改进如下: 客户端使用用户名密码进行认证 服务端生成有效时间较短的 Access Token（例如 10 分钟），和有效时间较长的 Refresh Token（例如 7 天） 客户端访问需要认证的接口时，携带 Access Token 如果 Access Token 没有过期，服务端鉴权后返回给客户端需要的数据 如果携带 Access Token 访问需要认证的接口时鉴权失败（例如返回 401 错误），则客户端使用 Refresh Token 向刷新接口申请新的 Access Token 如果 Refresh Token 没有过期，服务端向客户端下发新的 Access Token 客户端使用新的 Access Token 访问需要认证的接口 常见的HTTP相应状态码总之：(一般标准用法是这样用哈, 但是真的写代码的时候其实跟get/post/put一样, 想怎么用全看自己, 前后端开发人员协商好就行) 1XX：消息 2XX：成功 3XX：重定向 4XX：请求错误 5XX、6XX：服务器错误 常见状态代码、状态描述的说明如下: 200 OK:请求已成功，请求所希望的响应头或数据体将随此响应返回。实际的响应将取决于所使用的请求方法。在GET请求中，响应将包含与请求的资源相对应的实体。在POST请求中，响应将包含描述或操作结果的实体。[7] 301 Moved Permanently:被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个URI之一。如果可能，拥有链接编辑功能的客户端应当自动把请求的地址修改为从服务器反馈回来的地址。[19]除非额外指定，否则这个响应也是可缓存的。新的永久性的URI应当在响应的Location域中返回。除非这是一个HEAD请求，否则响应的实体中应当包含指向新的URI的超链接及简短说明。如果这不是一个GET或者HEAD请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。注意：对于某些使用HTTP/1.0协议的浏览器，当它们发送的POST请求得到了一个301响应的话，接下来的重定向请求将会变成GET方式。 400 Bad Request由于明显的客户端错误（例如，格式错误的请求语法，太大的大小，无效的请求消息或欺骗性路由请求），服务器不能或不会处理该请求。[31] 401 Unauthorized（RFC 7235）参见：HTTP基本认证、HTTP摘要认证类似于403 Forbidden，401语义即“未认证”，即用户没有必要的凭据。[32]该状态码表示当前请求需要用户验证。该响应必须包含一个适用于被请求资源的WWW-Authenticate信息头用以询问用户信息。客户端可以重复提交一个包含恰当的Authorization头信息的请求。[33]如果当前请求已经包含了Authorization证书，那么401响应代表着服务器验证已经拒绝了那些证书。如果401响应包含了与前一个响应相同的身份验证询问，且浏览器已经至少尝试了一次验证，那么浏览器应当向用户展示响应中包含的实体信息，因为这个实体信息中可能包含了相关诊断信息。注意：当网站（通常是网站域名）禁止IP地址时，有些网站状态码显示的401，表示该特定地址被拒绝访问网站。 403 Forbidden主条目：HTTP 403服务器已经理解请求，但是拒绝执行它。与401响应不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。如果这不是一个HEAD请求，而且服务器希望能够讲清楚为何请求不能被执行，那么就应该在实体内描述拒绝的原因。当然服务器也可以返回一个404响应，假如它不希望让客户端获得任何信息。 404 Not Found主条目：HTTP 404请求失败，请求所希望得到的资源未被在服务器上发现，但允许用户的后续请求。[35]没有信息能够告诉用户这个状况到底是暂时的还是永久的。假如服务器知道情况的话，应当使用410状态码来告知旧资源因为某些内部的配置机制问题，已经永久的不可用，而且没有任何可以跳转的地址。404这个状态码被广泛应用于当服务器不想揭示到底为何请求被拒绝或者没有其他适合的响应可用的情况下。 500 Internal Server Error通用错误消息，服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。没有给出具体错误信息。[59] 503 Service Unavailable由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是暂时的，并且将在一段时间以后恢复。[62]如果能够预计延迟时间，那么响应中可以包含一个Retry-After头用以标明这个延迟时间。如果没有给出这个Retry-After信息，那么客户端应当以处理500响应的方式处理它。 get和post的本质区别从设计初衷上来说，GET 用来实现从服务端取数据，POST 用来实现向服务端提出请求对数据做某些修改，也因此如果你向nginx用post请求静态文件，nginx会直接返回 405 not allowed，但是服务端毕竟是人实现的，你可以让 POST 做 GET 相同的事情 get请求的参数一般放在url中，但是浏览器和服务器程序对url长度还是有限制的。post请求的参数一般放在body，你硬要放到url中也可以。 在RESTful风格中，get用于从服务器获获取数据，而post用于创建数据 Connection: keep-alive在早期的HTTP/1.0中，每次http请求都要创建一个连接，而创建连接的过程需要消耗资源和时间，为了减少资源消耗，缩短响应时间，就需要重用连接。在后来的HTTP/1.0中以及HTTP/1.1中，引入了重用连接的机制，就是在http请求头中加入Connection: keep-alive来告诉对方这个请求响应完成后不要关闭，下一次咱们还用这个请求继续交流。协议规定HTTP/1.0如果想要保持长连接，需要在请求头中加上Connection: keep-alive，而HTTP/1.1默认是支持长连接的，有没有这个请求头都行。 要实现长连接很简单，只要客户端和服务端都保持这个http长连接即可。但问题的关键在于保持长连接后，浏览器如何知道服务器已经响应完成？在使用短连接的时候，服务器完成响应后即关闭http连接，这样浏览器就能知道已接收到全部的响应，同时也关闭连接（TCP连接是双向的）。 在使用长连接的时候，响应完成后服务器是不能关闭连接的，那么它就要在响应头中加上特殊标志告诉浏览器已响应完成。一般情况下这个特殊标志就是Content-Length，来指明响应体的数据大小，比如Content-Length: 120表示响应体内容有120个字节，这样浏览器接收到120个字节的响应体后就知道了已经响应完成。 由于Content-Length字段必须真实反映响应体长度，但实际应用中，有些时候响应体长度并没那么好获得，例如响应体来自于网络文件，或者由动态语言生成。这时候要想准确获取长度，只能先开一个足够大的内存空间，等内容全部生成好再计算。但这样做一方面需要更大的内存开销，另一方面也会让客户端等更久。这时候Transfer-Encoding: chunked响应头就派上用场了，该响应头表示响应体内容用的是分块传输，此时服务器可以将数据一块一块地分块响应给浏览器而不必一次性全部响应，待浏览器接收到全部分块后就表示响应结束。 以分块传输一段文本内容：“人的一生总是在追求自由的一生 So easy”来说明分块传输的过程，如下图所示: url编码urlencode是什么RFC3986文档规定，Url中只允许包含英文字母（a-zA-Z）、数字（0-9）、-_.~4个特殊字符以及所有保留字符。那如何对Url中的非法字符进行编码呢? Url编码通常也被称为百分号编码（Url Encoding，also known as percent-encoding），是因为它的编码方式非常简单，使用%百分号加上两位的字符——0123456789ABCDEF——代表一个字节的十六进制形式。Url编码默认使用的字符集是US-ASCII。例如a在US-ASCII码中对应的字节是0x61，那么Url编码之后得到的就是%61，我们在地址栏上输入http://g.cn/search?q=%61%62%63， 实际上就等同于在google上搜索abc了。又如@符号在ASCII字符集中对应的字节为0x40，经过Url编码之后得到的是%40。 对于非ASCII字符，需要使用ASCII字符集的超集进行编码得到相应的字节，然后对每个字节执行百分号编码。对于Unicode字符，RFC文档建议使用utf-8对其进行编码得到相应的字节，然后对每个字节执行百分号编码。如”中文”使用UTF-8字符集得到的字节为0xE4 0xB8 0xAD 0xE6 0x96 0x87，经过Url编码之后得到”%E4%B8%AD%E6%96%87”。 如果某个字节对应着ASCII字符集中的某个非保留字符，则此字节无需使用百分号表示。例如”Url编码”，使用UTF-8编码得到的字节是0x55 0x72 0x6C 0xE7 0xBC 0x96 0xE7 0xA0 0x81，由于前三个字节对应着ASCII中的非保留字符”Url”，因此这三个字节可以用非保留字符”Url”表示。最终的Url编码可以简化成”Url%E7%BC%96%E7%A0%81” ，当然，如果你用”%55%72%6C%E7%BC%96%E7%A0%81”也是可以的。 很多HTTP监视工具或者浏览器地址栏等在显示Url的时候会自动将Url进行一次解码（使用UTF-8字符集），这就是为什么当你在Firefox中访问Google搜索中文的时候，地址栏显示的Url包含中文的缘故。但实际上发送给服务端的原始Url还是经过编码的。]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pycharm远程调试的非attach方法备忘]]></title>
    <url>%2F2021%2F01%2F07%2Fpycharm_remote_debug%2F</url>

    <encrypted>1</encrypted>

    <content type="text"><![CDATA[. . . 我们以aop项目为例子讲解, 其他A+项目类似: 搭建项目运行环境 先将windows上的项目切到dvp分支 在linux自己的home目录下建立以下目录 ~/aop/aop ~/aop/files ~/aop/logs 配置pycharm与主机远程代码同步 去项目中拷贝一份 requirement.txt放到~/aop 去/home/edt/aop/aop/backend下面拷贝一份settings_variables.py到~/aop/aop下面, 然后 把此文件中的PROJECT_ROOT = &#39;/home/edt/aop&#39;改为你自己的aop路径, 比如我的是PROJECT_ROOT = &#39;/home/hulinhong/aop&#39; 把 SITE_ROOT = os.path.join(PROJECT_ROOT, &#39;www&#39;) 改成 SITE_ROOT = os.path.join(PROJECT_ROOT, &#39;aop&#39;) 去/home/edt下面找到aop的venv的python版本, 然后去/home/edt下面找对应的版本建立venv, 如: cd ~/aop /home/edt/.pyenv/versions/3.6.8/bin/python3.6 -m venv venv source ./venv/bin/activate python --version 安装各种依赖: pip3 install -i https://pypi.python.org/simple/ --extra-index-url http://42.186.20.241:6900/simple/ --trusted-host 42.186.20.241 -r requirement.txt 从edt的aop下面拷贝前端生成好的文件到自己aop项目 cp -r /home/edt/aop/aop/backend/templates/ ~/aop/aop/ cp -r /home/edt/aop/aop/backend/static/ ~/aop/aop/ 启动服务器 cd ~/aop/aop flask run -h 0.0.0.0 -p 9507 打开浏览器, 打开网址看看是否对了 http://dev-edt.netease.com:9507/login_manager/ 使用OpenID登陆, 登录之后会报此站点的连接不安全dev-edt.netease.com 发送了无效的响应。, 此时需要在浏览器网址去掉https即可 搭建远程调试环境 修改backend目录下的manage.py, 在最底部加入以下代码 12345678910if __name__ == '__main__': import os from dotenv import find_dotenv, load_dotenv load_dotenv(find_dotenv()) params = &#123; 'host': os.environ.get('FLASK_RUN_HOST'), 'port': os.environ.get('FLASK_RUN_PORT'), 'ssl_context': (os.environ.get('FLASK_RUN_CERT'), os.environ.get('FLASK_RUN_KEY')) &#125; app.run(**params) 拷贝一份证书文件到你的home目录(其他地方也行, 下面.env文件里的证书路径跟这个对应上即可), cp -r /home/gzliurongzhi/projects/cert ~ 在backend目录下加一个.env文件, 写入以下内容 123456FLASK_RUN_HOST=dev-edt.netease.comFLASK_RUN_PORT=9507FLASK_RUN_CERT=../../cert/cert.pemFLASK_RUN_KEY=../../cert/key.pemFLASK_ENV=developmentFLASK_APP=manage 按照下图开启远程调试]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Pycharm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-Linux网络编程]]></title>
    <url>%2F2020%2F12%2F28%2Fself_cultivation_linux_net_programming%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Linux网络编程 I/O模式 水平触发 边缘触发 epoll ✓ ✓ select/poll ✓ 信号驱动 ✓ select一个常见的select例子如下:123456789101112131415161718192021222324252627282930313233343536373839404142#include "unp.h"voidstr_cli(FILE *fp, int sockfd)&#123; int maxfdp1, stdineof; fd_set rset; char buf[MAXLINE]; int n; stdineof = 0; FD_ZERO(&amp;rset); for ( ; ; ) &#123; if (stdineof == 0) FD_SET(fileno(fp), &amp;rset); FD_SET(sockfd, &amp;rset); maxfdp1 = max(fileno(fp), sockfd) + 1; Select(maxfdp1, &amp;rset, NULL, NULL, NULL); if (FD_ISSET(sockfd, &amp;rset)) &#123; /* socket is readable */ if ( (n = Read(sockfd, buf, MAXLINE)) == 0) &#123; if (stdineof == 1) return; /* normal termination */ else err_quit("str_cli: server terminated prematurely"); &#125; Write(fileno(stdout), buf, n); &#125; if (FD_ISSET(fileno(fp), &amp;rset)) &#123; /* input is readable */ if ( (n = Read(fileno(fp), buf, MAXLINE)) == 0) &#123; stdineof = 1; Shutdown(sockfd, SHUT_WR); /* send FIN */ FD_CLR(fileno(fp), &amp;rset); continue; &#125; Writen(sockfd, buf, n); &#125; &#125;&#125; 参考select poll epoll的区别 可以看出select的缺点如下: (遍)select返回的是含有整个句柄的数组，应用程序需要遍历整个数组才能发现哪些句柄发生了事件； fd_set 使用数组实现，数组大小使用 FD_SETSIZE 定义，所以只能监听少于 FD_SETSIZE 数量的描述符。FD_SETSIZE 大小默认为 1024，因此默认只能监听少于 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译 (内)内核/用户空间内存拷贝问题，每次调用select都需要将全部描述符从应用进程缓冲区复制到内核缓冲区 (数)单个进程能够监视的文件描述符的数量存在最大限制，通常是1024，当然可以更改数量，但由于select采用轮询的方式扫描文件描述符，文件描述符数量越多，性能越差； select的触发方式是水平触发，应用程序如果没有完成对一个已经就绪的文件描述符进行IO，那么之后再次select调用还是会将这些文件描述符通知进程。 相比于select模型，poll使用链表保存文件描述符，因此没有了监视文件数量的限制，但其他三个缺点依然存在。 epoll一个常见的epoll使用例子:12345678910111213141516171819202122232425262728293031323334353637383940414243444546while (numOpenFds &gt; 0) &#123; /* Fetch up to MAX_EVENTS items from the ready list of the epoll instance */ printf("About to epoll_wait()\n"); ready = epoll_wait(epfd, evlist, MAX_EVENTS, -1); if (ready == -1) &#123; if (errno == EINTR) continue; /* Restart if interrupted by signal */ else errExit("epoll_wait"); &#125; printf("Ready: %d\n", ready); /* Deal with returned list of events */ for (j = 0; j &lt; ready; j++) &#123; printf(" fd=%d; events: %s%s%s\n", evlist[j].data.fd, (evlist[j].events &amp; EPOLLIN) ? "EPOLLIN " : "", (evlist[j].events &amp; EPOLLHUP) ? "EPOLLHUP " : "", (evlist[j].events &amp; EPOLLERR) ? "EPOLLERR " : ""); if (evlist[j].events &amp; EPOLLIN) &#123; s = read(evlist[j].data.fd, buf, MAX_BUF); if (s == -1) errExit("read"); printf(" read %d bytes: %.*s\n", s, s, buf); &#125; else if (evlist[j].events &amp; (EPOLLHUP | EPOLLERR)) &#123; /* After the epoll_wait(), EPOLLIN and EPOLLHUP may both have been set. But we'll only get here, and thus close the file descriptor, if EPOLLIN was not set. This ensures that all outstanding input (possibly more than MAX_BUF bytes) is consumed (by further loop iterations) before the file descriptor is closed. */ printf(" closing fd %d\n", evlist[j].data.fd); // 关闭一个文件描述符会自动的将其从所有的 epoll 实例的兴趣列表中移除 if (close(evlist[j].data.fd) == -1) errExit("close"); numOpenFds--; &#125; &#125;&#125; epoll的设计和实现select完全不同。epoll把原先的select/poll调用分成了3个部分： 调用epoll_create()建立一个epoll对象（在epoll文件系统中为这个句柄对象分配资源） 调用epoll_ctl向epoll对象中添加这100万个连接的套接字 调用epoll_wait收集发生的事件的连接 总结: epoll_ctl 用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait 便可以得到事件完成的描述符。 从上面的描述可以看出，epoll 只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，并且进程不需要通过轮询来获得事件完成的描述符。 epoll 仅适用于 Linux OS。 epoll 比 select 和 poll 更加灵活而且没有描述符数量限制。 水平触发与边缘触发的区别默认情况下 epoll 提供的是水平触发通知.要使用边缘触发通知，我们在调用epoll_ctl()时在ev．events字段中指定EPOLLET标志. 例如 : 12345struct epoll_event ev;ev.data.fd = fd;ev.events = EPOLLIN | EPOLLET;if (epoll_ctl(epfd, EPOLL_CTL_ADD, fd, ev) == -1) errExit("epoll_ctl"); 我们通过一个例子来说明epoll的水平触发和边缘触发通知之间的区别。假设我们使用epoll来监视一个套接字上的输入（EPOLLIN），接下来会发生如下的事件。 套接字上有输入到来。 我们调用一次epoll_wait()。无论我们采用的是水平触发还是边缘触发通知，该调用都会告诉我们套接字已经处于就绪态了。 再次调用epoll_wait()。 如果我们采用的是水平触发通知，那么第二个epoll_wait()调用将告诉我们套接字处于就绪态。 而如果我们采用边缘触发通知，那么第二个epoll_wait()调用将阻塞，因为自从上一次调用epoll_wait()以来并没有新的输入到来。边缘触发通知通常和非阻塞的文件描述符结合使用。因而，采用epoll的边缘触发通知机制的程序基本框架如下: 1. 让所有待监视的文件描述符都成为非阻塞的。 2. 通过epoll_ctl()构建epoll的兴趣列表。 3. 通过epoll_wait()取得处于就绪态的描述符列表。 4. 针对每一个处于就绪态的文件描述符，不断进行I/O处理直到相关的系统调用( 例如read()、write()，recv()、send()或accept() )返回EAGAIN或EWOULDBLOCK错误。 水平触发需要处理的问题使用linux epoll模型，水平触发模式（Level-Triggered）；当socket可写时，会不停的触发socket可写的事件，如何处理？ 第一种最普通的方式： 当需要向socket写数据时，将该socket加入到epoll模型（epoll_ctl）；等待可写事件。 接收到socket可写事件后，调用write()或send()发送数据。。。 当数据全部写完后， 将socket描述符移出epoll模型。 这种方式的缺点是： 即使发送很少的数据，也要将socket加入、移出epoll模型。有一定的操作代价。 第二种方式，（是本人的改进方案， 叫做directly-write） 向socket写数据时，不将socket加入到epoll模型；而是直接调用send()发送； 只有当或send()返回错误码EAGAIN（系统缓存满），才将socket加入到epoll模型，等待可写事件后(表明系统缓冲区有空间可以写了)，再发送数据。 全部数据发送完毕，再移出epoll模型。 这种方案的优点： 当用户数据比较少时，不需要epool的事件处理。 在高压力的情况下，性能怎么样呢？ 对一次性直接写成功、失败的次数进行统计。如果成功次数远大于失败的次数， 说明性能良好。（如果失败次数远大于成功的次数，则关闭这种直接写的操作，改用第一种方案。同时在日志里记录警告） 在我自己的应用系统中，实验结果数据证明该方案的性能良好。 事实上，网络数据可分为两种到达/发送情况： 一是分散的数据包， 例如每间隔40ms左右，发送/接收3-5个 MTU（或更小，这样就没超过默认的8K系统缓存）。 二是连续的数据包， 例如每间隔1s左右，连续发送/接收 20个 MTU（或更多）。 第三种方式： 使用Edge-Triggered（边沿触发），这样socket有可写事件，只会触发一次。 可以在应用层做好标记。以避免频繁的调用 epoll_ctl( EPOLL_CTL_ADD, EPOLL_CTL_MOD)。 这种方式是epoll 的 man 手册里推荐的方式， 性能最高。但如果处理不当容易出错，事件驱动停止。 epoll实现细节epoll的高效就在于，当我们调用epoll_ctl往里塞入百万个句柄时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的句柄给我们用户。这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。 而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已，如何能不高效？！ 那么，这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。 如此，一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。 最后看看epoll独有的两种模式LT和ET。无论是LT和ET模式，都适用于以上所说的流程。区别是，LT模式下，只要一个句柄上的事件一次没有处理完，会在以后调用epoll_wait时次次返回这个句柄，而ET模式仅在第一次返回。 这件事怎么做到的呢？当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表，最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了。所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的。 select 和 epoll的区别select函数，必须得清楚select跟linux特有的epoll的区别， 有三点(遍内数)： 遍历 ： 每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大；当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd, 每次只需要简单的从列表里取出就行了 内存拷贝 ： select，poll每次调用都要把fd集合从用户态往内核态拷贝一次; epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次 数量限制 ： select默认只支持1024个；epoll并没有最大数目限制 非阻塞的connect和accept 非阻塞connect为啥要用?怎么用? 为啥要用: 因为connect是比较耗时的, 所以我们希望可以在connecting的时候并行的做点其他的事 怎么用: 调用非阻塞connect之后会立马返回EINPROCESS错误, 然后我们去epoll注册一个可写事件, 等待此套接字可写我们判断一下如果不是socket发生异常错误则即为connect连上了 非阻塞accept有啥用, 怎么用?为啥要用? 为啥要用: 如果调用阻塞accept，这样如果在select检测到有连接请求，但在调用accept之前，这个请求断开了，然后调用accept的时候就会阻塞在哪里，除非这时有另外一个连接请求，如果没有，则一直被阻塞在accept调用上, 无法处理任何其他已就绪的描述符。 怎么用: 我们去epoll注册一个监听套接字的fd可读事件, 等待此套接字的fd可写我们判断一下如果不是socket发生异常错误则即为准备好了一个新连接 注意 : 当socket异常错误的时候socket是可读并可写的, 所以在非阻塞connect(判断是否可写)/accept(判断是否可读)的时候要特别注意这种情况, 要用getsockopt函数, 使用SO_ERROR选项来检查处理. 阻塞和非阻塞的send和recv和sendto和recvfrom注意: 首先需要说明的是，不管阻塞还是非阻塞，在发送时都会将数据从应用进程缓冲区拷贝到内核套接字发送缓冲区（UDP并没有实际存在这个内核套接字发送缓冲区, UDP的套接字缓冲区大小仅仅是可写到该套接字UDP数据包的大小上限, TCP/UDP都可以用SO_SNDBUF选项来更改该内核缓冲区大小）。 发送, 我们发送选用send（这里特指TCP）以及sendto（这里特指UDP）来描述 阻塞 在阻塞模式下send操作将会等待所有数据均被拷贝到发送缓冲区后才会返回。阻塞的send操作返回的发送大小，必然是你参数中的发送长度的大小。 在阻塞模式下的sendto操作不会阻塞。 关于这一点的原因在于：UDP并没有真正的发送缓冲区，它所做的只是将应用缓冲区拷贝给下层协议栈，在此过程中加上UDP头，IP头，所以实际不存在阻塞。 非阻塞 在非阻塞模式下send操作调用会立即返回。关于立即返回大家都不会有异议。还是拿阻塞send的那个例子来看，当缓冲区只有192字节，但是却需要发送2000字节时，此时调用立即返回，并得到返回值为192。从中可以看到，非阻塞send仅仅是尽自己的能力向缓冲区拷贝尽可能多的数据，因此在非阻塞下send才有可能返回比你参数中的发送长度小的值。如果缓冲区没有任何空间时呢？这时肯定也是立即返回，但是你会得到WSAEWOULDBLOCK/EWOULDBLOCK 的错误，此时表示你无法拷贝任何数据到缓冲区，你最好休息一下再尝试发送。 在非阻塞模式下sendto操作 不会阻塞（与阻塞一致，不作说明）。 接收, 接收选用recv（这里特指TCP）以及recvfrom（这里特指UDP）来描述 阻塞 在阻塞模式下recv，recvfrom操作将会阻塞 到缓冲区里有至少一个字节（TCP）或者一个完整UDP数据报才返回。 在没有数据到来时，对它们的调用都将处于睡眠状态，不会返回。 非阻塞 在非阻塞模式下recv，recvfrom操作将会立即返回。 如果缓冲区有任何一个字节数据（TCP）或者一个完整UDP数据报，它们将会返回接收到的数据大小。而如果没有任何数据则返回错误 WSAEWOULDBLOCK/EWOULDBLOCK。 reuseaddr和reuseport reuseaddr的作用? 参考 https://zhuanlan.zhihu.com/p/35367402 主要是用于绑定TIME_WAIT状态的地址: 一个非常现实的问题是，假如一个systemd托管的service异常退出了，留下了TIME_WAIT状态的socket，那么systemd将会尝试重启这个service。但是因为端口被占用，会导致启动失败，造成两分钟的服务空档期，systemd也可能在这期间放弃重启服务。但是在设置了SO_REUSEADDR以后，处于TIME_WAIT状态的地址也可以被绑定，就杜绝了这个问题。因为TIME_WAIT其实本身就是半死状态，虽然这样重用TIME_WAIT可能会造成不可预料的副作用，但是在现实中问题很少发生，所以也忽略了它的副作用 reuseport有啥用? SO_REUSEPORT使用场景：linux kernel 3.9 引入了最新的SO_REUSEPORT选项，使得多进程或者多线程创建多个绑定同一个ip:port的监听socket，提高服务器的接收链接的并发能力,程序的扩展性更好；此时需要设置SO_REUSEPORT（注意所有进程都要设置才生效）。 1setsockopt(listenfd, SOL_SOCKET, SO_REUSEPORT,(const void *)&amp;reuse , sizeof(int)); 目的：每一个进程有一个独立的监听socket，并且bind相同的ip:port，独立的listen()和accept()；提高接收连接的能力。（例如nginx多进程同时监听同一个ip:port）解决的问题： 避免了应用层多线程或者进程监听同一ip:port的“惊群效应”。 内核层面实现负载均衡，保证每个进程或者线程接收均衡的连接数。 只有effective-user-id相同的服务器进程才能监听同一ip:port （安全性考虑）]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器开发自我修养专栏-Linux文件系统]]></title>
    <url>%2F2020%2F11%2F06%2Fself_cultivation_linux_file_system%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Linux文件系统详细的可以查看本博客的这篇文章哈文件描述符FD与Inodefd数目大小的限制可以改变, 参考 文件描述符限制 系统目录结构 Linux 系统目录结构 登录系统后，在当前命令窗口下输入命令： ls / 你会看到如下图所示: 树状目录结构： 以下是对这些目录的解释： /bin： bin 是 Binaries (二进制文件) 的缩写, 这个目录存放着最经常使用的命令。 以下是对这些目录的解释： /bin： bin 是 Binaries (二进制文件) 的缩写, 这个目录存放着最经常使用的命令。 /boot： 这里存放的是启动 Linux 时使用的一些核心文件，包括一些连接文件以及镜像文件。 /dev ： dev 是 Device(设备) 的缩写, 该目录下存放的是 Linux 的外部设备，在 Linux 中访问设备的方式和访问文件的方式是相同的。 /etc： etc 是 Etcetera(等等) 的缩写, 这个目录用来存放所有的系统管理所需要的配置文件和子目录。 /home： 用户的主目录，在 Linux 中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的，如上图中的 alice、bob 和 eve。 /lib： lib 是 Library(库) 的缩写这个目录里存放着系统最基本的动态连接共享库，其作用类似于 Windows 里的 DLL 文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found： 这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /media： linux 系统会自动识别一些设备，例如 U 盘、光驱等等，当识别后，Linux 会把识别的设备挂载到这个目录下。 /mnt： 系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在 /mnt/ 上，然后进入该目录就可以查看光驱里的内容了。 /opt： opt 是 optional(可选) 的缩写，这是给主机额外安装软件所摆放的目录。比如你安装一个 ORACLE 数据库则就可以放到这个目录下。默认是空的。 /proc： proc 是 Processes(进程) 的缩写，/proc 是一种伪文件系统（也即虚拟文件系统），存储的是当前内核运行状态的一系列特殊文件，这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。 这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的 ping 命令，使别人无法 ping 你的机器： 1echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all /root： 该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin： s 就是 Super User 的意思，是 Superuser Binaries (超级用户的二进制文件) 的缩写，这里存放的是系统管理员使用的系统管理程序。 /selinux： 这个目录是 Redhat/CentOS 所特有的目录，Selinux 是一个安全机制，类似于 windows 的防火墙，但是这套机制比较复杂，这个目录就是存放 selinux 相关的文件的。 /srv： 该目录存放一些服务启动之后需要提取的数据。 /sys： 这是 Linux2.6 内核的一个很大的变化。该目录下安装了 2.6 内核中新出现的一个文件系统 sysfs 。 sysfs 文件系统集成了下面 3 种文件系统的信息：针对进程信息的 proc 文件系统、针对设备的 devfs 文件系统以及针对伪终端的 devpts 文件系统。 该文件系统是内核设备树的一个直观反映。 当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。 /tmp： tmp 是 temporary(临时) 的缩写这个目录是用来存放一些临时文件的。 /usr： usr 是 unix shared resources(共享资源) 的缩写，这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于 windows 下的 program files 目录。 /usr/bin： 系统用户使用的应用程序。 /usr/sbin： 超级用户使用的比较高级的管理程序和系统守护程序。 /usr/src： 内核源代码默认的放置目录。 /var： var 是 variable(变量) 的缩写，这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 /run： 是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。 在 Linux 系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。/etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。/bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在 /bin/ls 目录下的。值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除 root 外的通用户），而 / sbin, /usr/sbin 则是给 root 使用的指令。/var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在 /var/log 目录下，另外 mail 的预设放置也是在这里。 inode硬盘的最小存储单位是扇区(Sector)，块(block)由多个扇区组成。文件数据存储在块中。块的最常见的大小是 4kb，约为 8 个连续的扇区组成（每个扇区存储 512 字节）。一个文件可能会占用多个 block，但是一个块只能存放一个文件。 虽然，我们将文件存储在了块(block)中，但是我们还需要一个空间来存储文件的 元信息 metadata ：如某个文件被分成几块、每一块在的地址、文件拥有者，创建时间，权限，大小等。这种 存储文件元信息的区域就叫 inode，译为索引节点：i（index）+node。 每个文件都有一个 inode，存储文件的元信息。 可以使用 stat 命令可以查看文件的 inode 信息。每个 inode 都有一个号码，Linux/Unix 操作系统不使用文件名来区分文件，而是使用 inode 号码区分不同的文件。 简单来说：inode 就是用来维护某个文件被分成几块、每一块在的地址、文件拥有者，创建时间，权限，大小等信息。简单总结一下： inode ：记录文件的属性信息，可以使用 stat 命令查看 inode 信息。 block ：实际文件的内容，如果一个文件大于一个块时候，那么将占用多个 block，但是一个块只能存放一个文件。（因为数据是由 inode 指向的，如果有两个文件的数据存放在同一个块中，就会乱套了） 软链接与硬链接详细的可参考: https://blog.csdn.net/yangxjsun/article/details/79681229 硬链接普通链接一般就是指硬链接, 硬链接是新的目录条目，其引用系统中的现有文件。文件系统中的每一文件默认具有一个硬链接。为节省空间，可以不复制文件，而创建引用同一文件的新硬链接。新硬链接如果在与现有硬链接相同的目录中创建，则需要有不同的文件名，否则需要在不同的目录中。指向同一文件的所有硬链接具有相同的权限、连接数、用户/组所有权、时间戳以及文件内容。指向同一文件内容的硬链接需要在相同的文件系统中。简单说，硬链接就是一个 inode 号对应多个文件名。就是同一个文件使用了多个别名（上图中 hard link 就是 file 的一个别名，他们有共同的 inode）。 由于硬链接是有着相同 inode 号仅文件名不同的文件，因此硬链接存在以下几点特性： 文件有相同的 inode 及 data block； 只能对已存在的文件进行创建； 不能交叉文件系统进行硬链接的创建； 不能对目录进行创建，只可对文件创建； 删除一个硬链接文件并不影响其他有相同 inode 号的文件, 只是相应的链接计数器（link count)减1 软链接(又称符号链接，即 soft link 或 symbolic link） 软链接与硬链接不同，若文件用户数据块中存放的内容是另一文件的路径名的指向，则该文件就是软连接。软链接就是一个普通文件，只是数据块内容有点特殊。软链接有着自己的 inode 号以及用户数据块。（见图2）软连接可以指向目录，而且软连接所指向的目录可以位于不同的文件系统中。 软链接特性： 软链接有自己的文件属性及权限等； 可对不存在的文件或目录创建软链接； 软链接可交叉文件系统； 软链接可对文件或目录创建； 创建软链接时，链接计数 i_nlink 不会增加； 删除软链接并不影响被指向的文件，但若被指向的原文件被删除，则相关软连接被称为死链接或悬挂的软链接（即 dangling link，若被指向路径文件被重新创建，死链接可恢复为正常的软链接）。 Linux 为什么多进程能够读写正在删除的文件参考进程表_文件表_inode_vnode Linux中多进程环境下，打开同一个文件，当一个进程进行读写操作，如果另外一个进程删除了这个文件，那么读写该文件的进程会发生什么呢? 因为文件被删除了，读写进程发生异常? 正在读写的进程仍然正常读写，好像没有发生什么？ 学操作系统原理的时候，我们知道，linux是通过link的数量来控制文件删除，只有当一个文件不存在任何link的时候，这个文件才会被删除。 而每个文件都会有2个link计数器: i_count: i_count的意义是当前使用者的数量，也就是打开文件进程的个数。 i_nlink: i_nlink的意义是介质连接的数量. 或者可以理解为 i_count是内存引用计数器，i_nlink是硬盘引用计数器。再换句话说，当文件被某个进程引用时，i_count 就会增加；当创建文件的硬连接的时候，i_nlink 就会增加。 对于 rm 而言，就是减少 i_nlink。这里就出现一个问题，如果一个文件正在被某个进程调用，而用户却执行 rm 操作把文件删除了，会出现什么结果呢？ 当用户执行 rm 操作后，ls 或者其他文件管理命令不再能够找到这个文件，但是进程却依然在继续正常执行，依然能够从文件中正确的读取内容。这是因为，rm 操作只是将 i_nlink 置为 0 了；由于文件被进程引用的缘故，i_count 不为 0，所以系统没有真正删除这个文件。i_nlink 是文件删除的充分条件，而 i_count 才是文件删除的必要条件。 基于以上只是，大家猜一下，如果在一个进程在打开文件写日志的时候，手动或者另外一个进程将这个日志删除，会发生什么情况？ 是的，数据库并没有停掉。虽然日志文件被删除了，但是有一个进程已经打开了那个文件，所以向那个文件中的写操作仍然会成功，数据仍然会提交。 文件操作偏移lseeklseek的函数用于设置文件偏移量。 每个打开的文件都有一个与其相关联的“当前文件偏移量”（当前文件偏移量）。它通常是一个非负整数，用以度量从文件开始处计算的字节数。通常，读写操作都从当前文件偏移量处开始，并使偏移量增加所读写的字节数。按系统默认的情况，当打开一个文件时，除非制定O_APPEND选项，否则该偏移量被设置为0。 文件空洞我们知道lseek()系统调用可以改变文件的偏移量，但如果程序调用使得文件偏移量跨越了文件结尾，然后再执行I/O操作，将会发生什么情况？ read()调用将会返回0，表示文件结尾。令人惊讶的是，write()函数可以在文件结尾后的任意位置写入数据。在这种情况下，对该文件的下一次写将延长该文件，并在文件中构成一个空洞，这一点是允许的。从原来的文件结尾到新写入数据间的这段空间被成为文件空洞。调用write后文件结尾的位置已经发生变化。 文件空洞不占用任何磁盘空间，直到后续某个时点，在文件空洞中写入了数据，文件系统才会为之分配磁盘块。空洞的存在意味着一个文件名义上的大小可能要比其占用的磁盘存储总量要大（有时大出许多）。向文件空洞中写入字节，内核需要为其分配存储单元，即使文件大小不变，系统的可用磁盘空间也将减少。这种情况并不常见，但也需要了解。 实际中的空洞文件会在哪里用到呢?常见的场景有: 一是在下载电影的时候,发现刚开始下载,文件的大小就已经到几百M了. 二是在创建虚拟机的磁盘镜像的时候,你创建了一个100G的磁盘镜像,但是其实装起来系统之后,开始也不过只占用了3,4G的磁盘空间,如果一开始把100G都分配出去的话,无疑是很大的浪费. 空洞文件方法对多线程共同操作文件是及其有用的。有时候我们创建一个很大的文件(比如视频文件)，如果从头开始依次构建时间很长。有一种思路就是将文件分为多段，然后多线程来操作每个线程负责其中一段的写入。（就像修100公里的高速公路，分成20个段来修，每个段就只负责5公里，就可以大大提高效率）。 习题Linux下两个进程可以同时打开同一个文件，这时如下描述错误的是(答案是4)： 两个进程中分别产生生成两个独立的fd 两个进程可以任意对文件进行读写操作，操作系统并不保证写的原子性 进程可以通过系统调用对文件加锁，从而实现对文件内容的保护 任何一个进程删除该文件时，另外一个进程会立即出现读写失败 两个进程可以分别读取文件的不同部分而不会相互影响 一个进程对文件长度和内容的修改另外一个进程可以立即感知 proc文件夹参考: https://www.cnblogs.com/liushui-sky/p/9354536.html 下面是作者系统（RHEL5.3）上运行的一个PID为2674的进程saslauthd的相关文件，其中有些文件是每个进程都会具有的，后文会对这些常见文件做出说明。123456789101112131415161718192021222324252627[root@rhel5 ~]# ll /proc/2674total 0dr-xr-xr-x 2 root root 0 Feb 8 17:15 attr-r-------- 1 root root 0 Feb 8 17:14 auxv-r--r--r-- 1 root root 0 Feb 8 17:09 cmdline-rw-r--r-- 1 root root 0 Feb 8 17:14 coredump_filter-r--r--r-- 1 root root 0 Feb 8 17:14 cpusetlrwxrwxrwx 1 root root 0 Feb 8 17:14 cwd -&gt; /var/run/saslauthd-r-------- 1 root root 0 Feb 8 17:14 environlrwxrwxrwx 1 root root 0 Feb 8 17:09 exe -&gt; /usr/sbin/saslauthddr-x------ 2 root root 0 Feb 8 17:15 fd-r-------- 1 root root 0 Feb 8 17:14 limits-rw-r--r-- 1 root root 0 Feb 8 17:14 loginuid-r--r--r-- 1 root root 0 Feb 8 17:14 maps-rw------- 1 root root 0 Feb 8 17:14 mem-r--r--r-- 1 root root 0 Feb 8 17:14 mounts-r-------- 1 root root 0 Feb 8 17:14 mountstats-rw-r--r-- 1 root root 0 Feb 8 17:14 oom_adj-r--r--r-- 1 root root 0 Feb 8 17:14 oom_scorelrwxrwxrwx 1 root root 0 Feb 8 17:14 root -&gt; /-r--r--r-- 1 root root 0 Feb 8 17:14 schedstat-r-------- 1 root root 0 Feb 8 17:14 smaps-r--r--r-- 1 root root 0 Feb 8 17:09 stat-r--r--r-- 1 root root 0 Feb 8 17:14 statm-r--r--r-- 1 root root 0 Feb 8 17:10 statusdr-xr-xr-x 3 root root 0 Feb 8 17:15 task-r--r--r-- 1 root root 0 Feb 8 17:14 wchan cmdline — 启动当前进程的完整命令，但僵尸进程目录中的此文件不包含任何信息； 12[root@rhel5 ~]# more /proc/2674/cmdline /usr/sbin/saslauthd cwd — 指向当前进程运行目录的一个符号链接； environ — 当前进程的环境变量列表，彼此间用空字符（NULL）隔开；变量用大写字母表示，其值用小写字母表示； 12[root@rhel5 ~]# more /proc/2674/environ TERM=linuxauthd exe — 指向启动当前进程的可执行文件（完整路径）的符号链接，通过/proc/N/exe可以启动当前进程的一个拷贝； fd — 这是个目录，包含当前进程打开的每一个文件的文件描述符（file descriptor），这些文件描述符是指向实际文件的一个符号链接； 123456789[root@rhel5 ~]# ll /proc/2674/fdtotal 0lrwx------ 1 root root 64 Feb 8 17:17 0 -&gt; /dev/nulllrwx------ 1 root root 64 Feb 8 17:17 1 -&gt; /dev/nulllrwx------ 1 root root 64 Feb 8 17:17 2 -&gt; /dev/nulllrwx------ 1 root root 64 Feb 8 17:17 3 -&gt; socket:[7990]lrwx------ 1 root root 64 Feb 8 17:17 4 -&gt; /var/run/saslauthd/saslauthd.pidlrwx------ 1 root root 64 Feb 8 17:17 5 -&gt; socket:[7991]lrwx------ 1 root root 64 Feb 8 17:17 6 -&gt; /var/run/saslauthd/mux.accept limits — 当前进程所使用的每一个受限资源的软限制、硬限制和管理单元；此文件仅可由实际启动当前进程的UID用户读取；（2.6.24以后的内核版本支持此功能）； maps — 当前进程关联到的每个可执行文件和库文件在内存中的映射区域及其访问权限所组成的列表； 123456[root@rhel5 ~]# cat /proc/2674/maps 00110000-00239000 r-xp 00000000 08:02 130647 /lib/libcrypto.so.0.9.8e00239000-0024c000 rwxp 00129000 08:02 130647 /lib/libcrypto.so.0.9.8e0024c000-00250000 rwxp 0024c000 00:00 0 00250000-00252000 r-xp 00000000 08:02 130462 /lib/libdl-2.5.so00252000-00253000 r-xp 00001000 08:02 130462 /lib/libdl-2.5.so mem — 当前进程所占用的内存空间，由open、read和lseek等系统调用使用，不能被用户读取； root — 指向当前进程运行根目录的符号链接；在Unix和Linux系统上，通常采用chroot命令使每个进程运行于独立的根目录； stat — 当前进程的状态信息，包含一系统格式化后的数据列，可读性差，通常由ps命令使用； statm — 当前进程占用内存的状态信息，通常以“页面”（page）表示； status — 与stat所提供信息类似，但可读性较好，如下所示，每行表示一个属性信息；其详细介绍请参见 proc的man手册页； 123456789101112131415161718[root@rhel5 ~]# more /proc/2674/status Name: saslauthdState: S (sleeping)SleepAVG: 0%Tgid: 2674Pid: 2674PPid: 1TracerPid: 0Uid: 0 0 0 0Gid: 0 0 0 0FDSize: 32Groups:VmPeak: 5576 kBVmSize: 5572 kBVmLck: 0 kBVmHWM: 696 kBVmRSS: 696 kB………… task — 目录文件，包含由当前进程所运行的每一个线程的相关信息，每个线程的相关信息文件均保存在一个由线程号（tid）命名的目录中，这类似于其内容类似于每个进程目录中的内容；（内核2.6版本以后支持此功能）]]></content>
      <categories>
        <category>Self-cultivation</category>
      </categories>
      <tags>
        <tag>Self-cultivation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[muduo详解之网络编程难点解读]]></title>
    <url>%2F2020%2F09%2F24%2Fmuduo_qa%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[时间有限, 有空待续… . . . 网络编程难点TCP 网络编程这其中有很多难点，也有很多细节需要注意，比方说： 如果要主动关闭连接，如何保证对方已经收到全部数据？如果应用层有缓冲（这在非阻塞网络编程中是必须的，见下文），那么如何保证先发送完缓冲区中的数据，然后再断开连接。直接调用 close(2) 恐怕是不行的。 走shutdown的SHUT_WR, 称为半关闭half-close, 当前留在套接字发送缓冲区的数据会被发送掉, 后跟正常的tcp连接终止序列, 不管套接字描述符的引用计数是否为0, 写半部关闭照样执行. 如果主动发起连接，但是对方主动拒绝，如何定期 (带 back-off) 重试？ 重试的间隔应该逐渐延长，例如0.5s、1s、2s、4s，直至30s。这就是所谓的back-off 非阻塞网络编程该用边沿触发(edge trigger)还是电平触发(level trigger)？（这两个中文术语有其他译法，我选择了一个电子工程师熟悉的说法。）如果是电平触发，那么什么时候关注 EPOLLOUT 事件？会不会造成 busy-loop？如果是边沿触发，如何防止漏读造成的饥饿？epoll 一定比 poll 快吗？ 在非阻塞网络编程中，为什么要使用应用层缓冲区？假如一次读到的数据不够一个完整的数据包，那么这些已经读到的数据是不是应该先暂存在某个地方，等剩余的数据收到之后再一并处理？假如数据是一个字节一个字节地到达，间隔 10ms，每个字节触发一次文件描述符可读 (readable) 事件，程序是否还能正常工作？lighttpd 在这个问题上出过安全漏洞。 pass待续 在非阻塞网络编程中，如何设计并使用缓冲区？一方面我们希望减少系统调用，一次读的数据越多越划算，那么似乎应该准备一个大的缓冲区。另一方面，我们系统减少内存占用。如果有 10k 个连接，每个连接一建立就分配 64k 的读缓冲的话，将占用 640M 内存，而大多数时候这些缓冲区的使用率很低。muduo 用 readv 结合栈上空间巧妙地解决了这个问题。 buffer设计要点 如果使用发送缓冲区，万一接收方处理缓慢，数据会不会一直堆积在发送方，造成内存暴涨？如何做应用层的流量控制？ 做了一个高水位的回调, 让应用层处理 12345678910111213141516assert(remaining &lt;= len);if (!faultError &amp;&amp; remaining &gt; 0)&#123; size_t oldLen = outputBuffer_.readableBytes(); if (oldLen + remaining &gt;= highWaterMark_ &amp;&amp; oldLen &lt; highWaterMark_ &amp;&amp; highWaterMarkCallback_) &#123; loop_-&gt;queueInLoop(std::bind(highWaterMarkCallback_, shared_from_this(), oldLen + remaining)); &#125; outputBuffer_.append(static_cast&lt;const char*&gt;(data)+nwrote, remaining); if (!channel_-&gt;isWriting()) &#123; channel_-&gt;enableWriting(); &#125;&#125; 如何设计并实现定时器？并使之与网络 IO 共用一个线程，以避免锁。 不用锁的话就走runInLoop 这些问题在 muduo 的代码中可以找到答案。 buffer设计要点123456789101112131415161718192021222324252627282930313233ssize_t Buffer::readFd(int fd, int* savedErrno)&#123; // saved an ioctl()/FIONREAD call to tell how much to read char extrabuf[65536]; struct iovec vec[2]; const size_t writable = writableBytes(); vec[0].iov_base = begin()+writerIndex_; vec[0].iov_len = writable; vec[1].iov_base = extrabuf; vec[1].iov_len = sizeof extrabuf; // when there is enough space in this buffer, don't read into extrabuf. // when extrabuf is used, we read 128k-1 bytes at most. const int iovcnt = (writable &lt; sizeof extrabuf) ? 2 : 1; const ssize_t n = sockets::readv(fd, vec, iovcnt); if (n &lt; 0) &#123; *savedErrno = errno; &#125; else if (implicit_cast&lt;size_t&gt;(n) &lt;= writable) &#123; writerIndex_ += n; &#125; else &#123; writerIndex_ = buffer_.size(); append(extrabuf, n - writable); &#125; // if (n == writable + sizeof extrabuf) // &#123; // goto line_30; // &#125; return n;&#125; 备注: readv则将从fd读入的数据按同样的顺序散布到各缓冲区中，readv总是先填满一个缓冲区，然后再填下一个1234567#include &lt;sys/uio.h&gt;ssize_t readv(int fd, const struct iovec *iov, int iovcnt);ssize_t writev(int fd, const struct iovec *iov, int iovcnt);struct iovec &#123; void *iov_base; /* Starting address */ size_t iov_len; /* Number of bytes to transfer */&#125;; buffer数据结构如下:12345678910/// A buffer class modeled after org.jboss.netty.buffer.ChannelBuffer////// @code/// +-------------------+------------------+------------------+/// | prependable bytes | readable bytes | writable bytes |/// | | (CONTENT) | |/// +-------------------+------------------+------------------+/// | | | |/// 0 &lt;= readerIndex &lt;= writerIndex &lt;= size/// @endcode 在非阻塞网络编程中，如何设计并使用缓冲区？ 一方面希望减少系统调用，一次读取的数据越多越划算；另一方面希望减少内存的占用。这两方面似乎是矛盾的，假设C10K ，每个连接一建立就分配50KB 的内存的话，那么将占用1GB 内存，但是大多数的连接并不需要这么多内存。muduo 巧妙的使用了readv() 结合栈上空间巧妙的解决了这个问题。 在栈上准备一个65535 字节(64k)的extrabuf , 然后利用readv() 来读取数据，iovec有两块，第一块是指向muduo Buffer （为每个连接准备1024字节的buf）中的writeable 字节，另一块是指向extrabuf。这样如果读入的数据不多，直接读到内置的buf；如果长度超过内置buf 的大小，就会读到栈上的extrabuf 中，然后程序再把extrabuf 里的数据append() 到 buf 中。 由于muduo采用level trigger, 因此readFd函数不会反复调用read()直到其返回EAGAIN, 从而节省系统调用次数, 降低延迟 为啥是64k? 因为平时一般都阻塞在poll上, 来了数据马上就处理的话也就几k而已, 即使是千兆网卡100MB/s, 500微秒也就是500/1000000秒也就是0.5毫秒, 100MB/s * (500/1000000) = 50000B, 也就是说0.5毫秒就是50000字节的数据, 64k已经足够容纳千兆网在0.5毫秒全速收到的数据了. 一般来说, 一次readv就能读完这次过来的数据了, 如果用完了writeable和extrabuf还是不够, 那就再readv一次嘛 异步日志双缓冲技术muduo日志库采用的是双缓冲技术，基本思路是准备两块缓冲：A与B，前端负责往buffer A中填数据（日志消息），后端负责将buffer B中的数据写入文件。当buffer A写满之后，交换A与B，让后端将buffer A中的数据写入文件，而前端负责往buffer B中填入新的日志文件。如此往复。 用两个buffer的好处是在新建日志消息的时候不必等待磁盘文件操作，也避免每条消息都触发（唤醒）了后端日志线程。换言之，前端不是将一条条消息分别传送给后端，而是将多个日志消息拼成一个大的buffer传送给后端，相当于批处理，减少了线程唤醒的频率，降低了开销。另外，为了及时将消息写入文件，即使前端的buffer A未写满，日志库也会每三秒执行一次上述交换写入操作。 connector编程难点1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768void Connector::handleWrite()&#123; LOG_TRACE &lt;&lt; "Connector::handleWrite " &lt;&lt; state_; if (state_ == kConnecting) &#123; int sockfd = removeAndResetChannel(); int err = sockets::getSocketError(sockfd); if (err) &#123; LOG_WARN &lt;&lt; "Connector::handleWrite - SO_ERROR = " &lt;&lt; err &lt;&lt; " " &lt;&lt; strerror_tl(err); retry(sockfd); &#125; else if (sockets::isSelfConnect(sockfd)) &#123; LOG_WARN &lt;&lt; "Connector::handleWrite - Self connect"; retry(sockfd); &#125; else &#123; setState(kConnected); if (connect_) &#123; newConnectionCallback_(sockfd); &#125; else &#123; sockets::close(sockfd); &#125; &#125; &#125; else &#123; // what happened? assert(state_ == kDisconnected); &#125;&#125;void Connector::handleError()&#123; LOG_ERROR &lt;&lt; "Connector::handleError state=" &lt;&lt; state_; if (state_ == kConnecting) &#123; int sockfd = removeAndResetChannel(); int err = sockets::getSocketError(sockfd); LOG_TRACE &lt;&lt; "SO_ERROR = " &lt;&lt; err &lt;&lt; " " &lt;&lt; strerror_tl(err); retry(sockfd); &#125;&#125;void Connector::retry(int sockfd)&#123; sockets::close(sockfd); setState(kDisconnected); if (connect_) &#123; LOG_INFO &lt;&lt; "Connector::retry - Retry connecting to " &lt;&lt; serverAddr_.toIpPort() &lt;&lt; " in " &lt;&lt; retryDelayMs_ &lt;&lt; " milliseconds. "; loop_-&gt;runAfter(retryDelayMs_/1000.0, std::bind(&amp;Connector::startInLoop, shared_from_this())); retryDelayMs_ = std::min(retryDelayMs_ * 2, kMaxRetryDelayMs); &#125; else &#123; LOG_DEBUG &lt;&lt; "do not connect"; &#125;&#125; 通过上述代码能看出: 用于建立连接的socket是一次性的，一旦出错，就无法恢复，只能关闭重来。但Connector是可以反复使用的，因此每次尝试连接都要使用新的socket和新的channel。 错误代码与accept不同，EAGAIN是真的错误，表明本机的临时端口号暂时用完，要关闭socket再延期重试。 即便出现socket可写，也不一定意味着连接已成功建立，还需要用getsockopt(…, SO_ERROR)再次确认一下。 重试的间隔应该逐渐延长，例如0.5s、1s、2s、4s，直至30s。这就是所谓的back-off 要处理自连接，处理的办法是断开连接再重试 首先，要认命，目前在linux网络协议栈开发者眼中，TCP自连接行为不被认为是TCP协议实现的bug。。。。为什么会出现自连接？当对一个TCP socket调用connect函数时，如果这个socket没有bind指定的端口号，操作系统会为它选择一个当前未被使用的端口号，这个端口号被称为ephemeral port, 范围可以在/proc/sys/net/ipv4/ip_local_port_range里查看。假设32769这个端口被选为ephemeral port，当connect函数被调用时，tcp向目标地址发起三次握手过程，发送SYN分节到对端，由于源端口和目标端口相同都是32769，32769端口是被打开的，这样32769端口能收到SYN分节，返回的是SYN＋ACK，而不会返回RST，最后源地址再返回ACK，三次握手流程完毕，完成了一个TCP自连接。如何避免自连接？可以加上一段判断保护代码，对描述符分别调用getsockname和getpeername，然后检查返回的地址结构是否相等，如果相等，说明是自连接，close描述符取消该连接。 Linux给没有bind过的socket自动分配一个端口号，默认是32768-61000。你或者把服务端listen监听的端口号改到这个范围以外，比如30008；或者改自动分配的范围：echo &quot;40001 61000&quot; &gt; /proc/sys/net/ipv4/ip_local_port_range 就不会出这个问题了。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>noodle</tag>
        <tag>muduo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑CPU与核心备忘]]></title>
    <url>%2F2020%2F07%2F06%2Fcpu_core_processor_thread_info%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[. . . cpu、core、processor、thread 等概念，有的是物理的有的是逻辑的，在不同语境中含义不尽相同。 “电脑有几个 cpu ?” “多线程程序设置多少个线程数效果好？” “linux cpuinfo / top 里展示的 cpu 的信息如何理解？” 物理 cpu 数（physical cpu）指主板上实际插入的 cpu 硬件个数（socket）。（但是这一概念经常被泛泛的说成是 cpu 数，这很容易导致与 core 数，processor 数等概念混淆，所以此处强调是物理 cpu 数）。 由于在主板上引入多个 cpu 插槽需要更复杂的硬件支持（连接不同插槽的 cpu 到内存和其他资源），通常只会在服务器上才这样做。在家用电脑中，一般主板上只会有一个 cpu 插槽。 核心（core）一开始，每个物理 cpu 上只有一个核心（a single core），对操作系统而言，也就是同一时刻只能运行一个进程 / 线程。 为了提高性能，cpu 厂商开始在单个物理 cpu 上增加核心（实实在在的硬件存在），也就出现了双核心 cpu（dual-core cpu）以及多核心 cpu（multiple cores），这样一个双核心 cpu 就是同一时刻能够运行两个进程 / 线程的。 同时多线程与超线程 同时多线程技术（simultaneous multithreading）和 超线程技术（hyper–threading/HT） 本质一样，是为了提高单个 core 同一时刻能够执行的多线程数的技术（充分利用单个 core 的计算能力，尽量让其 “一刻也不得闲”）。 simultaneous multithreading 缩写是 SMT，AMD 和其他 cpu 厂商的称呼。 hyper–threading 是 Intel 的称呼，可以认为 hyper–threading 是 SMT 的一种具体技术实现。 在类似技术下，产生了如下等价术语： 虚拟 core： virtual core 逻辑 processer： logical processor 线程：thread 所以可以这样说：某款采用 SMT 技术的 4 核心 AMD cpu 提供了 8 线程同时执行的能力；某款采用 HT 技术的 2 核心 Intel cpu 提供了 4 线程同时执行的能力。 查看 cpu 信息的方法与命令linux 系统lscpu 命令可以同时看到各种信息。比如： 1234567...CPU(s): 24On-line CPU(s) list: 0-23Thread(s) per core: 2Core(s) per socket: 6Socket(s): 2... 查看物理 cpu 数： cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l 查看每个物理 cpu 中 核心数 (core 数)： cat /proc/cpuinfo | grep &quot;cpu cores&quot; | uniq 查看总的逻辑 cpu 数（processor 数）： cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l 查看 cpu 型号： cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 判断 cpu 是否 64 位： 检查 cpuinfo 中的 flags 区段，看是否有 lm （long mode） 标识 window 系统任务管理器-性能-CPU: Sockets Cores Logical processors x86 与 x86_64 (摘自 维基百科)x86泛指一系列基于 Intel 8086 且向后兼容的中央处理器指令集架构。 x86 的 32 位架构一般又被称作 IA-32，全名为 “Intel Architecture, 32-bit”。 值得注意的是，Intel 也推出过 IA-64 架构，虽然名字上与 “IA-32” 相似，但两者完全不兼容，并不属于 x86 指令集架构家族。 x86-64又称 x64，即英文词 64-bit extended，64 位拓展 的简写，是 x86 架构的 64 位拓展，向后兼容于 16 位及 32 位的 x86 架构。 不同厂商有不同的称呼： x64 于 1999 年由 AMD 设计，AMD 首次公开 64 位集以扩展给 x86，称为 “AMD64” 其后也为 Intel 所采用，Intel 称为 “Intel 64” 苹果公司和 RPM 包管理员称为 “x86-64” 或 “x86_64” 甲骨文公司及 Microsoft 称为 “x64” BSD 家族及其他 Linux 发行版则使用 “amd64”，32 位版本则称为 “i386”（或 i486/586/686） Arch Linux 成为 x86_64 多线程程序线程数为了让我们的多线程程序更好的利用 cpu 资源，我们通常会先了解机器拥有的 processor 数，有若干手段可以获取这一信息： cpuinfo 中查看：比如上文中的 cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l top 命令查看：cpu0,cpu1,… 编程：比如在 Java 中用 Runtime.getRuntime().availableProcessors() 具体在多线程程序中设置线程数多大，对计算密集型的程序有的建议是 processor count + 1，有的建议是 processor count 的 1.5 倍，都是经验值，实测为准。 小结 一台完整的计算机可能包含一到多个物理 cpu 从单个物理 cpu （physical cpu）的角度看，其可能是单核心、双核心甚至多核心的 从单个核心（core）的角度看，还有 SMT / HT 等技术让每个 core 对计算机操作系统而言用起来像多个物理 core 差不多 总的逻辑 cpu 数 = 物理 cpu 数 每颗物理 cpu 的核心数 每个核心的超线程数 线程 cpu-basics-multiple-cpus-cores-and-hyper-threading-explained Simultaneous multithreading Hyper-threading Linux 查看物理 CPU 个数、核数、逻辑 CPU 个数 x86 x86-64]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>CPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python单例实现之详解元类和type和__call__和__new__和__init__]]></title>
    <url>%2F2020%2F06%2F17%2Fpython_call_new_init%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[python的单例实现方式茫茫多, 讲道理, 其实是违背python之禅的: There should be one– and preferably only one –obvious way to do it. 用一种方法，最好是只有一种方法来做一件事 12345678910111213141516171819202122&gt;&gt;&gt; import thisThe Zen of Python, by Tim PetersBeautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.Special cases aren&apos;t special enough to break the rules.Although practicality beats purity.Errors should never pass silently.Unless explicitly silenced.In the face of ambiguity, refuse the temptation to guess.There should be one-- and preferably only one --obvious way to do it.Although that way may not be obvious at first unless you&apos;re Dutch.Now is better than never.Although never is often better than *right* now.If the implementation is hard to explain, it&apos;s a bad idea.If the implementation is easy to explain, it may be a good idea.Namespaces are one honking great idea -- let&apos;s do more of those! . . . 任何事物都有一个从创建，被使用，再到消亡的过程，在程序语言面向对象编程模型中，对象也有相似的命运：创建、初始化、使用、垃圾回收，不同的阶段由不同的方法（角色）负责执行。 定义一个类时，大家用得最多的就是 __init__ 方法，而 __new__ 和 __call__ 使用得比较少，这篇文章试图帮助大家把这 3 个方法的正确使用方式和应用场景分别解释一下。 关于 Python 新式类和老式类在这篇文章不做过多讨论，因为老式类是 Python2 中的概念，现在基本没人再会去用老式类，新式类必须显示地继承 object，而 Python3 中，只有新式类，默认继承了 object，无需显示指定，本文代码都是基于 Python3 来讨论。 __init__方法 __init__ 方法负责对象的初始化，系统执行该方法前，其实该对象已经存在了，要不然初始化什么东西呢？先看例子： 1234567891011121314# class A(object): python2 必须显示地继承objectclass A: def __init__(self): print("__init__ ") super(A, self).__init__() def __new__(cls): print("__new__ ") return super(A, cls).__new__(cls) def __call__(self): # 可以定义任意参数 print('__call__ ')A() 输出 12__new____init__ 从输出结果来看， __new__先被调用，返回一个实例对象，接着 __init__ 被调用。 __call__ 方法并没有被调用，这个我们放到最后说，先来说说前面两个方法，稍微改写成： 12345678910def __init__(self): print("__init__ ") print(self) super(A, self).__init__()def __new__(cls): print("__new__ ") self = super(A, cls).__new__(cls) print(self) return self 输出： 1234__new__ &lt;__main__.A object at 0x1007a95f8&gt;__init__ &lt;__main__.A object at 0x1007a95f8&gt; 从输出结果来看， __new__ 方法的返回值就是类的实例对象，这个实例对象会传递给 __init__ 方法中定义的 self 参数，以便实例对象可以被正确地初始化。 如果 __new__ 方法不返回值（或者说返回 None）那么 __init__ 将不会得到调用，这个也说得通，因为实例对象都没创建出来，调用 __init__ 也没什么意义，此外，Python 还规定， __init__ 只能返回 None 值，否则报错，这个留给大家去试。 __init__方法可以用来做一些初始化工作，比如给实例对象的状态进行初始化： 1234def __init__(self, a, b): self.a = a self.b = b super(A, self).__init__() __new__方法 一般我们不会去重写该方法，除非你确切知道怎么做，什么时候你会去关心它呢，它作为构造函数用于创建对象，是一个工厂函数，专用于生产实例对象。著名的设计模式之一，单例模式，就可以通过此方法来实现。在自己写框架级的代码时，可能你会用到它，我们也可以从开源代码中找到它的应用场景，例如微型 Web 框架 Bootle 就用到了。 123456class BaseController(object): _singleton = None def __new__(cls, *a, **k): if not cls._singleton: cls._singleton = object.__new__(cls, *a, **k) return cls._singleton 这段代码出自 https://github.com/bottlepy/bottle/blob/release-0.6/bottle.py 这就是通过 __new__ 方法是实现单例模式的的一种方式，如果实例对象存在了就直接返回该实例即可，如果还没有，那么就先创建一个实例，再返回。当然，实现单例模式的方法不只一种，Python 之禅有说： There should be one– and preferably only one –obvious way to do it. 用一种方法，最好是只有一种方法来做一件事 __call__ 方法 关于 __call__ 方法，不得不先提到一个概念，就是可调用对象（callable），我们平时自定义的函数、内置函数和类都属于可调用对象，但凡是可以把一对括号 () 应用到某个对象身上都可称之为可调用对象，判断对象是否为可调用对象可以用函数 callable 如果在类中实现了 __call__ 方法，那么实例对象也将成为一个可调用对象，我们回到最开始的那个例子： 12a = A()print(callable(a)) # True a 是实例对象，同时还是可调用对象，那么我就可以像函数一样调用它。试试： 1a() # __call__ 很神奇不是，实例对象也可以像函数一样作为可调用对象来用，那么，这个特点在什么场景用得上呢？这个要结合类的特性来说，类可以记录数据（属性），而函数不行（闭包某种意义上也可行），利用这种特性可以实现基于类的装饰器，在类里面记录状态，比如，下面这个例子用于记录函数被调用的次数： 1234567891011121314151617class Counter: def __init__(self, func): self.func = func self.count = 0 def __call__(self, *args, **kwargs): self.count += 1 return self.func(*args, **kwargs)@Counterdef foo(): passfor i in range(10): foo()print(foo.count) # 10 在 Bottle 中也有 __call__ 方法 的使用案例，另外，stackoverflow 也有一些关于 __call__ 的实践例子，推荐看看，如果你的项目中，需要更加抽象化、框架代码，那么这些高级特性往往能发挥出它作用。 typetype有一种完全不同的能力，它也能动态的创建类。type可以接受一个类的描述作为参数，然后返回一个类。（我知道，根据传入参数的不同，同一个函数拥有两种完全不同的用法是一件很傻的事情，但这在Python中是为了保持向后兼容性） type可以像这样工作：1type(类名, 父类的元组（针对继承的情况，可以为空），包含属性的字典（名称和值）) 比如下面的代码：123456789&gt;&gt;&gt; class MyShinyClass(object):… pass 可以手动像这样创建：&gt;&gt;&gt; MyShinyClass = type(&apos;MyShinyClass&apos;, (), &#123;&#125;) # 返回一个类对象&gt;&gt;&gt; print MyShinyClass&lt;class &apos;__main__.MyShinyClass&apos;&gt;&gt;&gt;&gt; print MyShinyClass() # 创建一个该类的实例&lt;__main__.MyShinyClass object at 0x8997cec&gt; 元类除了使用type()动态创建类以外，要控制类的创建行为，还可以使用metaclass。 metaclass，直译为元类，简单的解释就是： 当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。 但是如果我们想创建出类呢？那就必须根据metaclass创建出类，所以：先定义metaclass，然后创建类。 连接起来就是：先定义metaclass，就可以创建类，最后创建实例。 所以，metaclass允许你创建类或者修改类。换句话说，你可以把类看成是metaclass创建出来的“实例”。 基础知识 类由type创建，创建类时，type的 __init__ 方法自动执行，类() 执行type的 __call__ 方法(类的 __new__ 方法,类的 __init__ 方法) 对象由类创建，创建对象时，类的 __init__ 方法自动执行，对象()执行类的 __call__ 方法 例子1123456789101112class Foo: def __init__(self): pass def __call__(self, *args, **kwargs): pass# 执行type的 __call__ 方法，调用 Foo类（是type的对象）的 `__new__`，用于创建对象，然后调用 Foo类（是type的对象）的 `__init__`方法，用于对对象初始化。obj = Foo()# 执行Foo的 __call__ 方法 obj() 例子2我们再看一个简单的例子，这个metaclass可以给我们自定义的MyList增加一个add方法： 定义ListMetaclass，按照默认习惯，metaclass的类名总是以Metaclass结尾，以便清楚地表示这是一个metaclass： 12345# metaclass是类的模板，所以必须从`type`类型派生：class ListMetaclass(type): def __new__(cls, name, bases, attrs): attrs['add'] = lambda self, value: self.append(value) return type.__new__(cls, name, bases, attrs) 有了ListMetaclass，我们在定义类的时候还要指示使用ListMetaclass来定制类，传入关键字参数metaclass：12class MyList(list, metaclass=ListMetaclass): pass 当我们传入关键字参数metaclass时，魔术就生效了，它指示Python解释器在创建MyList时，要通过ListMetaclass.__new__()来创建，在此，我们可以修改类的定义，比如，加上新的方法，然后，返回修改后的定义。 __new__()方法接收到的参数依次是： 当前准备创建的类的对象； 类的名字； 类继承的父类集合； 类的方法集合。 测试一下MyList是否可以调用add()方法：1234&gt;&gt;&gt; L = MyList()&gt;&gt;&gt; L.add(1)&gt;&gt; L[1] 而普通的list没有add()方法：12345&gt;&gt;&gt; L2 = list()&gt;&gt;&gt; L2.add(1)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &apos;list&apos; object has no attribute &apos;add&apos; 元类的使用-单例Python做了如下的操作： Foo中有__metaclass__这个属性吗？如果是，Python会在内存中通过metaclass创建一个名字为Foo的类对象（我说的是类对象，请紧跟我的思路）。如果Python没有找到__metaclass__，它会继续在Bar（父类）中寻找__metaclass__属性，并尝试做和前面同样的操作。如果Python在任何父类中都找不到__metaclass__，它就会在模块层次中去寻找__metaclass__，并尝试做同样的操作。如果还是找不到__metaclass__,Python就会用内置的type来创建这个类对象。 现在的问题就是，你可以在__metaclass__中放置些什么代码呢？答案就是：可以创建一个类的东西。那么什么可以用来创建一个类呢？type，或者任何使用到type或者子类化type的东东都可以。 12345678910111213141516171819202122232425class SingletonType(type): _instance = None def __call__(cls, *args, **kwargs): if cls._instance is None: cls._instance = type.__call__(cls, *args, **kwargs) # 或者也可以写成下面这样: # cls._instance = super(SingletonType, cls).__call__(*args, **kwargs) return cls._instanceclass Foo(object): # 指定创建Foo的type为SingletonType __metaclass__ = SingletonType def __init__(self,name): self.name = nameobjxx = Foo('xx')print objxx.nameobjtt = Foo('tt')print objtt.nameprint(id(objxx) == id(objtt))print objxx.name 元类就是用来创建类的“东西”。你创建类就是为了创建类的实例对象，不是吗？但是我们已经学习到了Python中的类也是对象。好吧，元类就是用来创建这些类（对象）的，元类就是类的类，你可以这样理解 为：12MyClass = MetaClass()MyObject = MyClass() 经过上述基础知识我们知道:因为我们没有重新定义SingletonType的__new__, 所以Foo类本身还是type直接生成, 然后 那这里Foo(‘xx’) 和 Foo(‘tt’)其实执行SingletonType的__call__方法里的type的 __call__ 方法，调用 Foo类（是type的对象）的 __new__方法，用于创建obj对象，然后调用 Foo类（是type的对象）的 __init__方法，用于对obj对象初始化。]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows共享教程]]></title>
    <url>%2F2020%2F05%2F06%2Fwin_share_not_found_issue%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[曾在使用Win0系统进行数据交换过程中发现，网上邻居上怎么找都找不着其他电脑，为何呢？也是用了很多办法，一次次失败后最后终于解决了，原来在很多优化/安全软件“整理”过的电脑中，有他们认为危险但实际非常重要的设置被强行关闭了。接下来我们看看怎么处理。 先参考下面几张图设置一波 . . . 然后, 右键点击“此电脑”，选择“属性”，并依次进入：高级电脑设置 - 计算机名，确保局域网内所有计算机的工作组名称一致。如不同，需要点击“网络ID”来进行修改。 右键点击“此电脑”，选择“管理”，并依次进入：服务和应用程序 - 服务，确保 TCP/IP NetBIOS Helper 以及 Computer Browser 服务处于正在运行状态。如显示为已停止，点击启动即可。若发现服务项中竟然没有 Computer Browser ，此时需要向系统中添加 SMB 功能。 进入：控制面板 - 程序 - 启用或关闭系统功能，选中 SMB 1.0/CIFS 文件共享支持，并点击确定。不出意外的话会提示需要重启生效，在重启后局域网共享即可恢复正常。 这样设置后，其他电脑应该就能看到这台共享电脑了。要是还不行的话，就需要进一步打开共享服务了。按Win+R后输入services.msc。在右侧找到“Function Discovery Resource Publication”并双击，点击“启动”]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内省排序]]></title>
    <url>%2F2020%2F03%2F23%2Fintrosort%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[基于比较的排序算法复杂度的理论下界为 O(nlog n)，同时指出了： 每一次判定 a &lt; b ，都相当于回答了一次「是否问题」。按照已有的知识，若要尽可能快地完成排序，就要让每一次大小判断的结果落在两种答案之一的概率接近；若不然，则这次比较带来的信息量较小，也就需要更多次的比较来完成排序。 此篇建立在这些知识的基础上，首先探讨以下三个问题，而后引出号称「在所有情况下，都能较快完成排序任务的内省式排序（Introspective Sort）」： 为什么堆排序一般快不过快速排序？ 快速排序快得无懈可击吗？ 插入排序什么时候快？ 为什么在平均情况下快速排序比堆排序要优秀堆排序是渐进最优的比较排序算法，达到了O(nlgn)这一下界，而快排有一定的可能性会产生最坏划分，时间复杂度可能为O(n^2)，那为什么快排在实际使用中通常优于堆排序？ 虽然quick_sort会n^2（其实有稳定的nlgn的版本），但这毕竟很少出现。heap_sort大多数情况下比较次数都多于quick_sort，尽管大家都是nlgn。那就让倒霉蛋倒霉好了，大多数情况下快才是硬道理。 堆排比较的几乎都不是相邻元素，对cache极不友好，这才是很少被采用的原因。数学上的时间复杂度不代表实际运行时的情况.快排是分而治之，每次都在同一小段进行比较，最后越来约接近局部性。反观堆排，堆化过程中需要一直拿index的当前元素A和处于index*2 + 1 的子元素B比较, 两个元素距离较远。(局部性原理是指CPU访问存储器时，无论是存取指令还是存取数据，所访问的存储单元都趋于聚集在一个较小的连续区域中。) 在快排的迭代过程中，我们所处理的 [比基准大的数]，[比基准小的数] 序列中，在进行两个数之间大小比较时，在该局部范围内，产生“大于”或者“小于”的可能性是一样的。这意味着每比较一次必然会产生一次有意义的比较结果，会缩减接下来迭代的扫描工作量。 我们再来看看堆排序。在每一次进行重新堆调整的时候，我们在迭代时其实就已经知道，上一层的结点值一定是比下面大的。为了打乱堆结构把最后一个元素与顶堆互换时，此时我们也已经知道，互换后的元素是一定比下一层的数要小的。而在迭代时为了调整堆我们还是要进行一次已经知道结果的比较，这无疑是没有什么价值的，也就是产生了一次没有意义的比较，对接下来的迭代工作量并没有任何进展。 . . . 回顾三种排序算法堆排序慢在哪首先回顾一下堆排序的大致流程： 在未排序部分建堆——一棵二叉树，父节点总是比子节点大； 将堆顶元素与最后一个未排序元素兑换； 回到 1，直至排序完成。 这里需要注意，在用数组实现的堆当中，父节点 i 的左右子节点的位置分别是 2i + 1 与 2i + 2, 按照前述的原则，不难发现，堆底元素几乎是必然要小于堆顶元素的两个子节点元素。因此，在重新建堆时，原本的堆底元素与上述两个元素的比较 a &lt; b 成立的概率几乎为 0。这就意味着，在堆排序中，存在诸多类似这样「不平衡」的判断，而这些判断带来的信息量很小，因此需要额外的比较次数来提供足够的信息量。 这就是堆排序不够快的原因。具体来说，尽管在平均情况下，堆排序的时间复杂度与快速排序都是 O(nlog n)，但是它时间复杂度的常数项要比快速排序大不少。不过，由于堆排序所需的比较次数是恒定的，所以它在最坏的情况下，复杂度也是 O(nlog n)。这算是堆排序的一个优点。 快速排序也没有快得无懈可击快速排序的核心是选取主元（pivot），而后将小于主元的元素置于左边以及大于等于主元的元素置于右边，而后递归这个过程。 现在我们来看元素 a 的比较过程。在全部 n! 种排列中，满足 a &lt; pivot 的排列有一半，不满足的也有一半。因此这次比较干掉了一半的可能性，nice shot！ 不失一般性，现在假定 a &lt; pivot 成立，我们来看元素 b 的比较过程。在剩下的 frac{n!}{2} 种排列中，满足 b &lt; a &lt; pivot 、 a &lt; b &lt; pivot 和 a &lt; pivot &lt; b 的各占三分之一。这也就是说，若是 b &lt; pivot ，则这一次判断只能排除剩下的三分之一的可能性。这次比较的效果，就不那么令人满意了。 继续下去，则每次比较所能获得的信息量会逐渐下降，距离最优的情形越来越远。特别地，若是 pivot 是序列中最大或最小的元素，则这一次分割没有排除任何可能性——完全是白费功夫。这就是为什么说快排也不是快得无懈可击，以及这就是为什么说 pivot 选择最值时是快速排序的最坏情况。 插入排序在几乎排好序的序列上很快插入排序某种意义上是最生动的排序算法了。在玩扑克牌的时候，大多数人都会使用插入排序的办法，将分派到自己的扑克牌按顺序整理好。 对于一个几乎已经排好序的序列（逆序对很少），使用堆排或快排仍然能达到 O(nlog n) 的时间复杂度。但是插入排序在这种情况下，只需要从头到尾扫描一遍，交换、移动少数元素即可；时间复杂度近乎 O(n)。究其原因，堆排或快排按照各自的要求，将已经近似排好序的序列打乱，而后又排序整理，没有用到「几乎已经排好序」的先验知识，所以在这种情况下不如插入排序快就是自然的了。 内省式排序（Introspective Sort） 回顾上一节的内容，我们发现： 快速排序在大多数情况下效率最高，应当是首选的排序算法。但是它在某些情况下，会掉入陷阱，复杂度恶化到 O(n^2)。 堆排序虽然在大多数情况下不如快速排序效率高，但在所有的情况下复杂度都是 O(nlog n)。因此若能检测到快速排序掉入陷阱，则堆排序会是一个很好的补充。 插入排序虽然复杂度虽然只能达到 O(n^2)，但若能已知「几乎已经排好序」，切换到插入排序的效率又要比快速排序和堆排序高出不少，能做到 O(n)。 显然，三种排序各有优点也各有缺点。若能将它们的优点组合起来，同时避免它们各自的缺点，形成内省式排序，那就能做到在所有情况下都能以较快的速度完成排序任务了。 不难归纳，这样的内省式排序，策略应该如下： 在数据量足够大的情况使用快速排序； 在快速排序掉入陷阱时，主动切换到堆排序； 在快速排序和堆排序已经做到基本有序的情况下，或者数据量较小的情况下，主动切换到插入排序。 于是，问题就变成了，如何定义数据量足够大或者说基本有序，以及如何确定快速排序掉入了陷阱，而对效率没有伤害。现在我们来解决这些问题，从而完善整个内省式排序。 数据量足够大或者基本有序是什么意思？一般来说，当递归调用带来的开销大于递归调用后实际操作的开销时，调用快排、堆排就不太恰当了。因此，如果存在一个阈值，当待排序元素的数量小于该阈值时递归调用的开销相对较大，则该阈值的大小应当取决于机器硬件的特性（位宽、cache 性能）和待排序元素本身的特性（体积、是否对缓存友好）。 这一阈值某种意义上可以算作是算法的「超参数」，它不会在算法执行时带来额外的开销。 如何确定快速排序掉入了陷阱？通过上文的分析，我们知道，快速排序的效率主要取决于 pivot 的选择。若 pivot 恰好是待分割区间内的最大值或最小值，则这种分割没有排除任何可能的排序，因而是白费力气。既然如此，那么最平凡的方式，就是去检查所选的 pivot 是否为待分割区间内的最值即可判定快速排序是否掉入了陷阱。 然而，判定区间最值的问题，不可避免地要遍历区间内的所有元素。这也就是说，我们为了避免快速排序掉入陷阱，而使得复杂度从 O(nlog n) 恶化到 O(n^2)，我们在每一次递归中，都要遍历一次所有元素。这相当于额外增加了 O(nlog n) 的遍历操作。诚然，整个算法还是 O (nlog n) 的复杂度，但是无疑增加了常数倍数。考虑到我们的指导原则之一就是尽可能在大多数情况下，避免常数高的堆排序；主动去推高时间常数的做法是不可取的。 行文至此，我们又需要转换一次看待问题的角度。正如数学中有「正难则反」的说法。 我们来回顾一下计算机上的「杀毒软件」。早期的计算机病毒，更新速度较慢；但计算机「小白」太多，所以病毒的威力还是很大的。这时候的杀毒软件，会对病毒样本进行脱壳、反编译分析等操作，获取病毒的特征代码，而后加入特征库中。杀毒软件将更新的特征库分发给用户后，用户的杀毒软件就有能力查杀新的病毒了。这种方式的优点是精准，不易误杀；但是缺点也很明显——滞后性。 为了解决这个问题，后来反病毒工程师就想了一个办法。我们说，判断一个事物可以有两种思路。一个是判断其本质特征，例如使用特征码判断病毒；二个是观察其行为特性。对于病毒来说，它总是要潜伏下来搞破坏的，所以必然有某些行为特征。杀毒软件可以利用这些行为特征来判断一个可执行文件是否是计算机病毒。而这件事情是可以不依赖中心服务器，交由杀毒软件客户端自己处理的。这就解决了传统杀毒软件滞后性的问题。 杀毒软件的例子应当给予我们有一些启发。既然正面处理问题有困难，那就反过来，看看快速排序掉入陷阱会有什么行为特征。这似乎也不难，是显而易见的。快速排序调入陷阱，意味着在递归时快速排序算法会连续多次选中带分割区间的最值元素，从而导致多次「无效」分割，进而导致递归层数快速增加。因此，我们可以设置一个阈值；一旦递归深度超出该阈值，则认为快速排序掉入陷阱了并切换到堆排序算法。 快速排序在理想状态下，应当递归约 log n 次。因此，我们可以说，如果递归深度明显大于 log n，快速排序就掉进陷阱了。于是，我们可以将该阈值设置为 log n 的某一倍数，比如 2log n；一旦递归深度超过 2log n，就从快速排序切换到堆排序。]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>Algo</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模板能放cpp文件里吗之为什么]]></title>
    <url>%2F2020%2F03%2F21%2Ftemplate_in_cpp_file%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[《C++ 编程思想》第 15 章 (第 300 页) 说明了原因： 模板定义很特殊。由 template&lt;…&gt; 处理的任何东西都意味着编译器在当时不为它分配存储空间，它一直处于等待状态直到被一个模板实例告知。在编译器和连接器的某一处， 有一机制能去掉指定模板的多重定义。所以为了容易使用，几乎总是在头文件中放置全部的模板声明和定义。 上面这段话的意思就是说，只有模板实例化时，编译器才会得知 T 实参是什么。编译器在处理模板实例化时，不仅仅要看到模板的定义式，还需要模版的实现体。 . . . 例 1（声明和定义分离，错误示范）1234567891011// CTest.htemplate &lt;class T&gt;class CTest&#123;public: T&amp; get_value(); void set_value(const T&amp; _value);protected: T m_value;&#125;; 1234567891011// CTest.cpp#include "CTest.h"template &lt;class T&gt;T&amp; CTest&lt;T&gt;::get_value()&#123; return m_value;&#125; template &lt;class T&gt;void CTest&lt;T&gt;::set_value(const T&amp; _value)&#123; m_value = _value;&#125; 1234567// main.cpp#include "CTest.h"int main () &#123; CTest&lt;int&gt; t; t.set_value(2); return 0;&#125; 运行g++ -o main main.cpp CTest.h CTest.cpp , 结果如下: 12345ruiy@ruiy-All-Series:~/store/test/test_tpl$ g++ -o main main.cpp CTest.h CTest.cpp /tmp/ccb7QKY6.o：在函数‘main’中：main.cpp:(.text+0x2d)：对‘CTest&lt;int&gt;::set_value(int const&amp;)’未定义的引用collect2: error: ld returned 1 exit statusruiy@ruiy-All-Series:~/store/test/test_tpl$ 这里报错为 set_value 未定义的引用，编译器这里并不知道 CTest::set_value 的定义是什么，因为 CTest.h 中只有该函数的声明而没有其定义，编译器此时希望链接器能够在其他 obj 文件中找到该函数的定义，但是由 CTest.cpp 中并没有使用该函数，所有 CTest.obj 中也无法找到该函数的定义，因此就会报该链接错误。 对于模板来说，编译器在处理 CTest.cpp 文件时，编译器无法预知 T 的实参是什么，所以编译器对其实现是不作处理的（即 CTest.obj 中没有为编译器为实现体生成的对应的二进制代码）。 现在有 main.cpp 真正使用了该模板（比方说，生成模板类的一个对象，并调用其函数），如果定义和实现分离，则编译器可以根据定义式生成模板类的对象（因为此处仅仅需要定义式就知道该对象在内存中需要多少空间并进一步分配了），但是调用对象的函数（即真正使用）需要该函数的定义，由于 main.cpp 仅仅 include 了模板的声明（所以只能找到该函数的声明），所以无法找到该函数的定义，此时编译器会寄希望于链接器在其他 obj 文件（这里就是指 CTest.obj 文件）中寻找该模板的实现体，但是就像之前说过的，CTest.obj 中也没有实现体生成的二进制代码。如果定义和实现是在同一个文件（比如说 CTest.h）中，那么编译器在编译时就可以寻找到模板的实现体。 例 2（声明和定义不分离，正确的示范） 1234567891011121314151617181920// CTest.htemplate &lt;class T&gt;class CTest&#123;public: T&amp; get_value(); void set_value(const T&amp; _value);protected: T m_value;&#125;;template &lt;class T&gt;T&amp; CTest&lt;T&gt;::get_value()&#123; return m_value;&#125;template &lt;class T&gt;void CTest&lt;T&gt;::set_value(const T&amp; _value)&#123; m_value = _value;&#125; 1234567// main.cpp#include "CTest.h"int main () &#123; CTest&lt;int&gt; t; t.set_value(2); return 0;&#125; 此时执行 g++ -o main main.cpp CTest.h便不会再报错。 例 3（声明和定义分离，但是在定义所在文件中添加实例化声明） 1234567891011// CTest.htemplate &lt;class T&gt;class CTest&#123;public: T&amp; get_value(); void set_value(const T&amp; _value);protected: T m_value;&#125;; 123456789101112// CTest.cpp#include "CTest.h"template &lt;class T&gt;T&amp; CTest&lt;T&gt;::get_value()&#123; return m_value;&#125; template &lt;class T&gt;void CTest&lt;T&gt;::set_value(const T&amp; _value)&#123; m_value = _value;&#125;template class CTest&lt;int&gt;; // 实例化声明 1234567// main.cpp#include "CTest.h"int main () &#123; CTest&lt;int&gt; t; t.set_value(2); return 0;&#125; 此时执行 g++ -o main main.cpp CTest.h 便不会再报错。与例 1 的差别仅仅是在 test.cpp 后面添加了一行实例化声明，执行g++ -o main main.cpp CTest.h CTest.cpp 不报错。由于 CTest.cpp 中添加了实例化声明，因此编译器在编译 CTest.cpp 时，会在 CTest.obj 加入对应实现体的二进制代码（因为此时 T 已知）。所以在链接时，链接器可以找到该模板类的具体实现。 普通对象或者函数为啥可分离?对于普通对象或者函数而言，声明和实现可以分离到 *.h 和 *.cpp（比如说这里写做 CommonClass.h 和 CommomClass.cpp）中去，其中 CommonClass.cpp 会 include这个 CommonClass.h，因为编译器会根据 CommonClass.cpp 生成对应 CommonClass.obj，因此 obj 文件中会包含实现体对应的二进制代码。如果现有 main.cpp 使用了该类生成的对象，那么链接器可以在 CommonClass.obj 找到实现体对应的二进制代码。可是对于模板来说，并不是这样，先看下面这段话。 《C++ 编程思想》第 15 章 (第 300 页) 说明了原因： 模板定义很特殊。由 template&lt;…&gt; 处理的任何东西都意味着编译器在当时不为它分配存储空间，它一直处于等待状态直到被一个模板实例告知。在编译器和连接器的某一处， 有一机制能去掉指定模板的多重定义。所以为了容易使用，几乎总是在头文件中放置全部的模板声明和定义。 上面这段话的意思就是说，只有模板实例化时，编译器才会得知 T 实参是什么。编译器在处理模板实例化时，不仅仅要看到模板的定义式，还需要模版的实现体。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>noodle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python玩MsgPack和ProtoBuf]]></title>
    <url>%2F2020%2F03%2F14%2Fmsgpack_protobuf_intro%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[MsgPack for python3It’s like JSON.but fast and small.MessagePack is an efficient binary serialization format. It lets you exchange data among multiple languages like JSON. But it’s faster and smaller. Small integers are encoded into a single byte, and typical short strings require only one extra byte in addition to the strings themselves. msgpack 比 json 模块序列化速度更快，所得到的数据体积更小 It’s like JSON,but fast and small msgpack 用起来像 json，但是却比 json 快，并且序列化以后的数据长度更小，言外之意，使用 msgpack 不仅序列化和反序列化的速度快，数据传输量也比 json 格式小，msgpack 同样支持多种语言。 . . . 安装msgpack 可以使用 pip 安装，安装命令如下： 1pip install msgpack 使用对数据流进行反序列化msgpack 提供了一个 Unpacker 方法，可以对数据流进行反序列化，下面的代码改自官网的例子 123456789101112import msgpackfrom io import BytesIObuf = BytesIO()for i in range(100): buf.write(msgpack.packb(i, use_bin_type=True))buf.seek(0)unpacker = msgpack.Unpacker(buf, raw=False)for unpacked in unpacker: print(unpacked) 自定义类型数据的序列化msgpack 序列化函数提供了一个 default 参数，反序列化函数提供了一个 object_hook，其用法，与上一篇 json 中的 default 和 objec_thook 一样 12345678910111213141516171819202122import datetimeimport msgpackuseful_dict = &#123; "id": 1, "created": datetime.datetime.now(),&#125;def decode_datetime(obj): if b'__datetime__' in obj: obj = datetime.datetime.strptime(obj["as_str"], "%Y%m%dT%H:%M:%S.%f") return objdef encode_datetime(obj): if isinstance(obj, datetime.datetime): return &#123;'__datetime__': True, 'as_str': obj.strftime("%Y%m%dT%H:%M:%S.%f")&#125; return objpacked_dict = msgpack.packb(useful_dict, default=encode_datetime, use_bin_type=True)this_dict_again = msgpack.unpackb(packed_dict, object_hook=decode_datetime, raw=False)print(this_dict_again) 打印结果为:1&#123;&apos;id&apos;: 1, &apos;created&apos;: &#123;&apos;__datetime__&apos;: True, &apos;as_str&apos;: &apos;20210218T16:45:33.992339&apos;&#125;&#125; Extented类型12345678910111213141516171819202122import msgpackimport arraydef default(obj): if isinstance(obj, array.array) and obj.typecode == 'd': return msgpack.ExtType(42, obj.tostring()) raise TypeError("Unknown type: %r" % (obj,))def ext_hook(code, data): if code == 42: a = array.array('d') a.fromstring(data) return a return ExtType(code, data)data = array.array('d', [1.2, 3.4])packed = msgpack.packb(data, default=default, use_bin_type=True)unpacked = msgpack.unpackb(packed, ext_hook=ext_hook, raw=False)print(data == unpacked) # True ProtoBufprotobuf 是谷歌开源的一套序列化框架，基于二进制，速度快，体积小 protobuf protobuf 是 google 开源的一个序列化框架，基于二进制，因此相比于 XML,json 要高效，它支持多种语言，php,java,c++,python，谷歌自己的许多系统间消息的传递就是用的它。 安装 protobuf（1）git clone https://github.com/google/protobuf.git （2）cd protobuf/ （3）./autogen.sh 第三步可能会出现一些问题，打开 autogen.sh 文件，会看到下面一段内容 123456789curl $curlopts -L -O https://github.com/google/googlemock/archive/release-1.7.0.zipunzip -q release-1.7.0.ziprm release-1.7.0.zipmv googlemock-release-1.7.0 gmockcurl $curlopts -L -O https://github.com/google/googletest/archive/release-1.7.0.zipunzip -q release-1.7.0.ziprm release-1.7.0.zipmv googletest-release-1.7.0 gmock/gtest 这里要下载一个 gmock，有些朋友会在这一步上遇到障碍，这里一定要保证这两个文件下载成功，否则后续的安装无法成功 （4）make （5）make check （6）make install （7）export LD_LIBRARY_PATH=/usr/local/lib:/usr/lib:/usr/local/lib64:/usr/lib64 完成这 7 步，就安装成功了，安装过程需要一点耐心，速度快慢视机器性能而定 定义. proto 文件json，msgpack 在使用前都不需要定义数据格式，但 protobuf 需要，.proto 文件里定义的是可串行化的数据结构信息，新建一个名为 person.proto 的文件，内容为 12345678910111213141516171819syntax = &quot;proto2&quot;;message Person &#123; required string name=1; required int32 id=2; optional string email=3; enum PhoneType &#123; MOBILE=0; HOME=1; WORK=2; &#125; message PhoneNumber &#123; required string number=1; optional PhoneType type=2 [default=HOME]; &#125; repeated PhoneNumber phone=4;&#125; 消息定义遵照固定的语法格式，字段定义格式如下 限定修饰符① | 数据类型② | 字段名称③ | = | 字段编码值④ | [字段默认值⑤] （1）修饰符有 3 种，required 表示必须该字段必须赋值，optional 表示可选，允许不赋值，repeated 表示重复，相当于数组的意思 （2）数据类型，下图是数据类型表和与各种语言之间的对比关系 （3）字段名称 （4）字段编码值，从 1 开始，逐个递增 （5）字段默认值 编译. proto 文件(1) 编译. proto 文件 在 person.proto 文件所在的目录里执行命令 protoc -I=. –python_out=. person.proto 命令执行后，在同目录下会生成一个名为 person_pb2.py 的文件，这个就是我们想要的东西 安装 python 包pip install protobuf 序列化和反序列化新建一个 python 脚本，内容如下 12345678910111213141516171819202122232425262728293031323334from person_pb2 import Personperson = Person()person.name = 'sheng'person.id = 1person.email = '1123@163.com'phone = person.phone.add()phone.number = '88888'phone.type = Person.WORKprint person.nameprint person.idprint person.emailfor p in person.phone: print p.number print p.typeprint u'序列化'proto_str = person.SerializeToString()print proto_strprint u'反序列化'person2 = Person()person2.ParseFromString(proto_str)print person.nameprint person.idprint person.emailfor p in person.phone: print p.number print p.type 执行一下脚本看看效果吧 个人愚见，小的系统最好不要用 protobuf，各个接口间消息的传递皆需要先定义. proto 文件，如果有所修改，就需要重新编译，不仅如此，不同系统间需要同时持有一份编译生成的_pb2.py 文件，这样导致工作很繁琐，小系统，直接使用 json 或者 msgpack 就好了 但如果是大系统，需要对接口进行明确规范的情况，使用 protbuf 却是非常合适，除去性能不谈，单单是使用统一的_pb2.py 文件，就能让工作变得条理清晰，避免个别工程师心花怒放随意定义数据格式]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>MsgPack</tag>
        <tag>ProtoBuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd的从应用场景到实现原理的全方位解读]]></title>
    <url>%2F2020%2F03%2F02%2Fetcd_intro%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[. . . 随着 CoreOS 和 Kubernetes 等项目在开源社区日益火热，它们项目中都用到的 etcd 组件作为一个高可用强一致性的服务发现存储仓库，渐渐为开发人员所关注。在云计算时代，如何让服务快速透明地接入到计算集群中，如何让共享配置信息快速被集群中的所有机器发现，更为重要的是，如何构建这样一套高可用、安全、易于部署以及响应快速的服务集群，已经成为了迫切需要解决的问题。etcd 为解决这类问题带来了福音，本文将从 etcd 的应用场景开始，深入解读 etcd 的实现方式，以供开发者们更为充分地享用 etcd 所带来的便利。 经典应用场景要问 etcd 是什么？很多人第一反应可能是一个键值存储仓库，却没有重视官方定义的后半句，用于配置共享和服务发现。 A highly-available key value store for shared configuration and service discovery. 实际上，etcd 作为一个受到 ZooKeeper 与 doozer 启发而催生的项目，除了拥有与之类似的功能外，更专注于以下四点。 简单：基于 HTTP+JSON 的 API 让你用 curl 就可以轻松使用。 安全：可选 SSL 客户认证机制。 快速：每个实例每秒支持一千次写操作。 可信：使用 Raft 算法充分实现了分布式。 随着云计算的不断发展，分布式系统中涉及到的问题越来越受到人们重视。受阿里中间件团队对 ZooKeeper 典型应用场景一览一文的启发，笔者根据自己的理解也总结了一些 etcd 的经典使用场景。让我们来看看 etcd 这个基于 Raft 强一致性算法的分布式存储仓库能给我们带来哪些帮助。 值得注意的是，分布式系统中的数据分为控制数据和应用数据。使用 etcd 的场景默认处理的数据都是控制数据，对于应用数据，只推荐数据量很小，但是更新访问频繁的情况。 场景一：服务发现（Service Discovery）服务发现要解决的也是分布式系统中最常见的问题之一，即在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发现就是想要了解集群中是否有进程在监听 udp 或 tcp 端口，并且通过名字就可以查找和连接。要解决服务发现的问题，需要有下面三大支柱，缺一不可。 一个强一致性、高可用的服务存储目录。基于 Raft 算法的 etcd 天生就是这样一个强一致性高可用的服务存储目录。 一种注册服务和监控服务健康状态的机制。用户可以在 etcd 中注册服务，并且对注册的服务设置key TTL，定时保持服务的心跳以达到监控健康状态的效果。 一种查找和连接服务的机制。通过在 etcd 指定的主题下注册的服务也能在对应的主题下查找到。为了确保连接，我们可以在每个服务机器上都部署一个 Proxy 模式的 etcd，这样就可以确保能访问 etcd 集群的服务都能互相连接。 图 1 服务发现示意图 下面我们来看服务发现对应的具体场景。 微服务协同工作架构中，服务动态添加。随着 Docker 容器的流行，多种微服务共同协作，构成一个相对功能强大的架构的案例越来越多。透明化的动态添加这些服务的需求也日益强烈。通过服务发现机制，在 etcd 中注册某个服务名字的目录，在该目录下存储可用的服务节点的 IP。在使用服务的过程中，只要从服务目录下查找可用的服务节点去使用即可。 图 2 微服务协同工作 PaaS 平台中应用多实例与实例故障重启透明化。PaaS 平台中的应用一般都有多个实例，通过域名，不仅可以透明的对这多个实例进行访问，而且还可以做到负载均衡。但是应用的某个实例随时都有可能故障重启，这时就需要动态的配置域名解析（路由）中的信息。通过 etcd 的服务发现功能就可以轻松解决这个动态配置的问题。 图 3 云平台多实例透明化 场景二：消息发布与订阅在分布式系统中，最适用的一种组件间通信方式就是消息发布与订阅。即构建一个配置共享中心，数据提供者在这个配置中心发布消息，而消息使用者则订阅他们关心的主题，一旦主题有消息发布，就会实时通知订阅者。通过这种方式可以做到分布式系统配置的集中式管理与动态更新。 应用中用到的一些配置信息放到 etcd 上进行集中管理。这类场景的使用方式通常是这样：应用在启动的时候主动从 etcd 获取一次配置信息，同时，在 etcd 节点上注册一个 Watcher 并等待，以后每次配置有更新的时候，etcd 都会实时通知订阅者，以此达到获取最新配置信息的目的。 分布式搜索服务中，索引的元信息和服务器集群机器的节点状态存放在 etcd 中，供各个客户端订阅使用。使用 etcd 的key TTL功能可以确保机器状态是实时更新的。 分布式日志收集系统。这个系统的核心工作是收集分布在不同机器的日志。收集器通常是按照应用（或主题）来分配收集任务单元，因此可以在 etcd 上创建一个以应用（主题）命名的目录 P，并将这个应用（主题相关）的所有机器 ip，以子目录的形式存储到目录 P 上，然后设置一个 etcd 递归的 Watcher，递归式的监控应用（主题）目录下所有信息的变动。这样就实现了机器 IP（消息）变动的时候，能够实时通知到收集器调整任务分配。 系统中信息需要动态自动获取与人工干预修改信息请求内容的情况。通常是暴露出接口，例如 JMX 接口，来获取一些运行时的信息。引入 etcd 之后，就不用自己实现一套方案了，只要将这些信息存放到指定的 etcd 目录中即可，etcd 的这些目录就可以通过 HTTP 的接口在外部访问。 图 4 消息发布与订阅 场景三：负载均衡在场景一中也提到了负载均衡，本文所指的负载均衡均为软负载均衡。分布式系统中，为了保证服务的高可用以及数据的一致性，通常都会把数据和服务部署多份，以此达到对等服务，即使其中的某一个服务失效了，也不影响使用。由此带来的坏处是数据写入性能下降，而好处则是数据访问时的负载均衡。因为每个对等服务节点上都存有完整的数据，所以用户的访问流量就可以分流到不同的机器上。 etcd 本身分布式架构存储的信息访问支持负载均衡。etcd 集群化以后，每个 etcd 的核心节点都可以处理用户的请求。所以，把数据量小但是访问频繁的消息数据直接存储到 etcd 中也是个不错的选择，如业务系统中常用的二级代码表（在表中存储代码，在 etcd 中存储代码所代表的具体含义，业务系统调用查表的过程，就需要查找表中代码的含义）。 利用 etcd 维护一个负载均衡节点表。etcd 可以监控一个集群中多个节点的状态，当有一个请求发过来后，可以轮询式的把请求转发给存活着的多个状态。类似 KafkaMQ，通过 ZooKeeper 来维护生产者和消费者的负载均衡。同样也可以用 etcd 来做 ZooKeeper 的工作。 图 5 负载均衡 场景四：分布式通知与协调这里说到的分布式通知与协调，与消息发布和订阅有些相似。都用到了 etcd 中的 Watcher 机制，通过注册与异步通知机制，实现分布式环境下不同系统之间的通知与协调，从而对数据变更做到实时处理。实现方式通常是这样：不同系统都在 etcd 上对同一个目录进行注册，同时设置 Watcher 观测该目录的变化（如果对子目录的变化也有需要，可以设置递归模式），当某个系统更新了 etcd 的目录，那么设置了 Watcher 的系统就会收到通知，并作出相应处理。 通过 etcd 进行低耦合的心跳检测。检测系统和被检测系统通过 etcd 上某个目录关联而非直接关联起来，这样可以大大减少系统的耦合性。 通过 etcd 完成系统调度。某系统有控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改了 etcd 上某些目录节点的状态，而 etcd 就把这些变化通知给注册了 Watcher 的推送系统客户端，推送系统再作出相应的推送任务。 通过 etcd 完成工作汇报。大部分类似的任务分发系统，子任务启动后，到 etcd 来注册一个临时工作目录，并且定时将自己的进度进行汇报（将进度写入到这个临时目录），这样任务管理者就能够实时知道任务进度。 图 6 分布式协同工作 场景五：分布式锁因为 etcd 使用 Raft 算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。 保持独占即所有获取锁的用户最终只有一个可以得到。etcd 为此提供了一套实现分布式锁原子操作 CAS（CompareAndSwap）的 API。通过设置prevExist值，可以保证在多个节点同时去创建某个目录时，只有一个成功。而创建成功的用户就可以认为是获得了锁。 控制时序，即所有想要获得锁的用户都会被安排执行，但是获得锁的顺序也是全局唯一的，同时决定了执行顺序。etcd 为此也提供了一套 API（自动创建有序键），对一个目录建值时指定为POST动作，这样 etcd 会自动在目录下生成一个当前最大的值为键，存储这个新的值（客户端编号）。同时还可以使用 API 按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可以是代表客户端的编号。 图 7 分布式锁 场景六：分布式队列分布式队列的常规用法与场景五中所描述的分布式锁的控制时序用法类似，即创建一个先进先出的队列，保证顺序。 另一种比较有意思的实现是在保证队列达到某个条件时再统一按顺序执行。这种方法的实现可以在 /queue 这个目录中另外建立一个 /queue/condition 节点。 condition 可以表示队列大小。比如一个大的任务需要很多小任务就绪的情况下才能执行，每次有一个小任务就绪，就给这个 condition 数字加 1，直到达到大任务规定的数字，再开始执行队列里的一系列小任务，最终执行大任务。 condition 可以表示某个任务在不在队列。这个任务可以是所有排序任务的首个执行程序，也可以是拓扑结构中没有依赖的点。通常，必须执行这些任务后才能执行队列中的其他任务。 condition 还可以表示其它的一类开始执行任务的通知。可以由控制程序指定，当 condition 出现变化时，开始执行队列任务。 图 8 分布式队列 场景七：集群监控与 Leader 竞选通过 etcd 来进行监控实现起来非常简单并且实时性强。 前面几个场景已经提到 Watcher 机制，当某个节点消失或有变动时，Watcher 会第一时间发现并告知用户。 节点可以设置TTL key，比如每隔 30s 发送一次心跳使代表该机器存活的节点继续存在，否则节点消失。 这样就可以第一时间检测到各节点的健康状态，以完成集群的监控要求。 另外，使用分布式锁，可以完成 Leader 竞选。这种场景通常是一些长时间 CPU 计算或者使用 IO 操作的机器，只需要竞选出的 Leader 计算或处理一次，就可以把结果复制给其他的 Follower。从而避免重复劳动，节省计算资源。 这个的经典场景是搜索系统中建立全量索引。如果每个机器都进行一遍索引的建立，不但耗时而且建立索引的一致性不能保证。通过在 etcd 的 CAS 机制同时创建一个节点，创建成功的机器作为 Leader，进行索引计算，然后把计算结果分发到其它节点。 图 9 Leader 竞选 场景八：为什么用 etcd 而不用 ZooKeeper？阅读了 “ZooKeeper 典型应用场景一览” 一文的读者可能会发现，etcd 实现的这些功能， ZooKeeper 都能实现。那么为什么要用 etcd 而非直接使用 ZooKeeper 呢？ 相较之下，ZooKeeper 有如下缺点： 复杂。ZooKeeper 的部署维护复杂，管理员需要掌握一系列的知识和技能；而 Paxos 强一致性算法也是素来以复杂难懂而闻名于世；另外，ZooKeeper 的使用也比较复杂，需要安装客户端，官方只提供了 Java 和 C 两种语言的接口。 Java 编写。这里不是对 Java 有偏见，而是 Java 本身就偏向于重型应用，它会引入大量的依赖。而运维人员则普遍希望保持强一致、高可用的机器集群尽可能简单，维护起来也不易出错。 发展缓慢。Apache 基金会项目特有的 “Apache Way” 在开源界饱受争议，其中一大原因就是由于基金会庞大的结构以及松散的管理导致项目发展缓慢。 而 etcd 作为一个后起之秀，其优点也很明显。 简单。使用 Go 语言编写部署简单；使用 HTTP 作为接口使用简单；使用 Raft 算法保证强一致性让用户易于理解。 数据持久化。etcd 默认数据一更新就进行持久化。 安全。etcd 支持 SSL 客户端安全认证。 最后，etcd 作为一个年轻的项目，真正告诉迭代和开发中，这既是一个优点，也是一个缺点。优点是它的未来具有无限的可能性，缺点是无法得到大项目长时间使用的检验。然而，目前 CoreOS、Kubernetes 和 CloudFoundry 等知名项目均在生产环境中使用了 etcd，所以总的来说，etcd 值得你去尝试。 etcd 实现原理解读上一节中，我们概括了许多 etcd 的经典场景，这一节，我们将从 etcd 的架构开始，深入到源码中解析 etcd。 1 架构 图 10 etcd 架构图 从 etcd 的架构图中我们可以看到，etcd 主要分为四个部分。 HTTP Server： 用于处理用户发送的 API 请求以及其它 etcd 节点的同步与心跳信息请求。 Store：用于处理 etcd 支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是 etcd 对用户提供的大多数 API 功能的具体实现。 Raft：Raft 强一致性算法的具体实现，是 etcd 的核心。 WAL：Write Ahead Log（预写式日志），是 etcd 的数据存储方式。除了在内存中存有所有数据的状态以及节点的索引以外，etcd 就通过 WAL 进行持久化存储。WAL 中，所有的数据提交前都会事先记录日志。Snapshot 是为了防止数据过多而进行的状态快照；Entry 表示存储的具体日志内容。 通常，一个用户的请求发送过来，会经由 HTTP Server 转发给 Store 进行具体的事务处理，如果涉及到节点的修改，则交给 Raft 模块进行状态的变更、日志的记录，然后再同步给别的 etcd 节点以确认数据提交，最后进行数据的提交，再次同步。 2 新版 etcd 重要变更列表 获得了 IANA 认证的端口，2379 用于客户端通信，2380 用于节点通信，与原先的（4001 peers / 7001 clients）共用。 每个节点可监听多个广播地址。监听的地址由原来的一个扩展到多个，用户可以根据需求实现更加复杂的集群环境，如一个是公网 IP，一个是虚拟机（容器）之类的私有 IP。 etcd 可以代理访问 leader 节点的请求，所以如果你可以访问任何一个 etcd 节点，那么你就可以无视网络的拓扑结构对整个集群进行读写操作。 etcd 集群和集群中的节点都有了自己独特的 ID。这样就防止出现配置混淆，不是本集群的其他 etcd 节点发来的请求将被屏蔽。 etcd 集群启动时的配置信息目前变为完全固定，这样有助于用户正确配置和启动。 运行时节点变化 (Runtime Reconfiguration)。用户不需要重启 etcd 服务即可实现对 etcd 集群结构进行变更。启动后可以动态变更集群配置。 重新设计和实现了 Raft 算法，使得运行速度更快，更容易理解，包含更多测试代码。 Raft 日志现在是严格的只能向后追加、预写式日志系统，并且在每条记录中都加入了 CRC 校验码。 启动时使用的 _etcd/* 关键字不再暴露给用户 废弃集群自动调整功能的 standby 模式，这个功能使得用户维护集群更困难。 新增 Proxy 模式，不加入到 etcd 一致性集群中，纯粹进行代理转发。 ETCD_NAME（-name）参数目前是可选的，不再用于唯一标识一个节点。 摒弃通过配置文件配置 etcd 属性的方式，你可以用环境变量的方式代替。 通过自发现方式启动集群必须要提供集群大小，这样有助于用户确定集群实际启动的节点数量。 3 etcd 概念词汇表 Raft：etcd 所采用的保证分布式系统强一致性的算法。 Node：一个 Raft 状态机实例。 Member： 一个 etcd 实例。它管理着一个 Node，并且可以为客户端请求提供服务。 Cluster：由多个 Member 构成可以协同工作的 etcd 集群。 Peer：对同一个 etcd 集群中另外一个 Member 的称呼。 Client： 向 etcd 集群发送 HTTP 请求的客户端。 WAL：预写式日志，etcd 用于持久化存储的日志格式。 snapshot：etcd 防止 WAL 文件过多而设置的快照，存储 etcd 数据状态。 Proxy：etcd 的一种模式，为 etcd 集群提供反向代理服务。 Leader：Raft 算法中通过竞选而产生的处理所有数据提交的节点。 Follower：竞选失败的节点作为 Raft 中的从属节点，为算法提供强一致性保证。 Candidate：当 Follower 超过一定时间接收不到 Leader 的心跳时转变为 Candidate 开始竞选。 Term：某个节点成为 Leader 到下一次竞选时间，称为一个 Term。 Index：数据项编号。Raft 中通过 Term 和 Index 来定位数据。 4 集群化应用实践etcd 作为一个高可用键值存储系统，天生就是为集群化而设计的。由于 Raft 算法在做决策时需要多数节点的投票，所以 etcd 一般部署集群推荐奇数个节点，推荐的数量为 3、5 或者 7 个节点构成一个集群。 4.1 集群启动etcd 有三种集群化启动的配置方案，分别为静态配置启动、etcd 自身服务发现、通过 DNS 进行服务发现。 通过配置内容的不同，你可以对不同的方式进行选择。值得一提的是，这也是新版 etcd 区别于旧版的一大特性，它摒弃了使用配置文件进行参数配置的做法，转而使用命令行参数或者环境变量的做法来配置参数。 4.1.1. 静态配置这种方式比较适用于离线环境，在启动整个集群之前，你就已经预先清楚所要配置的集群大小，以及集群上各节点的地址和端口信息。那么启动时，你就可以通过配置initial-cluster参数进行 etcd 集群的启动。 在每个 etcd 机器启动时，配置环境变量或者添加启动参数的方式如下。 12ETCD_INITIAL_CLUSTER=&quot;infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380&quot;ETCD_INITIAL_CLUSTER_STATE=new 参数方法： 123-initial-cluster infra0=http://10.0.1.10:2380,http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \ -initial-cluster-state new 值得注意的是，-initial-cluster参数中配置的 url 地址必须与各个节点启动时设置的initial-advertise-peer-urls参数相同。（initial-advertise-peer-urls参数表示节点监听其他节点同步信号的地址） 如果你所在的网络环境配置了多个 etcd 集群，为了避免意外发生，最好使用-initial-cluster-token参数为每个集群单独配置一个 token 认证。这样就可以确保每个集群和集群的成员都拥有独特的 ID。 综上所述，如果你要配置包含 3 个 etcd 节点的集群，那么你在三个机器上的启动命令分别如下所示。 1234567891011121314151617$ etcd -name infra0 -initial-advertise-peer-urls http://10.0.1.10:2380 \ -listen-peer-urls http://10.0.1.10:2380 \ -initial-cluster-token etcd-cluster-1 \ -initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \ -initial-cluster-state new$ etcd -name infra1 -initial-advertise-peer-urls http://10.0.1.11:2380 \ -listen-peer-urls http://10.0.1.11:2380 \ -initial-cluster-token etcd-cluster-1 \ -initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \ -initial-cluster-state new$ etcd -name infra2 -initial-advertise-peer-urls http://10.0.1.12:2380 \ -listen-peer-urls http://10.0.1.12:2380 \ -initial-cluster-token etcd-cluster-1 \ -initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \ -initial-cluster-state new 在初始化完成后，etcd 还提供动态增、删、改 etcd 集群节点的功能，这个需要用到etcdctl命令进行操作。 4.1.2. etcd 自发现模式通过自发现的方式启动 etcd 集群需要事先准备一个 etcd 集群。如果你已经有一个 etcd 集群，首先你可以执行如下命令设定集群的大小，假设为 3. 1$ curl -X PUT http://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size -d value=3 然后你要把这个 url 地址http://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83作为-discovery参数来启动 etcd。节点会自动使用http://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83目录进行 etcd 的注册和发现服务。 所以最终你在某个机器上启动 etcd 的命令如下。 123$ etcd -name infra0 -initial-advertise-peer-urls http://10.0.1.10:2380 \ -listen-peer-urls http://10.0.1.10:2380 \ -discovery http://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83 如果你本地没有可用的 etcd 集群，etcd 官网提供了一个可以公网访问的 etcd 存储地址。你可以通过如下命令得到 etcd 服务的目录，并把它作为-discovery参数使用。 12$ curl http://discovery.etcd.io/new?size=3http://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de 同样的，当你完成了集群的初始化后，这些信息就失去了作用。当你需要增加节点时，需要使用etcdctl来进行操作。 为了安全，请务必每次启动新 etcd 集群时，都使用新的 discovery token 进行注册。另外，如果你初始化时启动的节点超过了指定的数量，多余的节点会自动转化为 Proxy 模式的 etcd。 4.1.3. DNS 自发现模式etcd 还支持使用 DNS SRV 记录进行启动。关于 DNS SRV 记录如何进行服务发现，可以参阅 RFC2782 ，所以，你要在 DNS 服务器上进行相应的配置。 (1) 开启 DNS 服务器上 SRV 记录查询，并添加相应的域名记录，使得查询到的结果类似如下。 1234$ dig +noall +answer SRV _etcd-server._tcp.example.com_etcd-server._tcp.example.com. 300 IN SRV 0 0 2380 infra0.example.com._etcd-server._tcp.example.com. 300 IN SRV 0 0 2380 infra1.example.com._etcd-server._tcp.example.com. 300 IN SRV 0 0 2380 infra2.example.com. (2) 分别为各个域名配置相关的 A 记录指向 etcd 核心节点对应的机器 IP。使得查询结果类似如下。 1234$ dig +noall +answer infra0.example.com infra1.example.com infra2.example.cominfra0.example.com. 300 IN A 10.0.1.10infra1.example.com. 300 IN A 10.0.1.11infra2.example.com. 300 IN A 10.0.1.12 做好了上述两步 DNS 的配置，就可以使用 DNS 启动 etcd 集群了。配置 DNS 解析的 url 参数为-discovery-srv，其中某一个节点地启动命令如下。 12345678$ etcd -name infra0 \-discovery-srv example.com \-initial-advertise-peer-urls http://infra0.example.com:2380 \-initial-cluster-token etcd-cluster-1 \-initial-cluster-state new \-advertise-client-urls http://infra0.example.com:2379 \-listen-client-urls http://infra0.example.com:2379 \-listen-peer-urls http://infra0.example.com:2380 当然，你也可以直接把节点的域名改成 IP 来启动。 4.2 关键部分源码解析etcd 的启动是从主目录下的main.go开始的，然后进入etcdmain/etcd.go，载入配置参数。如果被配置为 Proxy 模式，则进入 startProxy 函数，否则进入 startEtcd，开启 etcd 服务模块和 http 请求处理模块。 在启动 http 监听时，为了保持与集群其他 etcd 机器（peers）保持连接，都采用的transport.NewTimeoutListener启动方式，这样在超过指定时间没有获得响应时就会出现超时错误。而在监听 client 请求时，采用的是transport.NewKeepAliveListener，有助于连接的稳定。 在etcdmain/etcd.go中的 setupCluster 函数可以看到，根据不同 etcd 的参数，启动集群的方法略有不同，但是最终需要的就是一个 IP 与端口构成的字符串。 在静态配置的启动方式中，集群的所有信息都已经在给出，所以直接解析用逗号隔开的集群 url 信息就好了。 DNS 发现的方式类似，会预先发送一个 tcp 的 SRV 请求，先查看etcd-server-ssl._tcp.example.com下是否有集群的域名信息，如果没有找到，则去查看etcd-server._tcp.example.com。根据找到的域名，解析出对应的 IP 和端口，即集群的 url 信息。 较为复杂是 etcd 式的自发现启动。首先就用自身单个的 url 构成一个集群，然后在启动的过程中根据参数进入discovery/discovery.go源码的JoinCluster函数。因为我们事先是知道启动时使用的 etcd 的 token 地址的，里面包含了集群大小 (size) 信息。在这个过程其实是个不断监测与等待的过程。启动的第一步就是在这个 etcd 的 token 目录下注册自身的信息，然后再监测 token 目录下所有节点的数量，如果数量没有达标，则循环等待。当数量达到要求时，才结束，进入正常的启动过程。 配置 etcd 过程中通常要用到两种 url 地址容易混淆，一种用于 etcd 集群同步信息并保持连接，通常称为 peer-urls；另外一种用于接收用户端发来的 HTTP 请求，通常称为 client-urls。 peer-urls：通常监听的端口为2380（老版本使用的端口为7001），包括所有已经在集群中正常工作的所有节点的地址。 client-urls：通常监听的端口为2379（老版本使用的端口为4001），为适应复杂的网络环境，新版 etcd 监听客户端请求的 url 从原来的 1 个变为现在可配置的多个。这样 etcd 可以配合多块网卡同时监听不同网络下的请求。 4.3 运行时节点变更etcd 集群启动完毕后，可以在运行的过程中对集群进行重构，包括核心节点的增加、删除、迁移、替换等。运行时重构使得 etcd 集群无须重启即可改变集群的配置，这也是新版 etcd 区别于旧版包含的新特性。 只有当集群中多数节点正常的情况下，你才可以进行运行时的配置管理。因为配置更改的信息也会被 etcd 当成一个信息存储和同步，如果集群多数节点损坏，集群就失去了写入数据的能力。所以在配置 etcd 集群数量时，强烈推荐至少配置 3 个核心节点。 4.3.1. 节点迁移、替换当你节点所在的机器出现硬件故障，或者节点出现如数据目录损坏等问题，导致节点永久性的不可恢复时，就需要对节点进行迁移或者替换。当一个节点失效以后，必须尽快修复，因为 etcd 集群正常运行的必要条件是集群中多数节点都正常工作。 迁移一个节点需要进行四步操作： 暂停正在运行着的节点程序进程 把数据目录从现有机器拷贝到新机器 使用 api 更新 etcd 中对应节点指向机器的 url 记录更新为新机器的 ip 使用同样的配置项和数据目录，在新的机器上启动 etcd。 4.3.2. 节点增加增加节点可以让 etcd 的高可用性更强。举例来说，如果你有 3 个节点，那么最多允许 1 个节点失效；当你有 5 个节点时，就可以允许有 2 个节点失效。同时，增加节点还可以让 etcd 集群具有更好的读性能。因为 etcd 的节点都是实时同步的，每个节点上都存储了所有的信息，所以增加节点可以从整体上提升读的吞吐量。 增加一个节点需要进行两步操作： 在集群中添加这个节点的 url 记录，同时获得集群的信息。 使用获得的集群信息启动新 etcd 节点。 4.3.3. 节点移除有时你不得不在提高 etcd 的写性能和增加集群高可用性上进行权衡。Leader 节点在提交一个写记录时，会把这个消息同步到每个节点上，当得到多数节点的同意反馈后，才会真正写入数据。所以节点越多，写入性能越差。在节点过多时，你可能需要移除一个或多个。 移除节点非常简单，只需要一步操作，就是把集群中这个节点的记录删除。然后对应机器上的该节点就会自动停止。 4.3.4. 强制性重启集群当集群超过半数的节点都失效时，就需要通过手动的方式，强制性让某个节点以自己为 Leader，利用原有数据启动一个新集群。 此时你需要进行两步操作。 备份原有数据到新机器。 使用-force-new-cluster加备份的数据重新启动节点 注意：强制性重启是一个迫不得已的选择，它会破坏一致性协议保证的安全性（如果操作时集群中尚有其它节点在正常工作，就会出错），所以在操作前请务必要保存好数据。 5 Proxy 模式Proxy 模式也是新版 etcd 的一个重要变更，etcd 作为一个反向代理把客户的请求转发给可用的 etcd 集群。这样，你就可以在每一台机器都部署一个 Proxy 模式的 etcd 作为本地服务，如果这些 etcd Proxy 都能正常运行，那么你的服务发现必然是稳定可靠的。 图 11 Proxy 模式示意图 所以 Proxy 并不是直接加入到符合强一致性的 etcd 集群中，也同样的，Proxy 并没有增加集群的可靠性，当然也没有降低集群的写入性能。 5.1 Proxy 取代 Standby 模式的原因那么，为什么要有 Proxy 模式而不是直接增加 etcd 核心节点呢？实际上 etcd 每增加一个核心节点（peer），都会增加 Leader 节点一定程度的包括网络、CPU 和磁盘的负担，因为每次信息的变化都需要进行同步备份。增加 etcd 的核心节点可以让整个集群具有更高的可靠性，但是当数量达到一定程度以后，增加可靠性带来的好处就变得不那么明显，反倒是降低了集群写入同步的性能。因此，增加一个轻量级的 Proxy 模式 etcd 节点是对直接增加 etcd 核心节点的一个有效代替。 熟悉 0.4.6 这个旧版本 etcd 的用户会发现，Proxy 模式实际上是取代了原先的 Standby 模式。Standby 模式除了转发代理的功能以外，还会在核心节点因为故障导致数量不足的时候，从 Standby 模式转为正常节点模式。而当那个故障的节点恢复时，发现 etcd 的核心节点数量已经达到的预先设置的值，就会转为 Standby 模式。 但是新版 etcd 中，只会在最初启动 etcd 集群时，发现核心节点的数量已经满足要求时，自动启用 Proxy 模式，反之则并未实现。主要原因如下。 etcd 是用来保证高可用的组件，因此它所需要的系统资源（包括内存、硬盘和 CPU 等）都应该得到充分保障以保证高可用。任由集群的自动变换随意地改变核心节点，无法让机器保证性能。所以 etcd 官方鼓励大家在大型集群中为运行 etcd 准备专有机器集群。 因为 etcd 集群是支持高可用的，部分机器故障并不会导致功能失效。所以机器发生故障时，管理员有充分的时间对机器进行检查和修复。 自动转换使得 etcd 集群变得复杂，尤其是如今 etcd 支持多种网络环境的监听和交互。在不同网络间进行转换，更容易发生错误，导致集群不稳定。 基于上述原因，目前 Proxy 模式有转发代理功能，而不会进行角色转换。 5.2 关键部分源码解析从代码中可以看到，Proxy 模式的本质就是起一个 HTTP 代理服务器，把客户发到这个服务器的请求转发给别的 etcd 节点。 etcd 目前支持读写皆可和只读两种模式。默认情况下是读写皆可，就是把读、写两种请求都进行转发。而只读模式只转发读的请求，对所有其他请求返回 501 错误。 值得注意的是，除了启动过程中因为设置了proxy参数会作为 Proxy 模式启动。在 etcd 集群化启动时，节点注册自身的时候监测到集群的实际节点数量已经符合要求，那么就会退化为 Proxy 模式。 6 数据存储etcd 的存储分为内存存储和持久化（硬盘）存储两部分，内存中的存储除了顺序化的记录下所有用户对节点数据变更的记录外，还会对用户数据进行索引、建堆等方便查询的操作。而持久化则使用预写式日志（WAL：Write Ahead Log）进行记录存储。 在 WAL 的体系中，所有的数据在提交之前都会进行日志记录。在 etcd 的持久化存储目录中，有两个子目录。一个是 WAL，存储着所有事务的变化记录；另一个则是 snapshot，用于存储某一个时刻 etcd 所有目录的数据。通过 WAL 和 snapshot 相结合的方式，etcd 可以有效的进行数据存储和节点故障恢复等操作。 既然有了 WAL 实时存储了所有的变更，为什么还需要 snapshot 呢？随着使用量的增加，WAL 存储的数据会暴增，为了防止磁盘很快就爆满，etcd 默认每 10000 条记录做一次 snapshot，经过 snapshot 以后的 WAL 文件就可以删除。而通过 API 可以查询的历史 etcd 操作默认为 1000 条。 首次启动时，etcd 会把启动的配置信息存储到data-dir参数指定的数据目录中。配置信息包括本地节点的 ID、集群 ID 和初始时集群信息。用户需要避免 etcd 从一个过期的数据目录中重新启动，因为使用过期的数据目录启动的节点会与集群中的其他节点产生不一致（如：之前已经记录并同意 Leader 节点存储某个信息，重启后又向 Leader 节点申请这个信息）。所以，为了最大化集群的安全性，一旦有任何数据损坏或丢失的可能性，你就应该把这个节点从集群中移除，然后加入一个不带数据目录的新节点。 6.1 预写式日志（WAL）WAL（Write Ahead Log）最大的作用是记录了整个数据变化的全部历程。在 etcd 中，所有数据的修改在提交前，都要先写入到 WAL 中。使用 WAL 进行数据的存储使得 etcd 拥有两个重要功能。 故障快速恢复： 当你的数据遭到破坏时，就可以通过执行所有 WAL 中记录的修改操作，快速从最原始的数据恢复到数据损坏前的状态。 数据回滚（undo）/ 重做（redo）：因为所有的修改操作都被记录在 WAL 中，需要回滚或重做，只需要方向或正向执行日志中的操作即可。 WAL 与 snapshot 在 etcd 中的命名规则在 etcd 的数据目录中，WAL 文件以$seq-$index.wal的格式存储。最初始的 WAL 文件是0000000000000000-0000000000000000.wal，表示是所有 WAL 文件中的第 0 个，初始的 Raft 状态编号为 0。运行一段时间后可能需要进行日志切分，把新的条目放到一个新的 WAL 文件中。 假设，当集群运行到 Raft 状态为 20 时，需要进行 WAL 文件的切分时，下一份 WAL 文件就会变为0000000000000001-0000000000000021.wal。如果在 10 次操作后又进行了一次日志切分，那么后一次的 WAL 文件名会变为0000000000000002-0000000000000031.wal。可以看到-符号前面的数字是每次切分后自增 1，而-符号后面的数字则是根据实际存储的 Raft 起始状态来定。 snapshot 的存储命名则比较容易理解，以$term-$index.wal格式进行命名存储。term 和 index 就表示存储 snapshot 时数据所在的 raft 节点状态，当前的任期编号以及数据项位置信息。 6.2 关键部分源码解析从代码逻辑中可以看到，WAL 有两种模式，读模式（read）和数据添加（append）模式，两种模式不能同时成立。一个新创建的 WAL 文件处于 append 模式，并且不会进入到 read 模式。一个本来存在的 WAL 文件被打开的时候必然是 read 模式，并且只有在所有记录都被读完的时候，才能进入 append 模式，进入 append 模式后也不会再进入 read 模式。这样做有助于保证数据的完整与准确。 集群在进入到etcdserver/server.go的NewServer函数准备启动一个 etcd 节点时，会检测是否存在以前的遗留 WAL 数据。 检测的第一步是查看 snapshot 文件夹下是否有符合规范的文件，若检测到 snapshot 格式是 v0.4 的，则调用函数升级到 v0.5。从 snapshot 中获得集群的配置信息，包括 token、其他节点的信息等等，然后载入 WAL 目录的内容，从小到大进行排序。根据 snapshot 中得到的 term 和 index，找到 WAL 紧接着 snapshot 下一条的记录，然后向后更新，直到所有 WAL 包的 entry 都已经遍历完毕，Entry 记录到 ents 变量中存储在内存里。此时 WAL 就进入 append 模式，为数据项添加进行准备。 当 WAL 文件中数据项内容过大达到设定值（默认为 10000）时，会进行 WAL 的切分，同时进行 snapshot 操作。这个过程可以在etcdserver/server.go的snapshot函数中看到。所以，实际上数据目录中有用的 snapshot 和 WAL 文件各只有一个，默认情况下 etcd 会各保留 5 个历史文件。 7 Raft新版 etcd 中，raft 包就是对 Raft 一致性算法的具体实现。关于 Raft 算法的讲解，网上已经有很多文章，有兴趣的读者可以去阅读一下 Raft 算法论文非常精彩。本文则不再对 Raft 算法进行详细描述，而是结合 etcd，针对算法中一些关键内容以问答的形式进行讲解。有关 Raft 算法的术语如果不理解，可以参见概念词汇表一节。 7.1 Raft 常见问答一览 Raft 中一个 Term（任期）是什么意思？ Raft 算法中，从时间上，一个任期讲即从一次竞选开始到下一次竞选开始。从功能上讲，如果 Follower 接收不到 Leader 节点的心跳信息，就会结束当前任期，变为 Candidate 发起竞选，有助于 Leader 节点故障时集群的恢复。发起竞选投票时，任期值小的节点不会竞选成功。如果集群不出现故障，那么一个任期将无限延续下去。而投票出现冲突也有可能直接进入下一任再次竞选。 图 12 Term 示意图 Raft 状态机是怎样切换的？ Raft 刚开始运行时，节点默认进入 Follower 状态，等待 Leader 发来心跳信息。若等待超时，则状态由 Follower 切换到 Candidate 进入下一轮 term 发起竞选，等到收到集群多数节点的投票时，该节点转变为 Leader。Leader 节点有可能出现网络等故障，导致别的节点发起投票成为新 term 的 Leader，此时原先的老 Leader 节点会切换为 Follower。Candidate 在等待其它节点投票的过程中如果发现别的节点已经竞选成功成为 Leader 了，也会切换为 Follower 节点。 图 13 Raft 状态机 如何保证最短时间内竞选出 Leader，防止竞选冲突？ 在 Raft 状态机一图中可以看到，在 Candidate 状态下， 有一个 times out，这里的 times out 时间是个随机值，也就是说，每个机器成为 Candidate 以后，超时发起新一轮竞选的时间是各不相同的，这就会出现一个时间差。在时间差内，如果 Candidate1 收到的竞选信息比自己发起的竞选信息 term 值大（即对方为新一轮 term），并且新一轮想要成为 Leader 的 Candidate2 包含了所有提交的数据，那么 Candidate1 就会投票给 Candidate2。这样就保证了只有很小的概率会出现竞选冲突。 如何防止别的 Candidate 在遗漏部分数据的情况下发起投票成为 Leader？ Raft 竞选的机制中，使用随机值决定超时时间，第一个超时的节点就会提升 term 编号发起新一轮投票，一般情况下别的节点收到竞选通知就会投票。但是，如果发起竞选的节点在上一个 term 中保存的已提交数据不完整，节点就会拒绝投票给它。通过这种机制就可以防止遗漏数据的节点成为 Leader。 Raft 某个节点宕机后会如何？ 通常情况下，如果是 Follower 节点宕机，如果剩余可用节点数量超过半数，集群可以几乎没有影响的正常工作。如果是 Leader 节点宕机，那么 Follower 就收不到心跳而超时，发起竞选获得投票，成为新一轮 term 的 Leader，继续为集群提供服务。需要注意的是；etcd 目前没有任何机制会自动去变化整个集群总共的节点数量，即如果没有人为的调用 API，etcd 宕机后的节点仍然被计算为总节点数中，任何请求被确认需要获得的投票数都是这个总数的半数以上。 图 14 节点宕机 为什么 Raft 算法在确定可用节点数量时不需要考虑拜占庭将军问题？ 拜占庭问题中提出，允许 n 个节点宕机还能提供正常服务的分布式架构，需要的总节点数量为 3n+1，而 Raft 只需要 2n+1 就可以了。其主要原因在于，拜占庭将军问题中存在数据欺骗的现象，而 etcd 中假设所有的节点都是诚实的。etcd 在竞选前需要告诉别的节点自身的 term 编号以及前一轮 term 最终结束时的 index 值，这些数据都是准确的，其他节点可以根据这些值决定是否投票。另外，etcd 严格限制 Leader 到 Follower 这样的数据流向保证数据一致不会出错。 用户从集群中哪个节点读写数据？ Raft 为了保证数据的强一致性，所有的数据流向都是一个方向，从 Leader 流向 Follower，也就是所有 Follower 的数据必须与 Leader 保持一致，如果不一致会被覆盖。即所有用户更新数据的请求都最先由 Leader 获得，然后存下来通知其他节点也存下来，等到大多数节点反馈时再把数据提交。一个已提交的数据项才是 Raft 真正稳定存储下来的数据项，不再被修改，最后再把提交的数据同步给其他 Follower。因为每个节点都有 Raft 已提交数据准确的备份（最坏的情况也只是已提交数据还未完全同步），所以读的请求任意一个节点都可以处理。 etcd 实现的 Raft 算法性能如何？ 单实例节点支持每秒 1000 次数据写入。节点越多，由于数据同步涉及到网络延迟，会根据实际情况越来越慢，而读性能会随之变强，因为每个节点都能处理用户请求。 7.2 关键部分源码解析在 etcd 代码中，Node 作为 Raft 状态机的具体实现，是整个算法的关键，也是了解算法的入口。 在 etcd 中，对 Raft 算法的调用如下，你可以在etcdserver/raft.go中的startNode找到： 12storage := raft.NewMemoryStorage()n := raft.StartNode(0x01, []int64&#123;0x02, 0x03&#125;, 3, 1, storage) 通过这段代码可以了解到，Raft 在运行过程记录数据和状态都是保存在内存中，而代码中raft.StartNode启动的 Node 就是 Raft 状态机 Node。启动了一个 Node 节点后，Raft 会做如下事项。 首先，你需要把从集群的其他机器上收到的信息推送到 Node 节点，你可以在etcdserver/server.go中的Process函数看到。 123456func (s *EtcdServer) Process(ctx context.Context, m raftpb.Message) error &#123; if m.Type == raftpb.MsgApp &#123; s.stats.RecvAppendReq(types.ID(m.From).String(), m.Size()) &#125; return s.node.Step(ctx, m)&#125; 在检测发来请求的机器是否是集群中的节点，自身节点是否是 Follower，把发来请求的机器作为 Leader，具体对 Node 节点信息的推送和处理则通过node.Step()函数实现。 其次，你需要把日志项存储起来，在你的应用中执行提交的日志项，然后把完成信号发送给集群中的其它节点，再通过node.Ready()监听等待下一次任务执行。有一点非常重要，你必须确保在你发送完成消息给其他节点之前，你的日志项内容已经确切稳定的存储下来了。 最后，你需要保持一个心跳信号Tick()。Raft 有两个很重要的地方用到超时机制：心跳保持和 Leader 竞选。需要用户在其 raft 的 Node 节点上周期性的调用 Tick() 函数，以便为超时机制服务。 综上所述，整个 raft 节点的状态机循环类似如下所示： 12345678910111213for &#123; select &#123; case &lt;-s.Ticker: n.Tick() case rd := &lt;-s.Node.Ready(): saveToStorage(rd.State, rd.Entries) send(rd.Messages) process(rd.CommittedEntries) s.Node.Advance() case &lt;-s.done: return &#125;&#125; 而这个状态机真实存在的代码位置为etcdserver/server.go中的run函数。 对状态机进行状态变更（如用户数据更新等）则是调用n.Propose(ctx, data)函数，在存储数据时，会先进行序列化操作。获得大多数其他节点的确认后，数据会被提交，存为已提交状态。 之前提到 etcd 集群的启动需要借助别的 etcd 集群或者 DNS，而启动完毕后这些外力就不需要了，etcd 会把自身集群的信息作为状态存储起来。所以要变更自身集群节点数量实际上也需要像用户数据变更那样添加数据条目到 Raft 状态机中。这一切由n.ProposeConfChange(ctx, cc)实现。当集群配置信息变更的请求同样得到大多数节点的确认反馈后，再进行配置变更的正式操作，代码如下。 123var cc raftpb.ConfChangecc.Unmarshal(data)n.ApplyConfChange(cc) 注意：一个 ID 唯一性的表示了一个集群，所以为了避免不同 etcd 集群消息混乱，ID 需要确保唯一性，不能重复使用旧的 token 数据作为 ID。 8 StoreStore 这个模块顾名思义，就像一个商店把 etcd 已经准备好的各项底层支持加工起来，为用户提供五花八门的 API 支持，处理用户的各项请求。要理解 Store，只需要从 etcd 的 API 入手即可。打开 etcd 的 API 列表，我们可以看到有如下 API 是对 etcd 存储的键值进行的操作，亦即 Store 提供的内容。API 中提到的目录（Directory）和键（Key），上文中也可能称为 etcd 节点（Node）。 为 etcd 存储的键赋值 12345678910curl http://127.0.0.1:2379/v2/keys/message -XPUT -d value=&quot;Hello world&quot;&#123; &quot;action&quot;: &quot;set&quot;, &quot;node&quot;: &#123; &quot;createdIndex&quot;: 2, &quot;key&quot;: &quot;/message&quot;, &quot;modifiedIndex&quot;: 2, &quot;value&quot;: &quot;Hello world&quot; &#125;&#125; 反馈的内容含义如下： action: 刚刚进行的动作名称。 node.key: 请求的 HTTP 路径。etcd 使用一个类似文件系统的方式来反映键值存储的内容。 node.value: 刚刚请求的键所存储的内容。 node.createdIndex: etcd 节点每次有变化时都会自增的一个值，除了用户请求外，etcd 内部运行（如启动、集群信息变化等）也会对节点有变动而引起这个值的变化。 node.modifiedIndex: 类似 node.createdIndex，能引起 modifiedIndex 变化的操作包括 set, delete, update, create, compareAndSwap and compareAndDelete。 查询 etcd 某个键存储的值 1curl http://127.0.0.1:2379/v2/keys/message 修改键值：与创建新值几乎相同，但是反馈时会有一个prevNode值反应了修改前存储的内容。 1curl http://127.0.0.1:2379/v2/keys/message -XPUT -d value=&quot;Hello etcd&quot; 删除一个值 1curl http://127.0.0.1:2379/v2/keys/message -XDELETE 对一个键进行定时删除：etcd 中对键进行定时删除，设定一个 TTL 值，当这个值到期时键就会被删除。反馈的内容会给出 expiration 项告知超时时间，ttl 项告知设定的时长。 1curl http://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -d ttl=5 取消定时删除任务 1curl http://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -d ttl= -d prevExist=true 对键值修改进行监控：etcd 提供的这个 API 让用户可以监控一个值或者递归式的监控一个目录及其子目录的值，当目录或值发生变化时，etcd 会主动通知。 1curl http://127.0.0.1:2379/v2/keys/foo?wait=true 对过去的键值操作进行查询：类似上面提到的监控，只不过监控时加上了过去某次修改的索引编号，就可以查询历史操作。默认可查询的历史记录为 1000 条。 1curl &apos;http://127.0.0.1:2379/v2/keys/foo?wait=true&amp;waitIndex=7&apos; 自动在目录下创建有序键。在对创建的目录使用POST参数，会自动在该目录下创建一个以 createdIndex 值为键的值，这样就相当于以创建时间先后严格排序了。这个 API 对分布式队列这类场景非常有用。 12345678910curl http://127.0.0.1:2379/v2/keys/queue -XPOST -d value=Job1&#123; &quot;action&quot;: &quot;create&quot;, &quot;node&quot;: &#123; &quot;createdIndex&quot;: 6, &quot;key&quot;: &quot;/queue/6&quot;, &quot;modifiedIndex&quot;: 6, &quot;value&quot;: &quot;Job1&quot; &#125;&#125; 按顺序列出所有创建的有序键。 1curl -s &apos;http://127.0.0.1:2379/v2/keys/queue?recursive=true&amp;sorted=true&apos; 创建定时删除的目录：就跟定时删除某个键类似。如果目录因为超时被删除了，其下的所有内容也自动超时删除。 1curl http://127.0.0.1:2379/v2/keys/dir -XPUT -d ttl=30 -d dir=true 刷新超时时间。 1curl http://127.0.0.1:2379/v2/keys/dir -XPUT -d ttl=30 -d dir=true -d prevExist=true 自动化 CAS（Compare-and-Swap）操作：etcd 强一致性最直观的表现就是这个 API，通过设定条件，阻止节点二次创建或修改。即用户的指令被执行当且仅当 CAS 的条件成立。条件有以下几个。 prevValue 先前节点的值，如果值与提供的值相同才允许操作。 prevIndex 先前节点的编号，编号与提供的校验编号相同才允许操作。 prevExist 先前节点是否存在。如果存在则不允许操作。这个常常被用于分布式锁的唯一获取。 假设先进行了如下操作：设定了 foo 的值。 1curl http://127.0.0.1:2379/v2/keys/foo -XPUT -d value=one 然后再进行操作： 1curl http://127.0.0.1:2379/v2/keys/foo?prevExist=false -XPUT -d value=three 就会返回创建失败的错误。 条件删除（Compare-and-Delete）：与 CAS 类似，条件成立后才能删除。 创建目录 1curl http://127.0.0.1:2379/v2/keys/dir -XPUT -d dir=true 列出目录下所有的节点信息，最后以/结尾。还可以通过 recursive 参数递归列出所有子目录信息。 1curl http://127.0.0.1:2379/v2/keys/ 删除目录：默认情况下只允许删除空目录，如果要删除有内容的目录需要加上recursive=true参数。 1curl &apos;http://127.0.0.1:2379/v2/keys/foo_dir?dir=true&apos; -XDELETE 创建一个隐藏节点：命名时名字以下划线_开头默认就是隐藏键。 1curl http://127.0.0.1:2379/v2/keys/_message -XPUT -d value=&quot;Hello hidden world&quot; 相信看完这么多 API，读者已经对 Store 的工作内容基本了解了。它对 etcd 下存储的数据进行加工，创建出如文件系统般的树状结构供用户快速查询。它有一个Watcher用于节点变更的实时反馈，还需要维护一个WatcherHub对所有Watcher订阅者进行通知的推送。同时，它还维护了一个由定时键构成的小顶堆，快速返回下一个要超时的键。最后，所有这些 API 的请求都以事件的形式存储在事件队列中等待处理。 9 总结通过从应用场景到源码分析的一系列回顾，我们了解到 etcd 并不是一个简单的分布式键值存储系统。它解决了分布式场景中最为常见的一致性问题，为服务发现提供了一个稳定高可用的消息注册仓库，为以微服务协同工作的架构提供了无限的可能。相信在不久的将来，通过 etcd 构建起来的大型系统会越来越多。 10 作者简介孙健波，浙江大学 SEL 实验室硕士研究生，目前在云平台团队从事科研和开发工作。浙大团队对 PaaS、Docker、大数据和主流开源云计算技术有深入的研究和二次开发经验，团队现将部分技术文章贡献出来，希望能对读者有所帮助。 参考文献 https://github.com/coreos/etcd https://groups.google.com/forum/#!topic/etcd-dev/wmndjzBNdZo http://jm-blog.aliapp.com/?p=1232 http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/ http://devo.ps/blog/zookeeper-vs-doozer-vs-etcd/ http://jasonwilder.com/blog/2014/02/04/service-discovery-in-the-cloud/ http://www.infoworld.com/article/2612082/open-source-software/has-apache-lost-its-way-.html http://en.wikipedia.org/wiki/WAL http://www.infoq.com/cn/articles/coreos-analyse-etcd http://www.activestate.com/blog/2014/05/service-discovery-solutions https://ramcloud.stanford.edu/raft.pdf 转自: https://www.infoq.cn/article/etcd-interpretation-application-scenario-implement-principle]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结四种常见的 POST 提交数据方式]]></title>
    <url>%2F2020%2F02%2F27%2Fhttp_4_way_post_data%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[总结一波四种常见的 POST 提交数据方式, 这文章总结得不错转自: https://imququ.com/post/four-ways-to-post-data-in-http.html HTTP/1.1 协议规定的 HTTP 请求方法有 OPTIONS、GET、HEAD、POST、PUT、DELETE、TRACE、CONNECT 这几种。其中 POST 一般用来向服务端提交数据，本文主要讨论 POST 提交数据的几种方式。 我们知道，HTTP 协议是以 ASCII 码传输，建立在 TCP/IP 协议之上的应用层规范。规范把 HTTP 请求分为三个部分：~状态行~请求行、请求头、消息主体。类似于下面这样： 1234&lt;method&gt; &lt;request-URL&gt; &lt;version&gt;&lt;headers&gt;&lt;entity-body&gt; 协议规定 POST 提交的数据必须放在消息主体（entity-body）中，但协议并没有规定数据必须使用什么编码方式。实际上，开发者完全可以自己决定消息主体的格式，只要最后发送的 HTTP 请求满足上面的格式就可以。 但是，数据发送出去，还要服务端解析成功才有意义。一般服务端语言如 php、python 等，以及它们的 framework，都内置了自动解析常见数据格式的功能。服务端通常是根据请求头（headers）中的 Content-Type 字段来获知请求中的消息主体是用何种方式编码，再对主体进行解析。所以说到 POST 提交数据方案，包含了 Content-Type 和消息主体编码方式两部分。下面就正式开始介绍它们。 application/x-www-form-urlencoded这应该是最常见的 POST 提交数据的方式了。浏览器的原生 表单，如果不设置 enctype 属性，那么最终就会以 application/x-www-form-urlencoded 方式提交数据。请求类似于下面这样（无关的请求头在本文中都省略掉了）： 1234POST http://www.example.com HTTP/1.1Content-Type: application/x-www-form-urlencoded;charset=utf-8title=test&amp;sub%5B%5D=1&amp;sub%5B%5D=2&amp;sub%5B%5D=3 首先，Content-Type 被指定为 application/x-www-form-urlencoded；其次，提交的数据按照 key1=val1&amp;key2=val2 的方式进行编码，key 和 val 都进行了 URL 转码。大部分服务端语言都对这种方式有很好的支持。例如 PHP 中，$_POST[‘title’] 可以获取到 title 的值，$_POST[‘sub’] 可以得到 sub 数组。 很多时候，我们用 Ajax 提交数据时，也是使用这种方式。例如 JQuery 和 QWrap 的 Ajax，Content-Type 默认值都是「application/x-www-form-urlencoded;charset=utf-8」。 multipart/form-data这又是一个常见的 POST 数据提交的方式。我们使用表单上传文件时，必须让 表单的 enctype 等于 multipart/form-data。直接来看一个请求示例： 12345678910111213POST http://www.example.com HTTP/1.1Content-Type:multipart/form-data; boundary=----WebKitFormBoundaryrGKCBY7qhFd3TrwA------WebKitFormBoundaryrGKCBY7qhFd3TrwAContent-Disposition: form-data; title------WebKitFormBoundaryrGKCBY7qhFd3TrwAContent-Disposition: form-data; Content-Type: image/pngPNG ... content of chrome.png ...------WebKitFormBoundaryrGKCBY7qhFd3TrwA-- 这个例子稍微复杂点。首先生成了一个 boundary 用于分割不同的字段，为了避免与正文内容重复，boundary 很长很复杂。然后 Content-Type 里指明了数据是以 multipart/form-data 来编码，本次请求的 boundary 是什么内容。消息主体里按照字段个数又分为多个结构类似的部分，每部分都是以 --boundary 开始，紧接着是内容描述信息，然后是回车，最后是字段具体内容（文本或二进制）。如果传输的是文件，还要包含文件名和文件类型信息。消息主体最后以 --boundary-- 标示结束。关于 multipart/form-data 的详细定义，请前往 rfc1867 查看。 这种方式一般用来上传文件，各大服务端语言对它也有着良好的支持。 上面提到的这两种 POST 数据的方式，都是浏览器原生支持的，而且现阶段标准中原生 表单也只支持这两种方式（通过 元素的 enctype 属性指定，默认为 application/x-www-form-urlencoded。其实 enctype 还支持 text/plain，不过用得非常少）。 随着越来越多的 Web 站点，尤其是 WebApp，全部使用 Ajax 进行数据交互之后，我们完全可以定义新的数据提交方式，给开发带来更多便利。 application/jsonapplication/json 这个 Content-Type 作为响应头大家肯定不陌生。实际上，现在越来越多的人把它作为请求头，用来告诉服务端消息主体是序列化后的 JSON 字符串。由于 JSON 规范的流行，除了低版本 IE 之外的各大浏览器都原生支持 JSON.stringify，服务端语言也都有处理 JSON 的函数，使用 JSON 不会遇上什么麻烦。 JSON 格式支持比键值对复杂得多的结构化数据，这一点也很有用。记得我几年前做一个项目时，需要提交的数据层次非常深，我就是把数据 JSON 序列化之后来提交的。不过当时我是把 JSON 字符串作为 val，仍然放在键值对里，以 x-www-form-urlencoded 方式提交。 Google 的 AngularJS 中的 Ajax 功能，默认就是提交 JSON 字符串。例如下面这段代码： 1234var data = &#123;&apos;title&apos;:&apos;test&apos;, &apos;sub&apos; : [1,2,3]&#125;;$http.post(url, data).success(function(result) &#123; ...&#125;); 最终发送的请求是： 1234POST http://www.example.com HTTP/1.1 Content-Type: application/json;charset=utf-8&#123;&quot;title&quot;:&quot;test&quot;,&quot;sub&quot;:[1,2,3]&#125; 这种方案，可以方便的提交复杂的结构化数据，特别适合 RESTful 的接口。各大抓包工具如 Chrome 自带的开发者工具、Firebug、Fiddler，都会以树形结构展示 JSON 数据，非常友好。但也有些服务端语言还没有支持这种方式，例如 php 就无法通过 $_POST 对象从上面的请求中获得内容。这时候，需要自己动手处理下：在请求头中 Content-Type 为 application/json 时，从 php://input 里获得原始输入流，再 json_decode 成对象。一些 php 框架已经开始这么做了。 当然 AngularJS 也可以配置为使用 x-www-form-urlencoded 方式提交数据。如有需要，可以参考这篇文章。 text/xml我的博客之前提到过 XML-RPC（XML Remote Procedure Call）。它是一种使用 HTTP 作为传输协议，XML 作为编码方式的远程调用规范。典型的 XML-RPC 请求是这样的： 123456789101112POST http://www.example.com HTTP/1.1 Content-Type: text/xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;methodCall&gt; &lt;methodName&gt;examples.getStateName&lt;/methodName&gt; &lt;params&gt; &lt;param&gt; &lt;value&gt;&lt;i4&gt;41&lt;/i4&gt;&lt;/value&gt; &lt;/param&gt; &lt;/params&gt;&lt;/methodCall&gt; XML-RPC 协议简单、功能够用，各种语言的实现都有。它的使用也很广泛，如 WordPress 的 XML-RPC Api，搜索引擎的 ping 服务等等。JavaScript 中，也有现成的库支持以这种方式进行数据交互，能很好的支持已有的 XML-RPC 服务。不过，我个人觉得 XML 结构还是过于臃肿，一般场景用 JSON 会更灵活方便。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>noodle</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10禁止自动更新踩坑]]></title>
    <url>%2F2020%2F01%2F09%2Fwin10_home_disable_auto_update%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[写个定时命令(推荐此方法且已集成到nox)已经集成到nox中 You can also create a stop and start script for Windows Update. Create a Notepad file with each of the following and save them with the .bat extension. stop-upadtes.bat12345678910sc config wuauserv start= disablednet stop wuauservsc config bits start= disablednet stop bitssc config dosvc start= disablednet stop dosvcpause start-updates.bat 12345678910sc config wuauserv start= autonet start wuauservsc config bits start= autonet start bitssc config dosvc start= autonet start dosvcpause Just right click on stop-upadtes.bat and select “Run as Administrator” Source: http://sdbr.net/windows-10-update-failure/ My sincere thanks to MVP, Greg Carmack for the help with finding this. . . . 直接暴力删除(亲测有效但不建议)不建议原因: 貌似会引起蓝屏以及删除了这个服务之后就不可能从Microsoft Store安装uwp了 Click your Start Button, type services and hit Enter Scroll down to find Windows Update Service If it is Started, double click it and stop that service CLose the services App Click your Start Button, type cmd, right click Command Prompt and choose ‘Run as Administrator’ Run this command and hit Enter sc delete wuauserv Close Command Prompt Restart your PC If you also have the Update Assistant installed on your system: The Update Assistant in Windows 10 cannot be uninstalled, but you can get rid of it Click your Start Button, type Task Scheduler and hit enterIn the left Pane, navigate to: Windows - Update OrchestraterRight Pane double click ‘Update Assistant’Click on Triggers TabOn each of the triggers ( At login . . . etc.) double click and uncheck ‘Enabled’ Under ‘Update Assistant, there is another event ‘Update Assistant CalendarRun’Do the same for all triggers on that event Restart your system, that’s It! When finished and after a system restart, open the services app and make sure the Windows Update Service is now gone . Other options to consider(亲测过都不够彻底)Please note, Microsoft’s approach to updates is very different that earlier releases. They are basically mandatory because of Microsoft’s past history with customers sometimes failing to install important updates. See options for managing how and when Windows Updates are installed. Option 1: Set your Internet connection as metered:https://www.groovypost.com/howto/manage-windows… Option 2: Use Active Hours and Restart options:https://www.groovypost.com/howto/managing-windo… Option 3: Use Show/Hide Updates tool to block the update: Is there an option or work around to block updates or hardware drivers that might cause problems?Yes, Microsoft has released a KB update (KB3073930) that will let users block or hide Windows or driver updates. You can download it at the following link:https://support.microsoft.com/en-us/kb/3073930 最近被win10的更新给坑了, 好好一个电脑硬是瘫痪了, 不得已重置了, 所有东西重装一遍, 吃一堑长一智这回得禁止他更新了… 我的电脑右键-管理-服务和应用程序-服务-windows Update： windows Update上右键-属性：启动类型改为禁用，然后点击选项卡里的恢复，将第一次失败、第二次失败等改为无操作，修改好后点击：应用-确定。 这样我原本以为万事大吉了，然而第二天打开电脑，又提示windows请求重启电脑以完成更新，我打开上面修改的内容一看，昨天修改好的禁用今天又变成手动触发了~而且状态是正在运行!流氓啊，最后在win10吧找到了这个帖子win10家庭版，禁用windows update后还是会自动启动服务(因为有一些任务计划在重新开启update服务) ,下面是解决办法： 1.打开你的小娜； 2.搜索任务计划程序； 3.找到windows Update，在任务计划程序库-Microsoft-Windows下； 4.全部禁用： 如果禁用不了就切换到超级管理员然后来禁用(禁用不了就直接删除) 开启超级管理员账户方法为:1、通过Cortana搜索cmd，匹配出“命令提示符”，右键以管理员身份运行；2、在打开的命令提示符窗口输入net user administrator /active:yes ，按回车执行命令，提示命令成功完成，进行下一步；3、右键开始按钮—关机或注销—注销，注销系统；4、在这个界面，选择Administrator帐户登录，初次登录这个帐号，会执行系统配置，需要等待一段时间。 若要关闭则net user administrator /active:no ps：还有一种通过禁用组策略来禁用更新的方法：通过组策略禁用Windows 10系统的自动更新功能 因为 不适用于家庭版 ，所以我没操作，你也可以在网上找win10家庭版启用组策略的方法来启用组策略(我没成功)，再做尝试。 组策略禁用自动更新步骤如下： 1、鼠标右键点击开始菜单或者是快捷键Win+R打开运行，然后在运行的对话框中填写gpedit.msc并确定； 2、打开组策略后点击左侧菜单依次展开计算机配置—–&gt;管理模板—–&gt;Windows组件—–&gt;Windows更新； 3、双击配置自动更新即可打开如下图的新窗口，在新窗口左侧的选项里将默认的未配置更改为已禁用即可；]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个只有2M+大小的强大效率工具sux]]></title>
    <url>%2F2020%2F01%2F09%2Fsux_readme%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[sux 知乎链接 下载链接 sux 是一个只有2M+大小的强大效率工具同时拥有 翻译 历史剪切板 Everything搜索工具栏 截图 &amp; 贴图 类似listary/alfred/wox的快捷搜索: shift+空格 类似macos的触发角 屏幕边缘触发器 全局自定义快捷键实现各种操作 文本替换器 文本变换器 托盘菜单 快捷指令 可自定义的json配置 自定义主题 blabla… An alternative to Alfred/Wox/Listary/Capslock+/OneQuick . Inspired by Alfred/Wox/Listary/Capslock+/utools/OneQuick, thank u. . . . 重要 请以管理员身份运行sux 防止杀毒软件误杀处理 : 打开win10托盘的Windows安全中心-病毒和威胁防护-病毒和威胁防护设置的管理设置-排除项的添加或删除排除项-添加排除项-文件夹, 然后选中sux所在文件夹即可 如果被其他杀毒软件报杀则将sux列入白名单 如果是windows安全中心杀了的话则在它的病毒和威胁保护-保护历史记录-找到删除sux的历史记录-还原 Please run sux as administrator. Just download sux.zip and unzip it then run sux.exe as admin ! 快捷搜索search-plus大多数时候其实都是 shift+空格或者双击Capslock 然后空格搜东西, 如果要取消菜单则按alt或者esc, 所有的菜单都是可以选中某段文字然后直接查询的, 右边这一排q/w/e/r啥的都是快捷键 也可以先选中某段文字然后shift+空格或者双击Capslock然后直接查询的. 所有的默认快捷键都是可以改的, 在conf.user.json里找到ShowSuxMenu改, 改成capslock_q或者alt_space或者doublehit_ctrl (双击ctrl) 或者triplehit_shift (三击shift) 或者其他的任何你喜欢的快捷键都行, 不过不建议doublehit_alt (双击alt), 因为alt会丢失焦点. 为什么shift+空格 出来的不是搜索框? 原来是那样的, 后来我给一些用户(比如运营岗用户)用, 发现他们记不住key.比如百度是bd, 谷歌是gg这种对吧?后来我就做了个这种快捷菜单, 用过几次熟悉快捷键之后也十分迅捷方便, 省去了每次都要输入什么gg/bd的烦恼 翻译选中文字然后 capslock+t 亮点: 可以整句翻译, 也可以直接翻译带下划线的或者大小写无空格的都可以直接选中后翻译, 甚至可以直接翻译形如 TransformText_Detail_Menu-click, stand up 的复杂句子, 会自动解析为 transform text detail menu click, stand up 然后再翻译. 历史剪切板clipboard-plus shift+空格 弹出菜单之后, 按v这个历史粘贴板支持: 图片内容(在sux剪切板里会以[图片]标识, sux支持从浏览器或者任何地方直接复制的图片) 支持其他的二进制文件, 如图片文件, txt文件等等, 会以 [文件]或 [多文件] 标识 支持文件夹, 会以 [文件夹]标识 支持一键粘贴所有历史剪切板记录和清空所有, 有时候需要去各种地方去一次性复制很多东西, 然后一次性粘贴, 那这时就可以先清空历史然后一键粘贴所有了 Everything搜索工具栏使用演示: 如何设置才能有这个功能呢? 请按照下面两张图与以下步骤设置: 进入Everything的设置, 勾上下图中这三个选项 检查 .NET Framework 是否 大于等于 4.7 (Win+r然后输入cmd回车, 然后输入reg query &quot;HKLM\SOFTWARE\Microsoft\Net Framework Setup\NDP\v4&quot; /s查看Version REG_SZ那一行是否大于等于 4.7) 参考下图, 先通过任务栏的上下文菜单来开启 EverythingToolbar, 注意你需要打开这个菜单两次, 第一次它不显示的 右键任务栏, 解锁任务栏, 然后调整工具栏的大小和位置 设置完毕之后, 可以使用 capslock+f 来激活, 若已经选中文字的话, 则会自动输入该文字并搜索 Shortcut Function &#8593; &#8595; Navigate search results Return Open Ctrl+Return/Click Open path Shift+Return/Click Open in Everything Alt+Return/Click File properties Ctrl+Shift+Enter Run as admin (Shift+)Tab Cycle through filters Ctrl+0-9 Select filter Ctrl+Space Preview file in QuickLook Win+Alt+S Focus search box (customizable) 截图和贴图 按capslock_q是截图 按capslock_alt_q是贴图, 贴图窗口出来之后, 点击一下贴图图片则可以让他变得半透明, 贴图还是比较有用的, 对于有时候需要对照着贴图写代码或者对照画图等的需求的用户 类似macos的触发角hot-corner若要用的话, 需要去sux托盘菜单里开启触发角功能 当开启之后, 鼠标移动到屏幕左上/左下/右上右下都会触发不同的动作: 触发角 操作 左上 跳到浏览器前一个标签页 右上 跳到浏览器后一个标签页 左下 模拟按下win键 右下 模拟按下alt+tab 这些是默认动作, 你都可以改动自定义配置conf.user.json来更改12345678910111213141516"hot-corner": &#123; "action": &#123; "LeftTopCorner": &#123; "hover": "JumpToPrevTab" &#125;, "RightTopCorner": &#123; "hover": "JumpToNextTab" &#125;, "LeftBottomCorner": &#123; "hover": "win" &#125;, "RightBottomCorner": &#123; "hover": "GotoPreApp" &#125; &#125;&#125;, 屏幕边缘触发器hot-edge 比如你把鼠标放到屏幕左边缘, 然后滚轮, 你会发现可以调节音量 下面是预设的边缘触发表: 边缘 快捷键 操作 左边缘 滚轮 调节音量 左边缘上半部分 鼠标中键 把当前窗口移到屏幕左边 左边缘下半部分 鼠标中键 把当前窗口移到鼠标当前所在屏幕 右边缘 滚轮 向上翻页 / 向下翻页 右边缘上半部分 鼠标中键 把当前窗口移到屏幕右边 右边缘下半部分 鼠标中键 把当前窗口移到鼠标当前所在屏幕 上边缘左半部分 滚轮 回到页面顶部 / 去页面底部 上边缘右半部分 滚轮 向上翻页 / 向下翻页 上边缘左半部分 鼠标中键 最小化 上边缘右半部分 鼠标中键 最大化 下边缘 滚轮 切换桌面 下边缘 鼠标中键 显示桌面管理器 文字替换器replace-text capslock+r 填写电子邮箱的时候经常要敲很多字或者填写密码的时候总是需要重复输入一长串, 对于经常重复需要输入的文本, 这个时候就可以用文字替换器来,比如密码是abcd12349087234bghyymll这么长的密码就可以定义为abc::, 或者比如把h/替换为http://之类的,配置可以自由定义, 已经选中文字则是只替换选中文字, 否则替换整行, 默认配置如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159 "replace-text": &#123; "enable": 1, "buildin": &#123; "h/": "http://", "hs/": "https://", "qc@": "@qq.com", "gc@": "@gmail.com", "16@": "@163.com" &#125;, "custom": &#123;&#125; &#125;,``` # 文本变换器`shift+空格` 弹出菜单之后, 按`t` 经常写代码的朋友应该经常会有把驼峰命名的文本 转换为 蛇形命名文本之类的需求, 或者把小写的文本转为大写的需求![](https://github.com/no5ix/no5ix.github.io/blob/source/source/img/sux/transform_text.gif)# CMDs指令`shift+空格` 弹出菜单之后, 按`c` * `cmd` : open a command prompt window on the current explorer path, 打开命令行窗口, 如果当前在文件管理器则打开后会立即进入当前文件管理器路径* `sux` : sux official site, sux官网# 托盘菜单![](https://github.com/no5ix/no5ix.github.io/blob/source/source/img/sux/tray_menu.gif)直接鼠标点击sux托盘图标可以快速禁用sux## 禁用win10系统的自动更新win10的自动更新经常会搞得电脑蓝屏或者各种崩溃或者长时间占用电脑, 十分恼人. win10的自动更新用win10本身自带的机制是无法禁止的, 即使关闭了win10的 `Windows Update`服务, 他隔一段时间后也会自动开启. sux的这个功能就彻底解决了这个问题, 不再烦恼.## 窗口移动器-永远保持新窗口在鼠标所在的显示器打开对于多显示器的用户来说, 在2显示器上双击了某程序准备打开它, 很可能它这个程序窗口却会在1显示器上打开, sux的窗口移动器就是解决这个问题的注: 当检测到用户只有一个显示器的时候, 此选项会自动禁用(灰掉)# 快捷键完全自定义这个工具其实很重磅的功能是 `hotkey`- 实现文本输入增强, 你可以通过 Capslock 键配合以下辅助按键实现大部分文本操作需求，不再需要在鼠标和键盘间来回切换, 可以类似vim一样的, 各种光标移动都十分方便 - 也可以自定义各种快捷键来触发各种动作, 比如配合触摸板达到快速大小化窗口, 三指拖动的效果预设快捷按键: | 快捷键 | 操作 || ---- | --- ||`caps+g` | 是`Ctrl`+`Y` || `ctrl+8` | 模拟鼠标按下不放的操作. (建议笔记本的触摸板的`三指点击`设定为快捷键`ctrl+8`, 然后就可以模拟鼠标按下不放的操作了, 达到类似mac的`三指拖动`的效果) || `ctrl+shift+alt+m` | 最大化/最小化窗口. (建议笔记本的触摸板的`四指点击`设定为`ctrl+shift+alt+m`, 这样就可以快速最大/最小化窗口了) ||`caps+h/j/k/l` | 也可以来上下左右的, 比如`caps+alt+h`就是往左选中哈, 以下相同 ||`caps+s` | 是左, 比如`caps+alt+s`就是往左选中哈, 以下相同, 加`alt`就是选中, 不加就是移动 ||`caps+e` | 是上, 加`alt`就是选中 ||`caps+d` | 是下, 加`alt`就是选中 ||`caps+f` | 是右, 加`alt`就是选中 ||`caps+逗号` | 是光标移动到最左边, 加`alt`就是选中 ||`caps+句号` | 是光标移动到最右边, 加`alt`就是选中 ||`caps+i` | 就是往左跳一个单词 ||`caps+alt+i` | 就是往左选中一个单词 | |`caps+o` | 就是往右跳一个单词 ||`caps+alt+o` | 就是往右选中一个单词 | |`caps+n` | 就是往左删一个单词 ||`caps+alt+n` | 就是往左删除到行首 | |`caps+m` | 就是往右删一个单词 ||`caps+alt+m` | 就是往右删除到行末 | |`caps+w` | 是选择当前单词 | |`caps+alt+w` | 是选择当前行 | |`caps+c` | 是模拟`ctrl+c` | |`caps+alt+c` | 也是模拟复制, 但是当复制文件的时候会直接返回文件的路径 | |`caps+r` | 是模拟`ctrl+y` | |`caps+v` | 是模拟`shift+insert`(终端爱好者的福音) | |`caps+tab` | 就是整行缩进, 不管光标在当前行的任何地方 | |`caps+backspace` | 删除光标所在行所有文字 | |`capslock+enter` | 无论光标是否在行末都能新起一个换行而不截断原句子 | |`capslock+alt+enter` | 无论光标是否在行末都能在上面新起一行而不截断原句子 | 其他的待用户尝试, 也可以自行配置, 我自己用的配置是`conf.no5ix.json`也在项目中的`app_data/conf_bak`文件夹里, 你也可以参考默认配置概览: ``` json "capslock_q": "ScreenShot", "capslock_alt_q": "ScreenShotAndSuspend", "shift_space": "ShowSuxMenu", "doublehit_capslock": "ShowSuxMenu", "capslock_t": "QuickEntry_Translation_Menu_Click", "capslock_c": "ctrl_c", "capslock_e": "up", "capslock_alt_e": "shift_up", "capslock_s": "left", "capslock_alt_s": "shift_left", "capslock_f": "right", "capslock_alt_f": "shift_right", "capslock_d": "down", "capslock_alt_d": "shift_down", "ctrl_8": "SimulateClickDown", "ctrl_shift_alt_m": "MaxMinWindow", "capslock_alt_c": "SaveSelectedFilePathToClipboard", "capslock_w": ["ctrl_left", "ctrl_shift_right", "ctrl_c"], "capslock_shift_w": ["home", "shift_end", "ctrl_c"], "capslock_`": "SwitchCapsState", "capslock_tab": ["home", "tab"], "capslock_v": "shift_ins", "capslock_shift_v": "shift_6", "capslock_g": "ctrl_y", "capslock_r": "QuickEntry_ReplaceText_Menu_Click", "capslock_enter": "InsertLineBelow", "capslock_shift_enter": "InsertLineAbove", "capslock_backspace": ["home", "shift_end", "backspace"], "capslock_y": "shift_8", "capslock_alt_y": "shift_5", "capslock_u": "shift_1", "capslock_alt_u": "shift_2", "capslock_h": "left", "capslock_alt_h": "shift_left", "capslock_j": "down", "capslock_alt_j": "shift_down", "capslock_k": "up", "capslock_alt_k": "shift_up", "capslock_l": "right", "capslock_alt_l": "shift_right", "capslock_p": "shift_7", "capslock_alt_p": "shift_3", "capslock_i": "ctrl_left", "capslock_alt_i": "shift_ctrl_left", "capslock_o": "ctrl_right", "capslock_alt_o": "shift_ctrl_right", "capslock_9": "[", "capslock_alt_9": "&#123;", "capslock_0": "]", "capslock_alt_0": "&#125;", "capslock_n": "ctrl_bs", "capslock_alt_n": "shift_home_del", "capslock_m": "ctrl_del", "capslock_alt_m": "shift_end_del", "capslock_,": "home", "capslock_alt_,": "shift_home", "capslock_.": "end", "capslock_alt_.": "shift_end", "capslock_;": "_", "capslock_alt_;": "-", "capslock_'": "=", "capslock_alt_'": "shift_=", "capslock_/": "\\", "capslock_alt_/": "shift_\\" 自定义配置可以在托盘菜单里找到”编辑配置文件”的菜单的, 改了配置记得重启sux哈 配置编写规则: 让一个键按了之后啥也不做是配一个nil, 比如 &quot;capslock&quot;: &quot;nil&quot; 就是让capslock按了之后不做任何事 action类型: 直接从下方的所有action里选即可 ShowSuxMenu StartSuxAhkWithWin MoveWindowToLeftSide MoveWindowToRightSide OpenFileExplorer OpenActionCenter CloseCurrentWindow GoTop GoBottom GoBack GoForward LockPc OpenTaskView VolumeMute VolumeUp VolumeDown GotoNextDesktop GotoPreDesktop RefreshTab ReopenLastTab GotoPreApp JumpToPrevTab JumpToNextTab SwitchCapsState SwitchInputMethodAndDeleteLeft MaxMinWindow MaxWindow MinWindow ReloadSux SelectCurrentWordAndCopy SelectCurrentLineAndCopy InsertLineBelow InsertLineAbove DeleteCurrentLine IndentCurrentLine SimulateClickDown 发送的单个键盘操作: 比如要发送shift+下 就是shift_down 发送一段键盘操作序列, 比如要实现caps +w选中当前单词, 首先得移动到词的左边, 然后往右选中单词, 则配置为: &quot;capslock_w&quot;: [&quot;ctrl_left&quot;, &quot;ctrl_shift_right&quot;] 一些特殊的热键定义对照表: lbutton: 左键单击 rbutton: 右键单击 mbutton: 中键单击 wheelup: 滚轮上滑 wheeldown: 滚轮下滑 hover: 悬停, 只能用在触发角的配置里 doublehit_: 双击, 比如doublehit_alt表示双击alt triplehit_: 三击 捐赠捐赠! 让作者更有动力给sux加新功能! ^_^ 微信 支付宝 如何发布 改 ver.ini, 然后提交 打 git tag 打包 建立 build 文件夹 compile sux.ahk 成为 sux.exe, 放到 build 里 拷贝 app_data 文件夹到 build 里 删除 build 里的app_data 里的 temp_dir 文件夹 删除 build 里的app_data 里的 ev_sup 里的 Everything.db 和 Run History.csv 使用 7zip 压缩 build文件夹, 压缩参数如下: 压缩格式: 7z / zip 压缩等级: 极限压缩 压缩方法: LZMA2 / LZMA 字典大小: 256MB 单词大小: 256 固实数据大小: 4GB TODO List auto update add more action 点击tray menu 不消失menu ext ahk search_plus失焦则销毁 smart selection 双引号, 括号内, 单引号内 translation gui change color to gray/ dpi / voice audio / soundmark encoding &lt;!– - 报一个bug，caps+q，q，用百度查询时，弹窗出现在另一个屏幕，因为我有2个显示器，建议设置一下默认的弹窗。 上述bug包括everything –&gt; &lt;!– - donate width Capslock + Backspace（删除光标所在行所有文字） Capslock + Enter（无论光标是否在行末都能新起一个换行而不截断原句子） –&gt;]]></content>
      <categories>
        <category>GitHub</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>AutoHotKey</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10系统下怎么让所有程序都默认以管理员身份运行]]></title>
    <url>%2F2020%2F01%2F01%2Fwindows_run_everything_as_admin%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[当我们在win10系统中运行一些程序的时候，经常会提示需要管理员身份才能运行，但是每次都要右键选择以管理员身份运行很麻烦，那么Win10系统下怎么让所有程序都默认以管理员身份运行呢？. . . 专业版系统具体步骤如下: 使用小娜搜索 secpol 或 “本地安全策略” （或运行 secpol.msc ），右键选择以管理员身份打开（或许需要）； 展开 本地策略，选择 安全选项，在右边找到“用户帐户控制：以管理员批准模式运行所有管理员”，双击它，将本地安全设置更改为“已禁用”； 然后重新启动计算机即可 完成操作后，通过 Win + R 执行命令 cmd 会发现输入框下面有一行小字”以管理员身份运行” 家庭版系统步骤如下(如果是家庭版用户没有组策略是无法像上述的专业版系统一样操作的): 打开注册表编辑器(运行 regedit)，展开注册表到 HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\System ，选择项 System 后，在右侧找到 EnableLUA ，将其值更改为0； 然后重新启动计算机即可。 完成操作后，通过 Win + R 执行命令 cmd 会发现输入框下面有一行小字”以管理员身份运行”]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode用markdown画uml]]></title>
    <url>%2F2019%2F12%2F10%2Fvscode_uml%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[需要安装的vsc插件 Markdown All in One Markdown Preview Enhanced PlantUML 备注： Markdown All in One,VSCode 中支持 Markdown(键盘快捷键、目录、自动预览等) Markdown Preview Enhanced 可以对 Markdown 做增强预览, 比如支持各种绘图等 PlantUML, 一款很强大的，并且可以绘制各种图形的脚本语言。需要安装 java . . . 需要下载的软件 java : 因为oracle下载jdk-8需要登陆了, 很麻烦, 所以建议在百度直接搜索jdk-8下载然后去类似于中关村下载站这种网站去下一个, 然后按照下图配置环境变量 graphviz : 在 http://graphviz.org/下载之后安装， 然后配置环境变量 plantuml.jar : 在 http://plantuml.com/ 下载 plantuml.jar， 然后配置环境变量 重启电脑 配完之后类似如下图, 注意去除环境变量路径首尾多余的空格, 特别是path的最前方, 否则无法正确识别. 备注：Graphviz 是开源图形可视化软件。图形可视化是将结构信息表示为抽象图形和网络的图表的一种方式。它在网络，生物信息学，软件工程，数据库和网页设计，机器学习以及其他技术领域的可视化界面中有重要的应用。 测试一波打开vscode， 新建一个markdown文件， 用下面的代码填入（记得把 ^ 换成 ` )， 然后右键用Markdown Preview Enhanced查看一波1234567891011121314151617181920^^^ pumlclass xxd &#123; - mm4 - mm3 - mm2 + mm1()&#125;class dfa &#123; + con() + con2() + con33()&#125;object halo_ui &#123; + get()&#125;halo_ui &lt;.. xxd: calc aoi tag &amp;&amp; monitor info^^^ 就可以得到下图的效果: 扩展学习画图语法https://plantuml.com/zh/class-diagram]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS必装插件]]></title>
    <url>%2F2019%2F12%2F08%2Fvs_install_exts_vax%2F</url>

    <encrypted>1</encrypted>

    <content type="text"><![CDATA[. . . VS2017必装插件： 备注: Match Margin这个插件是控制高亮颜色的, 颜色是在 Tools-Options-Environment-Fonts And Colors-Match Color 调节的 vs2017的vax破解方法： 链接: https://pan.baidu.com/s/12NA4Tm-M9HjvIZhGP-NOdw 提取码: nxmm google cpp代码风格linter配置由于谷歌风格的检查文件cpplint.py（上去复制下来保存为.py就好啦 ）是Python来运行的，所以首先我们需要一个python~安装的时候记得把路径改好并记下，不能有中文 ，不然就有可能出错。 接下来就是主要步骤了。 打开vs，选择工具-&gt;外部工具-&gt;添加, 然后(下述路径根据自己情况修改哈) 标题: 取个名字, 如 Google Cpp Style Standard 命令: C:\Python27\python.exe 参数: C:\Users\hulinhong\Documents\github\cpplint\cpplint.py --output=vs7 --filter=-build/header_guard,-build/include,-readability/streams $(ItemPath) 初始目录: $(ItemDir) 打勾 使用输出窗口 记住此时Google Cpp Style Standard这个命令从上到下的顺序, 记为 n, 等下会用到 设置完之后保存，就会看到工具菜单下面多了个Google Cpp Style Standard的菜单项。但是这时候 …… 没快捷键你逗我？快捷键就要这样设置：首先 ，还是我们的工具菜单-&gt;选项，弹出的选项对话框中选择环境 -&gt;键盘,敲一下“外部”两个字 然后找到工具.外部命令n, 这个n就是上述的 n vax报错解决如果开启vs的时候报the security key for this program currently stored on your system does not appear to be valid, 则va出问题了, 查閱資料整理解決方案如下： 卸載visual assist 開始-運行-regedit-刪除HKEY_CURRENT_USER/SOFTWARE/WHOLE TOMATO項 該文件夾下面的文件： C:\Users\XXX\AppData\Local\Temp\1489AFE4.TMP，刪除掉 註冊表, 刪除HKEY_CURRENT_USER\Software\Licenses及其下面所有的子項：HKEY_CURRENT_USER\Software\Licenses{K7C0DB872A3F777C0}HKEY_CURRENT_USER\Software\Licenses{I7538681BD5988129}HKEY_CURRENT_USER\Software\Licenses{07538681BD5988129} 如果按照上述步骤还是报错则尝试这篇文章 https://www.jianshu.com/p/aed1fd997eb4 VS2015必装插件： visual Assist (各种跳转,各种..无敌插件 Word Highlight with margin 2015(双击单词高亮所有相同的词 mixEdit (多行编辑 以及类似sublime的ctrl+D Indent Guides (大括号的对齐线 Hot Commands (Ctrl+/来开关注释, 格式化代码 vstools (定位文档在solution的位置, 不用去solution慢慢打开文件翻了 hlsl tools for visual studio (写hlsl语言的时候才需要 vs2015的vax破解方法： 链接: https://pan.baidu.com/s/1isp9dYV963F-zFsfYIG2iQ 提取码: hksf]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>VS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Visual Studio 2017 各版本安装包离线下载、安装全解析]]></title>
    <url>%2F2019%2F12%2F08%2Fvs2017_offline_install%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[中文乱码解决方案问题: 如果你用非简体中文版(比如英文版/繁体中文版)的vs来打开含有简体中文的项目, 则会乱码. 下面是繁体vs的解决方案(英文版vs解决方案也是一样): 现在我来说怎么解决在繁体VS中打开简体VS中开发的项目： 首先要明白VS中项目文件的编码方式，并不是所有文件的编码方式都一样如web.config之类的xml会以UTF-8方式保存，其他文件则和你的vs设定有关。默认情况下vs是以系统语言为准，比如简体系统就会以gb2312存，繁体则以big来存或者以uft-8来存如果是以uft-8来存，则不存在乱码问题。 简体vs中保存带有中文简体的项目，文件一般被保存为gb2312，(不过建议是保存为UTF-8,这样根本不会乱码) 在繁体vs中打开时需要设定，默认情况vs会根据系统里的设置来打开非unicode文件。 gb2312就是非unicode，所以将操作系统设置为支持简体中文即可。 设置方法如下：控制面板–区域语言选项–高级–非unicode程式语言：设置为简体中文(不要勾下面的beta: USE unicode blabla...), 这样重启系统后，用vs打开原来的简体项目，就正常了。 这个设置其实就是告诉操作系统和相关软件，如果语言不是unicode，则以中文PRC即GB2312来显示，且不会影响本来就是unicode的程式和软件，但会影响简体中文以外的非unicode的，比如如果你的繁体软件用的是big5编码(非unicode)，则会乱码。 所以，最好还是用unicode来编码喽。 . . . 原文地址 https://www.ithome.com/html/win10/297093.htm 关于 Visual Studio 2017 各版本安装包离线下载、更新和安装的方法以及通过已下载版本减少下载量的办法 微软最近发布了正式版 Visual Studio 2017 并公开了其下载方式，不过由于 VS2017 采用了新的模块化安装方案，所以微软官方并未提供 ISO 镜像，但是官方提供了如何进行离线下载的方案给需要进行离线安装的用户，只不过都是英文。本文将对官方指南中的一部分进行翻译（这里说一句：翻译的部分就是最基础的离线下载和安装方案，另外，即使如以前微软提供了安装镜像，也仅会包含 Visual Studio 的基本核心组件和部分官方扩展，而微软、安卓和苹果等平台的 SDK、模拟器和第三方扩展功能等会在用户选择后联网下载。而 VS2017 的离线包是包含所有可选项的）。好了，接下来将会分为四部分来叙述，分别是离线下载安装文件、离线安装 VS2017、更新离线文件包以及如何通过已下载版本来减少下载其它版本下载量的方法。 第一部分：离线下载安装文件这里描述是包括所有版本，截图以下载 VS2017 社区版为例： ①登入 VS 官网下载页面，选择需要的版本点击下载，下载页点此进入。 ②下载完成后，打开下载文件所在文件夹，Windows 8.1 及以上版本用户点击资源管理器上的文件 - 打开命令提示符 - 以管理员身份打开命令提示符；Windows7 用户可在该文件夹空白处按住 Shift 键的情况下点击鼠标右键选择 “在此处打开命令窗口”。 ③根据自己下载的 VS2017 版本，在打开的命令提示符窗口输入下面对应的命令并点击回车，等待程序启动即会开始下载（以下命令用于下载完整版离线包，包含全功能以及全语言包，其中红色部分为下载文件存放路径，根据自身情况用户可自行更改） 企业版： vs_enterprise.exe –layoutc:\vs2017offline 专业版： vs_professional.exe –layoutc:\vs2017offline 社区版： vs_community.exe –layoutc:\vs2017offline 如果需要下载单一语言的或其中某几种语言的离线文件，可采用以下命令： 企业版：vs_enterprise.exe –layout c:\vs2017offline –langzh-CN 专业版：vs_professional.exe –layout c:\vs2017offline –langzh-CN 社区版：vs_community.exe –layout c:\vs2017offline –langzh-CN 红色代码为语言参数，这里提供三种语言的参数供大家选择 英语（美国）：en-US 中文：zh-CN（简体），zh-TW（繁体） 日语：ja-JP 如果需要同时下载多种语言，可以在 –lang 后面连续加上多个语言代码参数，用空格间隔开就行，比如—lang en-US zh-CN ja-JP，那么就会同时下载英文、中文和日文语言包。 ④如需更多语言包或者更多安装指令，请参考官方离线安装命令指南：点此进入。 ⑤等待文件下载，不要关闭文件下载窗口，等所有文件下载完成后该窗口会自动关闭，下载过程中不要断电断网，也不要关闭窗口，当然，如果断电断网或者关闭了下载窗口，没关系，输入命令重头来过，还是会继续下载的。 第二部分：离线安装本部分以安装社区版为例，其他版本安装过程一样，只是专业版和企业版需要激活授权。 ①打开刚刚存放离线文件的路径，比如 E:\vs2017offline，然后找到 certificates 文件夹并打开，依次安装该文件夹下的软件证书。双击证书并根据提示往下走就行了，实在懒得弄就一直下一步直到提示导入成功。 ②运行离线根目录下的安装程序，各版本位置如下： 企业版：离线文件存储文件夹 \ vs_Enterprise.exe 专业版：离线文件存储文件夹 \ vs_Professional.exe 社区版：离线文件存储文件夹 \ vs_Community.exe ③安装程序运行后，选择自己需要的模块和功能，确认所需后点击安装即可。顺便吐槽一下，社区版全工作负荷、全组件以及全语言包选择后需要空间 93.85GB，所以大家还是选择自己需要的安装就行了…… 另外，需要 Python 扩展支持的朋友，可以自己下载 Python 安装，或者安装完成后自行到扩展添加，官方安装文件对 Python 扩展的包含据说要等下次更新…… ④等待安装完成即可使用了。 第三部分：更新离线文件包这部分比较简单，步骤如下： ①当微软提示有版本更新后，到官网下载最新的在线安装执行文件，下载地址：点此进入。 ②按步骤执行本文第一部分下载离线安装文件的步骤，但是必须注意，–layout 后的下载文件夹路径必须是旧版本离线文件存储的位置（如果你之前下载好没移动过那就是之前的下载路径），执行命令后安装程序会扫描已有文件并下载更新文件和新增文件。 ③更新完成后可再次执行离线文件夹根目录下的安装程序进行软件更新，或者 VS 中直接检查更新，更新新版本理论上无需重新导入证书除非有新证书被下载。 第四部分：通过已下载版本减少其他版本离线下载量这部分也很简单，原理就是社区版、专业版和企业版有大多数组件其实是相同，如果用户想离线下载所有版本，那么没有那个必要，企业版包含了其它所有版本的组件，所以理论上用户只用下载企业版即可，安装时是可选择安装社区版、专业版或者企业版的。当然目前前面所述只是理论，毕竟我没试验，当然以下方法可以百分百保证需要的用户用最少的下载量完成下载所有版本的 VS2017。 如果你已经下载好了其中一版，均可进行如下操作： ①创建两个新文件夹，根据你自己的爱好重命名，最好是英文名。 ②将已经下好的版本的所有文件都分别复制到这两个新文件夹中，并删除由之前下载程序下载在根目录下的六个文件。 ③下载另外两个版本的安装程序，接下来就是重复第一部分的步骤，但是命令行命令—layout 后的路径参数根据版本分别设置为你刚刚新建的那两个文件夹。 ④回车执行命令，程序会扫描和检查已经下载的组件，并现在自己版本对应的缺少的以及匹配的组件，不过这部分大小不会超过 1GB，准确来说可能只有几十兆左右…… ⑤等待完成，反正不一会儿就好了，然后就可以收藏三个版本的离线包了。 ⑥至于更新嘛，参见第三部分。 第五部分：附加这部分呢是附加的一部分，信息是完全下载（包含全功能全语言包）的社区版和企业版的详细文件信息，以及他们的差集文件夹信息；最大的是企业版，最小的是企业版和社区版文件夹差集的信息。最后就是，各版本离线包里面至少含以下关键字的文件夹中的内容是完全一模一样的，就是：SDK、.NET、Xamarin、Unity、Cocos、Unreal、Linux、Mac、iOS、Android、emulator…… 包含这些关键词的文件夹，大小合计应该超过 16GB 了…… Cpp/Py混合调试所需勾选 Python 本机开发工具 先勾上 Python 开发, 然后在右侧勾上 “Python 本机开发工具” 2.ATL 库 类似的, 也选中 ATL 库 其他需要的东西应该默认都带着了. 开启Cpp/Py混合调试模式首先, 你需要重新 cmake 一下引擎 (目标改为 vs 2017), 并重新 build 一下. 与 12 升 15 不同, 这一步应该不会碰到什么问题. 然后右击 client 工程, 选属性. 在调试页面中, 将 debugger 改为 “Python/Native Debugging” 此外, 还需要将 Environment 设为游戏脚本的路径 三. 配置符号文件 Python27.pdb 先看一下引擎 link 的是哪一个 Python27, 然后看看有没有对应的 pdb. 比如我这里用的是 managed3rdparty\python\lib14\python27.lib, 并且这个目录下刚好有对应的 pdb. 那么只需要在调试 - 选项 - 符号中, 添加这个目录即可 如果找不到对应的 pdb, 那就自己 build 一下 python 吧, 然后也一样设置一下符号的位置. 也可以在启动游戏后在模块窗口里手动加载: 四. 验证 启动游戏后, 如果在输出窗口能看到加载 python 脚本的 log, 就表示配置成功了. 此时既可以调试 C++ 代码, 也可以调试 python 代码, 并且都能看到两边完整的栈. 为了调试方便, 可以把 python 脚本目录作为工程添加进来 要注意的是, 引擎里已经有个工程叫 “script” 了, 添加的时候需要改个名字, 不要重名了.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>VS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSR client]]></title>
    <url>%2F2019%2F12%2F08%2Fssr_client%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[. . . 本文为选摘文章，版权分别归 神代綺凜 &amp; HYBRID &amp; Mark 所有，转载请注明出处！ 本文链接：https://www.quchao.net/ShadowsocksR.html 友情提示：如果博客部分链接出现 404，请留言或者联系博主修复。 前言更新：给自建酸酸 / 酸酸乳的朋友们的一些忠告及建议 本篇说明仅供交流与学习使用，请勿作出任何违反国家法律的行为。 本教程包含 Windows、Android、iOS、MacOS 端的酸酸乳客户端的功能介绍和使用方法的详细介绍以及常见问题解决。 教程比较长，很长，长的不行，因此请按需查看，灵活运用右侧目录跳转至相应章节，否则可能会引起包括但不限于头晕眼花、心烦意乱、精神恍惚、直接点击右上角等不适现象。 实际上本文的定位大概并不是教程，而是说明书……建议萌新们先自己摸索着用用，有不懂的地方再按照目录查阅相关功能的说明解释。 如果还是有不懂的问题可以在评论中提出。 友情提示请仔细阅读相关信息 不要问我什么****cloud为什么不能用、*speed怎么用不了，这类问题请不要找我，去找他们，我与这些团体没有任何关系，这篇文章也仅仅是一篇单纯针对酸酸（乳）的说明书而已。 最近大家也都知道的，各种形式都很严峻，大批 VPS 被 Q，vu、瓦工、良心 cc 的 IP 都被 Q 的差不多了，而且也有很多人在评论询问我为什么自己的 VPS 总是被 Q。 觉得很有必要在这里说一些自建酸酸 / 酸酸乳所需要注意的事情、能提升 VPS 存活率的建议。 1. 不要认为加密 / 协议 / 混淆什么的随便都行酸酸1、加密请使用 AEAD 加密，包括以下几个 aes-128-gcm aes-192-gcm aes-256-gcm chacha20-ietf-poly1305 xchacha20-ietf-poly1305 对于移动设备来说，ARM v8 以后的 CPU 使用aes-gcm的效率要高于chacha20，因此更推荐使用aes-256-gcm 2、混淆请使用 plain，即不使用混淆插件，或者使用 http_simple 酸酸乳 1 加密请使用none 2 协议请使用chain_a 3 混淆请使用plain或者http_simple 能用plain没问题就用plain，当在plain情况下你察觉到网络有异样 *，且你了解http_simple混淆参数的用途，再酌情使用http_simple 2. （科普）这些参数的意义是什么有很多萌新同学直接拿一键脚本搭建的，脚本要求设置参数的时候也许萌新们就直接一路回车默认过去或者随便选了，殊不知这可能就正是 VPS 被 Q 的原因 混淆为什么不选tls这是有可能导致被 Q 的一个首要原因，因此我也放到第一位来讲，也许细心的同学们也发现了上述推荐配置中的混淆这一行都加粗了，是的，这非常重要 前段时间我与其他人讨论的时候，就发现了一个共同点，那就是被 Q 的人大多数都使用了tls1.2_ticket_auth混淆；而一些混淆plain的人，即使还在使用老旧甚至过时的协议，却安然无恙 很有理由猜测 GFW 已经掌握了 tls 混淆的特征 让我更确信了我的猜测的是，后来询问我被 Q 相关问题的人，大多数都用的是搬瓦工后台自带酸酸乳，因为我从没有用过搬瓦工，我询问了他们后才知道，搬瓦工的酸酸乳是强制tls1.2_ticket_auth混淆，嗯，真不愧是 GFW 最佳合作伙伴（而且搬瓦工自带酸酸居然没有 AEAD 加密） 后来，推特上也有人指出 酸酸乳的 tls 凭据复用已经成为安全问题 不要用了 而且，就 tls 混淆原本的用途来说，tls 混淆只是为了突破部分地区的网络环境才有的 QoS 限制，一般情况下根本不需要使用破娃酱也在文档里说的很明白，一切因使用混淆而产生的看似是网络加速了的效果都是因为绕过了限制，混淆实际上会减慢你的网络速度 并且，如果你并不明白 http(s) 协议的具体细节，没有这方面的计算机网络知识，那么也不要轻易使用http_simple之类的混淆；如果没有必要，也不要使用80和443端口；这些在 GFW 眼里很可能会成为一种明显的特征，会成为 “为什么我的 VPS 好好的就被 Q 了” 的直接原因 加密这块到底是怎么回事简单的来说，我们若干年前使用的非 AEAD 加密，都存在被主动探测到的风险（这一块如果感兴趣想了解，可以自行谷歌 AEAD 加密的相关科普博文） 因此，如果是酸酸，强烈推荐使用之前提到的那 5 种 AEAD 加密，为了防止今后（可能的）来自 GFW 的主动探测 而酸酸乳，虽然目前并没有使用到 AEAD 加密，但是破娃酱在设计协议的时候已经考虑到了主动探测问题并且针对这块进行了设计，因此目前来说还是相对安全的，前提是你使用的是chain_a或auth_aes128_md5或auth_aes128_sha1协议 对于酸酸乳，chain_a是目前最佳的协议；chain_b虽然说更难以被识别，但是仍是一个测试版协议，并且实际使用发现丢包现象莫名十分严重，并不能用；至于酸酸乳乳那些chain_c/d/e/f，可以看看这里 之前提到的推荐配置中，酸酸乳的加密为none的原因是，auth_chain系列协议已经自带了RC4加密，针对 UDP 部分也有加密及长度混淆，因此一般情况下不需要再进行额外的加密；如果你觉得RC4加密有安全隐患，再套一层其他加密也可 是为了帮助新入主机坑的朋友们少走点弯路 CloudCone 有很多优惠套餐，平均1.5刀/月主要优点是便宜实惠，亚洲优化线路（非 CN2，但是在亚洲优化线路中已经算很稳的了），被 Q 换 IP 一刀一次十分便宜，而且年付套餐可以删除退钱到余额，余额甚至还能提现（有比这更良心的吗这家的国人客户也占大多数，而且很明显是想做好中国市场的，支持支付宝付款，照顾得很周到 搬瓦工 CN2线路，平均2.4刀/月如果你还是觉得 CloudCone 的线路稳定性和延迟不满足你的需求可以考虑搬瓦工的 CN2，是真的稳（不像某 HostMyBytes 便宜没好货），而且被 Q 可以换 IP（好像是两刀一次），支持支付宝付款 Vultr 5刀/月如果需要日本 VPS 这家我认为是考虑性价比和使用体验的首选，在几个月前曾经优化过日本线路，目前走 ntt，听说晚高峰会爆炸但是我自己实际用起来并没什么感觉需要注意的一点是 Vultr 的 IP 都是被 P 站屏蔽了的，因此不能用来上 P 站，不过其实 P 站域名只是受到了 DNS 污染，自行加 Host 或者自行搭建无污染 DNS 都可以解决这个问题手游玩家注意，Vultr 日本过不去大部分稍严的日本限定 IP 认证，如果有这方面需求请看下面的 AWS对于电信联通用户只建议选日本机房，移动可以考虑新加坡虽然有很多开出来的 IP 是被 Q 的，因为是小时付机子所以可以不断的创建然后删除来刷，一次成本仅 0.01 刀，放心刷 AWS Lightsail 免费或5刀/月建议用 Github 学生包来薅羊毛，成功通过亚马逊学生认证可以得到 150 刀有效期一年的抵扣金额，机子随你开，不过注册亚马逊前请准备好信用卡，国内外皆可不限制，甚至可以用虚拟信用卡，如果是 0 额度的信用卡，请确保卡内余额 &gt;=1 刀也是和 Vultr 一样电信联通只建议日本，线路是 Vultr 优化前的线路，但是质量也并不差，移动可以考虑新加坡IP 基本没被墙，所以可以放心开不需要刷，就算要刷也是小时付，而且貌似现在 AWS Lightsail 是动态 IP，重启会换，那更不用怕了 Azure(微软) 只建议免费同样是学生认证福利，网上有很多教程可以撸到，一年有效期 100 刀，可以免费开到国内直连的香港，但是可用流量很少只有 15G（只计 Out），就算 100 刀用完了也可以继续使用免费的学生套餐机，流量 10G（只计 Out）缺点很明显就是流量少了点，而且微软家 Hyper-V 虚拟化再加上那种比较专业的资源管理面板（参考亚马逊 EC2）对新手来说可能会很难用…… 而且随时有翻车可能 阿里云国际 注册要外国信用卡，建议代购如果你想要稳定的香港 VPS，我只建议你上阿里云国际，找代购大概 9 刀 / 月，或者买礼金号 把 C# 版 SSR 下下来，把压缩包里面的所有文件解压到一个文件夹中。 请不要只解压 exe 文件，不然可能会发生一些科学无法解释的现象，比如说硬盘中一个叫 “马克思主义” 或者 “学习资料” 的文件夹消失了。——破娃酱 然后你会发现有两个 exe 文件，后缀为 “dotnet2.0” 和 “dotnet4.0”的（sig 文件是 MD5 验证文件，不用理会）。如果你的电脑上已经安装了 donet4.0（全名 NET Framework 4.0），那么就可以使用 “donet4.0” 后缀的版本，否则请去百度 “donet4.0” 并安装，再使用。如果是 XP 用户，请直接使用 “dotnet2.0” 版本。 打开程序之后先别着急，第一步先找到 SSR 的任务栏图标（纸飞机样子的图标，下文均简称为 “小飞机”），右击小飞机 - 服务器订阅 - SSR 服务器订阅设置，点击 “delete” 将自带订阅删除（自带订阅早已失效，已经不需要了）。 接着，你就可以按照你的 SSR 服务商的提示添加服务器了。双击小飞机图标可以直接打开 “编辑服务器” 界面以手动编辑服务器。在你添加完自己的服务器之后，要记得将 SSR 自带的示例配置删掉，并切换至你要使用的配置文件，因为那个也是没有用的。部分萌新可能还会在添加完服务器之后忘记切换到自己添加的那个配置就开始使用，导致无法使用。 功能、设置说明右击小飞机，你能看到 SSR 所有的功能选项，以下是各项功能及设置的介绍。 系统代理模式系统代理模式有四种模式可以选择： 直连模式会关闭系统 HTTP 代理，你的所有 HTTP 上网流量都不会通过 SSR 代理，在此模式下你只能使用 Socks5 代理方式连接 SSR 代理（详见后续的进阶使用）。 PAC 模式会修改系统 IE 代理，使用 PAC 文件控制代理。PAC 文件包含了规则列表，可以控制哪些流量走 SSR，哪些不走（例如国内流量直连，国外走代理），做到智能代理。但是实际上此功能已经可以被 “代理规则” 设置完全代替（除非你一定要用 gfwlist），因此一般不用这一模式。PAC 文件为 SSR 根目录下的pac.txt。 全局模式会开启系统 HTTP 代理，你的所有 HTTP 上网流量将会通过 SSR 代理。注意：仅能代理 HTTP 流量，即浏览网页的流量——例如浏览器浏览网页，或者某些应用程序的应用内网页（比如 QQ 的群文件、群公告这些就是），或者某些比较奇葩的使用 HTTP 方式进行通信的程序（比如 Steam 版的影之诗）。如果要代理应用程序，请详见 Windows -SSTap 教程。 保持当前状态不修改顾名思义，不会对你目前的系统 HTTP 代理状态进行任何的修改。 当你退出 SSR 后，系统 HTTP 代理会自动被恢复至原有状态；开启 SSR 后，同样的，系统 HTTP 代理会被设置成你所设定的系统代理模式状态。 PAC此菜单中的选项均为对 SSR 的 PAC 文件进行操作的选项，如果你不使用 “PAC 模式” 代理，那么这一节你可以直接跳过。 不过实际上也没什么好讲的，每个选项会执行什么操作、有什么效果，都在菜单中写的清清楚楚。 代理规则代理规则指的是，对于所有通过系统 HTTP 代理或者 SOCKS5 代理被发送至 SSR 的流量，按照你设置的规则进行代理，即智能代理。 实际上这个设置项也没有什么可以说明的内容，每个选项也写的很清楚。这里稍微解释下某些名词及选项： 绕过 xxx顾名思义，绕过就是不走代理，例如选择项中的 “绕过局域网“就是当你浏览局域网网页的时候不会走 SSR 代理（最常见的例如学校、公司的内部网网页），” 绕过大陆“就是所有国内大陆的流量都不会通过 SSR 代理。 绕过非大陆带有这个的一项实际上是给从国外翻回来的人用的，我们这些翻出去的并用不到。 用户自定义萌新可以直接无视这一项。自定义文件为 SSR 根目录下的user-rule.txt，你可以在这一文件中自己定义代理规则，格式详见：https://adblockplus.org/en/filter-cheatsheet 全局顾名思义，所有被发送至 SSR 的流量都会走代理。 日常使用我推荐使用 “绕过局域网和大陆” 的选项 服务器在这里你可以切换你已有的服务器配置，在你第一次使用的时候别忘了先切换至你自己添加的服务器配置。 编辑服务器直接双击小飞机也可以打开编辑窗口。 从文件导入服务器可以导入 SSR 服务商提供的服务器配置文件。 服务器连接统计直接鼠标中键单机小飞机也可以打开这一窗口，会显示你目前 SSR 的所有服务器的统计信息。 在这一界面，双击某个条目的服务器可以直接切换至该服务器；单击某一条目的 “连接” 可以断开当前条目的所有连接；双击某一条目的 “开关” 可以指定你在设置“服务器负载均衡”时是否使用这一服务器，而双击某一个 “组”(Group) 可以批量开关这一组；双击某一条目其他的列可以清零这一统计信息。 服务器订阅此处可以设置服务器订阅，初次使用请先删掉自带的服务器订阅（已经失效）。 如果你没有从 SSR 提供商处获得 SSR 服务器订阅地址，那么可以直接跳过本节。 SSR 服务器订阅设置点击 “ADD”，然后在右侧填入你的 SSR 服务器订阅地址，即可添加一条服务器订阅，然后直接点击确定即可。如果你勾选了左下角的 “自动更新”，在每次 SSR 启动的时候都会自动更新订阅，一般情况下无需选择。 更新服务器订阅将会从已经设置的订阅地址中获取配置信息自动添加 / 更新至服务器配置中，并且此操作会通过你当前的 SSR 代理进行！如果你目前的 SSR 代理不是可用的代理，那么将会更新失败！因此建议选择 “更新服务器订阅（不通过代理）”。一般只有 SSR 服务器订阅地址不能在墙内访问而你有可以使用的 SSR 代理配置的时候才需要使用 SSR 代理更新订阅。 服务器负载均衡这是个基本不会用到的功能。选中以后，在每次有流量通过 SSR 代理时，会自动在没有被 “关掉” 的代理中选择一个来使用（怎么叫“关掉”？请看服务器 - 服务器连接统计）。 如果想只在某一组服务器中切换，或者依据一定的条件来切换服务器，请右击小飞机 - 选项设置，然后看 “负载均衡” 的设置。 选项设置 二级（前置）代理使用代理来连接代理，就像盗梦空间那样。没有特殊需求就不用管这一项。 负载均衡没什么好说的，每个设置有什么用都写的简单明了。 本地代理这是个很重要的设置项。本地代理实际上就是在本机 (127.0.0.1) 开启一个 SOCKS5 代理端口，端口为 “本地端口” 设置项中的端口号。这一功能有什么用？请见后续进阶使用。如果勾选了 “允许来自局域网的连接”，则与你在同一局域网的其他设备一可以通过这一 SOCKS5 代理来连接，服务器地址为你的局域网 IP 地址。用户名和密码选填，如果你设置了，那么当你使用 SOCKS5 代理的时候就需要这一用户名密码了。 右下角的设置没啥好说的（。 端口设置可以设置端口转发以及对于不同的服务器配置开启不同的本地代理 SOCKS5 端口等等。萌新直接不用管。 二维码扫描可以扫描屏幕上的 SSR 配置信息二维码并导入服务器配置中。二维码通常由你的 SSR 供应商提供。 剪贴板批量导入 ssr:// 链接没啥好讲的，如果你的 SSR 供应商有提供批量 SSR 服务器配置链接，你就可以复制之后通过这里导入。 帮助由于众所周知的原因，破娃酱已经删除了 Github 上的所有 SSR 项目，并且停止了对 SSR 的开发及维护，因此这里的什么 “检查更新” 啊“打开 Wiki 文档”啊都是已经没用了的选项。 https://github.com/shadowsocksr-backup/shadowsocks-rss/wiki以上是 Wiki 文档备份 显示日志显示 SSR 通信日志。 自定义生成二维码这就是一个生成二维码的小工具而已。 设置客户端密码为你的 SSR 客户端设置密码保护。 常见问题使用了代理之后查询自己的 IP 发现还是国内的你大概是直接去百度 “IP” 或者使用了国内的查询 IP 的服务，而且你的 SSR 代理模式设置的是“绕过大陆”。 请将代理模式改为 “全局” 后再试，或者访问国外 IP 查询服务网站例如 https://whatismyipaddress.com 连上了代理但是无法上谷歌、推特等 1 按下键盘组合键 “Win + R” 调出运行窗口 2 输入cmd然后回车，这时会打开 “命令提示符” 3 输入ipconfig /flushdns然后回车 4 现在应该可以正常访问了 这是 DNS 污染所致，想更详细的了解有关于 DNS 污染的知识请看：DNS 污染是什么 打开 SSR 提示 xxxx 端口已被占用你的 SSR 的 SOCKS5 端口被系统中的其他程序占用了，去更改 SSR 选项设置 中的本地端口吧。推荐改为 1025~65535 之间的端口（1024 之前的端口有系统保留端口，防止误占用）。 无法更新 SSR 服务器订阅在关闭代理的情况下，直接把你的订阅网址扔到浏览器里，看看是否能正常访问，正常情况下会直接显示一串毫无规律的英文数字或者弹出一个文件下载框。 能√解决方法：选择 “更新 SSR 服务器订阅（不通过代理）”问题分析：你可能是在还没有添加有效服务器配置的时候直接选择了 “更新 SSR 服务器订阅”。如果不选含有 “（不通过代理）” 的那一项，那么 SSR 默认将会通过代理来更新订阅，然而你并没有有效的服务器配置，因此自然就无法更新了。 不能 ×那么检查一下你是不是手打打错了或者复制少了一部分网址……如果你很确信订阅地址没有错，如果你有可用的代理配置的话，就尝试通过代理更新订阅。如果还是不行，请找到你的 SSR 代理商询问原因。 正确添加了配置，但无法正常使用代理上网逐步排查问题： 你可能根本就连不上你的 ss 代理服务器打开命令提示符（Win+R 打开 “运行”，输入cmd然后回车），接着输入 1ping 你的SSR服务器地址 然后回车，查看是否能 Ping 通，否则说明你目前的网络情况根本无法连接代理。 你的 SSR 配置可能不正确如果你是手动填写的配置而不是通过 SSR 提供商的一键配置来添加，那可能会出现这种情况，请检查一下你的配置是否正确。 SSR 的系统代理模式和代理规则是否正确通常来讲推荐萌新的设置是，系统代理模式选择 “全局模式”，代理规则选择 “绕过局域网和大陆”。 浏览器可能没有使用系统代理如果你使用了 “SwitchyOmega” 并且确定你配置正确了并且启用了情景模式，那么可以跳过此项。如果你是比如 360 极速这种国产双核 / 多核浏览器，那么应该会有个 “代理服务器” 的设置项：检查一下这项，选择 “使用 IE 代理” 或者“使用系统代理”。 会不会是你的 SSR 代理到期了或者没流量了 你买的可能是个假代理 在 Chrome 浏览器中配合 SwitchyOmega 使用为什么要用 SwitchyOmega如果你想全局代理浏览器，而又不想设置系统 HTTP 代理（因为系统 HTTP 代理会代理到所有的 HTTP 连接，比如 QQ 的群公告、群文件、群相册），又或者你想强制让某些网站走 / 不走代理，则你可以在 Chrome 浏览器中安装 SwitchyOmega 插件。 另外，SwitchyOmega 还支持很多人性化的进阶设置，比较深层的用法具体就请自己研究吧，多用用百度，这里主要说明一下基础配置及用法。 然后咋整 1 右击 SwitchyOmega 的图标 - 选项，进入设置界面，点击 “添加情景模式”，填入一个名称（随便写），下面要选中 “代理服务器”，然后点击 “创建”。 2 然后在你新建的情景模式中，代理协议选择SOCKS5，代理服务器填写127.0.0.1（环回地址），代理端口填写 SSR 选项设置中的本地端口，然后点击左下角的 “应用选项” 就可以保存配置了。 再次强调一下，本地端口务必要填 SSR 选项设置中的本地端口，而不是现在这个示例图中的 1088！ 3 点击 SwitchyOmega 的图标，选择你刚才添加并设置好的那个情景模式，这样你的浏览器的流量都会通过 SSR 了，接着你只需设置 SSR 的系统代理模式为直连模式（如果你想全局翻 *）或者绕过局域网及大陆等你需要的模式，然后就可以开始愉快的上网了。 每当你不想使用 SSR 代理了以后，一定要记得将情景模式选回直接连接，不然当你关掉 SSR 客户端以后，浏览器将会上不了网。 SSTap 全称 SOCKSTap, 是一款利用虚拟网卡技术在网络层实现的代理工具。SSTap 能在网络层拦截所有连接并转发给 HTTP, SOCKS4/5, SHADOWSOCKS(R) 代理。而无需对被代理的应用程序做任何修改或设置。它能同时转发 TCP, UDP 数据包。它非常适合于游戏玩家使用。 SSTap 是一个为代理游戏而生的工具，能代理辣鸡 Profixier 代理不到的应用，并且简单易用，不像 Profixier 或者 Super Socks5Cap 那样使用复杂或者需要付费。 它的性质与 L2TP 这类系统全局代理很像，但是可以自定义路由以绕过大陆 IP（应用自带有这些规则），这点是十分赞的。 这是一个免费的工具，仅有应用内常驻非弹出式广告，并不会影响使用体验。 安装https://www.sockscap64.com/sstap/去上方的这个官网下载安装即可。第一次安装的话会弹出提示安装一个虚拟网卡驱动，同意即可。 使用 添加代理和 SSR 的添加方式基本相同，点击主界面那个原谅色的 “+”，会有三种添加方式： SOCKS5如果你是使用 SS/SSR 代理，那么建议直接使用下面的 SS/SSR 代理添加，不推荐使用 SOCKS5 去连接 SS/SSR 的 SOCKS5 本地端口，因为这样的话你还需要额外将 SS/SSR 的服务器 IP 添加到代理排除列表中，十分麻烦。如果你是在路由器里挂的仅 ss-local 那么就当然可以这样做。 SS/SSR手动输入你的 SS/SSR 服务器的参数。SSR 支持目前最新的chain_a和chain_b协议。 通过 SS/SSR 链接批量添加（推荐）十分懒人，复制链接进去一键添加。 SS/SSR 链接一般由你的 SS/SSR 商提供；或者你也可以直接在 SS/SSR 客户端中查看配置，然后复制配置中的 SS/SSR 链接。 通过 SS/SSR 订阅添加（强烈推荐）详见设置 - SSR 订阅 “+” 右边的三个按钮依次是删除代理、编辑代理、测试延迟。 模式 这是 SSTap 的一个核心功能之一。内置了数种规则，以上图中的规则除了正在选中的那条是我自己加的以外都是内置含有的规则。 校园网用户注意！！！由于校园网大多数是采用程序认证的方式上网，如果局域网流量也被代理，那么你的认证将会断开而导致无法上网！ 下面是如何修改规则让 SSTap 绕过局域网 IP 进行代理的步骤： 1 右击另存为 这个文件 2 右击 SSTap 快捷方式，属性，打开文件所在位置实际上就是打开 SSTap 程序所在目录…… 3 进入rules文件夹，将压缩包里的文件放进去 4 退出 SSTap 并重新进入，这时候应该就能看到多出了一个 “不代理中国和局域网 IP” 的模式了，使用这个模式连接代理即可。 本篇文章出处 https://www.quchao.net 转载请注明来源 程序菜单右击 SSTap 的任务栏图标或者点击主界面右下方的齿轮图标即可出现程序菜单。 设置 此处可以设置关闭 UDP 转发，自定义 DNS 以及设置开机启动等。 除非你知道自己在干什么，否则请不要更改关闭 UDP 转发以及手动更改 DNS。 配置智能 DNS当设置里的 DNS 为 “智能分流 DNS” 时，点击右侧的齿轮图标可以进入这个设置。 你可以分别设置国内域名使用哪个 DNS 来解析，国外的域名使用哪个 DNS 来解析以防止 DNS 污染，效果类似于 ChinaDNS。 事件管理 也不需要做什么说明了，一看就知道是做什么的了。 附加路由管理 可以设置指定 IP 的连接是直连还是走代理。只能单个单个 IP 设置，不能使用 CIDR 格式。 挺希望能支持 CIDR 格式，这样的话绕过局域网就可以直接在这里实现了，不需要改规则那么麻烦……（其实也不麻烦） SSR 订阅 和 Windows 客户端的 SSR 订阅一个用法。 频率即订阅更新频率，“每次” 指的是每次打开 SSTap 都更新。实际上正常使用的话根本不需要这么高的更新频率…… 要是有不自动更新的选项就好了。 OBFS 默认参数在更新订阅后自动帮你把每个配置的混淆参数改成这个。如果你不知道混淆以及混淆参数是什么那就无视掉这个设置项。 代理选择一个代理，并通过这个代理来更新订阅。如果你要设置此项，那么你必须已经至少添加了一个可用的代理配置。 常见问题Win10 开启代理后 CPU 占用很高这是由一个 Win10 的一个网络检测服务所致，SSTap 虚拟网卡创建的连接被误认为是受限网络从而触发了检测。（大概） 无需理会，数分钟以后会恢复正常。 准备工作不用多说，下载安卓的 SSR 客户端即可。 配置管理界面打开程序以后，点击上方的 “shadowsocks R ▼” 字样即可进入配置管理界面。 添加配置点击右下角的 “+” 之后就会出现图中所示的 5 种配置添加方式，根据情况自己选择即可，这部分就不需要过多解释了，基础配置之类的与 Windows 版无异。 删除配置能加当然就能删，左滑或者右滑一条配置即可删除该配置。 误删了怎么办？别担心，你删除之后下方会提示你可以点击 “撤销” 来撤销刚才的删除操作。 每条配置下方的文字以及右侧的按钮 下方的文字从左到右依次表示 累积上行流量、累积下载流量、节点延迟、服务器组名（如果有）。 闪电图标点击即可以检测此节点的延迟（ping）。 分享按钮能干什么我就不用说了，点进去一看就懂。 右上角的三个图标 一个闪电加上一个 Aping 列表中的所有服务器。 三道杠基于 ping 自动由低到高排序。 那个看上去像是复制的按钮对，就是复制来的。点一下就会把你目前所有配置的 ssr 链接复制到剪贴板中，然后你可以发给其他设备或者朋友什么的。 主界面点击右上角的小飞机图标即可开启代理。第一次连接会弹出一个请求 V*N 连接权限的窗口，允许即可。 服务器设置你可以改当前配置的配置（听起来有点怪怪的）。 功能设置 路由参见 Windows 版的代理规则。 自定义 ACL 文件（少用）类似于 Windows 版中的 PAC 代理模式，能根据文件中的规则智能翻 *。如果需要使用此模式，选中后会让你填写 ACL 文件地址，这个地址通常由 SSR 供应商给出。 IPv6 路由如果代理服务器支持 IPv6，那么开启此选项之后你可以访问 IPv6 网站（即使你在 IPv4 环境下）。 分应用代理开启后可以指定某几个应用的流量走代理。 UDP 转发在手机上很少需要使用。如果你要代理的外服手游有使用到 UDP 通信那么就需要开启（例如 BangDream 的多人联机）。 China DNS若你路由选了含有 “绕过大陆” 的选项，那么将使用 China DNS 来解析大陆域名。 DNS默认解析所使用的 DNS。 其他 自动连接开机就会自动连。 TCP Fast Open几乎不会用到的功能。 NAT 模式使用旧的、需要 ROOT 的代理模式。用于安卓 4.0 以下的系统，或者当你使用 V*N 模式无法正常连接时再考虑使用。 重置就是重置。当不小心玩坏了的时候可以试一下。 ACL 文件更新当你的 “路由” 选的是“自定义 ACL 文件时”，点击此项可以更新 ACL 文件。 前置代理参见 Windows 版的前置代理。 常见问题连上了代理但是无法上谷歌、推特等如果你确定你的代理是可用的，那么开关一次飞行模式即可解决 99% 的这类问题。 这是 DNS 污染所致，想更详细的了解有关于 DNS 污染的知识请看：DNS 污染是什么 网络正常但是无法正常使用代理具体表现为，能 Ping 通代理服务器，但是连上以后就是无法通过代理上网。 请依次尝试以下步骤看看是否能解决你的问题： 如果你开启了 UDP 转发，那么尝试关掉它 尝试使用 NAT 模式（需要 Root 权限） 如果你是动手能力稍差的对 iOS 不是很了解萌新，那可能在 iOS 上使用 SSR 并不是那么容易的事。并不是难以进行配置，而是难以购买代理工具…… 不过没关系，谁都有第一次，这里只给一句忠告：学会百度，善用百度 准备工作首先你需要下载一个支持 SS/SSR 的代理工具由于你懂的的原因，目前大多数的翻 工具在国区（中国大陆）Store 都*已被下架，因此你需要注册或者找你认识的人借一个其他区的 Apple 帐号来下载。 推荐注册 美区 / 日区 / 香港 / 台湾 的 Apple ID，其中美区和日区都是要挂对应地理位置的代理才能注册的，香港和台湾的应该是不需要的吧，大概…… 我也没有试过去注册过其他区的，因为像我这种玩日服手游的人手一个日区帐号是再正常不过的事了…… 怎么注册其他区的 Apple ID？去百度吧少年，百度~和某不存在的谷歌~是你一生的老师。 可是我不会百度啊…… 能不能直接给一篇教程？23333，我早就想到了这种情况了，所以，帮你们最后一次 注册美区 Apple ID 教程 注册日区 Apple ID 教程 注册香港 Apple ID 教程 注册台湾 Apple ID 教程 有账号了，然后呢推荐去下载 Shadowrocket，本教程后续也只以 Shadowrocket 为例。 除了 Shadowrocket 以外，你还可以使用 mume 或者 Potatso Lite 等支持 SSR 的代理工具，添加配置的本质方法都大同小异。 由于这些软件都为付费软件（有时候能逮到限免，比如 mume 和 Potatso Lite 就曾经限免过，我也是在这时候白嫖了一份来备用），而且在别的区并不能用微信支付宝之类的，如果你有 Store 支持的付款方式（信用卡、PayPal 等）那是最好不过的了，否则就需要去某宝购买对应商店区的礼品卡（即充值卡），充值进 Apple 账户之后再购买。 正式开搞有了 Shadowrocket 之后就可以开搞了，以下是 Shadowrocket 的正式教程。 打开 Shadowrocket，下方选项卡有四个标签，这里将分标签讲解 Shadowrocket 的功能。 主页 未连接emmmmmm，这个开关应该不用讲吧……第一次连接的时候会弹出是否允许创建 V*N 连接的窗口，允许，然后按个指纹 / 输个密码即可。 全局路由有 4 个选项： 配置（推荐）如果你没有自己改变过配置文件的话，那么这个选项就相当于 “绕过局域网及大陆” 代理即全局代理 直连直连，不主动代理任何流量。可以用于 “自己的手机不想翻 ，但是想把代理共享给同一局域网内需要翻 的设备” 的情况基本是不会用到的 场景根据下方 设置 - 场景 中的设置，在自己设定的情况下连接代理时会无视首页中的 “全局路由” 和“服务器”设置而使用你事先规定好的设置。 下面 “设置” 中的两个设置项就不多做介绍了，属于那种 “萌新基本用不上，要用的人自然会用” 的功能。 添加节点自动添加 点击链接添加根据你 SSR 商提供的 SSR 链接，在 Safari 中打开时会自动跳转到 Shadowrocket 中进行添加。 扫码添加如果你的 SSR 商有提供节点配置二维码的话，点击左上角的扫一扫。 手动添加点击右上角的 “+”，进入配置添加界面，点击类型，然后选择 “ShadowsocksR”，接着填入 SSR 商提供的节点信息，然后完成即可。 以下是目前（2018-06-12） Shadowrocket 支持的代理协议： 通过订阅添加（推荐）如果你的 SSR 商有提供 SSR 订阅链接的话，那是最好的。跟手动添加的方法一样，但是类型选择 “Subscribe”，然后填入订阅地址，然后完成即可。 选择节点 1 点击一个节点，前面出现了一个小橙点即代表你选中了这个节点。 2 点击 “选择节点” 右侧的 “···” 可以进入批量删除、排序模式。 3 左滑一个节点，可以进行 3 种操作： 复制复制出一份完全相同的配置供你修改（备注会自动重命名） 二维码展示这个服务器配置的二维码以供分享 删除节点：mmp 4 左滑一个订阅，也有 3 种操作： 更新更新订阅 二维码展示这个订阅的链接的二维码 删除订阅：mmp 5 点击一个配置右侧的 “i”，可以编辑这个配置。 ！！！进阶警告！！！以上即为 Shadowrocket 的基础使用说明，下面的是进阶功能使用教程。这部分比较复杂，但是是 Shadowrocket 的强大之处，不想折腾的萌新可以直接跳过，有兴趣的可以看看。 配置 前三项没什么好说的。 测试规则用于测试你当前选择的配置文件，输入域名或 IP 之后回车便会返回这个域名或 IP 所应用的那条规则，用于测试规则正确性。 本地文件此处列出了本机上所有的配置文件。默认会有default.conf配置文件，这个规则为 GFWList + 绕过大陆 + 常用广告域名屏蔽 + 绕过国内常见域名。点击以后会弹出二级菜单： 下面只讲几个重要的： 编辑配置下面会详细讲。 编辑纯文本以纯文本方式编辑配置，萌新不需理会，点进去大概会觉得是天书（。语法较为简单，如果了解各个设置的工作方式（下面详细讲），那么这个纯文本看一看就知道什么是什么了，大概。这个主要是方便大佬们批量修改设置之类的，毕竟可以拿出来用查询替换，甚至自己写个小程序移植其他类似代理工具的配置文件。 导出到云可以将这个配置保存到 iCloud 上，也就是备份啦。 添加配置可以把大佬们共享出来的配置文件地址放进去，导入到自己的设备里。 配置 - 本地文件 - 编辑配置Shadowrocket 的强大核心所在。 通用 一个配置的通用设置，一般情况下有可能需要改动的只有 DNS 覆写即你可以自定义 DNS （即时你使用的是移动数据），但是只有你在开启代理的时候才生效。 跳过代理看官方的说明即可。 规则、添加规则 规则可以在上一页中点击 “添加规则” 来添加，点击后你将会看到以下界面： 类型一共有 6 种类型： DOMAIN-SUFFIX（域名后缀）此时应在下面填入形如abc.com的域名后缀，则此规则会对所有以abc.com结尾的域名生效，即对*.abc.com以及abc.com（顶级域名）生效。 DOMAIN-KEYWORD（域名关键词）此时在下面填入一个关键词，所有包含这个关键词的域名都会应用这个规则。例如，填入abc，那么abc.com、abc.net、asd.abc.cn、123abc456.pw这样的域名都会应用规则，也就是有abc就符合条件。 DOMAIN（域名）最严格的条件，即必须 100% 符合你输入的域名才会应用这条规则。例如填入abc.com，那么只对abc.com生效，其他的多了任何一点字母数字符号的域名都不符合这条规则。 USER-AGENT（用户代理）用户代理即 UserAgent，简称 UA。一般不会用到，即与你设置的用户代理相同的所有流量都符合这条规则。不知道用户代理是什么的话可以去百度一下。 IP-CIDRIP-CIDR 表示的是某一 IP 地址或某一网段，格式形如192.168.1.100/24，斜杠前为 IP 地址或子网地址，斜杠后的数字表示的是子网掩码，目的地址符合这一 IP-CIDR 的流量将会应用此规则。建议具有关于这部分一定的网络知识后再去使用。 GEOIP（国家 / 地区）很粗暴的一种规则，填入国家 / 地区代码（比如中国是CN），只要流量目的地址所在地区符合，那么就应用此条规则。 FINAL（默认规则）FINAL 规则只应该存在一条，它的作用是，如果某流量不符合你前面规定的所有规则，那么它将符合这条规则。 选项选项的内容也很简单： PROXY符合这条规则就走代理。 REJECT符合这条规则就直接屏蔽，常用于屏蔽广告。 DIRECR符合这条规则就直连，不走代理。 （你代理节点名称）符合这条规则就走你所选的代理节点（即使你现在使用的节点并不是这个）。 其他选项 强制远程 DNS这个选项在 “类型” 为“DOMAIN”那三个时可开启，作用是，如果你开启了，那么符合规则的域名将会强行通过代理服务器进行解析，可以解决一般常见的 DNS 污染问题，甚至是某些网络环境下因上级路由重定向 DNS 而导致的 DNS 污染问题。 不解析域名仅 “IP-CIDR” 可开启，开启后，只有直接访问 IP 地址的时候才会去匹配这条规则，如果是通过域名访问的则不会匹配这条规则。 Hosts、添加映射 这个功能在 Shadowrocket 中用应用的实际上并不多~（都有代理了还要 host 干嘛）~，除非是一些特殊情况。 首先我们需要知道 Hosts 是什么，能做什么~不懂装懂~大致解释版：简单地说，DNS 能根据域名得到对应的 IP 地址，而如果你事先往 Hosts 里记录了域名以及他们的 IP 地址，那么系统就会优先使用 Hosts 记录充当域名解析结果，而不会再去向 DNS 发出解析请求。 说人话版：你可以用这玩意来自定义一个网址域名锁对应的 IP 地址。 因此 Host 也可以被用来解决 DNS 污染问题（如果需求域名不是很多的话……），甚至用来帮助反代一些 “不存在的” 网站以达到科学上网的效果。 添加映射实际上就是添加一条 Host，点击添加映射即可。界面很简单我也就不放图了，在 域名 中填入你想要指定 IP 的域名，在 IP 地址 中填入你自己指定的 IP 即可。例如域名填入abc.com，IP 地址填入123.123.123.123，那么当你连上代理后访问 abc.com，就会以 abc.com 这个域名访问 123.123.123.123 这个 IP。 编辑 Hosts过于简单，是个人都会用，不作解释~ URL 重写顾名思义，当访问你在其中指定的网址的时候，可以自动改写 URL 地址，以及对这些网址流量做一些奇怪的事情（x 点击 “添加 URL 重写” 可以添加一条 URL 重写规则。 URL填写一个 URL 匹配规则，必填，使用正则表达式书写。如果你不知道什么是正则表达式，去百度吧，不过这玩意对于非计算机专业 or 非理科生可能会很难理解。 TO符合规则的 URL 将会被重写成这个。 类型 有五种类型： header直接重写。 302使用 302 重定向来重写。如果不知道什么是 302 请百度 “302 重定向”。 reject（拒绝） proxy（代理） direct（直连） 后面三个选项存在的意义也许是可以让你用正则表达式规则来指定哪些 URL 走代理 / 直连 / 阻止 吧，会比规则使用起来灵活一些？我也没有用实际用过（懒人一个）所以不太清楚，你们就当前面这句是我在瞎 bb…… 感觉这三个根本就不在 URL 重写的范畴内…… HTTPS 解密 这个功能可以说是一个十分劲爆的功能了（我最早开始用 Shadowrocket 的时候还没有这个功能），Shadowrocket 会使用中间人攻击的方式来解密你所指定的域名的 HTTPS 流量。这个功能应该是用在代理日志抓 HTTPS 包上的。如果你改动了这个设置，就需要生成一个新的 CA 证书并在系统设置中信用它，Shadowrocket 会提示你如何操作，照做就行。 复制复制一份一模一样的配置并自动重命名。 测试规则与上一级菜单中的测试规则一致。 数据这里的功能都是辅助性的功能。只讲一些重点功能。 统计就是单纯的用量统计，你要开着代理的时候看这里才会有统计，关掉之后会立即清空。 开启 “启用存档” 后，每次使用代理的用量记录都会被存到下面的 “归档” 中。 代理（日志）这是一个好用的功能，可以用来当个临时的轻量级抓包工具。 开启 “启用日志记录” 后就会自动记录你每次请求的 URL 和端口，以及产生请求的 APP（即 UA）和应用的规则，可以按 全部 / 代理 / 直连 / 拒绝 分类查看，并可以一键根据这些记录快速创建规则（进入一条记录的详情界面后点击右上角的 “···”）。详细用法亲自一试便知。 DNS（日志）和代理日志很相似，只不过是记录了你的 DNS 请求以及解析结果。 可以用这个来分析是否遭到了 DNS 污染之类的。 设置 证书管理你在使用 Shadowrocket 时生成的 CA 证书（例如使用了 HTTPS 解密），或者手动生成新证书。 延迟测试方法更改测试延迟的请求方法，默认是使用 ICMP。 ICMP 是什么？ 你可以更改为 TCP，这样的话可以 ping 到一些正常情况下不给你 ping 的服务器（某些服务器可能会将 ICMP 包 DROP 掉以增加隐蔽性，即禁 ping），也可以达到在丢包率比较高的时候 ping 不至于次次超时的效果。 但是需要注意的是，使用 TCP 去 ping 的话得不到最准确的延迟。 Today 部件调整 Weight 部件的参数。 按需求连接一个比较赞的功能，但是我并不喜欢用它~，可能是我的可控欲比较强的缘故~。 简单的来说，比如按需求连接设置中有域名*.google.com，那么当你在未连接代理的情况下去访问www.google.com，那么 Shadowrocket 就会自动的帮你连上代理。 还有其他的设置功能，比如设置只有在使用 4G 的情况下才触发这个功能、锁屏不断代理之类的，具体请自行查看。 代理代理共享可以把你当前连接的代理在局域网中分享出去，比如你的手机和你的其他设备都连在同一个 WIFI 上，那么你就可以在其他设备在没有代理配置的情况下使用你手机上现有的代理。 可以自定义端口以及设置 IP 白名单。 我没有亲自试过这个功能所以不清楚这个代理是 SOCKS5 还是 HTTP 之类的，不过有提供二维码，拿另一台设备上的 Shadowrocket 扫一扫应该就清楚了。 前置代理不多讲了，跟 Windows 客户端的前置代理是一样的。 DNS就是自定义 Hosts，不过不是太清楚什么条件下才生效…… 我尝试过写 Hosts 进去但是明没有在代理的时候生效，也可能是因为我当时访问的域名都是在规则中强制使用远程 DNS 解析的？……或者可能我的用法错了…… TCP此处可以设置 TCP 最大连接数，默认为 16，如果你没有特殊需求就不要改动这个设置。 这个设置的作用是当你的 TCP 连接数多于设定值时，所有不活动的 TCP 连接将立即被关闭，以减轻代理服务器的负担。 如果这个值设置的过高，就无法关闭冗余的 TCP 连接，会对代理服务器造成一定负担并造成端口资源的浪费。 UDP开启 UDP 转发后，SS 和 SSR 代理将可以转发 UDP 流量，以代理 UDP 连接。 比如 BangDream 的多人联机通信使用的就是 UDP。 UDP 转发需要 SS/SSR 服务器服务端的支持，SS 服务端好像是由服务端配置来决定是否允许 UDP 转发的，而 SSR 服务端是默认允许的。如果你使用的是 SS，那么在使用这个功能前最好询问一下你的 SS 提供商，看看服务端是否允许 UDP 转发。 服务器订阅这个设置仅用于 SS/SSR 服务器订阅，可以设置每次开启 Shadowrocket 时是否自动更新订阅等。 用户代理用户代理即 UserAgent，简称 UA。 这个设置可以控制经过代理的请求是否需要隐藏 UA 以保护隐私，默认开启。 常见问题连上了代理但是无法上谷歌、推特等如果你确定你的代理是可用的，那么开关一次飞行模式即可解决 99% 的这类问题。 这是 DNS 污染所致，想更详细的了解有关于 DNS 污染的知识请看：DNS 污染是什么]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>SSR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode用cl来编译c++]]></title>
    <url>%2F2019%2F12%2F08%2Fvscode_cl_code_runner%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[在 Windows 下，想要编译 C++ 程序有很多种实现方式，Clang+LLVM，GCC，MSVC 等。一般而言，要想使用微软的 MSVC 编译 C++ 程序，需要用到庞大的 IDE：Visual Studio。然而如果平常随便写个代码都要调用 Visual Studio，无疑造成很大的不便。安装 Visual Studio 后，其实可以用命令行编译 C++ 程序，这需要一些小小的配置： . . . 参考文档微软的官方文档是最可靠的来源：C++ 生成参考按类别列出的编译器选项 装git的注意事项装git的时候选的terminal客户端选cmd那个， 否则编译的时候会出错 配置环境变量为了方便之后的配置，可以设置两个辅助变量（当然，这不是必须的） 变量名 路径 MSVC C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023 WK10 C:\Program Files (x86)\Windows Kits\10 此处路径仅作为参考。接下来配置 Path：在 Path 中添加 1%WK10%\bin\10.0.17763.0\x64;%MSVC%\bin\Hostx64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE 新建两个变量： 变量名 路径 LIB %WK10%\Lib\10.0.17763.0\ucrt\x64;%WK10%\Lib\10.0.17763.0\um\x64;%MSVC%\lib\x64 INCLUDE %WK10%\Include\10.0.17763.0\ucrt;%WK10%\Include\10.0.17763.0\um;%MSVC%\include; 备注:如果编译不成功的话, 比如报错include error之类的, 应该检查一下环境变量是否错误, 是否真的包含了各种标准库文件啥的 在 VS code 内置命令行实现快捷编译当然可以使用 Task.json 配置自定义生成任务，或者使用 Makefile 和 nmake.exe 来生成可执行文件。这里介绍通过扩展实现快捷编译的一种方法： 到 Extension 下载扩展 Code Runner，在 User\settings.json 中用如下代码替换原代码的对应部分： 123456&quot;code-runner.executorMap&quot;: &#123; &quot;c&quot;: &quot;cd $dir &amp;&amp; cl $fileName /Fe$fileNameWithoutExt &amp;&amp; $dir$fileNameWithoutExt&quot;, &quot;cpp&quot;: &quot;cd $dir &amp;&amp; cl /EHsc /nologo $fileName /Fe$fileNameWithoutExt &amp;&amp; $dir$fileNameWithoutExt&quot;,&#125;, 右键然后点击“Run Code” 即可编译当前文件。 （备注）测试单文件编译建立一个 C++ 文件 Hello.cpp 123456#include &lt;iostream&gt;int main()&#123; std::cout &lt;&lt; &quot;Hello World&quot; &lt;&lt; std::endl; return 0;&#125; 进入到文件所在的目录，打开命令行，输入编译命令： cl.exe 是 MSVC 编译器的编译命令。等编译完成（中间可能会报错，暂时不管它），目录生成 Hello.obj 和 Hello.exe，即可运行程序观察效果： 编译过程中我们可以为编译器指定参数。详细的参数列表可以查询微软的官方文档。这里列出常用的几个： 参数 说明 /nologo 不显示 “Microcoft 优化 C++ 编译器” 的字样。 /std 控制兼容的 C++ 版本。目前仅支持 / std:c++14、/std:c++17(/std:c++latest) /EH 指定异常处理模型。一般使用 / EHsc；尽量不要使用 / EHa /Fe 指定生成可执行文件的名称。 /Fo 指定生成中间输出文件的名称。 与 GCC 不同，MSVC 编译选项后直接跟内容，不加空格。例如，上面的 Hello.cpp 我们可以这样编译： 1cl /nologo /EHsc /FeHelloWorld /std:c++latest Hello.cpp 这样就会生成 HelloWorld.exe 文件而不是 Hello.exe。 如果编译不成功的话, 比如报错include error之类的, 应该检查一下环境变量是否错误, 是否真的包含了各种标准库文件啥的 （备注）测试多文件编译新建一个类 Class1，在目录下创建 Class1.h 和 Class1.cpp，调整 main() 函数调用 Class1 的成员函数输出 “Hello World”。尝试编译这个文件： MSVC 支持使用如下命令分离编译和链接的步骤：(大小写区分！) 12cl /EHsc /c Hello.cpp Class1.cpplink Hello.obj Class1.obj /OUT:Hello.exe link.exe 是 MSVC 的链接命令。/c 指令说明编译而不链接，/OUT:filename.exe 指示输出可执行文件的名称。这里. exe 后缀可以省略。 然而，更简便的做法是让 cl.exe 自动调用 link.exe： 1cl /nologo /EHsc /FeHello Hello.cpp Class1.cpp 这样也可以实现与先编译后链接相同的效果。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异步日志小结]]></title>
    <url>%2F2019%2F11%2F15%2Fasync_log_summary%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[最近工作涉及到生产环境的日志系统, 小结一波, 其实践为: cpp高性能的异步日志系统模块导出给py使用 此cpp模块另起线程 logger通过unix socket连接上address 为 “/dev/log”，与 rsyslogd 程序通信 log先缓存在一个buffer中, 通过一个任务来把log存入syslog这一事件封装起来 把这个任务加入一个线程间可保序输出的任务队列(常用asio的strand来实现), 即可返回了 之后会异步顺序输出到syslog, 然后/ etc/init.d/rsyslog 这个后台程序根据 / etc/rsyslog.conf 这个配置文件 将日志输出到不同的文件，包括网络文件，即其他服务器(分发到数据部门, 然后他们就可从中筛选提取数据, 并制作网站供各方查询日志了) . . . 关于syslog可参考以下记录, 以下文字转自 https://www.cnblogs.com/xybaby/p/6596431.html 正文 本文记录了因为一个简单的日志需求，继而对 linux 环境下 syslog、rsyslog、unix domain socket 的学习。本文关注使用层面，并不涉及 rsyslog 的实现原理，感兴趣的读者可以参考 rsyslog 官网。另外，本文实验的环境实在 debian8，如果是其他 linux 发行版本或者 debian 的其他版本，可能会稍微有些差异。 需求： 工作中有一个在 Linux（debian8）环境下运行的服务器程序，用 python 语言实现，代码中有不同优先级的日志需要记录，开发的时候都是使用 python 的 logging 模块输出到文件，示例代码如下： 123456789101112131415161718import logging, oslogger = Nonedef get_logger(): global logger if not logger: logger = logging.getLogger('ServerLog') logger.setLevel(logging.INFO) filehandler = logging.FileHandler(os.environ['HOME'] + '/Server.log', encoding='utf8') filehandler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")) logger.addHandler(filehandler) return loggerdef some_func(): get_logger().info("call some_func")if __name__ == '__main__': some_func() 运行上面这段代码，就会在 home 目录下面产生一个 server.log 文件。 后来数据分析的部门说他们希望能够实时拿到一部分日志，他们有一台专门处理日志的服务器，那么怎么把日志发给他们呢？笔者之前并没有相关经验，数据分析部门的同事说，这种需求他们都是找运维人员帮忙。运维同事给出的方案很简单：产品把日志写到 syslog，然后他们负责把带有某些关键字的日志转发给数据分析部门，在运维同事的指导下，把代码改成了这样: 12345678910111213141516171819202122import loggingimport logging.handlerslogger = Nonedef get_logger(): global logger if not logger: logger = logging.getLogger('ServerLog') logger.setLevel(logging.INFO) sys_handler = logging.handlers.SysLogHandler('/dev/log', facility=logging.handlers.SysLogHandler.LOG_LOCAL0) syslog_tag = 'ServerLog' sys_handler.setFormatter(logging.Formatter(syslog_tag + ":%(asctime)s - %(name)s - %(levelname)s - %(message)s")) logger.addHandler(sys_handler) return loggerdef some_func(): get_logger().info("call some_func")if __name__ == '__main__': some_func() 上面的代码修改了日志的输出形式，直观的感受就是从文件 server.log 到了 /dev/log，但 / dev/log 对应的是 SysLogHandler，并不是 FileHandler，所以肯定不是一个普通的文件。此时，我有两个疑问：第一，这里我并没有将日志输出到 home 目录下的 Server.log 文件，但是程序运行的时候生成了这么一个文件；第二，怎么讲日志发送到数据分析部门的服务器。 不懂就问： Q：新的代码下怎么生成 Server.log 文件，日志内容又是怎么转发到数据分析部门的服务器？ A: 这个是 / etc/init.d/rsyslog 这个后台程序根据 / etc/rsyslog.conf 这个配置文件 将日志输出到不同的文件，包括网络文件，即其他服务器。看 / etc/rsyslog.conf 这个配置就明白了。 Q：OK，那 python 代码将文件输出到 / dev/log 跟 rsyslog 又是什么关系呢？ A：python 的 sysloghandler 会将日志发送到 rsyslog，他们之间使用 unix domain socket 通信，具体看 logging 模块的源码就知道了 unix domain socket： 按照上面的对话的意思，python 程序先将日志发送给 rsyslog 这个程序，然后 rsyslog 再处理收到的日志数据，所以先看 logging 代码： SysLogHandler 这个类在 logging.handlers.py, 核心代码如下： 12345678910111213141516171819202122232425262728293031323334def __init__(self, address=('localhost', SYSLOG_UDP_PORT), facility=LOG_USER, socktype=socket.SOCK_DGRAM): """ Initialize a handler. If address is specified as a string, a UNIX socket is used. To log to a local syslogd, "SysLogHandler(address="/dev/log")" can be used. If facility is not specified, LOG_USER is used. """ logging.Handler.__init__(self) self.address = address self.facility = facility self.socktype = socktype if isinstance(address, basestring): self.unixsocket = 1 self._connect_unixsocket(address) else: self.unixsocket = 0 self.socket = socket.socket(socket.AF_INET, socktype) if socktype == socket.SOCK_STREAM: self.socket.connect(address) self.formatter = Nonedef _connect_unixsocket(self, address): self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM) # syslog may require either DGRAM or STREAM sockets try: self.socket.connect(address) except socket.error: self.socket.close() self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) self.socket.connect(address) 在init.doc 里面写得很清楚，如果 address 是一个字符串（默认值是一个 tuple），那么会建立一个 unix socket（unix domain socket）。如果 address 为 “/dev/log”（正如我们之前的 python 代码），那么输出到本机的 syslogd 程序。另外，在第 27 行 self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM) socket.socket 的第一个参数 family 的值为 AF_UNIX，而不是我们经常使用的 AF_INET(IPV4）或者 AF_INET6(IPV6)。那么什么是 unix domain socket 呢？ unix domain socket 是进程间通信（IPC：inter-process communication）的一种方式，其他还有管道、命名管道、消息队列、共享内存、socket 之类的。unix domain socket 与平常使用的 socket（狭义的 internet socket）有什么区别呢，那就是 unix domain socket 只能在同一台主机上的进程之间通信，普通的 socket 也可以通过’localhost’来在同一台主机通信，那么 unix domain socket 有哪些优势呢？ 第一：不需要经过网络协议栈 第二：不需要打包拆包、计算校验和、维护序号和应答等 所以，优势就是性能好，一个字，快。 下面用一个简单的服务器客户端例子来看看 unix domain socket 的使用方法与过程： 服务器：uds_server.py 123456789101112131415161718192021ADDR = '/tmp/uds_tmp'import socket, osdef main():try: sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) if os.path.exists(ADDR): os.unlink(ADDR) sock.bind(ADDR) sock.listen(5) while True: connection, address = sock.accept() print "Data : %s" % connection.recv(1024); connection.send("hello uds client") connection.close()finally: sock.close()if __name__ == '__main__': main() 客户端：uds_client.py 12345678910111213ADDR = '/tmp/uds_tmp'import socketdef main(): sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) sock.connect(ADDR) sock.send('hello unix domain socket server') print 'client recieve', sock.recv(1024) sock.close()if __name__ == '__main__': main() 首先：运行服务器 python uds_server.py，这个时候在 / tmp 目录下产生了文件，用 ls 查看详细信息如下： 可以看到，文件类型（第一个字段）为 s，代表 socket 文件。（PS： 如果进程间用命令管道通信，也是利用中间文件，ls 显示的文件类型为 p） 运行客户端 python uds_client.py，在客户端和服务器端都有相应的输出，使用方法与普通 socket 没有什么大的差异。 日志转发流程： 在了解了 unix domain socket 这个概念之后，下面就比较简单了，首先是 / dev/log 这个文件，我们用 ls 来查看这个文件的信息 可以看到这个文件是一个符号链接文件，真实的文件是 / run/systemd/journal/dev-log, 那么再来查看这个文件 ok，是一个 socket 文件，复合预期，按照之前的 unix domain socket 的例子，rsyslog 也应该咋这个文件上监听，我们来看看 lsof fd 可以列出所有使用了这个文件（linux 下文件的概念比较宽泛）的进程，事实上我们看到只有 systemd 和 systemd-j 两个不明所以的进程。那么直接看看 rsyslog 使用的 unix domain socket 吧 ![](/img/syslog_intro/1089769-20170324101545783-1464834217.png) 额，可以看到 rsyslogd 使用的 socket domain socket 是 / run/systemd/journal/syslog，并不是 / run/systemd/journal/dev-log，这两个文件在同一个目录下，那么再来看看还有哪些进程使用了 / run/systemd/journal/syslog。 so，systemd 和 rsyslogd 都使用了这个文件，感觉像是应用进程 (e.g. 上面的 python 程序）将日志通过 / run/systemd/journal/dev-log（/dev/log 背后真正的文件）发送到 systemd， 然后 systemd 再将日志通过 / run/systemd/journal/syslog 发送到 rsyslogd，是不是这样呢，google 了一下，发现了这篇文章 understand-logging-in-linux，确实是这么一个过程： systemd has a single monolithic log management program, systemd-journald. This runs as a service managed by systemd. It reads /dev/kmsg for kernel log data. It reads /dev/log (a symbolic link to /run/systemd/journal/dev-log) for application log data from the GNU C library’s syslog() function. It listens on the AF_LOCAL stream socket at /run/systemd/journal/stdout for log data coming from systemd-managed services. It listens on the AF_LOCAL datagram socket at /run/systemd/journal/socket for log data coming from programs that speak the systemd-specific journal protocol (i.e. sd_journal_sendv() et al.). It mixes these all together. It writes to a set of system-wide and per-user journal files, in /run/log/journal/ or /var/log/journal/. If it can connect (as a client) to an AF_LOCAL datagram socket at /run/systemd/journal/syslogit writes journal data there, if forwarding to syslog is configured. ok，到现在为止，我们知道了应用程序的日志是怎么转发到 rsyslog，那么 rsyslog 怎么处理接收到的日志，秘密就在 / etc/rsyslog.conf, 在打开这个配置文件之前，我们先看看 rsyslog 官网的简单描述： RSYSLOG is the rocket-fast system for log processing. 原来 R 是 rocket-fast 的意思！火箭一般快！官网声称每秒可以处理百万级别的日志。rsyslogd 在部分 linux 环境是默认的 syslogd 程序（至少在笔者的机器上），d 是 daemon 的意思，后台进程。系统启动的时候就会启动该进程来处理日志（包括操作系统自身和用户进程的日志）。打开修改过的 / etc/rsyslog.conf, 接下来就是见证奇迹的时刻 原来一举一动都在监控之中。这个文件是系统提供的，直接在这个文件上做修改显然不是明智之举。如上图红色部分，可以再 rysyslog.d 文件夹下增加自己的配置文件，定制日志过滤规则。那么看看的 rsyslog.d 文件夹下新增的 tmp.conf 1234567891011$FileOwner USERNAME$FileGroup USERNAME$FileCreateMode 0644$DirCreateMode 0755$Umask 0022$template serverLog,"/home/USERNAME/Server.log"$template LogFormat,"%msg%\n"if $syslogfacility-text == 'local0' and $syslogtag contains 'ServerLog' then -?serverLog;LogFormat#if $syslogfacility-text == 'local0' and $syslogtag contains 'ServerLog' then @someip:port&amp; stop 再来回顾一下对应的应用代码： 12345678910111213141516171819202122import loggingimport logging.handlerslogger = Nonedef get_logger(): global logger if not logger: logger = logging.getLogger('ServerLog') logger.setLevel(logging.INFO) sys_handler = logging.handlers.SysLogHandler('/dev/log', facility=logging.handlers.SysLogHandler.LOG_LOCAL0) syslog_tag = 'ServerLog' sys_handler.setFormatter(logging.Formatter(syslog_tag + ":%(asctime)s - %(name)s - %(levelname)s - %(message)s")) logger.addHandler(sys_handler) return loggerdef some_func(): get_logger().info("call some_func")if __name__ == '__main__': some_func() 注意：配置文件需要与应用代码配合，比如代码中第 11 行 facility=logging.handlers.SysLogHandler.LOG_LOCAL0 与 配置中 $syslogfacility-text == ‘local0’ 相对应；代码第 12 行 syslog_tag = ‘ServerLog’ 与 配置文件 $syslogtag contains ‘ServerLog’ 对应。关于 python 代码中 syslogtag 的设置，参考了 stackoverflow 上的这个问答。 当我们修改了配置时候需要通过命令 /etc/init.d/rsyslog restart 来重启 rsyslogd，重启之后再运行之前的 python 文件，就可以了。 发送到远端服务器： 上面的 tmp.conf 文件注释掉了第 10 行，这一行的作用是将满足条件的日志发送到指定的其他机器上，IP：Port 用来指定接受日志的远端 rsyslogd 程序。默认情况下 rsyslogd 在 514 端口监听。假设我需要给局域网内 10.240.10.10 发送 syslog，第 10 行改成这样就行了： 123&gt; if $syslogfacility-text == &apos;local0&apos; and $syslogtag contains &apos;ServerLog&apos; then @10.240.10.10&gt; &gt; 那么 10.240.10.10 主要开启 rsyslogd 的远程监听，并指定远端日志的输出规则，for example： 这个配置，让 rsyslogd 使用 UDP 和 TCP 协议同时在 514 端口上监听，并将非本机的日志输出到对应远端主机名的文件。注意，以上修改 都需要重启 rsyslogd 才能生效。 总结： 日志从应用程序到最终的日志文件（或者远程服务器）的流程如下： references：inter-process communication unix domain socket understand-logging-in-linux 在 Linux 上配置一个 syslog 服务器]]></content>
      <categories>
        <category>NP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pycharm & Goland 激活]]></title>
    <url>%2F2019%2F10%2F20%2Fgoland_pycharm_activation%2F</url>

    <encrypted>1</encrypted>

    <content type="text"><![CDATA[. . . Pycharm/Goland 2019激活方法注: 本激活方法下文虽以Pycharm为例, 但对Goland也是适用的, 已测试证实通过 0.下载文件jetbrains-agent.jar : 链接1, 适用于pycharm2019.2 与 goland2019.2 : https://pan.baidu.com/s/1EtGPMKBPyWjWyqs4BjaOPw 提取码: esrr 链接2, 适用于pycharm2019.1.1 : https://pan.baidu.com/s/10jraOk5krAa_feF4ljTh8A 提取码: vqs6 1.打开软件，按照习惯操作，直到激活界面，选择试用： 2.在弹出的界面中点击下侧的Configure，选择Edit Custom VM Options,在打开的文件最后一行添上内容：-javaagent:D:\JetBrains\jetbrains-agent.jar, 其中，D:\JetBrains\jetbrains-agent.jar为jetbrains-agent.jar的绝对路径，并非一定要放在软件安装目录下，只要不改动jetbrains-agent.jar的路径就行。 3.关闭软件 4.（可能不存在该步骤）再次打开软件，进入激活页面，仍旧选择免费试用，其他默认 5.进入软件（随便建一个空项目）打开软件，选择Help—Register pycharm在建空项目时可能因为没有解释器（No Python interpreter）而不能建立项目，这时可以关闭软件，随便建一个后缀名为.py的文件（也可建一个文本文件，将其后缀改为.py），然后右键选择打开方式，使用PyCharm打开该文件。 6.选择License server后稍等几秒（不到半分钟），会自动填入http://jetbrains-license-server。若没有填入，则点击Discover Server，会有旋转等待的标记圈（不到半分钟），标记圈消失后，出现该地址，然后点击Activate。 7.查看激活状态（若激活不成功，转到第8步）,完成激活后, 点击Help-About可以看到内容, 将上面图与下方图（来源于网络）对比可看到上面图中没有截止日期，因此是无期限。 8.激活码方式激活（未测试，激活码来源于网络） 若服务器方式无法激活，则选择Activation code方式激活，这种方式可离线，激活码为： 1520E5894E2-eyJsaWNlbnNlSWQiOiI1MjBFNTg5NEUyIiwibGljZW5zZWVOYW1lIjoicGlnNiIsImFzc2lnbmVlTmFtZSI6IiIsImFzc2lnbmVlRW1haWwiOiIiLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiJVbmxpbWl0ZWQgbGljZW5zZSB0aWxsIGVuZCBvZiB0aGUgY2VudHVyeS4iLCJjaGVja0NvbmN1cnJlbnRVc2UiOmZhbHNlLCJwcm9kdWN0cyI6W3siY29kZSI6IklJIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlMwIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiV1MiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJSRCIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJDIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiREMiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJEQiIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlJNIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiRE0iLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJBQyIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkRQTiIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IkdPIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUFMiLCJwYWlkVXBUbyI6IjIwODktMDctMDcifSx7ImNvZGUiOiJDTCIsInBhaWRVcFRvIjoiMjA4OS0wNy0wNyJ9LHsiY29kZSI6IlBDIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In0seyJjb2RlIjoiUlNVIiwicGFpZFVwVG8iOiIyMDg5LTA3LTA3In1dLCJoYXNoIjoiODkwNzA3MC8wIiwiZ3JhY2VQZXJpb2REYXlzIjowLCJhdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlLCJpc0F1dG9Qcm9sb25nYXRlZCI6ZmFsc2V9-DZ/oNHBfyho0XrrCJJvAOKg5Q1tLBgOdbCmzCKwkuM+Yryce0RoOi3OOmH6Ba/uTcCh/L37meyD0FJdJIprv59y4+n+k2kIeF/XKrKqg0dEsDUQRw0lUqqMt99ohqa+zmbJ44Yufdwwx/F1CtoRGvEQ2Mn0QjuqRoZJZ3wiT5Am22JiJW8MaNUl3wg9YPj+OPGARKKJUdUJ0NGUDQBcBAv5ds8LhbSbJSbPkbkwH/a1QMz4nEdn6lRDKI1aFIn43QhBSCFqvUq6TPJlbIJ0ZjE+PyZjHFBKCgkry0DHPXU2BbtIZPsksQnN3fx240a9K6sN7peZnLpEoMoq23FEz4g==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA5ndaik1GD0nyTdqkZgURQZGW+RGxCdBITPXIwpjhhaD0SXGa4XSZBEBoiPdY6XV6pOfUJeyfi9dXsY4MmT0D+sKoST3rSw96xaf9FXPvOjn4prMTdj3Ji3CyQrGWeQU2nzYqFrp1QYNLAbaViHRKuJrYHI6GCvqCbJe0LQ8qqUiVMA9wG/PQwScpNmTF9Kp2Iej+Z5OUxF33zzm+vg/nYV31HLF7fJUAplI/1nM+ZG8K+AXWgYKChtknl3sW9PCQa3a3imPL9GVToUNxc0wcuTil8mqveWcSQCHYxsIaUajWLpFzoO2AhK4mfYBSStAqEjoXRTuj17mo8Q6M2SHOcwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQBonMu8oa3vmNAa4RQP8gPGlX3SQaA3WCRUAj6Zrlk8AesKV1YSkh5D2l+yUk6njysgzfr1bIR5xF8eup5xXc4/G7NtVYRSMvrd6rfQcHOyK5UFJLm+8utmyMIDrZOzLQuTsT8NxFpbCVCfV5wNRu4rChrCuArYVGaKbmp9ymkw1PU6+HoO5i2wU3ikTmRv8IRjrlSStyNzXpnPTwt7bja19ousk56r40SmlmC04GdDHErr0ei2UbjUua5kw71Qn9g02tL9fERI2sSRjQrvPbn9INwRWl5+k05mlKekbtbu2ev2woJFZK4WEXAd/GaAdeZZdumv8T2idDFL7cAirJwcrbfpawPeXr52oKTPnXfi0l5+g9Gnt/wfiXCrPElX6ycTR6iL3GC2VR4jTz6YatT4Ntz59/THOT7NJQhr6AyLkhhJCdkzE2cob/KouVp4ivV7Q3Fc6HX7eepHAAF/DpxwgOrg9smX6coXLgfp0b1RU2u/tUNID04rpNxTMueTtrT8WSskqvaJd3RH8r7cnRj6Y2hltkja82HlpDURDxDTRvv+krbwMr26SB/40BjpMUrDRCeKuiBahC0DCoU/4+ze1l94wVUhdkCfL0GpJrMSCDEK+XEurU18Hb7WT+ThXbkdl6VpFdHsRvqAnhR2g4b+Qzgidmuky5NUZVfEaZqV/g== 按照第7步查看激活状态，这种激活方式About显示有截止日期，为2089年。 pycharm 2018.2激活方法(貌似已失效)1.下载pycharm2.下载补丁 链接: https://pan.baidu.com/s/1AOO359_TNRDEUl5mlgsCIA&amp;shfl=shareset 提取码: k6ab 将 JetbrainsIdesCrack-3.4-release-enc.jar 放置到 pycharm安装目录的\bin目录下 3.在 Pycharm安装目录的\bin目录下找到 pycharm.exe.vmoptions 和 pycharm64.exe.vmoptions；以文本格式打开并同时在两个文件最后追加 -javaagent:D:\JetBrains\PyCharm 2018.2.1\bin\JetbrainsIdesCrack-3.4-release-enc.jar，注意路径修改成你的pycharm安装路径，然后保存。 4.重启Pycharm ,选择激活码激活，输入如下内容激活:1BIG3CLIK6FeyJsaWNlbnNlSWQiOiJCSUczQ0xJSzZGIiwibGljZW5zZWVOYW1lIjoibGFuIHl1IiwiYXNzaWduZWVOYW1lIjoiIiwiYXNzaWduZWVFbWFpbCI6IiIsImxpY2Vuc2VSZXN0cmljdGlvbiI6IkZvciBlZHVjYXRpb25hbCB1c2Ugb25seSIsImNoZWNrQ29uY3VycmVudFVzZSI6ZmFsc2UsInByb2R1Y3RzIjpbeyJjb2RlIjoiQUMiLCJwYWlkVXBUbyI6IjIwMTctMTEtMjMifSx7ImNvZGUiOiJETSIsInBhaWRVcFRvIjoiMjAxNy0xMS0yMyJ9LHsiY29kZSI6IklJIiwicGFpZFVwVG8iOiIyMDE3LTExLTIzIn0seyJjb2RlIjoiUlMwIiwicGFpZFVwVG8iOiIyMDE3LTExLTIzIn0seyJjb2RlIjoiV1MiLCJwYWlkVXBUbyI6IjIwMTctMTEtMjMifSx7ImNvZGUiOiJEUE4iLCJwYWlkVXBUbyI6IjIwMTctMTEtMjMifSx7ImNvZGUiOiJSQyIsInBhaWRVcFRvIjoiMjAxNy0xMS0yMyJ9LHsiY29kZSI6IlBTIiwicGFpZFVwVG8iOiIyMDE3LTExLTIzIn0seyJjb2RlIjoiREMiLCJwYWlkVXBUbyI6IjIwMTctMTEtMjMifSx7ImNvZGUiOiJEQiIsInBhaWRVcFRvIjoiMjAxNy0xMS0yMyJ9LHsiY29kZSI6IlJNIiwicGFpZFVwVG8iOiIyMDE3LTExLTIzIn0seyJjb2RlIjoiUEMiLCJwYWlkVXBUbyI6IjIwMTctMTEtMjMifSx7ImNvZGUiOiJDTCIsInBhaWRVcFRvIjoiMjAxNy0xMS0yMyJ9XSwiaGFzaCI6IjQ3NzU1MTcvMCIsImdyYWNlUGVyaW9kRGF5cyI6MCwiYXV0b1Byb2xvbmdhdGVkIjpmYWxzZSwiaXNBdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlfQ==-iygsIMXTVeSyYkUxAqpHmymrgwN5InkOfeRhhPIPa88FO9FRuZosIBTY18tflChACznk3qferT7iMGKm7pumDTR4FbVVlK/3n1ER0eMKu2NcaXb7m10xT6kLW1Xb3LtuZEnuis5pYuEwT1zR7GskeNWdYZ0dAJpNDLFrqPyAPo5s1KLDHKpw+VfVd4uf7RMjOIzuJhAAYAG+amyivQt61I9aYiwpHQvUphvTwi0X0qL/oDJHAQbIv4Qwscyo4aYZJBKutYioZH9rgOP6Yw/sCltpoPWlJtDOcw/iEWYiCVG1pH9AWjCYXZ9AbbEBOWV71IQr5VWrsqFZ7cg7hLEJ3A==MIIEPjCCAiagAwIBAgIBBTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE1MTEwMjA4MjE0OFoXDTE4MTEwMTA4MjE0OFowETEPMA0GA1UEAwwGcHJvZDN5MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxcQkq+zdxlR2mmRYBPzGbUNdMN6OaXiXzxIWtMEkrJMO/5oUfQJbLLuMSMK0QHFmaI37WShyxZcfRCidwXjot4zmNBKnlyHodDij/78TmVqFl8nOeD5+07B8VEaIu7c3E1N+e1doC6wht4I4+IEmtsPAdoaj5WCQVQbrI8KeT8M9VcBIWX7fD0fhexfg3ZRt0xqwMcXGNp3DdJHiO0rCdU+Itv7EmtnSVq9jBG1usMSFvMowR25mju2JcPFp1+I4ZI+FqgR8gyG8oiNDyNEoAbsR3lOpI7grUYSvkB/xVy/VoklPCK2h0f0GJxFjnye8NT1PAywoyl7RmiAVRE/EKwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9 5.这样就配置成功了哦，点击 pycharm的help-&gt;about查看即可查看期限]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Pycharm</tag>
        <tag>GoLand</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[supervisor 备忘]]></title>
    <url>%2F2019%2F09%2F09%2Fsupervisor_intro%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Introduction 基于python编写，安装方便 进程管理工具，可以很方便的对用户定义的进程进行启动，关闭，重启，并且对意外关闭的进程进行重启 ，只需要简单的配置一下即可，且有web端，状态、日志查看清晰明了。 组成部分 supervisord[服务端，所以要通过这个来启动它]supervisorctl[客户端，可以来执行stop等命令] 官方文档地址：http://supervisord.org/ . . . 安装python 第三方包的安装方法，此处不详细描述 1pip install supervisor 使用说明使用 supervisor 很简单，只需要修改一些配置文件，就可以使用了。 查看默认配置运行 1echo_supervisord_conf 即可看到默认配置情况，但是一般情况下，我们都不要去修改默认的配置，而是将默认配置重定向到另外的文件中，不同的进程运用不同的配置文件去对默认文件进行复写即可。 1echo_supervisord_conf &gt; /etc/supervisord.conf 默认配置说明 默认的配置文件是下面这样的，但是这里有个坑需要注意，supervisord.pid 以及 supervisor.sock 是放在 /tmp 目录下，但是 /tmp 目录是存放临时文件，里面的文件是会被 Linux 系统删除的，一旦这些文件丢失，就无法再通过 supervisorctl 来执行 restart 和 stop 命令了，将只会得到 unix:///tmp/supervisor.sock 不存在的错误 。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091[unix_http_server];file=/tmp/supervisor.sock ; (the path to the socket file);建议修改为 /var/run 目录，避免被系统删除file=/var/run/supervisor.sock ; (the path to the socket file);chmod=0700 ; socket file mode (default 0700);chown=nobody:nogroup ; socket file uid:gid owner;username=user ; (default is no username (open server));password=123 ; (default is no password (open server));[inet_http_server] ; inet (TCP) server disabled by default;port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for ;all iface);username=user ; (default is no username (open server));password=123 ; (default is no password (open server))...[supervisord];logfile=/tmp/supervisord.log ; 日志文件(main log file;default $CWD/supervisord.log);建议修改为 /var/log 目录，避免被系统删除logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log)logfile_maxbytes=50MB ; 日志文件大小(max main logfile bytes b4 rotation;default 50MB)logfile_backups=10 ; 日志文件保留备份数量(num of main logfile rotation backups;default 10)loglevel=info ; 日志级别(log level;default info; others: debug,warn,trace);pidfile=/tmp/supervisord.pid ; (supervisord pidfile;default supervisord.pid);建议修改为 /var/run 目录，避免被系统删除pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid);设置启动supervisord的用户，一般情况下不要轻易用root用户来启动，除非你真的确定要这么做;user=chrism ; (default is current user, required if root)nodaemon=false ; (start in foreground if true;default false)minfds=1024 ; (min. avail startup file descriptors;default 1024)minprocs=200 ; (min. avail process descriptors;default 200);umask=022 ; (process file creation umask;default 022);identifier=supervisor ; (supervisord identifier, default is &apos;supervisor&apos;);directory=/tmp ; (default is not to cd during start);nocleanup=true ; (don&apos;t clean up tempfiles at start;default false);childlogdir=/tmp ; (&apos;AUTO&apos; child log dir, default $TEMP);environment=KEY=&quot;value&quot; ; (key value pairs to add to environment);strip_ansi=false ; (strip ansi escape codes in logs; def. false)[unix_http_server]file=/tmp/supervisor.sock ; (the path to the socket file);chmod=0700 ; socket file mode (default 0700);chown=nobody:nogroup ; socket file uid:gid owner;username=user ; (default is no username (open server));password=123 ; (default is no password (open server))[supervisorctl]; 必须和&apos;unix_http_server&apos;里面的设定匹配;serverurl=unix:///tmp/supervisor.sock ; use a unix:// URL for a unix socket;建议修改为 /var/run 目录，避免被系统删除serverurl=unix:///var/run/supervisor.sock ; use a unix:// URL for a unix socket;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket;username=chris ; should be same as http_username if set;password=123 ; should be same as http_password if set;[program:theprogramname];command=/bin/cat ; the program (relative uses PATH, can take args);process_name=%(program_name)s ; process_name expr (default %(program_name)s);numprocs=1 ; number of processes copies to start (def 1);directory=/tmp ; directory to cwd to before exec (def no cwd);umask=022 ; umask for process (default None);priority=999 ; the relative start priority (default 999);autostart=true ; start at supervisord start (default: true);startsecs=1 ; # of secs prog must stay up to be running (def. 1);startretries=3 ; max # of serial start failures when starting (default 3);autorestart=unexpected ; when to restart if exited after running (def: unexpected);exitcodes=0,2 ; &apos;expected&apos; exit codes used with autorestart (default 0,2);stopsignal=QUIT ; signal used to kill process (default TERM);stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10);stopasgroup=false ; send stop signal to the UNIX process group (default false);killasgroup=false ; SIGKILL the UNIX process group (def false);user=chrism ; setuid to this UNIX account to run the program;redirect_stderr=true ; redirect proc stderr to stdout (default false);stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB);stdout_logfile_backups=10 ; # of stdout logfile backups (default 10);stdout_capture_maxbytes=1MB ; number of bytes in &apos;capturemode&apos; (default 0);stdout_events_enabled=false ; emit events on stdout writes (default false);stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB);stderr_logfile_backups=10 ; # of stderr logfile backups (default 10);stderr_capture_maxbytes=1MB ; number of bytes in &apos;capturemode&apos; (default 0);stderr_events_enabled=false ; emit events on stderr writes (default false);environment=A=&quot;1&quot;,B=&quot;2&quot; ; process environment additions (def no adds);serverurl=AUTO ; override serverurl computation (childutils);[group:thegroupname];programs=progname1,progname2 ; each refers to &apos;x&apos; in [program:x] definitions;priority=999 ; the relative start priority (default 999)[include]files = /etc/supervisor/*.conf 配置文件都有说明，且很简单，就不做多的描述了，在上面有一些建议修改的目录，若做了修改，则应先创建这些文件，需要注意权限问题，很多错误都是没有权限造成的。 启动服务端现在，让我们来启动 supervisor 服务。 supervisord -c /etc/supervisord.conf 查看 supervisord 是否运行： ps aux|grep superviosrd 1output:xxxx 82039 1 0 11:22 ? 00:00:00 /usr/local/bin/python /usr/local/bin/supervisord -c /etc/supervisord.conf 项目配置及运行上面我们已经把 supervisrod 运行起来了，现在可以添加我们要管理的进程的配置文件。可以把所有配置项都写到 supervisord.conf 文件里，但并不推荐这样做，而是通过 include 的方式把不同的程序（组）写到不同的配置文件里，对，就是默认配置中的最后的那个 include。下面来对项目进行简单的配置。假设我们把项目配置文件放在这个目录中:/etc/supervisor/则我们需要修改 / etc/supervisord.conf 中的 include 为：[include]files = /etc/supervisor/*.conf 以下为本人的配置文件目录： /etc/supervisor/update_ip.conf 1234567891011121314151617181920212223242526272829[program:update_ip] ;项目名称directory = /home/xxxx/works/ip_update/ip_update_on_server_no_1/ ; 程序的启动目录command = python /home/xxxx/works/ip_update/ip_update_on_server_no_1/update_ip_internal.py ; 启动命令，可以看出与手动在命令行启动的命令是一样autostart = true ; 在 supervisord 启动的时候也自动启动startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了autorestart = true ; 程序异常退出后自动重启startretries = 3 ; 启动失败自动重试次数，默认是 3user = shimeng ; 用哪个用户启动redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 falsestdout_logfile_maxbytes = 50MB ; stdout 日志文件大小，默认 50MBstdout_logfile_backups = 20 ; stdout 日志文件备份数; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件）stdout_logfile = /home/xxxx/works/ip_update/ip_update_on_server_no_1/supervisor.logloglevel=info[supervisorctl]serverurl=unix:/[unix_http_server]file=/tmp/supervisor.sock ; (the path to the socket file)chmod=0777 ; socket file mode (default 0700);chown=nobody:nogroup ; socket file uid:gid owner;username=shimeng ; (default is no username (open server));password=123 ; (default is no password (open server))[inet_http_server] ; inet (TCP) server disabled by defaultport=127.0.0.1:9001 ; (ip_address:port specifier, *:port for all iface)username=shimeng ; (default is no username (open server))password=123 12345678910配置详解：a) 在supervisord.conf文件中，分号“；”后面的内容表示注释b) [group:组名]，设置一个服务分组，programs后面跟组内所有服务的名字，以分号分格。c) [program：服务名]，下面是这个服务的具体设置：Command:启用Tornado服务文件的命令，也就是我们手动启动的命令。Directory:服务文件所在的目录User:启用服务的用户Autorestart:是否自动重启服务stdout_logfile：服务的产生的日起文件loglevel:日志级别 配置完成以后，即可运行： supervisord -c /etc/supervisord.conf 查看运行状态 $ supervisorctl status 12out:update_ip RUNNING pid 62040, uptime 0:10:09 打开浏览器，输入 127.0.0.9001, 输入用户名与密码（如果配置文件中 inet_http_server 中作了设置），可以看到下面这个界面： image 使用 supervisorctl在启动服务之后，运行： supervisorctl -c /etc/supervisord.conf 12out:update_ip RUNNING pid 62040, uptime 0:10:09 若成功，则会进入 supervisorctl 的 shell 界面，有以下方法： 123456status # 查看程序状态stop update_ip # 关闭 update_ip 程序start update_ip # 启动 update_ip 程序restart update_ip # 重启 update_ip 程序reread ＃ 读取有更新（增加）的配置文件，不会启动新添加的程序update ＃ 重启配置文件修改过的程序 执行相关操作后，可以在 web 端看到具体的变化情况，如 stop 程序 stop update_ip 其实，也可以不使用 supervisorctl shell 界面，而在 bash 终端运行： 123456$ supervisorctl status$ supervisorctl stop usercenter$ supervisorctl start usercenter$ supervisorctl restart usercenter$ supervisorctl reread$ supervisorctl update 多个进程管理按照官方文档的定义，一个 [program:x] 实际上是表示一组相同特征或同类的进程组，也就是说一个 [program:x] 可以启动多个进程。这组进程的成员是通过 numprocs 和 process_name 这两个参数来确定的，这句话什么意思呢，我们来看这个例子。 123456789101112131415161718192021; 设置进程的名称，使用 supervisorctl 来管理进程时需要使用该进程名[program:foo] ; 可以在 command 这里用 python 表达式传递不同的参数给每个进程command=python server.py --port=90%(process_num)02ddirectory=/home/python/tornado_server ; 执行 command 之前，先切换到工作目录; 若 numprocs 不为1，process_name 的表达式中一定要包含 process_num 来区分不同的进程numprocs=2 process_name=%(program_name)s_%(process_num)02d; user=oxygen ; 使用 oxygen 用户来启动该进程autorestart=true ; 程序崩溃时自动重启redirect_stderr=true ; 重定向输出的日志stdout_logfile = /var/log/supervisord/tornado_server.logloglevel=info 上面这个例子会启动两个进程，process_name 分别为 foo:foo_01 和 foo:foo_02。通过这样一种方式，就可以用一个 [program:x] 配置项，来启动一组非常类似的进程。更详细配置，点击这里 Supervisor 同时还提供了另外一种进程组的管理方式，通过这种方式，可以使用 supervisorctl 命令来管理一组进程。跟 [program:x] 的进程组不同的是，这里的进程是一个个的 [program:x] 。 123[group:thegroupname]programs=progname1,progname2 ; each refers to &apos;x&apos; in [program:x] definitionspriority=999 ; the relative start priority (default 999) 当添加了上述配置后，progname1 和 progname2 的进程名就会变成 thegroupname:progname1 和 thegroupname:progname2 以后就要用这个名字来管理进程了，而不是之前的 progname1。 以后执行 supervisorctl stop thegroupname: 就能同时结束 progname1 和 progname2，执行 supervisorctl stop thegroupname:progname1 就能结束 progname1。 结尾 实际上，默认情况下，supervisored 也是一个进程，最理想的的情况应该是将其安装为系统服务，安装方法可以参考这里, 安装脚本参考这里, 由于没有做具体的实验，此处不展开说明。 其实还有一个简单的方法，因为 Linux 在启动的时候会执行 /etc/rc.local 里面的脚本，所以只要在这里添加执行命令就可以 12345# 如果是 Ubuntu 添加以下内容/usr/local/bin/supervisord -c /etc/supervisord.conf# 如果是 Centos 添加以下内容/usr/bin/supervisord -c /etc/supervisord.conf 以上内容需要添加在 exit 命令前，而且由于在执行 rc.local 脚本时，PATH 环境变量未全部初始化，因此命令需要使用绝对路径。 在添加前，先在终端测试一下命令是否能正常执行，如果找不到 supervisord，可以用如下命令找到 1234sudo find / -name supervisordoutput:/usr/local/bin/supervisord]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Supervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang的IDE搭建各种方式的比较与踩坑备忘]]></title>
    <url>%2F2019%2F09%2F08%2Fgolang_ide_vsc_goland%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Go 语言支持以下系统： Linux FreeBSD Mac OS X（也称为 Darwin） Windows 安装包下载地址为：https://golang.org/dl/。 如果打不开可以使用这个地址：https://golang.google.cn/dl/。 . . . Windows 系统下安装GoWindows 下可以使用 .msi 后缀(在下载列表中可以找到该文件，如go1.4.2.windows-amd64.msi)的安装包来安装。 默认情况下 .msi 文件会安装在 c:\Go 目录下。你可以将 c:\Go\bin 目录添加到 Path 环境变量中(默认msi会帮你加入到PATH中)。添加后你需要重启命令窗口才能生效。 Go项目注意事项当你遇到了一些奇怪的环境配置问题可以尝试下面的建议, 否则建议走 GoLand流) go项目最好放到GOPATH的src文件夹下, 可以免除很多奇奇怪怪的麻烦 如果是Win平台, 尽量用PowerShell编译, 可以免除很多奇奇怪怪的麻烦, 不建议用GoLand的terminal, 谁也不知道他干了啥, 如果目录下有 go.mod 文件, go build 的时候默认是会从网上下载最新依赖库的, 所以如果想直接用vendor文件夹里的本地的依赖库编译可执行文件可以用命令 go build -mod=vendor, 如果目录下没有 go.mod 文件, 则直接 go build即可, 他会优先以vendor里的依赖库编译的 GoLand流(推荐)激活方法: Pycharm & Goland 激活 GoLand几乎不需要什么特别的配置, 不过有几点要注意: 如果当前项目没有放在GOPATH的src下, 则必须放在其他某个文件夹(比如是example_folder)的src文件夹下, 然后在goland的 settings-GOPATH-Project GOPATH 下加上 example_folder的路径, 并且编译时候使用goland的terminal来编译 如果本身项目中的vendor或其他本地文件夹已经包含所有第三方抵赖了, 记得把Goland的 settings-Go-Go Modules(vgo)的enable的 √ 去掉, 不然goland不会直接引用本地的这些依赖, 不仅go build会出错, 而且goland还不能正确的函数跳转, 而且goland还有可能会疯狂提示要你登录相应的git仓库的账号密码啥的 当出现了一些奇怪的无法编译或无法跳转问题, 则可参考 Go项目注意事项 VSCode流在 vscode 扩展里面搜索 go(MicroSoft出品的那个)，然后下载安装扩展。 安装 go 插件 在$GOPATH目录下创建bin,pkg,src 切换到$GOPATH/bin目录下，打开终端输入以下命令，不需要翻墙： go get -u -v github.com/josharian/impl go get -u -v github.com/mdempsky/gocode go get -u -v github.com/rogpeppe/godef go get -u -v github.com/golang/lint/golint go get -u -v github.com/lukehoban/go-find-references go get -u -v github.com/lukehoban/go-outline go get -u -v github.com/sqs/goreturns go get -u -v golang.org/x/tools/cmd/gorename go get -u -v github.com/tpng/gopkgs go get -u -v github.com/newhook/go-symbols go get -v -u github.com/peterh/liner github.com/derekparker/delve/cmd/dlv go get -u -v golang.org/x/tools/cmd/guru一共 11 个插件，由于被墙和依赖的缘故，很多插件是没办法正常安装的，但是 go 官方在 github 是有镜像仓库的，所以我们可以借助 github 来安装。 在src下创建golang.org/x/两个文件夹，然后切换到此目录下，打开终端输入： 1git clone https://github.com/golang/tools.git 此时x目录下会出现tools文件夹。 切换到$GOPATH目录下，打开终端输入以下命令安装，安装那些你上一步没有成功的插件： go install github.com/mdempsky/gocode go install github.com/rogpeppe/godef go install github.com/lukehoban/go-find-references go install github.com/lukehoban/go-outline go install github.com/sqs/goreturns go install golang.org/x/tools/cmd/gorename go install github.com/tpng/gopkgs go install github.com/josharian/impl go install github.com/newhook/go-symbols go install golang.org/x/tools/cmd/guru golint比较特殊,通过上面的方式还是无法安装，所以我们在x目录下打开终端执行： 12git clone https://github.com/golang/lint.gitgo install golang.org/x/lint/golint 此时所有插件安装成功。 第三方库依赖(可选项) 安装net库解决警告，切换到x目录，然后打开终端执行： 1git clone git@github.com:golang/net.git --depth 1 然后重启 vscode 即可。 安装text库解决警告，切换到x目录，然后打开终端执行： 1git clone git@github.com:golang/text.git --depth 1 然后重启 vscode 即可 调试配置 备注：go 的调试器是dlv 进入调试界面，按F5或者点击调试按钮，进入后添加配置(项目需要以文件夹的形式打开)。 回到hello.go文件, 按F5, 出现以下界面代表成功： 用户设置如果设置了系统级别的$GOPATH可以在用户设置里面覆盖。12345678910111213141516&quot;go.buildTags&quot;: &quot;&quot;,&quot;go.buildFlags&quot;: [],&quot;go.lintFlags&quot;: [],&quot;go.vetFlags&quot;: [],&quot;go.liveErrors&quot;: &#123; &quot;enabled&quot;: true, &quot;delay&quot;: 500&#125;,&quot;go.coverOnSave&quot;: false,&quot;go.useCodeSnippetsOnFunctionSuggest&quot;: true,&quot;go.useCodeSnippetsOnFunctionSuggestWithoutType&quot;: true,&quot;go.formatTool&quot;: &quot;goreturns&quot;,&quot;go.goroot&quot;: &quot;C:\\Go&quot;,&quot;go.gopath&quot;: &quot;C:\\Users\\b\\go&quot;,&quot;go.gocodeAutoBuild&quot;: false,&quot;go.autocompleteUnimportedPackages&quot;: true]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>GoLand</tag>
        <tag>Go</tag>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Surface使用优化]]></title>
    <url>%2F2019%2F08%2F13%2Fsurface_optimization%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[其他设备投影不到surface上更新其他的设备的网卡驱动 windows更新之后windows hello出问题比如报出以下问题: 识别反应慢 解锁之后摄像头的指示灯还在亮 先尝试 进入 设置-系统-电源/睡眠-网络连接, 设置为睡眠总是断开网络 如果不行,则尝试回退或者重装以下驱动: Biometric devices(Windows Hello Face Software Device或者类似的名字) Cameras(Surface Camera Front / Surfa IR Camera Front 或者其他的名字) Display adapters(你没看错, 显卡驱动也会影响windows hello…) 都没用就去尝试还原系统,还是没用的话只能去setting里尝试recovery了, 最好的办法是 屏蔽更新 触摸板某些三指或四指手势在某些app中无法使用原因是触摸板的手势无法在以管理员启动的app中使用, 因为没找到触摸板是哪个程序启动的, 所以解决办法是以管理员运行所有程序(这样就包括触摸板也以管理员启动了), 参考Win10系统下怎么让所有程序都默认以管理员身份运行 亮度忽明忽暗的修复方法(都不完美): (这种方法会有个开机启动项, 而且好像会造成instant-on功能失效) 去 MicroSoft Store 下载安装 英特尔显卡控制中心, 然后 打开之后, 系统-功率-显示器节能 (这种方法每次更新系统都有可能又失效了, 而且有时候会无法调节亮度, 重启又好了) 注册表修改键值： 1. win+R，输入regedit，回车 2. 导航到 计算机\HKEY_LOCAL_MACHINE\SYSTEM\ControlSet001\Control\Class\{4d36e968-e325-11ce-bfc1-08002be10318}\0000 或者 0001 的右边找到FeatureTestControl 3. 双击键值，将9240或200更改为9250，重启即可。 . . . 打开高级电源计划调整选项不建议改, 用默认预装的电源计划即可, 自己配的有各种问题, 比如instant-on失效, 比如盒盖不能听歌, 等等等等… The limited power configuration in Advanced Settings because of Connected Standby feature that enabled by default on every Surface devices. In order to create a new power plan with optimized advanced settings, we need to disable connected standby via Registry Editor. To turn off connected standby: press Win + R Type regedit to open Register Editor. Now you need to go to 1HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Power Double click on CsEnabled and change Value data from 1 to 0, and click OK. Restart your computer to apply these changes to your system. After restarting your computer, now you can access the full list of power plans and individual advanced settings. 网速慢修复方法: Press Windows button. Search for Regedit. Open it Navigate to the following path: HKEY_LOCAL_MACHINE\SYSTEM\ControlSet001\Services\mrvlpcie8897 Find the item labeled “TXAMSDU.” Double tap and modify the value from 1 to 0 Restart the machine link: https://www.windowscentral.com/surface-pro-4-slow-wi-fi-fix 解决surface pro掉TF卡问题在解决之前搜了很多方法.包括什么高性能 更新驱动 改注册表 都不行 或许有的人因为兼容问题通过以上的方法可以解决.但是我依然还存在这个情况. 后来经过研究发现是插口断电导致的 现在已经解决 此贴分享给与我一样情况的小伙伴们 : 设备管理器-通用串行总线控制器-Realtek USB 3.0 Card Reader-更新驱动程序-浏览我的计算机查找驱动-让我从计算机上可用驱动列表查找-USB大容量设备(这个步骤把SD卡接口驱动改为USB驱动, 然后你会发现原来的 Realtek USB 3.0 Card Reader 变成了 USB大容量存储设备 设备管理器-通用串行总线控制器-USB大容量存储设备-属性-电源管理-允许计算机关闭此设备以节省电源(不勾选)]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Surface</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于SIGPIPE和SIGHUP]]></title>
    <url>%2F2019%2F08%2F12%2Fabout_sighup_sigpipe%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[SIGHUP 信号 在介绍 SIGHUP 信号之前，先来了解两个概念：进程组和会话。 . . . 进程组 进程组就是一系列相互关联的进程集合，系统中的每一个进程也必须从属于某一个进程组；每个进程组中都会有一个唯一的 ID(process group id)，简称 PGID；PGID 一般等同于进程组的创建进程的 Process ID，而这个进进程一般也会被称为进程组先导 (process group leader)，同一进程组中除了进程组先导外的其他进程都是其子进程； 进程组的存在，方便了系统对多个相关进程执行某些统一的操作，例如，我们可以一次性发送一个信号量给同一进程组中的所有进程。 会话 会话（session）是一个若干进程组的集合，同样的，系统中每一个进程组也都必须从属于某一个会话；一个会话只拥有最多一个控制终端（也可以没有），该终端为会话中所有进程组中的进程所共用。一个会话中前台进程组只会有一个，只有其中的进程才可以和控制终端进行交互；除了前台进程组外的进程组，都是后台进程组；和进程组先导类似，会话中也有会话先导 (session leader) 的概念，用来表示建立起到控制终端连接的进程。在拥有控制终端的会话中，session leader 也被称为控制进程(controlling process)，一般来说控制进程也就是登入系统的 shell 进程(login shell)； 执行睡眠后台进程 sleep 50 &amp; 之后，通过 ps 命令查看该进程及 shell 信息如上图： PPID 指父进程 id； PID 指进程 id； PGID 指进程组 id SID 指会话 id； TTY 指会话的控制终端设备； COMMAND 指进程所执行的命令 TPGID 指前台进程组的 PGID。 SIGHUP 信号的触发及默认处理 在对会话的概念有所了解之后，我们现在开始正式介绍一下 SIGHUP 信号，SIGHUP 信号在 用户终端连接 (正常或非正常) 结束 时发出, 通常是在终端的控制进程结束时, 通知同一 session 内的各个作业, 这时它们与控制终端不再关联. 系统对 SIGHUP 信号的默认处理是终止收到该信号的进程。所以若程序中没有捕捉该信号，当收到该信号时，进程就会退出。 SIGHUP 会在以下 3 种情况下被发送给相应的进程： 终端关闭时，该信号被发送到 session 首进程以及作为 job 提交的进程（即用 &amp; 符号提交的进程）； session 首进程退出时，该信号被发送到该 session 中的前台进程组中的每一个进程； 若父进程退出导致进程组成为孤儿进程组，且该进程组中有进程处于停止状态（收到 SIGSTOP 或 SIGTSTP 信号），该信号会被发送到该进程组中的每一个进程。 例如：在我们登录 Linux 时，系统会分配给登录用户一个终端 (Session)。在这个终端运行的所有程序，包括前台进程组和后台进程组，一般都属于这个 Session。当用户退出 Linux 登录时，前台进程组和后台有对终端输出的进程将会收到 SIGHUP 信号。这个信号的默认操作为终止进程，因此前台进 程组和后台有终端输出的进程就会中止。 此外，对于与终端脱离关系的守护进程，正常情况下是永远都收不到这个信号的, 所以可以人为的发SIGHUP信号给她用于通知它做一些想要的自定义的操作, 比较常见的如重新读取配置文件操作。 比如 xinetd 超级服务程序。 当 xinetd 程序在接收到 SIGHUP 信号之后调用 hard_reconfig 函数，它将循环读取 / etc/xinetd.d / 目录下的每个子配置文件，并检测其变化。如果某个正在运行的子服务的配置文件被修改以停止服务，则 xinetd 主进程讲给该子服务进程发送 SIGTERM 信号来结束它。如果某个子服务的配置文件被修改以开启服务，则 xinetd 将创建新的 socket 并将其绑定到该服务对应的端口上。 SIGPIPE 在网络编程中，SIGPIPE 这个信号是很常见的。当往一个写端关闭的管道或 socket 连接中连续写入数据时会引发 SIGPIPE 信号, 引发 SIGPIPE 信号的写操作将设置 errno 为 EPIPE。在 TCP 通信中，当通信的双方中的一方 close 一个连接时，若另一方接着发数据，根据 TCP 协议的规定，会收到一个 RST 响应报文，若再往这个服务器发送数据时，系统会发出一个 SIGPIPE 信号给进程，告诉进程这个连接已经断开了，不能再写入数据。 测试程序如下：简单的测试程序，函数未加错误判断server.c 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;signal.h&gt;#define port 8888void handle(int sig)&#123; printf("SIGPIPE : %d\n",sig);&#125;void mysendmsg(int fd)&#123; // 写入第一条消息 char* msg1 = "first msg"; int n = write(fd, msg1, strlen(msg1)); if(n &gt; 0) //成功写入第一条消息,server 接收到 client 发送的 RST &#123; printf("success write %d bytes\n", n); &#125; // 写入第二条消息,触发SIGPIPE char* msg2 = "second msg"; n = write(fd, msg2, strlen(msg2)); if(n &lt; 0) &#123; printf("write error: %s\n", strerror(errno)); &#125;&#125;int main()&#123; signal(SIGPIPE , handle); //注册信号捕捉函数 struct sockaddr_in server_addr; bzero(&amp;server_addr, sizeof(server_addr)); server_addr.sin_family = AF_INET; server_addr.sin_addr.s_addr = htonl(INADDR_ANY); server_addr.sin_port = htons(port); int listenfd = socket(AF_INET , SOCK_STREAM , 0); bind(listenfd, (struct sockaddr *)&amp;server_addr, sizeof(server_addr)); listen(listenfd, 128); int fd = accept(listenfd, NULL, NULL); if(fd &lt; 0) &#123; perror("accept"); exit(1); &#125; mysendmsg(fd); return 0;&#125; client.c123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;errno.h&gt;#include&lt;string.h&gt;#include&lt;sys/types.h&gt;#include&lt;netinet/in.h&gt;#include&lt;sys/socket.h&gt;#include&lt;sys/wait.h&gt;#include&lt;arpa/inet.h&gt;#include&lt;unistd.h&gt;#define PORT 8888#define MAX 1024int main()&#123; char buf[MAX] = &#123;'0'&#125;; int sockfd; int n; socklen_t slen; slen = sizeof(struct sockaddr); struct sockaddr_in seraddr; bzero(&amp;seraddr,sizeof(seraddr)); seraddr.sin_family = AF_INET; seraddr.sin_port = htons(PORT); seraddr.sin_addr.s_addr = htonl(INADDR_ANY); //socket() if((sockfd = socket(AF_INET,SOCK_STREAM,0)) == -1) &#123; perror("socket"); exit(-1); &#125; //connect() if(connect(sockfd,(struct sockaddr *)&amp;seraddr,slen) == -1) &#123; perror("connect"); exit(-1); &#125; int ret = shutdown(sockfd , SHUT_RDWR); if(ret &lt; 0) &#123; perror("shutdown perror"); &#125; return 0;&#125; 运行结果 此外，因为 SIGPIPE 信号的默认行为是结束进程，而我们绝对不希望因为写操作的错误而导致程序退出，尤其是作为服务器程序来说就更恶劣了。所以我们应该对这种信号加以处理，在这里，介绍两种处理 SIGPIPE 信号的方式： 给 SIGPIPE 设置 SIG_IGN 信号处理函数，忽略该信号:signal(SIGPIPE, SIG_IGN);前文说过，引发 SIGPIPE 信号的写操作将设置 errno 为 EPIPE,。所以，第二次往关闭的 socket 中写入数据时, 会返回 - 1, 同时 errno 置为 EPIPE. 这样，便能知道对端已经关闭，然后进行相应处理，而不会导致整个进程退出. 使用 send 函数的 MSG_NOSIGNAL 标志来禁止写操作触发 SIGPIPE 信号。send(sockfd , buf , size , MSG_NOSIGNAL);同样，我们可以根据 send 函数反馈的 errno 来判断 socket 的读端是否已经关闭。此外，我们也可以通过 IO 复用函数来检测管道和 socket 连接的读端是否已经关闭。以 POLL 为例，当 socket 连接被对方关闭时，socket 上的 POLLRDHUP 事件将被触发。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>NP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP安全性和幂等性]]></title>
    <url>%2F2019%2F07%2F11%2Fhttp_safety_idempotence%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[GET操作是安全的。 所谓安全是指不管进行多少次操作，资源的状态都不会改变。 比如我用GET浏览文章，不管浏览多少次，那篇文章还在那，没有变化。当然，你可能说每浏览一次文章，文章的浏览数就加一，这不也改变了资源的状态么？这并不矛盾，因为这个改变不是GET操作引起的，而是用户自己设定的服务端逻辑造成的。 . . . PUT，DELETE操作是幂等的。 所谓幂等是指不管进行多少次操作，结果都一样。 比如我用PUT修改一篇文章，然后在做同样的操作，每次操作后的结果并没有不同，DELETE也是一样。顺便说一句，因为GET操作是安全的，所以它自然也是幂等的。 POST操作既不是安全的，也不是幂等的，比如常见的POST重复加载问题：当我们多次发出同样的POST请求后，其结果是创建出了若干的资源。安全和幂等的意义在于：当操作没有达到预期的目标时，我们可以不停的重试，而不会对资源产生副作用。从这个意义上说，POST操作往往是有害的，但很多时候我们还是不得不使用它。 还有一点需要注意的就是，创建操作可以使用POST，也可以使用PUT，区别在于POST 是作用在一个集合资源之上的（/uri），而PUT操作是作用在一个具体资源之上的（/uri/xxx），再通俗点说，如果URL可以在客户端确定，那么就使用PUT，如果是在服务端确定，那么就使用POST，比如说很多资源使用数据库自增主键作为标识信息，而创建的资源的标识信息到底是什么只能由服务端提供，这个时候就必须使用POST。 关于GET POST 的混淆先说相同点，只有了解了相同点之后才能理解为什么会发生混淆。两者都能向服务器发送数据，提交的“内容”[注1]的格式相同，都是 param1=value1&amp;param2=value2&amp;.... get 和 post 区别如字面，一个是get（获取），一个是post（发送）。 get用来告诉服务器需要获取哪些内容（uri+query），向静态页面（uri）请求则直接返回文件内容给浏览器，向一个动态页面请求时可以提供查询参数（query）以获得相应内容。 post用来向服务器提交内容，主要是为了提交，而不是为了请求内容，就是说post的初衷并不要求服务器返回内容[注2]，只是提交内容让服务器处理（主要是存储或者处理之后再存储）。 get和post出现混淆是因为对提交的数据处理方法的滥用造成的，数据是无辜的。 混淆之一：将get提交的用来查询的字段当作是存储数据存入了服务器端文件或者数据库。然后就误以为get是用来提交用于存储的数据的。 混淆之二：编写脚本在服务器端通过处理post提交的数据并返回内容。只要有数据，就能用来进行判断，脚本怎写是程序员的事，而不在乎数据来源的形式（post、get，或者是自己预设值的常量）。这点功能上确实没问题，只是背离的其初始目的而已。 由于都是要传送数据，且数据格式相同（即使数据格式不同，只要能提取出相应数据）。使用的时候难免出现张冠李戴，将get数据用来存储、将post数据用来检索返回数据。但是二者还是有区别的（主要是根据其用途而“人为”造成的），get的长度限制在2048字节（由浏览器和服务器限制的，这是目前IE的数据，曾经是1024字节），很大程度上限制了get用来传递“存储数据”的数据的能力，所以还是老老实实用来做检索吧；post则无此限制（只是HTTP协议规范没有进行大小限制，但受限于服务器的处理能力），因此对于大的数据（一般来说需要存储的数据可能会比较大，比2048字节大）的传递有天然的优势，谁让它是 nature born post 呢。 get提交的数据是放在url里，目的是灵活的向服务其提交检索请求，可以在地址栏随时修改数据以变更需要获取的内容，比如直接修改分页的编号就跳到另外一个分页了（当然也可能是 404）。post提交的数据放在http请求的正文里，目的在于提交数据并用于服务器端的存储，而不允许用户过多的更改相应数据（主要是相对于在url 修改要麻烦很多，url的修改只要点击地址栏输入字符就可以了），除非是专门跑来编辑数据的。 PS：post和get的安全性在传输的层面上区别不大，但是采用url提交数据的get方式容易被人肉眼看到，或者出现在历史纪录里，还是可能被肉眼看到，都是一些本地的问题。 注1：我强调的是内容，至于http协议中的get和post的格式大家有兴趣就自己看看吧。 注2：get方式主要是为了获得预期内容，即uri+query相同时所得到的内容应该是相同的。而post主要是提交内容，至于是否有必要返回页面可能只是出于用户体验，比如注册时返回你的注册id，但是如果只是返回一个“您已注册成功”的相同页面（即使你post的数据不一样）也没什么好奇怪的。注3：关于这个“人为”，不是那么贴切，get和post还是有技术层面的区别的。但是从表象上看暂且这么说吧，毕竟二者的混淆也是“人为”的。 POST GET 本质区别一般在浏览器中输入网址访问资源都是通过GET方式；在FORM提交中，可以通过Method指定提交方式为GET或者POST，默认为GET提交 Http定义了与服务器交互的不同方法，最基本的方法有4种，分别是GET，POST，PUT，DELETE URL 全称是资源描述符，我们可以这样认为：一个URL地址，它用于描述一个网络上的资源，而HTTP中的GET，POST，PUT，DELETE就对应着对这个资源的查 ，改 ，增 ，删 4个操作。到这里，大家应该有个大概的了解了，GET一般用于获取/查询 资源信息，而POST一般用于更新 资源信息(个人认为这是GET和POST的本质区别，也是协议设计者的本意，其它区别都是具体表现形式的差异 )。 根据HTTP规范，GET用于信息获取，而且应该是安全的和幂等的 。 所谓安全的意味着该操作用于获取信息而非修改信息。换句话说，GET请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。 注意：这里安全的含义仅仅是指是非修改信息。 幂等的意味着对同一URL的多个请求应该返回同样的结果。这里我再解释一下幂等 这个概念: 幂等 （idempotent、idempotence）是一个数学或计算机学概念，常见于抽象代数中。 幂等有以下几种定义：对于单目运算，如果一个运算对于在范围内的所有的一个数多次进行该运算所得的结果和进行一次该运算所得的结果是一样的，那么我们就称该运算是幂等的。比如绝对值运算就是一个例子，在实数集中，有abs(a) = abs(abs(a)) 。 对于双目运算，则要求当参与运算的两个值是等值的情况下，如果满足运算结果与参与运算的两个值相等，则称该运算幂等，如求两个数的最大值的函数，有在在实数集中幂等，即max(x,x) = x 。 看完上述解释后，应该可以理解GET幂等的含义了。 但在实际应用中，以上2条规定并没有这么严格。引用别人文章的例子：比如，新闻站点的头版不断更新。虽然第二次请求会返回不同的一批新闻，该操作仍然被认为是安全的和幂等的，因为它总是返回当前的新闻。从根本上说，如果目标是当用户打开一个链接时，他可以确信从自身的角度来看没有改变资源即可。 根据HTTP规范，POST表示可能修改变服务器上的资源的请求 。 继续引用上面的例子：还是新闻以网站为例，读者对新闻发表自己的评论应该通过POST实现，因为在评论提交后站点的资源已经不同了，或者说资源被修改了。 上面大概说了一下HTTP规范中，GET和POST的一些原理性的问题。但在实际的做的时候，很多人却没有按照HTTP规范去做，导致这个问题的原因有很多，比如说： 很多人贪方便，更新资源时用了GET，因为用POST必须要到FORM（表单），这样会麻烦一点。 对资源的增，删，改，查操作，其实都可以通过GET/POST完成，不需要用到PUT和DELETE。 另外一个是，早期的但是Web MVC框架设计者们并没有有意识地将URL当作抽象的资源来看待和设计 。还有一个较为严重的问题是传统的Web MVC框架基本上都只支持GET和POST两种HTTP方法，而不支持PUT和DELETE方法。 转自: https://286.iteye.com/blog/1420713]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>noodle</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd的restful接口]]></title>
    <url>%2F2019%2F07%2F10%2Fetcd_api_note%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[. . . etcd api接口 基本操作api: https://github.com/coreos/etcd/blob/6acb3d67fbe131b3b2d5d010e00ec80182be4628/Documentation/v2/api.md 集群配置api:&nbsp;https://github.com/coreos/etcd/blob/6acb3d67fbe131b3b2d5d010e00ec80182be4628/Documentation/v2/members_api.md 鉴权认证api:&nbsp;https://github.com/coreos/etcd/blob/6acb3d67fbe131b3b2d5d010e00ec80182be4628/Documentation/v2/auth_api.md 配置项：https://github.com/coreos/etcd/blob/master/Documentation/op-guide/configuration.md https://coreos.com/etcd/docs/latest/runtime-configuration.html https://coreos.com/etcd/docs/latest/clustering.html https://coreos.com/etcd/docs/latest/runtime-configuration.html https://coreos.com/etcd/docs/latest/ https://coreos.com/etcd/docs/latest/admin_guide.html#disaster-recovery 采用标准的restful 接口，支持http 和 https 两种协议。运行单机etcd服务1 ./bin/etcd&nbsp; 监听localhost和从IANA分配的端口，2379用于同client通讯，2389用于server与server直接的通讯。&nbsp;获取版本 &nbsp;/version1 [root@vStack ~]# curl http://127.0.0.1:2379/version | python -m json.tool2 % Total % Received % Xferd Average Speed Time Time Time Current3 Dload Upload Total Spent Left Speed4 100 44 100 44 0 0 14093 0 –:–:– –:–:– –:–:– 220005 {6 “etcdcluster“: “2.3.0“,7 “etcdserver“: “2.3.7“8 }&nbsp;&nbsp;etcd 的基本API是一个分层的key空间。key空间由通常被称为”nodes”（节点）的keys和目录组成。对datastore的访问，即通过 /version/keys&nbsp;端点(endpoint) 访问key空间。1. PUT 为etcd存储的键赋值， 即创建 message 键值，赋值为”Hello world” 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message -X PUT -d value=”Hello world” | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 119 100 102 100 17 38230 6371 –:–:– –:–:– –:–:– 51000 5 { 6 “action“: “set“, 7 “node“: { 8 “createdIndex“: 30, 9 “key“: “/message“,10 “modifiedIndex“: 30,11 “value“: “Hello world“12 }13 } Response body返回值中的： action： &nbsp; 请求接口进行的动作名称。 通过 http &nbsp;PUT方法修改node.key的值，对应的action值为：”set&ldquo;。 &nbsp;PUT方法中，请求body中存在 prevExist=true时， action为update；&nbsp;prevExist=false时，action为create；&nbsp;其他为set。 node.createIndex: &nbsp;etcd每次变化时创建的，唯一的，单调递增的、整数值作为索引。这个特定的索引值反映了在etcd状态成员里创建了一个给定key。除了用户请求外，etcd内部运行（如启动服务，重启服务、集群信息变化：添加、删除、同步服务等）也可能会因为对节点有变动而引起该值的变化。所以即使我们首次请求，此值也不是从1开始。update、get&nbsp;action不引起 node.createIndex值的变化。&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;node.key: &nbsp;在请求的HTTP路径中，作为操作对象key。etcd使用一个类似文件系统的方式来反映键值存储的内容， 因此所有的key都是以&lsquo;/&rsquo;开始 。&nbsp; &nbsp; &nbsp; node.modifiedIndex: &nbsp;像&nbsp;node.createIndex, 这个属性也是etcd的索引。 引起这个值变化的Actions包括：set，delete，update，create，compareAndSwap 和 compareAndDelete。因为 get 和 watchcommands 在存储中不修改状态，所以这两个action不会修改mode.modifiedIndex值， 也不会修改 node.createIndex的值。 重启服务等也会修改此属性值。&nbsp; &nbsp; &nbsp;&nbsp;node.value: &nbsp;处理完请求后的key值。 在上面的实例中，成功请求后，修改节点的值为 Hello world。&nbsp;&nbsp; &nbsp; &nbsp; Response header返回值中： 在responses中包括一些的HTTP 的headers部，在header中提供了一些关于etcd集群的全部信息，集群提供服务请求。1 X-Etcd-Cluster-Id: 7e27652122e8b2ae2 X-Etcd-Index: 933 X-Raft-Index: 2236964 X-Raft-Term: 8&nbsp; &nbsp; X-Etcd-Cluster-Id: &nbsp;etcd 集群id。 X-Etcd-Index： &nbsp;当前etcd的索引，像前面的解释。当在key空间进行watch时，watch开始时，X-Etcd-Index是当前etcd的索引值，这意味着watched事件可能发生在X-Etcd-Index之后。 X-Raft-Index： 与X-Etcd-Index索引类似，是raft协议的索引。 X-Raft-Term： &nbsp;是一个在集群中发生master election时，将增长的整数。如果这个值增长的非常快，需要调优这个election超时。详见&nbsp;tuning&nbsp;部分。&nbsp;2. GET 查询etcd某个键存储的值[root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message | python -m json.tool % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 102 100 102 0 0 64110 0 –:–:– –:–:– –:–:– 99k{ “action“: “get“, “node“: { “createdIndex“: 19, “key“: “/message“, “modifiedIndex“: 19, “value“: “Hello world“ }}&nbsp;3. PUT 修改键值：与创建新值几乎相同，但是反馈时会有一个prevNode值反应了修改前存储的内容。 -d value=xxxx[root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message -X PUT -d value=”RECREATE” | python -m json.tool % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 202 100 188 100 14 57108 4252 –:–:– –:–:– –:–:– 62666{ “action“: “set“, “node“: { “createdIndex“: 33, “key“: “/message“, “modifiedIndex“: 33, “value“: “RECREATE“ }, “prevNode“: { “createdIndex“: 32, “key“: “/message“, “modifiedIndex“: 32, “value“: “Hello world“ }} Respone中新的字段 “prevNode”, 这个字段表示当前请求完成前的请求节点的状态。 prevNode的格式与node相同， 在访问的节点没有前面状态时将被忽略。&nbsp;&nbsp;4. DELETE 删除一个值&nbsp; 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message -X DELETE | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 177 100 177 0 0 73261 0 –:–:– –:–:– –:–:– 172k 5 { 6 “action“: “delete“, 7 “node“: { 8 “createdIndex“: 19, 9 “key“: “/message“,10 “modifiedIndex“: 2911 },12 “prevNode“: {13 “createdIndex“: 19,14 “key“: “/message“,15 “modifiedIndex“: 28,16 “value“: “test createIndex“17 }18 }&nbsp;5. PUT 对一个键进行定时删除：etcd中对键进行定时删除，设定一个ttl值，当这个值到期时键就会被删除。反馈的内容会给出expiration项告知超时时间，ttl项告知设定的时长。&nbsp; &nbsp; 在设定一个key时，设定其ttl（time to live), ttl时间后，自动删除。 -d&nbsp;ttl=xxx 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -d ttl=5 | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 159 100 144 100 15 60453 6297 –:–:– –:–:– –:–:– 72000 5 { 6 “action“: “set“, 7 “node“: { 8 “createdIndex“: 34, 9 “expiration“: “2016-04-23T12:01:57.992249507Z“,10 “key“: “/foo“,11 “modifiedIndex“: 34,12 “ttl“: 5,13 “value“: “bar“14 }15 }&nbsp; 在repsonse中有两个新的字段： expiration：key的有效截至日期。 ttl: &nbsp; &nbsp; &nbsp; key的ttl值，单位秒。&nbsp; &nbsp; &nbsp;注意|： key只有被cluster header设定过期，如果一个memeber 脱离的集群，它里面的key将没有过期，直到重新加入后才有过期功能。&nbsp;6. PUT 取消定时删除任务 -d ttl= 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -d ttl= -d prevExist=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 254 100 225 100 29 98944 12752 –:–:– –:–:– –:–:– 219k 5 { 6 “action“: “update“, 7 “node“: { 8 “createdIndex“: 38, 9 “key“: “/foo“,10 “modifiedIndex“: 39,11 “value“: “bar“12 },13 “prevNode“: {14 “createdIndex“: 38,15 “expiration“: “2016-04-23T12:07:05.415596297Z“,16 “key“: “/foo“,17 “modifiedIndex“: 38,18 “ttl“: 78,19 “value“: “bar“20 }21 }&nbsp;7. PUT 刷新key的 ttl&nbsp; ttl 到删除key和重新设置ttl，都会触发watcher。通过在请求的body中增加 refresh=true，更新ttl(必须存在)，不引起触发watcher事件。 -d refresh=true 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message -XPUT -d ttl=100 -d refresh=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 304 100 284 100 20 46973 3307 –:–:– –:–:– –:–:– 56800 5 { 6 “action“: “set“, 7 “node“: { 8 “createdIndex“: 145, 9 “expiration“: “2016-12-28T06:58:20.426383304Z“,10 “key“: “/message“,11 “modifiedIndex“: 145,12 “ttl“: 100,13 “value“: “”14 },15 “prevNode“: {16 “createdIndex“: 144,17 “expiration“: “2016-12-28T06:57:55.628682326Z“,18 “key“: “/message“,19 “modifiedIndex“: 144,20 “ttl“: 76,21 “value“: “”22 }23 }&nbsp;&nbsp;8. GET 对键值修改进行监控：etcd提供的这个API通过long polling(轮询)让用户可以监控一个值或者递归式(recursive=true 在url path中作为参数)地监控一个目录及其子目录的值，当目录或值发生变化时，etcd会主动通知。 ？wait=true &nbsp; &nbsp; 监听当前节点&nbsp; &nbsp; &nbsp; ？recursive=true&nbsp; 递归监听当前节点和子目录&nbsp; &nbsp; &nbsp; ？waitIndex=xxx 监听过去已经发生的。过去值的查询或监听， 必选与wait一起使用。 1 [root@vStack ~]# curl ‘http://127.0.0.1:2379/v2/keys/message?wait=true&amp;waitIndex=2230‘ | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 183 0 183 0 0 131k 0 –:–:– –:–:– –:–:– 178k 5 { 6 “action“: “set“, 7 “node“: { 8 “createdIndex“: 2230, 9 “key“: “/message“,10 “modifiedIndex“: 2230,11 “value“: “123“12 },13 “prevNode“: {14 “createdIndex“: 2229,15 “key“: “/message“,16 “modifiedIndex“: 2229,17 “value“: “123“18 }19 }&nbsp; watch 一个ttl自删除的key时，收到如下 &ldquo;expire&rdquo; action。 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/message?wait=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 215 0 215 0 0 19 0 –:–:– 0:00:10 –:–:– 45 5 { 6 “action“: “expire“, 7 “node“: { 8 “createdIndex“: 2223, 9 “key“: “/message“,10 “modifiedIndex“: 222411 },12 “prevNode“: {13 “createdIndex“: 2223,14 “expiration“: “2016-12-28T09:25:00.028597482Z“,15 “key“: “/message“,16 “modifiedIndex“: 2223,17 “value“: “”18 }19 }&nbsp;&nbsp;9. GET 对过去的键值操作进行查询：类似上面提到的监控，在其基础上指定过去某次修改的索引编号，就可以查询历史操作。默认可查询的历史记录为1000条。 ？ waitIndex=xxx &nbsp;&nbsp;监听过去已经发生的。 这个在确保在watch命令中，没有丢失事件非常有用。例如：我们反复watch 我们得到节点的 modifiedIndex+1。 因为 node 的modifiedIndex的值是不连续，如果waitIndex的值没有相应modifiedIndex，返回最大的modifedIndex的节点信息。&nbsp;如果大于节点中所有的modifiedIndex，等待，直到节点的modifiedIndex值大于等于waitIndex的值。 即使删除key后，也可以查询历史数据。 store中有一个全局的currentIndex，每次变更，index会加1.然后每个event都会关联到currentIndex. 当客户端调用watch接口（参数中增加 wait参数）时，如果请求参数中有waitIndex，并且waitIndex 小于 currentIndex，则从 EventHistroy 表中查询index小于等于waitIndex，并且和watch key 匹配的 event，如果有数据，则直接返回。如果历史表中没有或者请求没有带 waitIndex，则放入WatchHub中，每个key会关联一个watcher列表。 当有变更操作时，变更生成的event会放入EventHistroy表中，同时通知和该key相关的watcher。 注意： 1. 必须与 wait 一起使用；&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2. curl 中url需要使用引号。&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3. etcd 仅仅保留系统中所有key最近的1000条event，建议将获取到的response发送到另一个线程处理，而不是处理response而阻塞watch。&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4. 如果watch超出了etcd保存的最近1000条，建议get后使用response header中的&nbsp;X-Etcd-Index&nbsp;+ 1进行重新watch，而不是使用node中的modifiedIndex+1. 因为 &nbsp;X-Etcd-Index&nbsp; 永远大于等于modifiedIndex， 使用modifiedIndex可能会返回401错误码，同样超出。&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;5. long polling可能会被服务器关闭，如超时或服务器关闭。导致仅仅收到header 200OK，body为空，此时应重新watch。 1 [root@vStack ~]# curl ‘http://127.0.0.1:2379/v2/keys/foo?wait=true&amp;waitIndex=2‘ | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 144 0 144 0 0 102k 0 –:–:– –:–:– –:–:– 140k 5 { 6 “action“: “set“, 7 “node“: { 8 “createdIndex“: 34, 9 “expiration“: “2016-04-23T12:01:57.992249507Z“,10 “key“: “/foo“,11 “modifiedIndex“: 34,12 “ttl“: 5,13 “value“: “bar“14 }15 }&nbsp;如果超出了etcd保留的最近1000条，返回 401错误码 1 [root@vStack ~]# curl ‘http://127.0.0.1:2379/v2/keys/message?wait=true&amp;waitIndex=8‘ | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 154 100 154 0 0 56163 0 –:–:– –:–:– –:–:– 150k 5 { 6 “cause“: “the requested history has been cleared [1186/8]“, 7 “errorCode“: 401, 8 “index“: 2185, 9 “message“: “The event in requested index is outdated and cleared“10 }&nbsp; 10. PUT&nbsp;创建目录 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/dir -XPUT -d dir=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 95 100 87 100 8 21260 1955 –:–:– –:–:– –:–:– 29000 5 { 6 “action“: “set“, 7 “node“: { 8 “createdIndex“: 63, 9 “dir“: true,10 “key“: “/dir“,11 “modifiedIndex“: 6312 }13 }&nbsp;11. GET 列出目录下所有的节点信息，最后以/结尾(不是必须的)。还可以通过recursive参数递归列出所有子目录信息。 没有recursive，返回第二级。后面不在返回。 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/dir1/ | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 167 100 167 0 0 65234 0 –:–:– –:–:– –:–:– 83500 5 { 6 “action“: “get“, 7 “node“: { 8 “createdIndex“: 67, 9 “dir“: true,10 “key“: “/dir1“,11 “modifiedIndex“: 67,12 “nodes“: [13 {14 “createdIndex“: 67,15 “dir“: true,16 “key“: “/dir1/dir2“,17 “modifiedIndex“: 6718 }19 ]20 }21 }&nbsp;12. POST 自动在目录下创建有序键。在对创建的目录使用POST参数，会自动在该目录下创建一个以global etcd index值为键的值，这样就相当于根据创建时间的先后进行了严格排序。该API对分布式队列这类场景非常有用。 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/queue -XPOST -d value=Job1 | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 127 100 117 100 10 31655 2705 –:–:– –:–:– –:–:– 39000 5 { 6 “action“: “create“, 7 “node“: { 8 “createdIndex“: 47, 9 “key“: “/queue/00000000000000000047“,10 “modifiedIndex“: 47,11 “value“: “Job1“12 }13 }&nbsp;13. GET&nbsp;按顺序列出所有创建的有序键 ？ sorted=true ? recursive=true 1 [root@vStack ~]# curl -s ‘http://127.0.0.1:2379/v2/keys/queue?sorted=true‘ | python -m json.tool 2 { 3 “action“: “get“, 4 “node“: { 5 “createdIndex“: 46, 6 “dir“: true, 7 “key“: “/queue“, 8 “modifiedIndex“: 46, 9 “nodes“: [10 {11 “createdIndex“: 46,12 “key“: “/queue/00000000000000000046“,13 “modifiedIndex“: 46,14 “value“: “”15 },16 {17 “createdIndex“: 47,18 “key“: “/queue/00000000000000000047“,19 “modifiedIndex“: 47,20 “value“: “Job1“21 },22 {23 “createdIndex“: 48,24 “key“: “/queue/00000000000000000048“,25 “modifiedIndex“: 48,26 “value“: “aaaa“27 },28 {29 “createdIndex“: 49,30 “key“: “/queue/00000000000000000049“,31 “modifiedIndex“: 49,32 “value“: “aaaa“33 },34 {35 “createdIndex“: 50,36 “key“: “/queue/00000000000000000050“,37 “modifiedIndex“: 50,38 “value“: “aaaa“39 },40 {41 “createdIndex“: 51,42 “key“: “/queue/00000000000000000051“,43 “modifiedIndex“: 51,44 “value“: “aaaa“45 }46 ]47 }48 }&nbsp;14. DELETE&nbsp;删除目录：默认情况下只允许删除空目录，如果要删除有内容的目录需要加上recursive=true参数。 ？dir=true 删除目录&nbsp; ？recursive=true &nbsp;删除非空目录&nbsp; &nbsp; &nbsp; 删除非空目录必须使用 recursive=true 参数，删除空目录，dir=true或recursive=true至少有一个。 1 [root@vStack ~]# curl ‘http://127.0.0.1:2379/v2/keys/dir1?dir=true‘ -XDELETE | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 77 100 77 0 0 38557 0 –:–:– –:–:– –:–:– 77000 5 { 6 “cause“: “/dir1“, 7 “errorCode“: 108, 8 “index“: 67, 9 “message“: “Directory not empty“10 }11 [root@vStack ~]# curl ‘http://127.0.0.1:2379/v2/keys/dir1?dir=true&amp;recursive=true‘ -XDELETE | python -m json.tool12 % Total % Received % Xferd Average Speed Time Time Time Current13 Dload Upload Total Spent Left Speed14 100 166 100 166 0 0 62032 0 –:–:– –:–:– –:–:– 8300015 {16 “action“: “delete“,17 “node“: {18 “createdIndex“: 67,19 “dir“: true,20 “key“: “/dir1“,21 “modifiedIndex“: 6822 },23 “prevNode“: {24 “createdIndex“: 67,25 “dir“: true,26 “key“: “/dir1“,27 “modifiedIndex“: 6728 }29 }&nbsp;15. PUT 创建定时删除的目录：就跟定时删除某个键类似。如果目录因为超时被删除了，其下的所有内容也自动超时删除。&nbsp; &nbsp; &nbsp; 如果目录存在，创建时，返回 102 错误码&nbsp; &nbsp; &nbsp; -d ttl=xx 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/dir -XPUT -d ttl=30 -d dir=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 157 100 142 100 15 22873 2416 –:–:– –:–:– –:–:– 28400 5 { 6 “action“: “set“, 7 “node“: { 8 “createdIndex“: 52, 9 “dir“: true,10 “expiration“: “2016-04-23T13:37:51.502289114Z“,11 “key“: “/dir“,12 “modifiedIndex“: 52,13 “ttl“: 3014 }15 }&nbsp;16. PUT 设置刷新目录超时时间 &nbsp;&nbsp;开始创建时，没有设置ttl， 或刷新已设置ttl的目录的ttl的值。 -d ttl=xxx 设置或刷新的ttl值。 ttl为空是，取消ttl。 -d prevExist=true &nbsp; &nbsp;必选参数，否者报错102错误码 会触发watcher事件。 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/dir -XPUT -d ttl=30 -d dir=true -d prevExist=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 304 100 274 100 30 60392 6612 –:–:– –:–:– –:–:– 91333 5 { 6 “action“: “update“, 7 “node“: { 8 “createdIndex“: 56, 9 “dir“: true,10 “expiration“: “2016-04-23T13:42:56.395923381Z“,11 “key“: “/dir“,12 “modifiedIndex“: 61,13 “ttl“: 3014 },15 “prevNode“: {16 “createdIndex“: 56,17 “dir“: true,18 “expiration“: “2016-04-23T13:42:46.225222674Z“,19 “key“: “/dir“,20 “modifiedIndex“: 56,21 “ttl“: 2022 }23 } 当ttl时间到后，watcher将收到一个”expire” action. &nbsp; 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/dir?wait=true | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 207 0 207 0 0 16 0 –:–:– 0:00:12 –:–:– 43 5 { 6 “action“: “expire“, 7 “node“: { 8 “createdIndex“: 2219, 9 “key“: “/dir“,10 “modifiedIndex“: 222011 },12 “prevNode“: {13 “createdIndex“: 2219,14 “dir“: true,15 “expiration“: “2016-12-28T09:22:35.853484071Z“,16 “key“: “/dir“,17 “modifiedIndex“: 221918 }19 }&nbsp;&nbsp;17. 创建一个隐藏节点：命名时名字以下划线_开头的key或目录，默认就是隐藏键。&nbsp; &nbsp; &nbsp; list目录下时，将不显示。可以显示使用。 1 [root@vStack ~]# curl http://127.0.0.1:2379/v2/keys/_message -XPUT -d value=”Hello hidden world” | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 134 100 110 100 24 46948 10243 –:–:– –:–:– –:–:– 107k 5 { 6 “action“: “set“, 7 “node“: { 8 “createdIndex“: 69, 9 “key“: “/_message“,10 “modifiedIndex“: 69,11 “value“: “Hello hidden world“12 }13 }&nbsp;&nbsp;注意：&nbsp; &nbsp; &nbsp; 1. api url 区分大小写，包括其中的参数。&nbsp; 2. 如果key存在，通过 curl http://IP:PORT/v2/keys/message001&nbsp;-XPUT -d dir=true , 将会把key调整为dir属性，value值为None； 增加 -d prevExist=false，将报105错误码。 修改为dir后，无法在恢复为key。&nbsp; &nbsp; &nbsp;3. 不能对一个dir进行赋值，即&nbsp;curl http://127.0.0.1:2379/v2/keys/message001 -XPUT -d value=123 &nbsp; &nbsp;， 返回错误码 102， &ldquo;Not a file&rdquo;&nbsp; &nbsp; &nbsp;4. key相当于文件系统中的文件，可以赋值即向文件写内容。dir相当于文件系统的目录或路径，内容包括dir和key， 即文件系统中的目录和文件。&nbsp; &nbsp; &nbsp;5. 在api url中的path，体现了存储结构。如果目录不存在，直接创建。如：curl http://127.0.0.1:2379/v2/keys/fst/sec/thr -XPUT -d value=123 &nbsp;中的fst、sec会自动创建为dir。&nbsp;&nbsp; &nbsp; &nbsp;6. 创建dir与key的区别，即在 curl的body中是否有 dir=true，有即为dir, 否认则key； dir存在时，value无效。 创建key时，value可以不存在。&nbsp; &nbsp; &nbsp;7. 不能在key下创建dir或可以，否者报错误码：104，&ldquo;Not a directory&rdquo;&nbsp; &nbsp; &nbsp;8. 目录不能重复创建，即&nbsp;curl -v http://127.0.0.1:2379/v2/keys/message -XPUT -d dir=true &nbsp;如果 message 目录已经已经存在，返回错误码：102，&nbsp;&ldquo;Not a file&rdquo;&nbsp; &nbsp; &nbsp;9. 删除一个非空目录，返回错误码：102. 通过在url中增加 recursive=true 参数，可以参数非空目录。&nbsp;Statistics &nbsp;统计接口 etcd 集群记录大量的统计数据，包括：延时(latency)，带宽和正常运行时间。统计功能通过统计端点(/stats)去理解一个集群的内部健康状态。 An etcd cluster keeps track of a number of statistics including latency, bandwidth and uptime. These are exposed via the statistics endpoint to understand the internal health of a cluster.Leader Statistics 领导点统计&nbsp; 1 [root@localhost testectd]# curl http://127.0.0.1:2379/v2/stats/self | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 119 357 119 357 0 0 274k 0 –:–:– –:–:– –:–:– 348k 5 { 6 “id“: “45b967575ff25cb2“, 7 “leaderInfo“: { 8 “leader“: “45b967575ff25cb2“, 9 “startTime“: “2016-12-29T20:15:13.811259537+08:00“,10 “uptime“: “8m19.603722077s“11 },12 “name“: “infra0“,13 “recvAppendRequestCnt“: 18,14 “sendAppendRequestCnt“: 3670,15 “sendBandwidthRate“: 123950.52498801574,16 “sendPkgRate“: 7.5456304767920797,17 “startTime“: “2016-12-29T20:14:29.300999352+08:00“,18 “state“: “StateLeader“19 }&nbsp;&nbsp; 1 [root@localhost testectd]# curl http://127.0.0.1:2379/v2/stats/leader | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 132 398 132 398 0 0 133k 0 –:–:– –:–:– –:–:– 388k 5 { 6 “followers“: { 7 “3c828782a67e0043“: { 8 “counts“: { 9 “fail“: 1211,10 “success“: 011 },12 “latency“: {13 “average“: 0,14 “current“: 0,15 “maximum“: 0,16 “minimum“: 9.2233720368547758e+18,17 “standardDeviation“: 018 }19 },20 “b26f1b9a6c735437“: {21 “counts“: {22 “fail“: 0,23 “success“: 323124 },25 “latency“: {26 “average“: 0.0073246419065304607,27 “current“: 0.0032520000000000001,28 “maximum“: 1.713633,29 “minimum“: 0.0012520000000000001,30 “standardDeviation“: 0.03565460655054003631 }32 }33 },34 “leader“: “45b967575ff25cb2“35 }&nbsp;Memeber API1.&nbsp;List members 返回http 200 OK response，显示在 etcd 集群中的所有成员。 1 [root@vStack ~]# curl http://192.168.10.150:2379/v2/members | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 138 100 138 0 0 73287 0 –:–:– –:–:– –:–:– 134k 5 { 6 “members“: [ 7 { 8 “clientURLs“: [ 9 “http://192.168.10.150:2379“10 ],11 “id“: “8e9e05c52164694d“,12 “name“: “default“,13 “peerURLs“: [14 “http://localhost:2380“15 ]16 }17 ]18 }&nbsp;[root@vStack ~]# curl http://127.0.0.1:2379/v2/members | python -m json.tool % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 181 100 181 0 0 138k 0 –:–:– –:–:– –:–:– 176k{ “members“: [ { “clientURLs“: [ “http://localhost:2379“, “http://localhost:4001“ ], “id“: “ce2a822cea30bfca“, “name“: “default“, “peerURLs“: [ “http://localhost:2380“, “http://localhost:7001“ ] } ]}&nbsp; 1 [root@vStack ~]# curl http://192.168.10.150:2379/v2/members | python -m json.tool 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 100 227 100 227 0 0 116k 0 –:–:– –:–:– –:–:– 221k 5 { 6 “members“: [ 7 { 8 “clientURLs“: [], 9 “id“: “755ef544f1926e2e“,10 “name“: “”,11 “peerURLs“: [12 “http://127.0.0.1:2380“13 ]14 },15 {16 “clientURLs“: [17 “http://192.168.10.150:2379“18 ],19 “id“: “8e9e05c52164694d“,20 “name“: “default“,21 “peerURLs“: [22 “http://localhost:2380“23 ]24 }25 ]26 }&nbsp;2. Add a member 成功时返回 HTTP 201 response 状态码，及新建入成员的信息，对新加入的成员生成一个成员id。 失败时，返回失败状态的字符描述。 &nbsp;Returns an HTTP 201 response code and the representation of added member with a newly generated a memberID when successful. Returns a string describing the failure condition when unsuccessful.If the POST body is malformed an HTTP 400 will be returned. If the member exists in the cluster or existed in the cluster at some point in the past an HTTP 409 will be returned. If any of the given peerURLs exists in the cluster an HTTP 409 will be returned. If the cluster fails to process the request within timeout an HTTP 500 will be returned, though the request may be processed later.&nbsp;1 curl http://10.0.0.10:2379/v2/members -XPOST \2 -H “Content-Type: application/json“ -d ‘{“peerURLs”:[“http://10.0.0.10:2380&quot;]}‘&nbsp; 1.&nbsp;&nbsp;需要在header中设置&nbsp;Content-Type: application/json， 否则会报 405 错误&nbsp;Unsupported Media Type 2. &nbsp;如果已经存在相同的peerURLs，直接返回当前存在相同peerURLs的member。&nbsp; &nbsp; &nbsp; 3. 如果添加一个无法使用的peerURLs，导致服务挂掉，无法操作。重启也无法使用。解决方法删除物理文件，但这个会删除记录的数据，导致持久数据的就丢失。需要进一步寻求解决方法。 集群信息会记录到持久化信息文件中，重启问题依旧。除非使用不同的name或改变数据目录。 &nbsp;3. Delete a member 从集群中删除一个memeber。 member ID 必须是一个64位整数的16位编码的字符串。成功时，返回 204 状态码和没有内容。失败时，返回404状态码和字符描述的失败情况。 从集群中删除一个不存在的member，返回500错误。集群处理失败请求，包括超时，返回一个500错误码。即使请求可能后面会处理。1 [root@localhost testectd]# curl http://192.168.10.150:2379/v2/members/2ae1ee131894262b -XDELETE | python -m json.tool2 % Total % Received % Xferd Average Speed Time Time Time Current3 Dload Upload Total Spent Left Speed4 0 0 0 0 0 0 0 0 –:–:– –:–:– –:–:– 05 No JSON object could be decoded&nbsp; 1. 删除成员后，etcd使用的data-dir必须被删除。如下是删除最后一个member，etcd给出的输出，服务退出。 2016-12-29 16:10:59.544409 E | etcdserver: the member has been permanently removed from the cluster 2016-12-29 16:10:59.544480 I | etcdserver: the data-dir used by this member must be removed. 2. 通过etcdctl 删除一个成员后，服务会退出。通过 etcdctl重新加入，显示为unstart。 如果需要重新加入集群，先用命令加入，再启动，否则启动时报&nbsp;the member has been permanently removed from the cluster 加入后，启动前，需要删除其存储的数据(member id发生了改变，会将使用磁盘记录的id，与新加入的ID不一致)。并设置&nbsp;–initial-cluster-state existing 不能设置为 new 注意 cluster版本要一致。cluster{“etcdserver”:”3.0.15”,”etcdcluster”:”3.0.0”} 可以。&nbsp; {“etcdserver”:”2.3.7”,”etcdcluster”:”2.3.0”} 出现：&nbsp;{“etcdserver”:”3.0.15”,”etcdcluster”:”2.3.0”} &nbsp;在集群系统中出现不同版本的member 以上删除重新加入的操作，高版本的可以，单低版本的不支持，报&nbsp; failed to find member 3c828782a67e0043 in cluster 34b660d543ad1445 无法发现其他member&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; 即 在集群中，成员的版本不同。低版本的成员失败退出，重启启动可以重新加入集群。通过接口，被动从集群中移除，再次加入，只能停止所有的成员，删除其中磁盘数据，重新构建。导致数据的丢失，如何恢复？&nbsp;高版本的成员没有此问题。&nbsp; 即 在集群中，成员的版本不同。集群版本降低为 低版本 如：{“etcdserver”:”3.0.15”,”etcdcluster”:”2.3.0”}， 低版本的成员退出后，集群版本升级为高版本：{“etcdserver”:”3.0.15”,”etcdcluster”:”3.0.0”}&nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; 其版本的首先启动时，使用–initial-cluster-state new； &nbsp;高版本的在启动时&nbsp;–initial-cluster-state&nbsp;existing 会报 集群版本不兼容。需要使用&nbsp;–initial-cluster-state new。 再次启动高版本的可以使用&nbsp;existing 。 &nbsp;构建集群时，采用就低版本，在高版本加入时，需要使用&nbsp;–initial-cluster-state new 或 不设置， 使用existing ，报集群不兼容。&nbsp;&nbsp; 1 [root@centos7mini etcd]# ./etcdctl member remove 45b967575ff25cb2 2 Removed member 45b967575ff25cb2 from cluster 3 4 [root@centos7mini etcd]# ./etcdctl member add infra0 http://192.168.10.150:2380 5 Added member named infra0 with ID 700fb7bf97791e71 to cluster 6 7 ETCD_NAME=“infra0“ 8 ETCD_INITIAL_CLUSTER=“infra3=http://192.168.10.184:2380,infra0=http://192.168.10.150:2380,infra1=http://192.168.10.231:2380“ 9 ETCD_INITIAL_CLUSTER_STATE=“existing“1011 [root@centos7mini etcd]# ./etcdctl member list12 3c828782a67e0043: name=infra3 peerURLs=http://192.168.10.184:2380 clientURLs=http://192.168.10.184:2379 isLeader=true13 700fb7bf97791e71[unstarted]: peerURLs=http://192.168.10.150:238014 b26f1b9a6c735437: name=infra1 peerURLs=http://192.168.10.231:2380 clientURLs=http://192.168.10.231:2379 isLeader=false15 [root@centos7mini etcd]# &nbsp;4. Change the peer urls of a member 修改已存在成员的peer urls。成员ID必须是一个64位整数的16进制显示的字符串。成功时，返回204状态码，空内容。失败时，返回失败状态字符描述。 修改的成员不存在，返回400 错误码。 如果提供的peerlURL存在，将返回409错误码。500错误： 集群处理超时。 1 [root@localhost etcd-v3.0.15-linux-amd64]# ./etcdctl member list 2 3c828782a67e0043: name=infra3 peerURLs=http://192.168.10.184:2380 clientURLs=http://192.168.10.184:2379 isLeader=false 3 45b967575ff25cb2: name=infra0 peerURLs=http://192.168.10.150:2380 clientURLs=http://192.168.10.150:2379 isLeader=true 4 b26f1b9a6c735437: name=infra1 peerURLs=http://192.168.10.231:2380 clientURLs=http://192.168.10.231:2379 isLeader=false 5 [root@localhost etcd-v3.0.15-linux-amd64]# 6 [root@localhost etcd-v3.0.15-linux-amd64]# curl http://192.168.10.150:2379/v2/members/b26f1b9a6c735437 -XPUT -H “Content-Type: application/json” -d ‘{“peerURLs”:[“http://127.0.0.1:2380”]}’ 7 [root@localhost etcd-v3.0.15-linux-amd64]# 8 [root@localhost etcd-v3.0.15-linux-amd64]# ./etcdctl member list 9 3c828782a67e0043: name=infra3 peerURLs=http://192.168.10.184:2380 clientURLs=http://192.168.10.184:2379 isLeader=false10 45b967575ff25cb2: name=infra0 peerURLs=http://192.168.10.150:2380 clientURLs=http://192.168.10.150:2379 isLeader=true11 b26f1b9a6c735437: name=infra1 peerURLs=http://127.0.0.1:2380 clientURLs=http://192.168.10.231:2379 isLeader=false12 [root@localhost etcd-v3.0.15-linux-amd64]# &nbsp;&nbsp;&nbsp;&nbsp; 在启动etcd设置&nbsp;–listen-client-urls 值时，请将localhost:2379或127.0.0.1:2379 设置，否者 本地etcdctl会报错如下1 [root@centos7mini etcd]# ./etcdctl member list2 Error: client: etcd cluster is unavailable or misconfigured3 error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused4 error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused&nbsp;&nbsp;一个节点断开后，成为candicate，向其他member发起vote，重新选准 master/leader。2016-12-29 19:43:15.767905 I | raft: b26f1b9a6c735437 became candidate at term 1462016-12-29 19:43:15.767932 I | raft: b26f1b9a6c735437 received vote from b26f1b9a6c735437 at term 1462016-12-29 19:43:15.767961 I | raft: b26f1b9a6c735437 [logterm: 101, index: 688] sent vote request to 45b967575ff25cb2 at term 1462016-12-29 19:43:17.266905 I | raft: b26f1b9a6c735437 is starting a new election at term 146&nbsp;将一个memeber加入两个集群时，出现 cluster id 匹配问题。以下是静态创建cluster。 1 # etcd –name infra1 –initial-advertise-peer-urls http://192.168.10.231:2380 \ 2 –listen-peer-urls http://192.168.10.231:2380 \ 3 –listen-client-urls http://192.168.10.231:2379,http://127.0.0.1:2379 \ 4 –advertise-client-urls http://192.168.10.231:2379 \ 5 –initial-cluster-token etcd-cluster-1 \ 6 –initial-cluster infra0=http://192.168.10.150:2380,infra1=http://192.168.10.231:2380,infra3=http://192.168.10.184:2380 \ 7 –initial-cluster-state new 8 9 # ./etcd –debug –name infra3 –initial-advertise-peer-urls http://192.168.10.184:2380 \10 –listen-peer-urls http://192.168.10.184:2380 –initial-cluster infra3=http://192.168.10.184:2380 \11 –listen-client-urls http://192.168.10.184:2379 –advertise-client-urls http://192.168.10.184:2379 \12 –initial-cluster-state new –initial-cluster-token etcd-cluster-1&nbsp;&nbsp;2016-12-30 11:47:30.939730 E | rafthttp: request sent was ignored (cluster ID mismatch: remote[3c828782a67e0043]=625ac7f9082c643, local=34b660d543ad1445)2016-12-30 11:47:30.977766 E | rafthttp: request sent was ignored (cluster ID mismatch: remote[3c828782a67e0043]=625ac7f9082c643, local=34b660d543ad1445)&nbsp;&nbsp;如下建立集群后，–debug 提示：150上的iptables防火墙导致。2016-12-30 12:07:19.479241 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:19.914614 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:20.781345 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:21.216792 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:21.885187 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:22.518689 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:22.620358 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:23.187832 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:23.925031 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:24.490547 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:24.592188 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:25.226673 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:25.696521 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:26.528616 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:26.630548 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 12:07:27.000087 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:27.932728 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:28.302774 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 12:07:28.404591 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no r如下调试信息是在leader 上的。 由于iptables防火墙的原因导致。2016-12-30 14:03:10.838829 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:11.353601 W | etcdserver: failed to reach the peerURL(http://192.168.10.150:2380) of member 45b967575ff25cb2 (Get http://192.168.10.150:2380/version: dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 14:03:11.353640 W | etcdserver: cannot get the version of member 45b967575ff25cb2 (Get http://192.168.10.150:2380/version: dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 14:03:11.672262 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:12.140697 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:12.974912 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:13.445167 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:13.547340 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 14:03:14.278497 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:14.380259 D | rafthttp: failed to dial 45b967575ff25cb2 on stream MsgApp v2 (dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 14:03:14.850132 D | rafthttp: failed to dial 45b967575ff25cb2 on stream Message (dial tcp 192.168.10.150:2380: i/o timeout)2016-12-30 14:03:15.358565 W | etcdserver: failed to reach the peerURL(http://192.168.10.150:2380) of member 45b967575ff25cb2 (Get http://192.168.10.150:2380/version: dial tcp 192.168.10.150:2380: getsockopt: no route to host)2016-12-30 14:03:15.358613 W | etcdserver: cannot get the version of member 45b967575ff25cb2 (Get http://192.168.10.150:2380/version: dial tcp 192.168.10.150:2380: getsockopt: no route to host)如下调试信息，是由于 192.168.10.231:2380 无法访问，&nbsp;b26f1b9a6c735437 为member id。2016-12-30 21:38:43.130791 D | rafthttp: failed to dial b26f1b9a6c735437 on stream Message (dial tcp 192.168.10.231:2380: getsockopt: connection refused)2016-12-30 21:38:43.191227 D | rafthttp: failed to dial b26f1b9a6c735437 on stream MsgApp v2 (dial tcp 192.168.10.231:2380: getsockopt: connection refused)2016-12-30 21:38:43.232280 D | rafthttp: failed to dial b26f1b9a6c735437 on stream Message (dial tcp 192.168.10.231:2380: getsockopt: connection refused)2016-12-30 21:38:43.292289 D | rafthttp: failed to dial b26f1b9a6c735437 on stream MsgApp v2 (dial tcp 192.168.10.231:2380: getsockopt: connection refused)2016-12-30 21:38:43.334129 D | rafthttp: failed to dial b26f1b9a6c735437 on stream Message (dial tcp 192.168.10.231:2380: getsockopt: connection refused)2016-12-30 21:38:43.393576 D | rafthttp: failed to dial b26f1b9a6c735437 on stream MsgApp v2 (dial tcp 192.168.10.231:2380: getsockopt: connection refused)&nbsp;2016-12-30 14:18:36.328628 W | rafthttp: the clock difference against peer 3c828782a67e0043 is too high [2.00405135s &gt; 1s]2016-12-30 14:19:06.329559 W | rafthttp: the clock difference against peer 3c828782a67e0043 is too high [2.003973758s &gt; 1s]2016-12-30 14:19:36.331189 W | rafthttp: the clock difference against peer 3c828782a67e0043 is too high [2.004098356s &gt; 1s]&nbsp;2016-12-30 21:38:22.857546 W | rafthttp: the clock difference against peer 3c828782a67e0043 is too high [7h59m58.924003117s &gt; 1s]2016-12-30 21:38:22.892541 W | rafthttp: health check for peer b26f1b9a6c735437 failed2016-12-30 21:38:22.892848 W | rafthttp: the clock difference against peer b26f1b9a6c735437 is too high [7h59m56.920465483s &gt; 1s]&nbsp; 转自: https://www.cnblogs.com/doscho/p/6227351.html]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gevent总结]]></title>
    <url>%2F2019%2F07%2F09%2Fgevent_intro%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[. . . gevent，它是一个并发网络库。它的协程是基于 greenlet 的，并基于 libev 实现快速事件循环（Linux 上是 epoll，FreeBSD 上是 kqueue，Mac OS X 上是 select）。有了 gevent，协程的使用将无比简单，你根本无须像 greenlet 一样显式的切换，每当一个协程阻塞时，程序将自动调度，gevent 处理了所有的底层细节。让我们看个例子来感受下吧。 12345678910111213141516import geventdef test1(): print 12 gevent.sleep(0) print 34def test2(): print 56 gevent.sleep(0) print 78gevent.joinall([ gevent.spawn(test1), gevent.spawn(test2),]) 解释下，gevent.spawn()方法会创建一个新的 greenlet 协程对象，并运行它。gevent.joinall()方法会等待所有传入的 greenlet 协程运行结束后再退出，这个方法可以接受一个timeout参数来设置超时时间，单位是秒。运行上面的程序，执行顺序如下： 先进入协程 test1，打印 12 遇到gevent.sleep(0)时，test1 被阻塞，自动切换到协程 test2，打印 56 之后 test2 被阻塞，这时 test1 阻塞已结束，自动切换回 test1，打印 34 当 test1 运行完毕返回后，此时 test2 阻塞已结束，再自动切换回 test2，打印 78 所有协程执行完毕，程序退出 所以，程序运行下来的输出就是： 123412563478 注意，这里与上一篇 greenlet 中第一个例子运行的结果不一样，greenlet 一个协程运行完后，必须显式切换，不然会返回其父协程。而在 gevent 中，一个协程运行完后，它会自动调度那些未完成的协程。 我们换一个更有意义的例子： 12345678import geventimport socketurls = ['www.baidu.com', 'www.gevent.org', 'www.python.org']jobs = [gevent.spawn(socket.gethostbyname, url) for url in urls]gevent.joinall(jobs, timeout=5)print [job.value for job in jobs] 我们通过协程分别获取三个网站的 IP 地址，由于打开远程地址会引起 IO 阻塞，所以 gevent 会自动调度不同的协程。另外，我们可以通过协程对象的”value” 属性，来获取协程函数的返回值。 猴子补丁MonkeyPatching细心的朋友们在运行上面例子时会发现，其实程序运行的时间同不用协程是一样的，是三个网站打开时间的总和。可是理论上协程是非阻塞的，那运行时间应该等于最长的那个网站打开时间呀？其实这是因为 Python 标准库里的 socket 是阻塞式的，DNS 解析无法并发，包括像 urllib 库也一样，所以这种情况下用协程完全没意义。那怎么办？ 一种方法是使用 gevent 下的 socket 模块，我们可以通过from gevent import socket来导入。不过更常用的方法是使用猴子布丁（Monkey patching）: 123456789from gevent import monkey; monkey.patch_socket()import geventimport socketurls = ['www.baidu.com', 'www.gevent.org', 'www.python.org']jobs = [gevent.spawn(socket.gethostbyname, url) for url in urls]gevent.joinall(jobs, timeout=5)print [job.value for job in jobs] 上述代码的第一行就是对 socket 标准库打上猴子补丁，此后 socket 标准库中的类和方法都会被替换成非阻塞式的，所有其他的代码都不用修改，这样协程的效率就真正体现出来了。Python 中其它标准库也存在阻塞的情况，gevent 提供了monkey.patch_all()方法将所有标准库都替换。 1from gevent import monkey; monkey.patch_all() 使用猴子补丁褒贬不一，但是官网上还是建议使用patch_all()，而且在程序的第一行就执行。 gevent原理获取协程状态协程状态有已启动和已停止，分别可以用协程对象的started属性和ready()方法来判断。对于已停止的协程，可以用successful()方法来判断其是否成功运行且没抛异常。如果协程执行完有返回值，可以通过value属性来获取。另外，greenlet 协程运行过程中发生的异常是不会被抛出到协程外的，因此需要用协程对象的exception属性来获取协程中的异常。下面的例子很好的演示了各种方法和属性的使用。 1234567891011121314151617181920212223242526272829303132import geventdef win(): return 'You win!'def fail(): raise Exception('You failed!')winner = gevent.spawn(win)loser = gevent.spawn(fail)print winner.started print loser.started try: gevent.joinall([winner, loser])except Exception as e: print 'This will never be reached'print winner.ready() print loser.ready() print winner.value print loser.value print winner.successful() print loser.successful() print loser.exception 协程运行超时之前我们讲过在gevent.joinall()方法中可以传入timeout参数来设置超时，我们也可以在全局范围内设置超时时间： 123456789101112import geventfrom gevent import Timeouttimeout = Timeout(2) timeout.start()def wait(): gevent.sleep(10)try: gevent.spawn(wait).join()except Timeout: print('Could not complete') 上例中，我们将超时设为 2 秒，此后所有协程的运行，如果超过两秒就会抛出Timeout异常。我们也可以将超时设置在 with 语句内，这样该设置只在 with 语句块中有效： 12with Timeout(1): gevent.sleep(10) 此外，我们可以指定超时所抛出的异常，来替换默认的Timeout异常。比如下例中超时就会抛出我们自定义的TooLong异常。 12345class TooLong(Exception): passwith Timeout(1, TooLong): gevent.sleep(10) 协程间通讯greenlet 协程间的异步通讯可以使用事件（Event）对象。该对象的wait()方法可以阻塞当前协程，而set()方法可以唤醒之前阻塞的协程。在下面的例子中，5 个 waiter 协程都会等待事件 evt，当 setter 协程在 3 秒后设置 evt 事件，所有的 waiter 协程即被唤醒。 123456789101112131415161718192021222324import geventfrom gevent.event import Eventevt = Event()def setter(): print 'Wait for me' gevent.sleep(3) print "Ok, I'm done" evt.set() def waiter(): print "I'll wait for you" evt.wait() print 'Finish waiting'gevent.joinall([ gevent.spawn(setter), gevent.spawn(waiter), gevent.spawn(waiter), gevent.spawn(waiter), gevent.spawn(waiter), gevent.spawn(waiter)]) 除了 Event 事件外，gevent 还提供了AsyncResult事件，它可以在唤醒时传递消息。让我们将上例中的setter和waiter作如下改动: 1234567891011121314from gevent.event import AsyncResultaevt = AsyncResult()def setter(): print 'Wait for me' gevent.sleep(3) print "Ok, I'm done" aevt.set('Hello!') def waiter(): print("I'll wait for you") message = aevt.get() print 'Got wake up message: %s' % message 队列Queue队列 Queue 的概念相信大家都知道，我们可以用它的put和get方法来存取队列中的元素。gevent 的队列对象可以让 greenlet 协程之间安全的访问。运行下面的程序，你会看到 3 个消费者会分别消费队列中的产品，且消费过的产品不会被另一个消费者再取到： 123456789101112131415161718192021import geventfrom gevent.queue import Queueproducts = Queue()def consumer(name): while not products.empty(): print '%s got product %s' % (name, products.get()) gevent.sleep(0) print '%s Quit'def producer(): for i in xrange(1, 10): products.put(i)gevent.joinall([ gevent.spawn(producer), gevent.spawn(consumer, 'steve'), gevent.spawn(consumer, 'john'), gevent.spawn(consumer, 'nancy'),]) put和get方法都是阻塞式的，它们都有非阻塞的版本：put_nowait和get_nowait。如果调用get方法时队列为空，则抛出gevent.queue.Empty异常。 信号量信号量可以用来限制协程并发的个数。它有两个方法，acquire和release。顾名思义，acquire就是获取信号量，而release就是释放。当所有信号量都已被获取，那剩余的协程就只能等待任一协程释放信号量后才能得以运行： 12345678910111213import geventfrom gevent.coros import BoundedSemaphoresem = BoundedSemaphore(2)def worker(n): sem.acquire() print('Worker %i acquired semaphore' % n) gevent.sleep(0) sem.release() print('Worker %i released semaphore' % n)gevent.joinall([gevent.spawn(worker, i) for i in xrange(0, 6)]) 上面的例子中，我们初始化了BoundedSemaphore信号量，并将其个数定为2。所以同一个时间，只能有两个 worker 协程被调度。程序运行后的结果如下： 123456789101112Worker 0 acquired semaphoreWorker 1 acquired semaphoreWorker 0 released semaphoreWorker 1 released semaphoreWorker 2 acquired semaphoreWorker 3 acquired semaphoreWorker 2 released semaphoreWorker 3 released semaphoreWorker 4 acquired semaphoreWorker 4 released semaphoreWorker 5 acquired semaphoreWorker 5 released semaphore 如果信号量个数为 1，那就等同于同步锁。 协程本地变量同线程类似，协程也有本地变量，也就是只在当前协程内可被访问的变量： 12345678910111213141516171819import geventfrom gevent.local import localdata = local()def f1(): data.x = 1 print data.xdef f2(): try: print data.x except AttributeError: print 'x is not visible'gevent.joinall([ gevent.spawn(f1), gevent.spawn(f2)]) 通过将变量存放在local对象中，即可将其的作用域限制在当前协程内，当其他协程要访问该变量时，就会抛出异常。不同协程间可以有重名的本地变量，而且互相不影响。因为协程本地变量的实现，就是将其存放在以的greenlet.getcurrent()的返回为键值的私有的命名空间内。 实际应用讲到这里，大家肯定很想看一个 gevent 的实际应用吧，这里有一个简单的聊天室程序，基于 Flask 实现，大家可以参考下。 更多参考资料gevent 的官方文档gevent 社区提供的教程 转自: http://www.bjhee.com/gevent.html]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>GEvent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的with与__enter__以及__exit__关系]]></title>
    <url>%2F2019%2F06%2F28%2Fpython_with%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[有一些任务，可能事先需要设置，事后做清理工作。对于这种场景，Python的with语句提供了一种非常方便的处理方式。一个很好的例子是文件处理，你需要获取一个文件句柄，从文件中读取数据，然后关闭文件句柄。 . . . 引子如果不用with语句，代码如下： 123file = open("/tmp/foo.txt")data = file.read()file.close() 这里有两个问题。一是可能忘记关闭文件句柄；二是文件读取数据发生异常，没有进行任何处理。下面是处理异常的加强版本： 12345file = open("/tmp/foo.txt")try: data = file.read()finally: file.close() 虽然这段代码运行良好，但是太冗长了。这时候就是with一展身手的时候了。除了有更优雅的语法，with还可以很好的处理上下文环境产生的异常。下面是with版本的代码： 12with open("/tmp /foo.txt") as file: data = file.read() with如何工作这看起来充满魔法，但不仅仅是魔法，Python对with的处理还很聪明。基本思想是with所求值的对象必须有一个__enter__()方法，一个__exit__()方法。 紧跟with后面的语句被求值后，返回对象的__enter__()方法被调用，这个方法的返回值将被赋值给as后面的变量。当with后面的代码块全部被执行完之后，将调用前面返回对象的__exit__()方法。 下面例子可以具体说明with如何工作： 1234567891011121314151617# -*- coding: UTF-8 -*-class Sample: def __enter__(self): print "In __enter__()" return "Foo" def __exit__(self, type, value, trace): print "In __exit__()"def get_sample(): return Sample()with get_sample() as sample: print "sample:", sample 输出如下1234bash-3.2$ ./with_example01.pyIn __enter__()sample: FooIn __exit__() 正如你看到的， __enter__()方法被执行__enter__()方法返回的值 - 这个例子中是”Foo”，赋值给变量’sample’执行代码块，打印变量”sample”的值为 “Foo”__exit__()方法被调用 with还可以处理异常with真正强大之处是它可以处理异常。可能你已经注意到Sample类的__exit__方法有三个参数val, type 和 trace。 这些参数在异常处理中相当有用。我们来改一下代码，看看具体如何工作的。 1234567891011121314151617# -*- coding: UTF-8 -*-class Sample: def __enter__(self): return self def __exit__(self, type, value, trace): print "type:", type print "value:", value print "trace:", trace def do_something(self): bar = 1/0 return bar + 10with Sample() as sample: sample.do_something() 这个例子中，with后面的get_sample()变成了Sample()。这没有任何关系，只要紧跟with后面的语句所返回的对象有 __enter__()和__exit__()方法即可。此例中，Sample()的__enter__()方法返回新创建的Sample对象，并赋值给变量sample。 代码执行后：12345678910bash-3.2$ ./with_example02.pytype: &lt;type &apos;exceptions.ZeroDivisionError&apos;&gt;value: integer division or modulo by zerotrace: &lt;traceback object at 0x1004a8128&gt;Traceback (most recent call last): File &quot;./with_example02.py&quot;, line 19, in &lt;module&gt; sample.do_somet hing() File &quot;./with_example02.py&quot;, line 15, in do_something bar = 1/0ZeroDivisionError: integer division or modulo by zero 实际上，在with后面的代码块抛出任何异常时，__exit__()方法被执行。正如例子所示，异常抛出时，与之关联的type，value和stack trace传给__exit__()方法，因此抛出的ZeroDivisionError异常被打印出来了。开发库时，清理资源，关闭文件等等操作，都可以放在__exit__方法当中。 因此，Python的with语句是提供一个有效的机制，让代码更简练，同时在异常产生时，清理工作更简单。]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Etcd安装备忘]]></title>
    <url>%2F2019%2F06%2F27%2Fhello_etcd%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[参考https://skyao.gitbooks.io/learning-etcd3/content/installation/linux_single.html 下载/配置简而言之就是 : 先去 etcd 的 github找到他的release, 然后复制链接, 下载然后配置 注： 以 etcd-v3.2.1 为例，后续更新版本时可能细节有所不同。 . . . 下载执行下面的命令，下载(大概10M)解压即可，无需安装： 12345678910curl -L https://github.com/coreos/etcd/releases/download/v3.2.1/etcd-v3.2.1-linux-amd64.tar.gz -o etcd-v3.2.1-linux-amd64.tar.gztar xzvf etcd-v3.2.1-linux-amd64.tar.gzmv etcd-v3.2.1-linux-amd64 etcdcd etcd./etcd --versionetcd Version: 3.2.1Git SHA: 61fc123Go Version: go1.8.3Go OS/Arch: linux/amd64 安装目录文件列表如下： 123$ lsdefault.etcd etcd README-etcdctl.md READMEv2-etcdctl.mdDocumentation etcdctl README.md 运行直接运行命令 ./etcd 就可以启动了，非常简单。 默认使用2379端口为客户端提供通讯， 并使用端口2380来进行服务器间通讯。 配置为了方便使用，将 etcd 加入 PATH，另外设置 ETCDCTL_API 为3(后面解释)。 在 /etc/profile 中加入以下内容： 123# etcdexport PATH=/home/sky/work/soft/etcd:$PATHexport ETCDCTL_API=3 然后执行 source /etc/profile 重新加载。 客户端访问配置etcdctletcdctl 是 etcd 的客户端命令行。 特别提醒：使用 etcdctl 前，务必设置环境变量 ETCDCTL_API=3 ! 注意：如果不设置 ETCDCTL_API=3，则默认是的API版本是2： 123$ etcdctl versionetcdctl version: 3.2.1API version: 2 正确设置后，API版本变成3： 123$ etcdctl versionetcdctl version: 3.2.1API version: 3.2 使用etcdctl通过下面的put和get命令来验证连接并操作etcd： 12345$ ./etcdctl put aaa 1OK$ ./etcdctl get aaaaaa1 总结上面操作完成之后，就有一个可运行的简单 etcd 服务器和一个可用的 etcdctl 客户端。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Go安装备忘]]></title>
    <url>%2F2019%2F06%2F27%2Flinux_install_go%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[装go因为 ubuntu 默认装 go 是1.6 的, 不想装 1.6, 准备装 1.8, . . . step1wget https://storage.googleapis.com/golang/go1.8.3.linux-amd64.tar.gz step2sudo tar -xvf go1.8.3.linux-amd64.tar.gzsudo mv go /usr/local step3vi ~/.bashrcexport GOROOT=/usr/local/goexport GOPATH=/home/zhangxiao/goworkspace/mygoexport PATH=$GOPATH/bin:$GOROOT/bin:$PATHsource ~/.bashrc step4go versiongo version go1.8.3 linux/amd64 参考：https://tecadmin.net/install-go-on-ubuntu/]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[久违pybind11]]></title>
    <url>%2F2019%2F05%2F19%2Fpybind11_intro%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[boost.python 迟暮, 久违 pybind11 , 来玩玩 官方介绍pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code. Its goals and syntax are similar to the excellent Boost.Python library by David Abrahams: to minimize boilerplate code in traditional extension modules by inferring type information using compile-time introspection. The main issue with Boost.Python—and the reason for creating such a similar project—is Boost. Boost is an enormously large and complex suite of utility libraries that works with almost every C++ compiler in existence. This compatibility has its cost: arcane template tricks and workarounds are necessary to support the oldest and buggiest of compiler specimens. Now that C++11-compatible compilers are widely available, this heavy machinery has become an excessively large and unnecessary dependency. 编写供 python 调用的 C++ 模块下载好 pybind11 之后，我们就可以开始对着官方的 pybind11 Tutorial 进行学习了，详细的入门教程及语法请参考官方文档，这里，我们简单演示下如何编写供 python 调用的 C++ 模块. . . . 首先，我们编写一个 C++ 源文件，命名为hello_pybind11.cpp hello_pybind11.cpp12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;pybind11/pybind11.h&gt;int add(int i, int j) &#123; return i + j;&#125;namespace py = pybind11;PYBIND11_MODULE(hello_pybind11, m) &#123; m.doc() = R"pbdoc( Pybind11 hello_pybind11 plugin ----------------------- .. currentmodule:: hello_pybind11 .. autosummary:: :toctree: _generate add subtract )pbdoc"; m.def("add", &amp;add, R"pbdoc( Add two numbers Some other explanation about the add function. )pbdoc"); m.def("subtract", [](int i, int j) &#123; return i - j; &#125;, R"pbdoc( Subtract two numbers Some other explanation about the subtract function. )pbdoc"); // exporting variables m.attr("the_answer") = 42; py::object world = py::cast("World"); m.attr("what") = world;#ifdef VERSION_INFO m.attr("__version__") = VERSION_INFO;#else m.attr("__version__") = "dev";#endif&#125; CMake 的编译方法我们使用 CMake 进行编译。如果 hello_pybind11.cpp 放在和 pybind11 同一级的目录下,首先像这样写一个 CMakeLists.txt 12345678910111213141516171819202122232425262728cmake_minimum_required(VERSION 2.8.12)project(hello_pybind11)set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++1y&quot;)find_package(PythonLibs REQUIRED) include_directories($&#123;PYTHON_INCLUDE_DIRS&#125;) include_directories($&#123;PROJECT_SOURCE_DIR&#125;)include_directories($&#123;PROJECT_SOURCE_DIR&#125;/pybind11/include)if (WIN32) add_definitions( -DNOMINMAX -DWIN32_LEAN_AND_MEAN -D_WIN32_WINNT=0x0600 -D_CRT_SECURE_NO_WARNINGS -D_SCL_SECURE_NO_WARNINGS -D_WINSOCK_DEPRECATED_NO_WARNINGS )else() set(CMAKE_BUILD_TYPE &quot;Debug&quot;) set(CMAKE_CXX_FLAGS_DEBUG &quot;$ENV&#123;CXXFLAGS&#125; -O0 -Wall -g -ggdb&quot;) set(CMAKE_CXX_FLAGS_RELEASE &quot;$ENV&#123;CXXFLAGS&#125; -O3 -Wall&quot;)endif()add_subdirectory(pybind11)pybind11_add_module(hello_pybind11 hello_pybind11.cpp) 然后 CMake，便会生成一个 vs 2015 的工程文件，用 vs 打开工程文件进行 build，就可以生成hello_pybind11.pyd了。若是在 Linux 下记得安装python-dev, sudo apt-get install python-dev即可 加入py测试脚本123456789101112# -*- coding:utf-8 -*-import syssys.path.append(r"C:\Users\hulinhong\Documents\github\wheel_timer_py\build\Debug") # hello_pybind11.pyd 在这个路径import hello_pybind11print hello_pybind11.add(1, 2)print hello_pybind11.subtract(12, 22)print hello_pybind11.the_answerprint hello_pybind11.what 踩坑点汇总 运行py时报错: ImportError: dynamic module does not define init function (initfizzbuzz) 解决方案: The error also occurs, when using boost::python, if the module name is different to the compiled .so file name. 要调试python的c++扩展记得 cmakelist.txt 里记得加上相应的标记 $gdb python (gdb)run main.py // 记得要先run 相关的py, 不然后面断点不到 (gdb)b CallbackMgr::callback // 此处可能提示没有符号, 在找到符号之后阻塞进程, 具体的方法名后面不要加括号, 就写成callback即可, 不要写成callback() (gdb)b WheelTimer.cpp:251 用gdb调试python的话, sudo apt-get install python2.7-dbg, 只能看看调用栈, 但其实没法设置断点的 如果遇到py脚本无法结束或无法继续, 基本就是c++扩展的某个地方死循环了 重载的时候编译不过有可能是因为const的原因 参考文献 pybind11 github pybind11 official Tutorial python 调用 C++ 之 pybind11 入门]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[asio none boost 浅踩坑]]></title>
    <url>%2F2019%2F04%2F01%2Fasio_non_boost_intro%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[一晃2年过去了, 记得曾经看过 boost.asio, 现在 asio 已经可以完全脱离 boost 了,不过它项目里的一些例子还是依赖 boost 的, 比如他 src 文件夹里的 tests 里的 除了 unit , 其他的大多数还是老的例子,都是直接包含boost的一些头文件, 也就是依赖boost的 编译注意事项官网说支持c++11的编译器会自动检测, 然后走asio的standalone模式, 测试了一下, 显然不会.所以 ASIO_STANDALONE 这个宏是必须得自己加上的, define ASIO_STANDALONE on your Preprocessor Settings (如: g++ -DASIO_STANDALONE) or as part of the project options. . . . 包含 asio 的目录 (如: g++ -I) In C/C++ Preprocessor Settings, defined:1234567891011121314ASIO_STANDALONEASIO_HAS_STD_ADDRESSOFASIO_HAS_STD_ARRAYASIO_HAS_CSTDINTASIO_HAS_STD_SHARED_PTRASIO_HAS_STD_TYPE_TRAITSASIO_HAS_VARIADIC_TEMPLATESASIO_HAS_STD_FUNCTIONASIO_HAS_STD_CHRONOBOOST_ALL_NO_LIB_WIN32_WINNT=0x0501_WINSOCK_DEPRECATED_NO_WARNINGS 还可以参考: https://nnarain.github.io/2015/11/03/Building-ASIO-Standalone-with-Visual-Studio-2015.html https://segmentfault.com/a/1190000013031005 如何fix例子里的boost依赖或已过时的代码12asio::placeholders::error, asio::placeholders::bytes_transferred 上面代码里的可替换为 std::placeholders::_1 和 std::placeholders::_2 123#include &lt;boost/array.hpp&gt;#include &lt;boost/bind.hpp&gt;#include &lt;boost/shared_ptr.hpp&gt; 这种就可以替换为12#include &lt;array&gt;#include &lt;memory&gt;]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Boost</tag>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSR server]]></title>
    <url>%2F2019%2F02%2F14%2Fssr_server%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[更多好用的一键脚本请转 https://github.com/ToyoDAdoubi/doubi ssr.sh 脚本说明: ShadowsocksR 一键安装管理脚本，支持单端口/多端口切换和管理 系统支持: CentOS6+ / Debian6+ / Ubuntu14+ 使用方法: https://doub.io/ss-jc42/ 项目地址: https://github.com/ToyoDAdoubiBackup/shadowsocksr . . . 脚本特点:目前网上的各个ShadowsocksR脚本基本都是只有 安装/启动/重启 等基础功能，对于小白来说还是不够简单方便。既然是一键脚本，那么就要尽可能地简单，小白更容易接受使用！ 支持 限制 用户速度 支持 限制 端口设备数 支持 显示 当前连接IP 支持 显示 SS/SSR连接+二维码 支持 切换管理 单/多端口 支持 一键安装 锐速 支持 一键安装 BBR 支持 一键封禁 垃圾邮件(SMAP)/BT/PT 下载安装:1wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh 安装ssr软件 &nbsp; 复制上面的代码到VPS服务器里，安装脚本后，以后只需要运行bash ssr.sh这个快捷命令就可以出现下图的界面进行设置管理了 &nbsp; 如上图出现管理界面后，输入数字1来安装SSR服务端。如果输入1后不能进入下一步，那么请重新连接VPS服务器，然后输入快捷管理命令 bash ssr.sh 再尝试 选择端口直接回车选择默认的就好了。当然了，你也可以设置其他的端口，这个根据个人需求 &nbsp; 然后设置密码，选择加密方式 &nbsp; 接下来选择协议插件 &nbsp; 再然后根据自己需求选择是否兼容原版ss客户端 &nbsp; 之后选择混淆插件 &nbsp; 进行混淆插件的设置后，会依次提示你对设备数、单线程限速和端口总限速进行设置，默认值是不进行限制，个人使用的话，选择默认即可，一路敲回车键。 &nbsp; 耐心等待一会，出现下面的界面即部署完成： &nbsp; 输入快捷管理命令：bash ssr.sh 进入管理界面 &nbsp; 根据上图就可以看到自己设置的ssr账号信息，包括IP、端口、密码、加密方式、协议插件、混淆插件等等，如果之后想修改账号信息，选择相应的数字来进行一键修改 加速VPS服务器 (谷歌BBR加速) wget –no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh &nbsp; &nbsp; &nbsp; chmod +x bbr.sh &nbsp; &nbsp; &nbsp;./bbr.sh 执行上面的代码，然后耐心等待，安装成功后重启VPS服务器即可 &nbsp; &nbsp; &nbsp; 最后重启服务器 ssr客户端下载 Windows SSR客户端 点击下载地址 MacOS SSR客户端 点击下载地址 Linux SSR客户端 点击下载地址 安卓 SSR客户端 点击下载地址 苹果手机SSR客户端：Potatso Lite、Potatso、shadowrocket都可以作为SSR客户端，但这些软件目前已经在国内的app商店下架，可以用美区的appid账号来下载。但是，如果你配置的SSR账号兼容SS客户端，或者协议选择origin且混淆选择plain，那么你可以选择苹果SS客户端软件(即协议和混淆可以不填)，APP商店里面有很多，比如：openwingy、superwingy、bestwingy、wingy+、greatwingy等。 有了账号后，打开SSR客户端，填上信息，这里以Windows版的SSR客户端为例子： &nbsp; 在对应的位置，填上服务器IP、服务器端口、密码、加密方式、协议和混淆，最后点击确认 常见问题解决方法 1、用了一段时间发现SSR账号用不了了? 多半是被墙了，即ip失效。首先ping一下自己的ip，看看能不能ping的通，ping不通那么就是ip被墙了，遇到这种情况重新部署一个新的服务器，新的服务器就是新的ip。关于怎么ping ip的方法，可以自行网上搜索，很简单。vultr服务商是折算成小时计费，且开通和删除服务器非常方便(新服务器即新ip。大多数vps服务商都没有这样的服务，一般的vps服务商可能会提供更换1次ip的服务，如果你买的是别家的vps，一定要了解是否能够更换ip，假如不能，那么万一你的ip不幸被墙，钱就打水漂了) 2、刚搭建好的ssr账号，ip能ping通，但是还是用不了? 首选排除杀毒软件的干扰，尤其是国产杀毒软件，比如360安全卫生、360杀毒软件、腾讯管家、金山卫士等。这些东西很容易干扰翻墙上网，如果你的电脑安装了这样的东西，建议至少翻墙时别用。其次，检查下SSR信息是否填写正确。浏览器的代理方式是否是ssr代理，即127.0.0.1 1080端口。如果以上条件都排除，还是用不了，那么可以更换端口、加密方式、协议、混淆，或者更换服务器位置 3、vultr服务商提供的 VPS 服务器是单向流量计算，有的vps服务商是双向流量计算，单向流量计算对于用户来说更实惠。因为我们是在 VPS 服务器上部署SSR服务端后，再用SSR客户端翻墙，所以SSR服务端就相当于中转，比如我们看一个视频，必然会产生流量，假如消耗流量80M，那么VPS服务器会产生上传80M和下载80M流量，vultr服务商只计算单向的80M流量。如果是双向计算流量，那么会计算为160M流量 4、如果你想把搭建的账号给多人使用，不用额外设置端口，因为一个账号就可以多人使用 &nbsp;]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>VPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些脚本小工具]]></title>
    <url>%2F2018%2F11%2F17%2Fsome_script_tool%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[可以把当前脚本目录下的gif转为mp4脚本会递归目录的子目录123456789101112131415161718192021222324import moviepy.editor as mp# vfc = mp.VideoFileClip("binary_tree_preorder_traversal.gif")# vfc.write_videofile("binary_tree_preorder_traversal.mp4")# vfc = mp.VideoFileClip(".\\mp/binary_tree_preorder_traversal.gif")# vfc.write_videofile(".\\mp/binary_tree_preorder_traversal1231.mp4")import osdef getfilelist(rlist,path, ex_filter): for dir,folder,file in os.walk(path): for i in file: if ex_filter not in i: continue t = "%s/%s"%(dir,i) rlist.append(t)all_gif_path = []getfilelist(all_gif_path, ".", ".gif")print all_gif_pathfor _gif_path in all_gif_path: vfc = mp.VideoFileClip(_gif_path) vfc.write_videofile(_gif_path.replace(".gif", ".mp4"))]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法企业真题实战练习]]></title>
    <url>%2F2018%2F11%2F01%2Falgo_practice%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[算法白话总结推荐参考本博客总结的 algo_newbie 本文完整参考代码https://github.com/no5ix/no5ix.github.io/blob/source/source/code/test_algo_practice.py 实战练习 pending_fini 比狗线程排列题 刷完剑指offer 回溯法递归强化 二叉树递归强化 lc4题-两个排序数组找中位数, 困难 一个桶里面有白球. 黑球各 100 个，现在按下述的规则取球： i . 每次从桶里面拿出来两个球； ii. 如果取出的是两个同色的球，就再放入一个黑球； iii. 如果取出的是两个异色的球，就再放入一个白球。 问：最后桶里面只剩下一个黑球的概率是多少？ 公司有内部 bbs，员工都会在上面发帖交流。据统计，有三个员工 ID 发帖很多，他们各自的发帖量都超过帖子总数 N 的 1/4。如果给到你所有帖子的发帖人 ID 列表，请写代码找出这三个 ID，要求时间复杂度 O（n），空间复杂度 O（1）。 finished leetcode64题最短路径和 判断是否有一个数num在有序数组中出现次数多于数组长度的一半 判断num是否等于中间的数即可 数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。 根据数组特点得到时间复杂度为O(n)的算法。根据数组特点，数组中有一个数字出现的次数超过数组长度的一半，也就是说它出现的次数比其他所有数字出现的次数之和还要多。因此，我们可以在遍历数组的时候设置两个值：一个是数组中的数result，另一个是出现次数times。当遍历到下一个数字的时候，如果与result相同，则次数加1,不同则次数减一，当次数变为0的时候说明该数字不可能为多数元素，将result设置为下一个数字，次数设为1。这样，当遍历结束后，最后一次设置的result的值可能就是符合要求的值（如果有数字出现次数超过一半，则必为该元素，否则不存在），因此，判断该元素出现次数是否超过一半即可验证应该返回该元素还是返回0。这种思路是对数组进行了两次遍历，复杂度为O(n)。 给定一个含有n个元素的整型数组a，其中只有一个元素出现奇数次，找出这个元素。 解决问题的关键是要想明白，按位异或运算满足结合律，偶数个异或结果是0，奇数个异或结果是本身，如1 ^ 2 ^ 3 ^ 1 ^ 2 ^ 3 ^ 3 = (3 ^ 3 ^ 3) ^ (1 ^ 1) ^ (2 ^ 2) = 3^ 0 ^ 0 = 3。 一个无序数组找其子序列构成的和最大，要求子序列中的元素在原数组中两两都不相邻, 和是多少?具体是哪个子序列? 状态转移方程讲解-打家劫舍 lc445, 两个单向链表，返回求和后的链表结构, 例如2-&gt;3-&gt;1-&gt;5，和3-&gt;6，结果返回2-&gt;3-&gt;5-&gt;1 用栈 反转链表再相加 现有一个随机数生成器可以生成0到4的数，现在要让你用这个随机数生成器生成0到6的随机数，要保证生成的数概率均匀。 别被这个题目唬住了, 其实很简单 大多数利用一个等概率随机数构造另外一个等概率随机数的题目，只需两次使用概率函数即可 代码如下:1234567891011def rand4(): # 这是题目给的 passdef rand6(): res = rand4() # 只要0和1和2, 注意这个0不可少, 不然`res + rand4()`生成的数至少为1 while res &gt; 2: res = rand4() # 这样就能保证是等概率0到4 , 等概率的加(0, 1, 2), 得到等概率的0-6 return res + rand4() 连续整数求和(leetcode第829题, hard实际有思路就easy)，要求时间复杂度小于O(N) 123456789101112class Solution: def consecutiveNumbersSum(self, N: int) -&gt; int: # 1个数时，必然有一个数可构成N # 2个数若要构成N，第2个数与第1个数差为1，N减掉这个1能整除2则能由商与商+1构成N # 3个数若要构成N，第2个数与第1个数差为1，第3个数与第1个数的差为2，N减掉1再减掉2能整除3则能由商、商+1与商+2构成N # 依次内推，当商即第1个数小于等于0时结束 res, i = 0, 1 while N &gt; 0: res += N % i == 0 N -= i i += 1 return res 智力题 有 64 匹马，赛场只有 8 条赛道，请问最少需要比赛多少场才能确定跑得最快的那 4 匹马，不可以借助计时器给每一匹马一一计时； https://www.jianshu.com/p/148439ddcb07 有 N 枚棋子，每个人一次可以拿1到 M 个，谁拿完后棋子的数量为0谁就获胜。现在有1000颗棋子，每次最多拿8个，A 先拿，那么 A 有必胜的拿法吗？ 这个是个智力题不是算法题, 是倍数的思想 这类题得先求得 N % (M+1) 的余数, 此处为 1000 % (8+1) = 1, 求得此余数Y后, 先拿的人第一次就拿Y个, 然后假如B同学第二次拿X个比如是4个, 不管B拿多少个, A之后都拿 (M+1)-X个即 (8+1)-4=5个和B同学拿的4个凑成(8+1)=9个, 这样就保证了A是最后一个拿完棋子的人 misc寻找第K大有一个整数数组，请你根据快速排序的思路，找出数组中第K大的数。给定一个整数数组a,同时给定它的大小n和要找的K(K在1到n之间)，请返回第K大的数，保证答案存在。 测试样例：[1,3,5,2,2],5,3返回：2 12345678910111213141516171819202122232425262728class Solution_find_top_k_num(object): def find_top_k_num(self, nums_arr, nums_arr_len, top_k): assert nums_arr, "nums is empty." assert nums_arr_len &gt; 0, "nums is empty." if top_k &gt; nums_arr_len or top_k &lt; 0: return None left_index = 0 right_index = nums_arr_len - 1 _p_index = self._partition(nums_arr, left_index, right_index) while _p_index != top_k-1: _p_index = self._partition(nums_arr, left_index, right_index) if _p_index &lt; top_k-1: left_index = _p_index + 1 elif _p_index &gt; top_k-1: right_index = _p_index - 1 return nums_arr[_p_index] def _partition(self, nums_arr, left_index, right_index): pivot_index = left_index _partition_index = pivot_index for i in range(pivot_index+1, right_index+1): if nums_arr[i] &lt;= nums_arr[pivot_index]: nums_arr[_partition_index+1], nums_arr[i] = \ nums_arr[i], nums_arr[_partition_index+1] _partition_index += 1 nums_arr[_partition_index], nums_arr[pivot_index] = \ nums_arr[pivot_index], nums_arr[_partition_index] return _partition_index leetcodelc41-缺失的第一个正数lc41, hard 缺失的第一个正数（leetcode第41题）给你一个未排序的整数数组，请你找出其中没有出现的最小的正整数。示例 1:输入: [1,2,0]输出: 3 示例 2:输入: [3,4,-1,1]输出: 2 示例 3:输入: [7,8,9,11,12]输出: 1 提示：你的算法的时间复杂度应为O(n)，并且只能使用常数级别的额外空间。 参考我们可以采取这样的思路：就把 11 这个数放到下标为 00 的位置， 22 这个数放到下标为 11 的位置，按照这种思路整理一遍数组。然后我们再遍历一次数组，第 11 个遇到的它的值不等于下标的那个数，就是我们要找的缺失的第一个正数。这个思想就相当于我们自己编写哈希函数，这个哈希函数的规则特别简单，那就是数值为 i 的数映射到下标为 i - 1 的位置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution_lc41(object): def firstMissingPositive(self, nums): """ :type nums: List[int] :rtype: int 思路: 使用座位交换法 根据题意可知，缺失的第一个整数是在 [1, len + 1] 之间， 那么我们可以遍历数组，然后将对应的数据填充到对应的位置上去，比如 1 就填充到 nums[0] 的位置， 2 就填充到 nums[1] 如果填充过程中， nums[i] &lt; 1 &amp;&amp; nums[i] &gt; len，那么直接舍弃 填充完成，我们再遍历一次数组，如果对应的 nums[i] != i + 1，那么这个 i + 1 就是缺失的第一个正数 比如 nums = [7, 8, 9, 10, 11], len = 5 我们发现数组中的元素都无法进行填充，直接舍弃跳过， 那么最终遍历数组的时候，我们发现 nums[0] != 0 + 1，即第一个缺失的是 1 比如 nums = [3, 1, 2], len = 3 填充过后，我们发现最终数组变成了 [1, 2, 3]，每个元素都对应了自己的位置，那么第一个缺失的就是 len + 1 == 4 """ if not nums: return 1 n = len(nums) for i in range(n): _pending_swap_index = nums[i] - 1 # 只有在 nums[i] 是 [1, len] 之间的数，并且不在自己应该呆的位置， nums[i] != i + 1 ， # 并且 它应该呆的位置没有被相同的值占有（即存在重复值占有） nums[nums[i] - 1] != nums[i] 的时候才进行交换 # 为什么使用 while ？ 因为交换后，原本 i 位置的 nums[i] 已经交换到了别的地方， # 交换后到这里的新值不一定是适合这个位置的，因此需要重新进行判断交换 # 如果使用 if，那么进行一次交换后，i 就会 +1 进入下一个循环，那么交换过来的新值就没有去找到它该有的位置 # 比如 nums = [3, 4, -1, 1] 当 3 进行交换后， nums 变成 [-1，4，3，1]， # 此时 i == 0，如果使用 if ，那么会进入下一个循环， 这个 -1 就没有进行处理 while(n &gt;= nums[i] &gt;= 1 and nums[i] != i+1 and \ nums[i] != nums[_pending_swap_index]): # `nums[i] != nums[_pending_swap_index]` 是为了防止 # nums[i] 和 nums[_pending_swap_index] 这两个数是相等的导致 # while死循环 self._swap(nums, i, _pending_swap_index) _pending_swap_index = nums[i] - 1 for i in range(n): if nums[i] != i + 1: return i + 1 return n + 1 def _swap(self, arr, index1, index2): arr[index1], arr[index2] = arr[index2], arr[index1]]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>Algo</tag>
        <tag>noodle</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[algo_newbie]]></title>
    <url>%2F2018%2F10%2F23%2Falgo_newbie%2F</url>

    <encrypted>1</encrypted>

    <content type="text"><![CDATA[实战练习推荐参考本博客总结的 算法企业真题实战练习 本文完整参考代码https://github.com/no5ix/no5ix.github.io/blob/source/source/code/test_algo_newbie.py . . . python解题常用标准库模块与函数 sorted函数, 用来排序, 注意sorted不会更改原有数组, 返回的才是排序好的数组 基本操作: num1 = [2, 1, 3]; sorted_num1 = sorted(num1) 按key排序: 123nums = [ [3,4,5], [3,2,6], [2,2,1] ] sorted_nums = sorted(nums, key=lambda x: x[0])# out: [[2, 2, 1], [3, 4, 5], [3, 2, 6]] heapq模块, 最小堆 heapq有两种方式创建堆， 一种是使用一个空列表，然后使用heapq.heappush(test_heap_list, test_num)函数把值加入堆中， 另外一种就是使用heap.heapify(test_list) 转换列表成为堆结构 如果只是想获取最小值而不是弹出，使用test_heap_list[0] 弹出使用heapq.heappop(test_heap_list)1234567891011121314151617181920212223242526import heapq# 第一种"""函数定义：heapq.heappush(heap, item) - Push the value item onto the heap, maintaining the heap invariant.heapq.heappop(heap) - Pop and return the smallest item from the heap, maintaining the heap invariant. If the heap is empty, IndexError is raised. To access the smallest item without popping it, use heap[0]."""nums = [2, 3, 5, 1, 54, 23, 132]heap = []for num in nums: heapq.heappush(heap, num) # 加入堆print(heap[0]) # 如果只是想获取最小值而不是弹出，使用heap[0]print([heapq.heappop(heap) for _ in range(len(nums))]) # 堆排序结果# out: [1, 2, 3, 5, 23, 54, 132]# 第二种nums = [2, 3, 5, 1, 54, 23, 132]heapq.heapify(nums)print([heapq.heappop(nums) for _ in range(len(nums))]) # 堆排序结果# out: [1, 2, 3, 5, 23, 54, 132] 数据结构哈希表发生碰撞的时候的解决方案: 拉链表法 开放寻址法 线性探查法, 此法并不好（Linear Probing）：di = 1,2,3,…,m-1 简单地说，就是以当前冲突位置为起点，步长为1循环查找，直到找到一个空的位置，如果循环完了都占不到位置，就说明容器已经满了。举个栗子，就像你在饭点去街上吃饭，挨家去看是否有位置一样。如果遍历到尾部都没有找到空闲的位置，那么我们就再从表头开始找，直到找到为止。 散列表中查找元素的时候，我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置还没有找到，就说明要查找的元素并没有在散列表中。 对于删除操作稍微有些特别，不能单纯地把要删除的元素设置为空。因为在查找的时候，一旦我们通过线性探测方法，找到一个空闲位置，我们就可以认定散列表中不存在这个数据。但是，如果这个空闲位置是我们后来删除的，就会导致原来的查找算法失效。这里我们可以将删除的元素，特殊标记为 deleted。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。 线性探测法存在很大问题。当散列表中插入的数据越来越多时，其散列冲突的可能性就越大，极端情况下甚至要探测整个散列表,因此最坏时间复杂度为O(N) 平方探测法（Quadratic Probing）：di = ±12, ±22, ±32…，±k2（k≤m/2） 相对于线性探查法，这就相当于的步长为di = i2来循环查找，直到找到空的位置。以上面那个例子来看，现在你不是挨家去看有没有位置了，而是拿手机算去第i2家店，然后去问这家店有没有位置。 伪随机探测法：di = 伪随机数序列, 这个就是取随机数来作为步长 再哈希法 Hi = RHi(key), 其中i=1,2,…,k RHi()函数是不同于H()的哈希函数，用于同义词发生地址冲突时，计算出另一个哈希函数地址，直到不发生冲突位置。这种方法不容易产生堆集，但是会增加计算时间。 所以再哈希法的缺点是：增加了计算时间。 负载因子与rehash负载因子计算公式为: 负载因子 = 哈希表已保存节点数量 / 哈希表大小比如说当前的容器初始容量initCapacity是16，负载因子是0.75(这个负载因子是口语中的负载因子, 实际上指的是该扩容了的负载因子临界值), 根据元素数量的扩容临界值（threshold） = 负载因子（loadFactor） * 初始容量(initCapacity)则16*0.75=12，也就是说，当容器中元素数量达到了12的时候就会进行扩容操作。他的作用很简单，相当于是一个扩容机制的阈值。当超过了这个阈值，就会触发扩容机制。 为什么java的HashMap(使用开放寻址法解决碰撞)负载因子一定是0.75？而不是0.8，0.6？ loadFactor太大，比如等于1，也就意味着，只有当容器全部填充了，才会发生扩容。那么就会有很高的哈希冲突的概率，会大大降低查询速度。 loadFactor太小，比如等于0.5，那么频繁扩容没，就会大大浪费空间。 开放寻址法与链表法比较 对于开放寻址法解决冲突的散列表: 优势: 由于数据都存储在数组中，因此可以有效地利用 CPU 缓存加快查询速度(数组占用一块连续的空间)。 缺点: 但是删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，负载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。 对于链表法解决冲突的散列表: 优势: 对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。 链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于1的情况。接近1时，就可能会有大量的散列冲突，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。 缺点: 但是，链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，而且链表中的结点是零散分布在内存中的，不是连续的，所以对CPU缓存是不友好的，这对于执行效率有一定的影响。 跳表 跳表增加数据时索引怎么变化 从上面skiplist的创建和插入过程可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点不会影响其它节点的层数。因此，插入操作只需要修改插入节点前后的指针，而不需要对很多节点都进行调整。这就降低了插入操作的复杂度。实际上，这是skiplist的一个很重要的特性，这让它在插入性能上明显优于平衡树的方案。这在后面我们还会提到。 执行插入操作时计算随机数的过程，是一个很关键的过程，它对 skiplist 的统计特性有着很重要的影响。这并不是一个普通的服从均匀分布的随机数，它的计算过程如下： 首先，每个节点肯定都有第 1 层指针（每个节点都在第 1 层链表里）。 如果一个节点有第 i 层 (i&gt;=1) 指针（即节点已经在第 1 层到第 i 层链表中），那么它有第 (i+1) 层指针的概率为 p。 节点最大的层数不允许超过一个最大值，记为 MaxLevel。 这个计算随机层数的伪码如下所示： 123456randomLevel() level := 1 // random()返回一个[0...1)的随机数 while random() &lt; p and level &lt; MaxLevel do level := level + 1 return level randomLevel() 的伪码中包含两个参数，一个是 p，一个是 MaxLevel。在 Redis 的 skiplist 实现中，这两个参数的取值为： 12p = 1/4MaxLevel = 32 跳表怎么支持查询排名的跳表数据结构里存了一个span值, 它表示当前的指针跨越了多少个节点。注意：图中前向指针上面括号中的数字，表示对应的span的值。即当前指针跨越了多少个节点，这个计数不包括指针的起点节点，但包括指针的终点节点。假设我们在这个skiplist中查找score=89.0的元素（即Bob的成绩数据），在查找路径中，我们会跨域图中标红的指针，这些指针上面的span值累加起来，就得到了Bob的排名(2+2+1)-1=4（减1是因为rank值以0起始）。需要注意这里算的是从小到大的排名，而如果要算从大到小的排名，只需要用skiplist长度减去查找路径上的span累加值，即6-(2+2+1)=1。可见，在查找skiplist的过程中，通过累加span值的方式，我们就能很容易算出排名。相反，如果指定排名来查找数据（类似zrange和zrevrange那样），也可以不断累加span并时刻保持累加值不超过指定的排名，通过这种方式就能得到一条O(log n)的查找路径。 AVL树AVL树是带有平衡条件的二叉严格平衡查找树，一般是用平衡因子差值判断是否平衡并通过旋转来实现平衡，左右子树树高不超过1，和红黑树相比，它是严格的平衡二叉树，平衡条件必须满足（所有节点的左右子树高度差不超过1）。不管我们是执行插入还是删除操作，只要不满足上面的条件，就要通过旋转来保持平衡，而旋转是非常耗时的，由此我们可以知道AVL树适合用于插入删除次数比较少，但查找多的情况。 红黑树一种二叉弱平衡查找树，但在每个节点增加一个存储位表示节点的颜色，可以是red或black。通过对任何一条从根到叶子的路径上各个节点着色的方式的限制，红黑树确保没有一条路径会比其它路径长出两倍。它是一种弱平衡二叉树(由于是若平衡，可以推出，相同的节点情况下，AVL树的高度低于红黑树)，相对于要求严格的AVL树来说，它的旋转次数变少，所以对于搜索、插入、删除操作多的情况下，我们就用红黑树。实际应用如下: 广泛用于C++的STL中，Map和Set都是用红黑树实现的； 著名的Linux进程调度Completely Fair Scheduler，用红黑树管理进程控制块，进程的虚拟内存区域都存储在一颗红黑树上，每个虚拟地址区域都对应红黑树的一个节点，左指针指向相邻的地址虚拟存储区域，右指针指向相邻的高地址虚拟地址空间； IO多路复用epoll的实现采用红黑树组织管理sockfd，以支持快速的增删改查； Nginx中用红黑树管理timer，因为红黑树是有序的，可以很快的得到距离当前最小的定时器； B树和B+树 B树(也叫B-树, 这个-只是个符号…不是B减树哈) B+树: B+树是应文件系统所需而产生的一种B树的变形树（文件的目录一级一级索引，只有最底层的叶子节点（文件）保存数据）非叶子节点只保存索引，不保存实际的数据，数据都保存在叶子节点中，所有叶子节点都有一个链表指针把实际的数据用链表连在一起使得遍历整棵树只需要遍历叶子节点就行. 为什么说B类树更适合数据库索引参考: 为什么MySQL数据库索引选择使用B+树? 为什么说B类树更适合数据库索引 我们可以根据B类树的特点，构造一个多阶的B类树，然后在尽量多的在结点上存储相关的信息，保证层数尽量的少，以便后面我们可以更快的找到信息，磁盘的I/O操作也少一些，而且B类树是平衡树，每个结点到叶子结点的高度都是相同，这也保证了每个查询是稳定的。 总的来说，B/B+树是为了磁盘或其它存储设备而设计的一种平衡多路查找树(相对于二叉，B树每个内节点有多个分支)，与红黑树相比，在相同的的节点的情况下，一颗B/B+树的高度远远小于红黑树的高度(在下面B/B+树的性能分析中会提到)。B/B+树上操作的时间通常由存取磁盘的时间和CPU计算时间这两部分构成，而CPU的速度非常快，所以B树的操作效率取决于访问磁盘的次数，关键字总数相同的情况下B树的高度越小，磁盘I/O所花的时间越少。 为什么说B+树比B树更适合数据库索引 B+树的磁盘读写代价更低：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。 B+树的查询效率更加稳定：由于b+树非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要对b树进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。 B树在提高了IO性能的同时并没有解决元素遍历的效率低下的问题，正是为了解决这个问题，B+树应用而生。B+树只需要去遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作或者说效率太低。 二叉树 遍历 深度优先遍历dfs 前序非递归 中序非递归 后序非递归 广度优先遍历bfs 层序遍历 非递归反转 找普通二叉树的两个结点的最近公共祖先LCA问题 二叉搜索树: 它的左、右子树也分别为二叉排序树 其中序遍历是个从小到大的有序序列 找二叉搜索树的任意两个结点的最近公共祖先 在遍历过程中，遇到的第一个值介于n1和n2之间的节点n，也即n1 =&lt; n &lt;= n2, 就是n1和n2的LCA。 在遍历过程中，如果节点的值比n1和n2都大，那么LCA在节点的左子树。 在遍历过程中，如果节点的值比n1和n2都小，那么LCA在节点的右子树。 记住一点, 其中序遍历是一个有序数组, 所以涉及到各种二叉搜索树(如AVL树/红黑树/B树/B+树)总是说要中序遍历扫描结点啥的 类似于 给你一棵所有节点为非负值的二叉搜索树，请你计算树中任意两节点的差的绝对值的最小值 这种题目就可以中序遍历之后得到一个有序数组然后遍历此数组求相邻元素的最小差值即可 二叉树的代码表示:12345class TreeNode(object): def __init__(self, val): self.left = None self.right = None self.val = val 如上图得到的相应的三种深度优先遍历的序列分别为 ： 先(根)序遍历 ： ABCDEGF 中(根)序遍历 ： CBEGDFA 后(根)序遍历 ： CGEFDBA 而得到的广度优先遍历的序列为 : ABCDEFG 统一形式的二叉树前中后序迭代遍历 cpp版本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162class TreeNode&#123;public: TreeNode(int _val): val(_val), left(nullptr), right(nullptr) &#123;&#125; char val; TreeNode *left, *right;&#125;;void swap_tree(TreeNode *tn)&#123;// if(tn == nullptr)// return;// std::cout &lt;&lt; tn-&gt;val &lt;&lt; std::endl;// pre_order_traverse(tn-&gt;left);// pre_order_traverse(tn-&gt;right);// std::stack&lt;std::pair&lt;const char*, TreeNode*&gt;&gt; st = stack&lt;std::pair&lt;const char*, TreeNode*&gt;&gt;(); auto st = std::stack&lt;std::tuple&lt;const char*, TreeNode*&gt;&gt;(); st.push(make_tuple("go", tn)); // todo std::tuple// // st.push(make_pair("go", tn));// while(!st.empty())&#123;// auto [cmd, tree_node] = st.top();// const char *cmd;// TreeNode* tree_node;// std::tie(cmd, tree_node) = st.top(); auto poped_elem = st.top(); st.pop(); auto elem = std::get&lt;1&gt;(poped_elem); if(strcmp(std::get&lt;0&gt;(poped_elem), "print") == 0)&#123;// std::cout &lt;&lt; std::get&lt;1&gt;(poped_elem)-&gt;val &lt;&lt; std::endl; auto temp = elem-&gt;left; elem-&gt;left = elem-&gt;right; elem-&gt;right = temp; continue; &#125; if(elem-&gt;right) st.push(make_pair("go", elem-&gt;right)); if(elem-&gt;left) st.push(make_pair("go", elem-&gt;left)); st.push(make_pair("print", elem)); &#125;&#125;void bfs(TreeNode* tn)&#123; auto qu = std::queue&lt;TreeNode*&gt;(); qu.push(tn); while (!qu.empty()) &#123; auto front_elem = qu.front(); qu.pop(); std::cout &lt;&lt; front_elem-&gt;val &lt;&lt; std::endl; if(front_elem-&gt;left) qu.push(front_elem-&gt;left); if (front_elem-&gt;right) &#123; qu.push(front_elem-&gt;right); &#125; &#125;&#125; python版本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def binary_tree_preorder_traversal(root): _result_arr = [] if not root: return _result_arr _temp_stack = [] _temp_stack.append(("go", root)) while _temp_stack: _cmd, _cur_node = _temp_stack.pop(-1) if _cmd == "print": _result_arr.append(_cur_node.val) continue if _cur_node.right: _temp_stack.append(("go", _cur_node.right)) if _cur_node.left: _temp_stack.append(("go", _cur_node.left)) _temp_stack.append(("print", _cur_node)) return _result_arrdef binary_tree_inorder_traversal(root): _result_arr = [] if not root: return _result_arr _temp_stack = [] _temp_stack.append(("go", root)) while _temp_stack: _cmd, _cur_node = _temp_stack.pop(-1) if _cmd == "print": _result_arr.append(_cur_node.val) continue if _cur_node.right: _temp_stack.append(("go", _cur_node.right)) _temp_stack.append(("print", _cur_node)) if _cur_node.left: _temp_stack.append(("go", _cur_node.left)) return _result_arrdef binary_tree_postorder_traversal(root): _result_arr = [] if not root: return _result_arr _temp_stack = [] _temp_stack.append(("go", root)) while _temp_stack: _cmd, _cur_node = _temp_stack.pop(-1) if _cmd == "print": _result_arr.append(_cur_node.val) continue _temp_stack.append(("print", _cur_node)) if _cur_node.right: _temp_stack.append(("go", _cur_node.right)) if _cur_node.left: _temp_stack.append(("go", _cur_node.left)) return _result_arr 二叉树层序遍历 注意看上图中的文字思路cpp版本1234567891011121314151617void bfs(TreeNode* tn)&#123; auto qu = std::queue&lt;TreeNode*&gt;(); qu.push(tn); while (!qu.empty()) &#123; auto front_elem = qu.front(); qu.pop(); std::cout &lt;&lt; front_elem-&gt;val &lt;&lt; std::endl; if(front_elem-&gt;left) qu.push(front_elem-&gt;left); if (front_elem-&gt;right) &#123; qu.push(front_elem-&gt;right); &#125; &#125;&#125; 与 python版本1234567891011121314def binary_tree_levelorder_traversal(root): _result_arr = [] if not root: return _result_arr _temp_queue = [] _temp_queue.append(root) while _temp_queue: _cur_node = _temp_queue.pop(0) _result_arr.append(_cur_node.val) if _cur_node.left: _temp_queue.append(_cur_node.left) if _cur_node.right: _temp_queue.append(_cur_node.right) return _result_arr 二叉树反转值得一提的是，如果把交换左右子节点的代码放在后序遍历的位置也是可以的，但是放在中序遍历的位置是不行的，请你想一想为什么？因为中序遍历换节点 根据左根右的遍历顺序 相当于左侧节点交换了两次 右侧节点没换 因为遍历根的时候交换了左右节点 遍历右侧的时候还是之前那个左节点, 所以右子树没有被翻转, 以下是递归写法:123456def binary_tree_swap_recursive(root): if not root: return root.left, root.right = root.right, root.left binary_tree_swap(root.left) binary_tree_swap(root.right) 可以看到二叉树反转的递归写法跟前序遍历的递归写法很像,所以反转的迭代写法也可以对着前序遍历的迭代写法如法炮制:12345678910111213141516def binary_tree_swap_iterative(root): if not root: return _temp_stack = [] _temp_stack.append(("go", root)) while _temp_stack: _cmd, _cur_node = _temp_stack.pop(-1) if _cmd == "print": # 参考前序遍历的迭代写法, 就只有这里改成了swap操作 _cur_node.left, _cur_node.right = _cur_node.right, _cur_node.left continue if _cur_node.right: _temp_stack.append(("go", _cur_node.right)) if _cur_node.left: _temp_stack.append(("go", _cur_node.left)) _temp_stack.append(("print", _cur_node)) 链表链表的代码表示:1234class LinkList(object): def __init__(self, val): self.next = None self.val = val 虚头结点的优点: 虚头结点是为了操作的统一与方便而设立的，放在第一个元素结点之前，其数据域一般无意义（当然有些情况下也可存放链表的长度、用做监视哨等等）。 有了虚头结点后，对在第一个元素结点前插入结点和删除第一个结点，其操作与对其它结点的操作统一了。 常见考题与解题思路 可以使用虚头结点来处理问题 链表反转 虚头节点方便处理问题的思想 双指针思想, 适用于下面这种题: 打印倒数第n个结点: 比如链表长度为6, 求打印倒数第3个结点, 则指针p1先走, 走到4的时候, 指针p2才开始走, 这样p1到尾结点的时候, p2刚好再倒数第三个结点 快慢指针的思想: 判断链表中是否有环 找一个单链表的中间结点 判断两个链表是否相交, 假设两个链表均不带环 最后一个元素必相同 给定一个头结点h和结点指针p, 怎么删除该结点 判断是否只有一个结点, 即判断if h == p and not h.next 判断是否为末尾结点, 即判断if not p.next, 是的话还是得从头遍历找到p的前一个结点 如果都不是, 则直接删除p后面的节点b, 并把b的内容复制到p上即可 链表反转 思路: 先设置一个虚头节点 pre, 先暂存好cur的next为temp_next 然后开始用cur去连接他即cur.next = pre, 把暂存好的 temp_next 赋值给 cur, 继续下一轮 while cur: 循环 cpp版本1234567891011121314151617181920212223struct LinkedList;typedef shared_ptr&lt;LinkedList&gt; llp;typedef struct LinkedList&#123; LinkedList(char _val): val(_val), next(nullptr) &#123;&#125; char val; llp next;&#125; ll;llp reverse_linked_list(llp test_ll)&#123; if(test_ll == nullptr) return test_ll; llp head = nullptr; auto cur = test_ll; while(cur)&#123; auto temp_next = cur-&gt;next; cur-&gt;next = head; head = cur; cur = temp_next; &#125; return head;&#125; 与 python版本12345678910111213141516def linklist_reverse(head): if not head: return _pre = None _cur = head _temp_next = head.next while _cur: #先用_temp_next保存_cur的下一个节点的信息， #保证单链表不会因为失去_cur节点的next而就此断裂 _temp_next = _cur.next #保存完_temp_next，就可以让_cur的next指向_pre了 _cur.next = _pre #让_pre，_cur依次向后移动一个节点，继续下一次的指针反转 _pre = _cur _cur = _temp_next return _pre 图论 广度优先遍历dfs可以得到最短路径 深度优先遍历bfs有啥用? 图的深度优先遍历dfs 图的表示下面这个图结构就有三个连通分量: 邻接表适合表示稀疏图(Sparse Graph): 邻接矩阵适合表示稠密图(Dense Graph): 图的代码实现代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181class GraphBase(object): # 图的基类 def __init__(self, point_count, is_directed): # 因为稀疏图一般用邻接表来保存图的顶点数据, # 而稠密图一般用邻接矩阵来表示, 所以他们的保存邻接点容器是不同的, # 留给具体子类来指定, 此处用 `self.adjacency_container = None`表示即可 self.adjacency_container = None self.is_directed = is_directed # 是否为有向图 self.connected_components_count = 0 # 连通分量个数 self.point_count = point_count def graph_dfs(self): """ 图的深度优先遍历（DFS）, 深度优先遍历尽可能优先往深层次进行搜索； 1. 首先访问出发点v，并将其标记为已访问过； 2. 然后依次从v出发搜索v的每个邻接点w。若w未曾访问过，则以w为新的出发点继续进行深度优先遍历，直至图中所有和源点v有路径相通的顶点均已被访问为止。 3. 若此时图中仍有未访问的顶点，则另选一个尚未访问的顶点为新的源点重复上述过程，直至图中所有的顶点均已被访问为止。 """ visited_arr = [] for _cur_point_index in xrange(0, len(self.adjacency_container)): if _cur_point_index not in visited_arr: self._dfs_by_point(_cur_point_index, visited_arr) # 运行到此处, 说明已经把所有和_cur_point_index 相连接的点都遍历完了, # A-B-C 也算作 A和C相连接的, # 其他的点肯定在另一个连接分量中 self.connected_components_count += 1 return visited_arr def _dfs_by_point(self, cur_point_index, visited_arr): visited_arr.append(cur_point_index) for _next_point_index in self._iter_adjacent_points(cur_point_index): if _next_point_index not in visited_arr: self._dfs_by_point(_next_point_index, visited_arr) def graph_bfs(self): """ 图的广度优先遍历, 也可以称为图的层序遍历, 广度优先遍历按层次优先搜索最近的结点，一层一层往外搜索: 1. 首先访问出发点v，接着依次访问v的所有邻接点w1、w2......wt， 2. 然后依次访问w1、w2......wt邻接的所有未曾访问过的顶点。 3. 以此类推，直至图中所有和源点v有路径相通的顶点都已访问到为止。此时从v开始的搜索过程结束。 4. 若此时图中仍有未访问的顶点，则另选一个尚未访问的顶点为新的源点重复上述过程，直至图中所有的顶点均已被访问为止。 """ result_arr = [] visited_set = set() # 用来记录某个顶点是否已经访问过 _temp_queue = [] for _cur_point_index in xrange(0, len(self.adjacency_container)): if _cur_point_index not in visited_set: _temp_queue.append(_cur_point_index) visited_set.add(_cur_point_index) while _temp_queue: _pt = _temp_queue.pop(0) result_arr.append(_pt) for _next_pt_index in self._iter_adjacent_points(_pt): if _next_pt_index not in visited_set: _temp_queue.append(_next_pt_index) visited_set.add(_next_pt_index) return result_arr def _iter_adjacent_points(self, cur_point_index): """ 因为稀疏图一般用邻接表来保存图的顶点数据, 而稠密图一般用邻接矩阵来表示, 所以他们的遍历邻接点的方式是不同的, 留给具体的子类来实现. """ raise NotImplementedError def add_edge(self, start_point_index, end_point_index): raise NotImplementedErrorclass SparseGraph(GraphBase): # 稀疏图 def __init__(self, point_count, is_directed): super(SparseGraph, self).__init__(point_count, is_directed) self.adjacency_container = [[] for _ in xrange(point_count)] # 邻接表 self.indegree_list = [ 0 for _ in range(point_count) ] # 每个顶点的入度, 初始化为0 def set_adjacency_list(self, adjacency_list): self.adjacency_container = adjacency_list def _iter_adjacent_points(self, cur_point_index): for _adjacent_point_index in self.adjacency_container[cur_point_index]: yield _adjacent_point_index def add_edge(self, start_point_index, end_point_index): self.adjacency_container[start_point_index].append(end_point_index) self.indegree_list[end_point_index] += 1 if not self.is_directed: self.adjacency_container[end_point_index].append(start_point_index) def topologic_sort(self): result_arr = [] zero_indegree_list = [] for point_index, cur_indegree in enumerate(self.indegree_list): if cur_indegree == 0: zero_indegree_list.append(point_index) # 将所有入度为0的顶点加入列表 while zero_indegree_list: cur_point_index = zero_indegree_list.pop() # 从列表中取出一个顶点 result_arr.append(cur_point_index) # 将所有v指向的顶点的入度减1，并将入度减为0的顶点加入列表 for j in self.adjacency_container[cur_point_index]: self.indegree_list[j] -= 1 if self.indegree_list[j] == 0: zero_indegree_list.append(j) # 若入度为0，则加入列表 if len(result_arr) != self.point_count: return False, result_arr # 没有输出全部顶点，说明有向图中有回路 else: return True, result_arr class DenseGraph(GraphBase): # 稠密图 def __init__(self, point_count, is_directed): super(DenseGraph, self).__init__(point_count, is_directed) self.adjacency_container = [ [0 for _ in xrange(point_count)] for _ in xrange(point_count) ] # 邻接矩阵 def set_adjacency_matrix(self, adjacency_matrix): self.adjacency_container = adjacency_matrix def _iter_adjacent_points(self, cur_point_index): for _adjacent_point_index, _is_point_adjacent in enumerate(self.adjacency_container[cur_point_index]): if not _is_point_adjacent: continue yield _adjacent_point_index def add_edge(self, start_point_index, end_point_index): self.adjacency_container[start_point_index][end_point_index] = 1 if not self.is_directed: self.adjacency_container[end_point_index][start_point_index] = 1 def topologic_sort(self): pass # TODOif __name__ == "__main__": temp_adjacency_list = [ [1, 2, 5, 6], [0], [0], [4, 5], [3, 5, 6], [0, 3, 4], [0, 4], ] test_sparse_graph = SparseGraph(point_count=len(temp_adjacency_list), is_directed=False) test_sparse_graph.set_adjacency_list(temp_adjacency_list) print "test_sparse_graph graph dfs:" print test_sparse_graph.graph_dfs() print "test_sparse_graph graph bfs:" print test_sparse_graph.graph_bfs() test_sparse_graph = SparseGraph(point_count=6, is_directed=True) test_sparse_graph.add_edge(5, 2) test_sparse_graph.add_edge(5, 0) test_sparse_graph.add_edge(4, 0) test_sparse_graph.add_edge(4, 1) test_sparse_graph.add_edge(2, 3) test_sparse_graph.add_edge(3, 1) print "test_sparse_graph topologic_sort: --------------" print test_sparse_graph.topologic_sort() # 这个邻接矩阵表示的和上面那个邻接表 temp_adjacency_list 是同一个图 temp_adjacency_matrix = [ [0, 1, 1, 0, 0, 1, 1], [1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 1, 0, 1, 1], [1, 0, 0, 1, 1, 0, 0], [1, 0, 0, 0, 1, 0, 0], ] test_dense_graph = DenseGraph(point_count=len(temp_adjacency_matrix), is_directed=False) test_dense_graph.set_adjacency_matrix(temp_adjacency_matrix) print "test_dense_graph graph dfs:" print test_dense_graph.graph_dfs() print "test_dense_graph graph bfs:" print test_dense_graph.graph_bfs() 打印结果:12345678910test_sparse_graph graph dfs:[0, 1, 2, 5, 3, 4, 6]test_sparse_graph graph bfs:[0, 1, 2, 5, 6, 3, 4]test_sparse_graph topologic_sort: --------------(True, [4, 5, 2, 0, 3, 1])test_dense_graph graph dfs:[0, 1, 2, 5, 3, 4, 6]test_dense_graph graph bfs:[0, 1, 2, 5, 6, 3, 4] 图的深度优先遍历dfs 图的深度优先遍历（DFS）, 深度优先遍历尽可能优先往深层次进行搜索； 首先访问出发点v，并将其标记为已访问过； 然后依次从v出发搜索v的每个邻接点w。若w未曾访问过，则以w为新的出发点继续进行深度优先遍历，直至图中所有和源点v有路径相通的顶点均已被访问为止。 若此时图中仍有未访问的顶点，则另选一个尚未访问的顶点为新的源点重复上述过程，直至图中所有的顶点均已被访问为止。 代码在上方已经有了, 其代码中的 graph_dfs 就是. 图dfs用途: 可以获得两点之间的一条路径 判断图是否有环: pending_fini leetcode原题201与题解 大致算法思想: 一条深度遍历路线中如果有结点被第二次访问到，那么有环。我们用一个变量来标记某结点的访问状态（未访问，访问过，其后结点都被访问过），然后判断每一个结点的深度遍历路线即可。 图的广度优先遍历bfs 也可以称为层序遍历, 广度优先遍历按层次优先搜索最近的结点，一层一层往外搜索: 首先访问出发点v，接着依次访问v的所有邻接点w1、w2……wt， 然后依次访问w1、w2……wt邻接的所有未曾访问过的顶点。 以此类推，直至图中所有和源点v有路径相通的顶点都已访问到为止。此时从v开始的搜索过程结束。 若此时图中仍有未访问的顶点，则另选一个尚未访问的顶点为新的源点重复上述过程，直至图中所有的顶点均已被访问为止。 图的bfs一般要用一个队列来实现, 代码在上方已经有了, 其代码中的 graph_bfs 就是. 图bfs用途: 可以获得两点之间的最短路径 拓扑排序参考拓扑排序通常用来 “排序” 具有依赖关系的任务.比如，如果用一个 DAG 图来表示一个工程，其中每个顶点表示工程中的一个任务，用有向边 表示在做任务 B 之前必须先完成任务 A。故在这个工程中，任意两个任务要么具有确定的先后关系，要么是没有关系，绝对不存在互相矛盾的关系（即环路）。 在图论中，拓扑排序（Topological Sorting）是一个有向无环图（DAG, Directed Acyclic Graph）的所有顶点的线性序列。且该序列必须满足下面两个条件： 每个顶点出现且只出现一次。 若存在一条从顶点 A 到顶点 B 的路径，那么在序列中顶点 A 出现在顶点 B 的前面。 有向无环图（DAG）才有拓扑排序，非 DAG 图没有拓扑排序一说。例如，下面这个图： 它是一个 DAG 图，那么如何写出它的拓扑排序呢？这里说一种比较常用的方法： 从 DAG 图中选择一个 没有前驱（即入度为 0）的顶点并输出。 从图中删除该顶点和所有以它为起点的有向边。 重复 1 和 2 直到当前的 DAG 图为空或当前图中不存在无前驱的顶点为止。后一种情况说明有向图中必然存在环。 于是，得到拓扑排序后的结果是 {1, 2, 4, 3, 5}。通常，一个有向无环图可以有一个或多个拓扑排序序列。 根据上面讲的方法，我们关键是要维护一个入度为 0 的顶点的列表. 代码实现思路如下: 每次在入度为0的列表中取顶点, 取出一个顶点v, 便输出v 然后将所有v指向的顶点的入度减1，并将入度减为0的顶点加入列表 重复步骤1和2 如果最终输出的顶点数量小于总顶点数量, 说明有环 取顶点的顺序不同会得到不同的拓扑排序序列，当然前提是该图存在多个拓扑排序序列。代码在上面有了, 其中的topologic_sort便是, 我们尝试用此代码来测试了如下DAG图： 输出结果是 4, 5, 2, 0, 3, 1。这是该图的拓扑排序序列之一。 单调栈参考 定义：栈内元素保持有序的状态的栈称为单调栈，如下图所示： 单调栈主要应用：在一个一维数组中，帮助我们找到某个元素的左侧或右侧第一个大于或小于该元素的数。 而所谓 单调栈 则是在栈的 先进后出 基础之上额外添加一个特性：从栈顶到栈底的元素是严格递增（or递减）。具体进栈过程如下： 对于单调递减栈，若当前进栈元素为 e，从栈顶开始遍历元素，把小于 e 或者等于 e 的元素弹出栈，直接遇到一个大于 e 的元素或者栈为空为止，然后再把 e 压入栈中。 对于单调递增栈，则每次弹出的是大于 e 或者等于 e 的元素。 以 单调递减栈 为例进行说明, 现在有一组数 3，4，2，6，4，5，2，3, 让它们从左到右依次入栈。具体过程如下： 第i步 操作 结果(栈底-&gt;栈顶) 1 3进 3 2 3出, 4进 4 3 2进 4 2 4 4 2出, 6进 6 5 4进 6 4 6 4出, 5进 6 5 7 2进 6 5 2 8 2出, 3进 6 5 3 队列中数帽子问题现有一条排好的队伍，从队首到队尾，队员们都戴着帽子，身高是无序的。假设每个人能看到队伍中在他前面的比他个子矮的人的帽子，（如果出现一个比这个人个子高的人挡住视线，那么此人不能看到高个子前面的任何人的帽子。）现在请计算出这个队伍中一共可以看到多少个帽子？例如给定数组为：[2,1,5,6,2,3]（顺序为从队尾到队首）。 如图示，答案为3。从暴力角度尝试去解这道题，显然可以做到。对于数组中每个元素，向右去找所有比它小的元素（找第第一个比它大的元素），这样总的时间复杂度为O(n^2)，最坏情况是这是一个单调递减数组，每次都要向右找到数组的最末尾。显然这不是理想的解法，我们可以应用单调栈来解决这个问题。其代码如下：1234567891011121314int countHats(vector&lt;int&gt;&amp; heights) &#123; heights.push_back(INT_MAX); stack&lt;int&gt; stk; int sum = 0; for(int i=0; i&lt;heights.size(); i++) &#123; while( !stk.empty() &amp;&amp; heights[i] &gt; heights[stk.top()]) ) &#123; int top = stk.top(); stk.pop(); sum += i – top – 1; &#125; stk.push(i); &#125; return sum;&#125; 在以上代码中，我们维护了一个单调递减栈，在栈中的元素都是单调递减的，这表明栈内的元素还可能看到比它更小的元素（帽子）。当遇到一个比栈顶元素大的元素时，说明栈顶元素不可能看到比它更小的元素了（因为遮挡作用），这时将栈顶元素pop出来，同时更新sum的值，sum += i – top – 1，表示栈顶元素与这个新元素间的距离，也就是栈顶元素能看到的最多的帽子数。在for循环中，每个元素都会入栈和出栈，在出栈过程中总会计算出栈顶元素能看到的最多的帽子数，并更新sum值，当整个队列循环结束后，得到的sum值就是最后队伍中能看到的帽子总数。注意为了使所有元素都能出栈，（糟糕情况是单调递减数列，这时似乎一次出栈都没有发生，原因是最后一个元素后面不可能有新的元素出现了，但单调栈还在期待新的元素出现，为了反映元素不再出现这一事实，我们假设最后一个元素后面出现了一个无穷大的元素），即heights.push_back(INT_MAX)。 寻找第一个比自己大的数给一个数组，返回一个大小相同的数组。返回的数组的第i个位置的值应当是，对于原数组中的第i个元素，至少往右走多少步，才能遇到一个比自己大的元素（如果之后没有比自己大的元素，或者已经是最后一个元素，则在返回数组的对应位置放上-1）。例如给定数组为：[2,1,5,6,2,3]返回数组应该为：[2,1,1,-1,1,-1]其实这个问题本质上和数帽子问题是一样的，本质都是找到元素右边第一个比它大的数，代码稍作改动即可。123456789101112int countSteps(vector&lt;int&gt;&amp; heights) &#123; stack&lt;int&gt; stk; vector&lt;int&gt; results(heights.size(), -1); for(int i=0; i&lt;heights.size(); i++) &#123; while( !stk.empty() &amp;&amp; heights[i] &gt; heights[stk.top()]) ) &#123; results[stk.top()] = i – stk.top(); stk.pop(); &#125; stk.push(i); &#125; return results;&#125; 接雨水-经典单调栈题lc42, hard给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。示例 1：输入：height = [0,1,0,2,1,0,1,3,2,1,2,1]输出：6解释：上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。示例 2：输入：height = [4,2,0,3,2,5]输出：9 参考理解题目，参考图解，注意题目的性质，当后面的柱子高度比前面的低时，是无法接雨水的当找到一根比前面高的柱子，就可以计算接到的雨水, 所以使用单调递减栈 对更低的柱子入栈: 更低的柱子以为这后面如果能找到高柱子，这里就能接到雨水，所以入栈把它保存起来 平地相当于高度 0 的柱子，没有什么特别影响 当出现高于栈顶的柱子时: 说明可以对前面的柱子结算了 计算已经到手的雨水，然后出栈前面更低的柱子 计算雨水的时候需要注意的是: 雨水区域的右边 r 指的自然是当前索引 i 底部是栈顶 st.top() ，因为遇到了更高的右边，所以它即将出栈，使用 cur 来记录它，并让它出栈 左边 l 就是新的栈顶 st.top() 雨水的区域全部确定了，水坑的高度就是左右两边更低的一边减去底部，宽度是在左右中间 使用乘法即可计算面积 1234567891011121314151617181920class Solution(object): def trap(self, height): """ :type height: List[int] :rtype: int """ rain_sum = 0 _monotone_stack = [] for i in range(len(height)): while _monotone_stack and height[i] &gt; height[_monotone_stack[-1]]: cur = _monotone_stack.pop(-1) if not _monotone_stack: break r = i l = _monotone_stack[-1] w = r - l - 1 h = min(height[l], height[r]) - height[cur] rain_sum += w * h _monotone_stack.append(i) return rain_sum 柱状图中最大矩形问题lc84给定 n 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。求在该柱状图中，能够勾勒出来的矩形的最大面积。如上图，矩形最大面积为10。这个问题同样可以借助单调栈来解决。在这之前，需要理解如何找到这个最大面积矩形，这个矩形的限制条件有两个，一个是高度（也即组成矩形的最短的那根柱子高度），一个是宽度，（也即组成矩形的柱子个数）。为了找到这个全局最大值，我们遍历所有局部最优情况。那么什么是局部最优解呢，我们将每个柱子的高度作为包含它的矩形的高度，也即这个柱子一定是这个矩形中最低的一个柱子，那么我们下一步是求解这个矩形的宽度，显然我们只需找到这个柱子左边，右边第一个比它低的柱子，就可以求出宽度。这显然让我们想到使用单调栈的数据结构。代码如下：1234567891011121314int largestRectangleArea(vector&lt;int&gt; heights) &#123; int maxArea = 0; heights.push_back(0); stack&lt;int&gt; stk; //monotone stack(ascending) for(int i=0; i&lt;heights.size(); i++) &#123; while(!stk.empty() &amp;&amp; heights[i] &lt; heights[stk.top()]) &#123; int top = stk.top(); stk.pop(); //find the smaller element to the left of the current element maxArea = max(maxArea,heights[top]*(stk.empty() ? i : (i - stk.top()-1))); &#125; stk.push(i); &#125; return maxArea;&#125; 我们维护一个单调递增栈，当遇到一个新元素小于栈顶元素时，发生出栈行为，表示栈顶元素向右遇到了第一个小于它的元素，同时在栈内的栈顶元素的下面一个元素即是栈顶元素向左寻找时第一个小于它的元素。（这一点的原因值得仔细思考，其实是因为栈顶元素与其下面的元素间在原数组中或许存在很多的元素，但它们必然是比栈顶元素大且比栈顶元素下面的元素小的，它们都在之前被弹出了栈。）在出栈行为发生后，我们需要计算以栈顶元素的高度值作为矩形高度时的矩形面积，而矩形宽度已经可以计算了，因为我们找到了栈顶元素左右两侧小于它的第一个元素，于是局部最优解得到计算。在整个循环中，所有元素进栈一次，出栈一次，时间复杂度为O(n)。 并查集参考 假设有 n 个村庄，有些村庄之间有连接的路，有些村庄之间并没有连接的路 设计一个数据结构，能够快速执行 2 个操作: 查询 2 个村庄之间是否有连接的路 连接 2 个村庄 使用数组、链表、平衡二叉树、集合 (Set), 查询、连接的时间复杂度都是: O(n), 但是: 并查集能够办到查询、连接的均摊时间复杂度都是 O(α(n)), α(n) &lt; 5 并查集非常适合解决这类 “连接” 相关的问题 并查集有2个核心操作: 查找(Find): 查找元素所在的集合(这里的集合并不是特指Set这种数据结构, 是指广义的数据集合) 合并(Union): 将两个元素所在的集合合并为一个集合 假设并查集处理的数据都是整型，那么可以用整型数组来存储数据, 每个数组坐标index表示某个node, 而每个数组元素的值表示node的parent. 初始化时，每个元素各自属于一个单元素集合, 父节点parent都是自己: 举个普通例子, 如果有下图这种情况: 则, 从上图中不难看出: 0、1、3 属于同一集合, 这三个node的根节点都是1 2 单独属于一个集合, 其根节点是自己也就是2 4、5、6、7 属于同一集合, 这四个node的根节点都是6 因此，并查集是可以用数组实现的树形结构 (二叉堆、优先级队列也是可以用数组实现的树形结构) Find操作并查集的find查找操作指的是: 通过parent链条不断地向上找，直到找到根节点.如并查集例子图1, 则 find(0) == 1 find(1) == 1 find(3) == 1 find(2) == 2 find(5) == 6 find(4) == 6 find(7) == 6 Union操作Quick Union 的 union(v1, v2)：让 v1 的根节点指向 v2 的根节点 在Union的过程中，可能会出现树不平衡的情况，甚至退化成链表 所以一般都会基于rank(也翻译为秩, 其实就是这棵树的层数)的优化, 即层数少的连到层数多的根节点去, 比如下图: 并查集优化-路径压缩路径减半（Path Halving）：使路径上每隔一个节点就指向其祖父节点(parent的parent), 这是靠在find的时候顺便压缩的 并查集代码实现12345678910111213141516171819202122232425262728293031323334353637class UnionFind: def __init__(self, node_count): # 初始化时，每个元素各自属于一个单元素集合, 父节点parent都是自己 self.parent_list = [ i for i in range(node_count) ] # 秩都初始化为1, 因为只有自己一个节点的时候, 层数为1 self.rank_list = [1 for _ in range(node_count) ] # 连通分量个数初始化为node个数, 等做union合并操作的时候再减少 self.connected_component_count = node_count # 找到val的根节点 def find(self, val): while(val != self.parent_list[val]): # 这一步就是路径压缩了, 路径减半 self.parent_list[val] = self.parent_list[self.parent_list[val]] val = self.parent_list[val] return val # 把val1所属的集合与val2所属的集合合并, 也就是把val1的根节点指向val2的根节点(或者反之) def unite(self, val1, val2): # 先找到val1和val2的根节点 parent1 = self.find(val1) parent2 = self.find(val2) if parent1 == parent2: return if self.rank_list[parent1] &lt; self.rank_list[parent2]: self.parent_list[parent1] = parent2 elif self.rank_list[parent1] &gt; self.rank_list[parent2]: self.parent_list[parent2] = parent1 else: # parent1和parent2的秩相等 # 这里反过来`self.parent_list[parent1] = parent2`也行 self.parent_list[parent2] = parent1 # parent2指向parent1了, 则显而易见的parent2的秩要加1 self.rank_list[parent2] += 1 self.connected_component_count -= 1 def is_connected(self, val1, val2): return self.find(val1) == self.find(val2) 并查集实战岛屿数量和朋友圈数量问题 朋友圈问题, lc547, medium 岛屿数量问题, lc200, medium 这两题虽然表述不一样但是其实是同一个问题…好奇leetcode为啥没标注为一样参考 解决本题的思路可以用dfs思路(见本文岛屿数量-经典floodfill问题),也可以用并查集, 新建一个并查集类 包括parent母结点数组，rank秩数组(优化用)和 count 数量构造初始化时 需要全图遍历一次 把parent对应的有陆地的标号置为和parent数组下标一样的值。海洋都是-1比如:121 1 1 00 0 1 1 则 parent = [0,1,2,-1,-1,-1,6,7]rank为秩 默认为0 若数有2个结点 则秩为1比如前两个陆地 1 1 合并后 变为 parent = [1,1,2,-1,-1,-1,6,7] 第一块秩为1count 初始化 遇到陆地就+1 比如 之前的count = 5正式求解时 再遍历全图 每个点向上下左右四个方向合并 合并一块count--之前的 会合并4次 count减去4次 就变成1了 只剩1块岛屿具体见下方cpp代码, 注释写的很详细了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122// 定义并查集class Djset &#123;private: // 数目 int count; // 母结点集合 vector&lt;int&gt; parent; // 秩（优化用） vector&lt;int&gt; rank;public: // 初始构造函数 主要初始化3个私有成员 // 默认parent数值为-1 一维，大小 是grid矩阵行数*列数 // 默认rank 秩为 0 一维，大小 是grid矩阵行数*列数 // 默认count数量为0 Djset(vector&lt;vector&lt;char&gt;&gt;&amp; grid): count(0), parent(vector&lt;int&gt;((grid.size()) * (grid[0].size()),-1)), rank(vector&lt;int&gt;((grid.size()) * (grid[0].size()), 0)) &#123; int row = grid.size(); int col = grid[0].size(); for(int i=0; i&lt;row; ++i)&#123; for(int j=0; j&lt;col; ++j)&#123; if(grid[i][j]=='1')&#123;// 初始化 若有陆地块 则母结点等于自己 否则是默认值-1// 因为是二维 所以映射到一维数组需要转换一下关系// : 行号*行数 + 列号 parent[i*col+j] = i*col+j;// 遇到一块陆地就+1 后面根据连通分量再删// 这个count 不是最终答案 count++; &#125; &#125; &#125; &#125; // 查 (找结点所在树的根节点) 如1-&gt;2-&gt;3-&gt;5 find(1) 得到 5 int find(int x)&#123; // if(x!=parent[x])&#123; // （1）路径压缩 优化 // 所有子结点全部指向根节点 减少树的深度 但开销较大 不推荐 // parent[x] = find(parent[x]); // &#125; // return parent[x]; // （2）路径减半 优化 // 使路径上每隔一个节点就指向其祖父节点(parent的parent) // 以 1-&gt;2-&gt;3-&gt;4-&gt;5 为例 若find(1) 路径查找优化为 // 1-&gt;3-&gt;5 路径减半 减少树的深度 while(x!=parent[x])&#123; parent[x] = parent[parent[x]]; x = parent[x]; &#125; return x; &#125; // 并 (一个结点树并到另一个结点树上) void unite(int x1, int x2)&#123; // 使用秩优化 按秩合并 避免合成后变成单链表 O(n)复杂度 // 找到 x1 和 x2两个树的根结点 int f1 = find(x1); int f2 = find(x2); // 不相等才合并 相等就不需要合并了 证明在一棵树上 if(f1!=f2)&#123; // 秩f1&gt;f2 f1长一些 把f2的树并在f1上 秩不增加 树总深度不变深 if(rank[f1]&gt;rank[f2])&#123; parent[f2] = f1; // 理解为 f2-&gt;f1 &#125;else&#123; // 秩f1&lt;=f2 把f1的树并在f2上 parent[f1] = f2; // f1-&gt;f2 // 若 秩f1=f2 合并后秩会+1 // 例： f1:1-&gt;2 f2:3&lt;-4 合： 1-&gt;2-&gt;3&lt;-4 if(rank[f1]==rank[f2])&#123; rank[f2]++ ; &#125; &#125; // 两块陆地合并成一块 减去一个数量 很重要！ count--; &#125; &#125; int get_count() const&#123; return count; &#125;&#125;;class Solution &#123;public: int numIslands(vector&lt;vector&lt;char&gt;&gt;&amp; grid) &#123; int row = grid.size(); if(!row)&#123;return 0;&#125; int col = grid[0].size(); // 初始化并查集对象 Djset djs(grid); // 全图遍历 for(int i=0; i&lt;row; ++i)&#123; for(int j=0; j&lt;col; ++j)&#123; // 当前块为陆地 if(grid[i][j]=='1')&#123; // 遍历过 避免重复 grid[i][j]='2'; // 向四个方向合并 有合并就会减去陆地数目得到最终的数目 // 向上 if(i-1&gt;=0 &amp;&amp; grid[i-1][j]=='1')&#123; djs.unite(i*col+j, (i-1)*col + j); &#125; // 向左 if(j-1&gt;=0 &amp;&amp; grid[i][j-1]=='1')&#123; djs.unite(i*col+j, (i)*col + j-1); &#125; // 向下 if(i+1&lt;row &amp;&amp; grid[i+1][j]=='1')&#123; djs.unite(i*col+j, (i+1)*col + j); &#125; // 向右 if(j+1&lt;col &amp;&amp; grid[i][j+1]=='1')&#123; djs.unite(i*col+j, (i)*col + j+1); &#125; &#125; &#125; &#125; // 得到最终数量 return djs.get_count(); &#125;&#125;; 等式方程的可满足性lc990, medium给定一个由表示变量之间关系的字符串方程组成的数组，每个字符串方程 equations[i] 的长度为 4，并采用两种不同的形式之一：”a==b” 或 “a!=b”。在这里，a 和 b 是小写字母（不一定不同），表示单字母变量名。只有当可以将整数分配给变量名，以便满足所有给定的方程时才返回 true，否则返回 false。示例 1：输入：[“a==b”,”b!=a”]输出：false解释：如果我们指定，a = 1 且 b = 1，那么可以满足第一个方程，但无法满足第二个方程。没有办法分配变量同时满足这两个方程。示例 2：输入：[“b==a”,”a==b”]输出：true解释：我们可以指定 a = 1 且 b = 1 以满足满足这两个方程。示例 3：输入：[“a==b”,”b==c”,”a==c”]输出：true示例 4：输入：[“a==b”,”b!=c”,”c==a”]输出：false示例 5：输入：[“c==c”,”b==d”,”x!=z”]输出：true 核心思想是，将 equations 中的算式根据 == 和 != 分成两部分，先处理 == 算式，使得他们通过相等关系各自勾结成门派；然后处理 != 算式，检查不等关系是否破坏了相等关系的连通性。 12345678910111213141516171819# 26 个英文字母unionFind = UnionFind(26)# 先让相等的字母形成连通分量for equation in equations: if equation[1] == '=': # ord() 函数它以一个字符作为参数，返回对应的 ASCII 数值 index1 = ord(equation[0]) - ord('a') index2 = ord(equation[3]) - ord('a') unionFind.union(index1, index2)# 检查不等关系是否打破相等关系的连通性for equation in equations: if equation[1] == '!': index1 = ord(equation[0]) - ord('a') index2 = ord(equation[3]) - ord('a') if (unionFind.is_connected(index1, index2)): # 如果相等关系成立，就是逻辑冲突 return Falsereturn True 位运算位运算的套路技巧 异或 异或的性质: 两个数字异或的结果a^b是将 a 和 b 的二进制每一位进行运算，得出的数字。 运算的逻辑是: 如果同一位的数字相同则为 0，不同则为 1 异或的规律: 任何数和本身异或则为0 任何数和 0 异或是本身 任何数和 1 异或 相当与取反 异或运算满足交换律，即: a ^ b ^ c = a ^ c ^ b 异或还可以模拟不算进位的加法: 12 二进制：1100 15 二进制：1111 各位置上的数字分别相加先不管进位的问题, 得到临时二进制结果： 1100 + 1111 = 0011 这也可以用1100 ^ 1111 = 0011得到. 本文算法题不用加减乘除做加法有应用 移除最后一个1: a=n&amp;(n-1), 比如n = 0b11010; print bin(n&amp;(n-1)), 则打印0b11000 获取最后一个 1: diff=(n&amp;(n-1))^n, 可以看出来是与 移除最后一个1了之后的数做个异或. python负数存储特殊性首先python/cpp/java语言中的数字都是以补码形式存储的, 但python没有int/long等不同长度的整形, python编程无需关心整形变量位数. py的整形数字可以视为是以一个无限长的位存储方式来实现的: 比如正数1其实是000000000000000000000000...000000000001, 远不止32位, 而..如果是c++的32位的正数1则只是0x00000001 而比如py的负数-1的补码存储则是 111111111111111111111111111...111, 远不止32位, 而..如果是c++的32位的负数-1则只是0xffffffff 但是python: Python 中 bin 一个负数（十进制表示），打印输出的却是它的原码的二进制表示加上个负号，方便查看（方便个鬼啊） 所以想看python负数的补码得用她和0xffffffff进行与操作, 可以理解为超过32位的东西就不进行考虑了，直接来查看后32位 重点: 那如果想从一个负数的补码还原成python的负数, 比如把-3的补码0xfffffffd还原成python的负数, 因为py的整形数字可以视为是以一个无限长的位存储方式来实现的, 所以直接print 0xfffffffd他会打印4294967293, 因为python把0xfffffffd当成了0x000000000fffffffd, 符号位在最前面为0, 当成正数了, 所以我们得对它的后32位之前的所有0都取反变为1, 这样符号位为1才是python存储-1的真正补码形式, 所以对于一个负数res来说, 得这么还原: ~(res ^ 0xffffffff), 要先将 末尾32 位取反（即 res ^ 0xffffffff ），再将所有位取反（即 ~ ). 两个组合操作实质上是将数字 末尾32 以前的位取反， 末尾32 位不变。 1234567891011121314151617181920a = bin(3)print(a)# out: 0b11# Python 中 bin 一个负数（十进制表示），# 打印输出的却是它的原码的二进制表示加上个负号，方便查看（方便个鬼啊）a = bin(-3)print(a)# out: -0b11 # 所以想看python负数的补码得用她和0xffffffff进行与操作,# 可以理解为超过32位的东西就不进行考虑了，直接来查看后32位b = bin(-3 &amp; 0xffffffff)print(b)# out: 0b11111111111111111111111111111101b = 0xfffffffdb = ~(b ^ 0xffffffff)print(b)# out: -3 二进制中1的个数剑指15请实现一个函数，输入一个整数，输出该数二进制表示中 1 的个数。例如，把 9 表示成二进制是 1001，有 2 位是 1。因此，如果输入 9，则该函数输出 2。示例 1：输入：00000000000000000000000000001011输出：3解释：输入的二进制串 00000000000000000000000000001011 中，共有三位为 ‘1’。 其实这道题用python来解不是很舒服, 接下来我们用适用于大多数语言的思路来写代码.因为python的int型是无限长度的, 所以我们假定n位64位, 我们不能用把n右移位的思路, 因为大多数语言对于负数的二进制表达都是补码其符号位是1(python的负数表达不太一样, -2打印出来表示为-0b10, 但实际上内存中还是以补码来存的, 为11111110, 所以下方代码对于python也是对的), 右移的话, 左边是要补1的, 代码不好写.因此我们采取用flag=1每次左移一位然后与n做位与运算即可1234567891011121314151617181920class Solution_jzo15(object): def hammingWeight(self, n): """ :type n: int :rtype: int """ # 我们采取用1每次左移一位然后与n做位与运算即可 _flag = 1 cnt = 0 move_cnt = 0 while _flag: if n &amp; _flag: cnt += 1 _flag = _flag &lt;&lt; 1 move_cnt += 1 if move_cnt &gt;= 64: # 因为python的int型是无限长度的... # 所以要用64次限制一下.. break return cnt 不用加减乘除做加法剑指65写一个函数，求两个整数之和，要求在函数体内不得使用 “+”、“-”、“*”、“/” 四则运算符号。示例:输入: a = 1, b = 1输出: 2 参考举个例子:12 二进制为：110015 二进制为：1111各位置上的数字分别相加先不管进位的问题则：1100 + 1111 = 0011得到临时不管进位的二进制结果temp: 0011(十进制位3), 那不用加法如何模拟? 可以用异或模拟1100 ^ 1111 = 0011计算进位的数字, 得到进位结果: 11000(十进制为24), 进位计算如何不用加法模拟?相与，左移一位则可得到进位结果carry:(1100 &amp; 1111) &lt;&lt; 1 = 11000然后temp + carry 则为 十进制的3+24=27, 用上述的方法再算一次temp + carry则cpp代码如下:12345678910111213class Solution &#123;public: int add(int a, int b) &#123; while (b) &#123; // LeetCode c++ 不允许负数左移操作，所以要转换成无符号整数 // 当然面试的时候不需要转换哈 int carry = (unsigned int)(a &amp; b) &lt;&lt; 1; a ^= b; b = carry; &#125; return a; &#125;&#125;; 因为python负数存储特殊性, 需要特殊处理一哈, 如下:123456789101112131415class Solution(object): def add(self, a, b): a &amp;= 0xffffffff b &amp;= 0xffffffff while b != 0: temp = a ^ b carry = (a &amp; b) &lt;&lt; 1 &amp; 0xffffffff a = temp b = carry # return a if (a &amp; 0xffffffff) &gt;&gt; 31 == 0 else ~(a ^ 0xffffffff) if a &lt; 0x80000000:#如果是正数的话直接返回 return a else: return ~(a^0xffffffff)#是负数的话,转化成其原码 只出现一次的数字系列出现奇数次题目: 给定一个含有n个元素的整型数组a，其中只有一个元素出现奇数次，找出这个元素。解决问题的关键是要想明白，按位异或运算满足结合律，偶数个异或结果是0，奇数个异或结果是本身，如1 ^ 2 ^ 3 ^ 1 ^ 2 ^ 3 ^ 3 = (3 ^ 3 ^ 3) ^ (1 ^ 1) ^ (2 ^ 2) = 3^ 0 ^ 0 = 3 只出现一次的数字系列1lc136给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。1234567class Solution: def singleNumber(self, nums: List[int]) -&gt; int: # 我们执行一次全员异或即可 single_number = 0 for num in nums: single_number ^= num return single_number 只出现一次的数字系列2lc137给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现了三次。找出那个只出现了一次的元素。说明：你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？示例 1:输入: [2,2,3,2]输出: 3示例 2:输入: [0,1,0,1,0,1,99]输出: 99 参考建立一个长度为 32 的数组 counts ，通过以下方法可记录所有数字的各二进制位的 1 的出现次数。将 counts 各元素对 3 求余，则结果为 “只出现一次的数字” 的各二进制位。利用 左移操作 和 或运算 ，可将 counts 数组中各二进位的值恢复到数字 res 上最终返回 res 即可。实际上，只需要修改求余数值 m ，即可实现解决 除了一个数字以外，其余数字都出现 m 次 的通用问题.1234567891011121314151617181920212223242526272829303132333435class Solution_lc137(object): def singleNumber(self, nums): """ :type nums: List[int] :rtype: int """ counts = [ 0 for _ in range(32) ] # 建立一个长度为 32 的数组 counts ，通过以上方法可记录所有数字的各二进制位的 1 的出现次数。 for cur_num in nums: _flag = 1 for j in range(32): if cur_num &amp; _flag: counts[j] += 1 _flag = _flag &lt;&lt; 1 res = 0 m = 3 # 将 counts 各元素对 3 求余，则结果为 “只出现一次的数字” 的各二进制位。 # 利用 左移操作 和 或运算 ，可将 counts 数组中各二进位的值恢复到数字 res 上 # 最终返回 res 即可。 # 实际上，只需要修改求余数值 m ，即可实现解决 除了一个数字以外， # 其余数字都出现 m 次 的通用问题. for i in range(32): res &lt;&lt;= 1 res |= counts[31-i] % m # 那如果想从一个负数的补码还原成python的负数, # 比如把`-3`的补码`0xfffffffd`还原成python的负数, # 因为py的整形数字可以视为是以一个无限长的位存储方式来实现的, # 所以直接`print 0xfffffffd`他会打印`4294967293`, # 因为python把`0xfffffffd`当成了`0x000000000fffffffd`, # 符号位在最前面为0, 当成正数了, 所以我们得对它的后32位之前的所有0都取反变为1, # 这样符号位为1才是python存储`-1`的真正补码形式, # 所以对于一个负数`res`来说, 得这么还原: `~(res ^ 0xffffffff)`, # 要先将 末尾32 位取反（即 res ^ 0xffffffff ），再将所有位取反（即 ~ ). # 两个组合操作实质上是将数字 末尾32 以前的位取反， 末尾32 位不变。 return res if (counts[31] % m) == 0 else ~(res ^ 0xffffffff) 只出现一次的数字系列3lc260给定一个整数数组 nums，其中恰好有两个元素只出现一次，其余所有元素均出现两次。 找出只出现一次的那两个元素。示例 :输入: [1,2,1,3,2,5]输出: [3,5]注意：结果输出的顺序并不重要，对于上面的例子， [5, 3] 也是正确答案。你的算法应该具有线性时间复杂度。你能否仅使用常数空间复杂度来实现？ 参考现在数组中有两个数字只出现1次，直接异或一次只能得到这两个数字的异或结果，但光从这个结果肯定无法得到这个两个数字。因此基于single number I 的思路——数组只能有一个数字出现1次。 设题目中这两个只出现1次的数字分别为A和B，如果能将A，B分开到二个数组中，那显然符合“异或”解法的关键点了。 因此这个题目的关键点就是将A，B分开到二个数组中。由于A，B肯定是不相等的，因此在二进制上必定有一位是不同的。根据这一位是0还是1可以将A，B分开到A组和B组。而这个数组中其它数字要么就属于A组，要么就属于B组。再对A组和B组分别执行“异或”解法就可以得到A，B了。而要判断A，B在哪一位上不相同，只要根据A异或B的结果就可以知道了，这个结果在二进制上为1的位就说明A，B在这一位上是不相同的。 比如 int a[] = {1, 1, 3, 5, 2, 2}, 整个数组异或的结果为3^5，即 0b0011 ^ 0b0101 = 0b0110, 而0b0110则表示3和5这两个数在第1位和第2位不同。我们取第1位来分组(当然取第2位来分组也可以), 因此整个数组根据这一位是0还是1分成两组。123456a[0] =1 0b0001 第一组a[1] =1 0b0001 第一组a[2] =3 0b0011 第二组a[3] =5 0b0101 第一组a[4] =2 0b0010 第二组a[5] =2 0b0010 第二组 第一组有{1,1,5}，第二组有{3,2,2}，然后对这二组分别执行“异或”解法就可以得到5和3了。 代码如下:1234567891011121314151617def singleNumber(self, nums: List[int]) -&gt; List[int]: ret = 0 # 所有数字异或的结果 a = 0 b = 0 for n in nums: ret ^= n # 找到第一位不是0的 h = 1 while(ret &amp; h == 0): h &lt;&lt;= 1 for n in nums: # 根据该位是否为0将其分为两组 if (h &amp; n == 0): a ^= n else: b ^= n return [a, b] 排序算法各类排序总览 比较交换类排序: 堆排序 快排 插排 冒泡排序 选择排序 希尔排序 桶思想类排序: 基数排序：根据键值的每位数字来分配桶； 计数排序：每个桶只存储单一键值； 桶排序：每个桶存储一定范围的数值； 不常用算法一览冒泡排序每一轮循环都会有一个最大的数慢慢移动到最后, 很像是冒出一个泡泡, 因而得名.算法步骤: 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 1234567891011void bubble_sort(int* arr, int arr_len)&#123; for(int i = 0; i &lt; arr_len; ++i)&#123; for(int j = i+1; j &lt; arr_len; ++j)&#123; if(arr[i] &gt; arr[j])&#123; auto temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; &#125; &#125;&#125; 选择排序首先在未排序序列中找到最小（大）元素，然后选择它存放到排序序列的起始位置。再从剩余未排序元素中继续寻找最小（大）元素，然后选择它放到已排序序列的末尾。重复第二步，直到所有元素均排序完毕。 12345678910111213void select_sort(int arr[], int arr_len)&#123; for(int i = 0; i &lt; arr_len; ++i)&#123; auto cur_min_index = i; for(int j = i + 1; j &lt; arr_len; ++j)&#123; if(arr[cur_min_index] &gt; arr[j])&#123; cur_min_index = j; &#125; &#125; auto temp = arr[i]; arr[i] = arr[cur_min_index]; arr[cur_min_index] = temp; &#125;&#125; 希尔排序希尔排序(Shell Sort)是插入排序的一种算法，是对直接插入排序的一个优化，也称缩小增量排序。希尔排序是非稳定排序算法。希尔排序因DL．Shell于1959年提出而得名。 简单插入排序很循规蹈矩，不管数组分布是怎么样的，依然一步一步的对元素进行比较，移动，插入，比如[5,4,3,2,1,0]这种倒序序列，数组末端的0要回到首位置很是费劲，比较和移动元素均需n-1次。 而希尔排序在数组中采用跳跃式分组的策略，通过某个增量将数组元素划分为若干组，然后分组进行插入排序，随后逐步缩小增量，继续按组进行插入排序操作，直至增量为1时排完就完毕了。 希尔排序通过这种策略使得整个数组在初始阶段达到从宏观上看基本有序，小的基本在前，大的基本在后。然后缩小增量，到增量为1时，其实多数情况下只需微调即可，不会涉及过多的数据移动, 此时排序完毕了. 计数排序让我们先来回顾一下经典的排序算法，无论是归并排序，冒泡排序还是快速排序等等，都是基于元素之间的比较来进行排序的。但是有一种特殊的排序算法叫计数排序，这种排序算法不是基于元素比较，而是利用数组下标来确定元素的正确位置。 有这样一道排序题：数组里有20个随机数，取值范围为从0到10，要求用最快的速度把这20个整数从小到大进行排序。请问怎么做? 在这个题目里，随即整数的取值范围是从0到10，那么这些整数的值肯定是在0到10这11个数里面。于是我们可以建立一个长度为11的数组，数组下标从0到10，元素初始值全为0，如下所示：先假设20个随机整数的值是：9, 3, 5, 4, 9, 1, 2, 7, 8，1，3, 6, 5, 3, 4, 0, 10, 9, 7, 9让我们先遍历这个无序的随机数组，每一个整数按照其值对号入座，对应数组下标的元素进行加1操作。比如第一个整数是9，那么数组下标为9的元素加1：第二个整数是3，那么数组下标为3的元素加1：继续遍历数列并修改数组……最终，数列遍历完毕时，数组的状态如下：数组中的每一个值，代表了数列中对应整数的出现次数。有了这个统计结果，排序就很简单了，直接遍历数组，输出数组元素的下标值，元素的值是几，就输出几次：0, 1, 1, 2, 3, 3, 3, 4, 4, 5, 5, 6, 7, 7, 8, 9, 9, 9, 9, 10显然，这个输出的数列已经是有序的了。 这就是计数排序的基本过程，它适用于一定范围的整数排序。在取值范围不是很大的情况下，它的性能在某些情况甚至快过那些O(nlogn)的排序，例如快速排序、归并排序。 桶排序桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点： 在额外空间充足的情况下，尽量增大桶的数量 使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中 同时，对于桶中元素的排序，选择何种比较排序算法对于性能的影响至关重要。 什么时候最快: 当输入的数据可以均匀的分配到每一个桶中。 什么时候最慢: 当输入的数据被分配到了同一个桶中 如何解决分布不平均的情况: 运用多层桶的思想, 比如游戏排行榜就是一个典型的桶排序适用场景, 针对这些划分之后还是有一些桶区间数量非常多，我们可以继续划分，比如，战力在1到1000之间的玩家比较多，我们就将这个区间继续划分为10个小区间，1到100，101到200，201到300…901到1000。如果划分之后，101到200元之间的还是太多，那就继续再划分 元素分布在桶中：然后，元素在每个桶中排序： 基数排序基数排序是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。 代码如下:12345678910111213141516171819202122232425262728def radix_sort(s): i = 0 # 记录当前正在排拿一位,最低位为1 max_num = max(s) # 最大值 j = len(str(max_num)) # 记录最大值的位数 while i &lt; j: bucket_list = [[] for _ in range(10)] # 初始化桶数组 for x in s: bucket_list[int(x / (10 ** i)) % 10].append(x) # 找到位置放入桶数组 print(bucket_list) s.clear() for x in bucket_list: # 放回原序列 for y in x: s.append(y) i += 1a = [334, 5, 67, 345, 7, 345345, 99, 4, 23, 78, 45, 1, 3453, 23424]radix_sort(a)print('最后的结果是:', a)'''[[], [1], [], [23, 3453], [334, 4, 23424], [5, 345, 345345, 45], [], [67, 7], [78], [99]][[1, 4, 5, 7], [], [23, 23424], [334], [345, 345345, 45], [3453], [67], [78], [], [99]][[1, 4, 5, 7, 23, 45, 67, 78, 99], [], [], [334, 345, 345345], [23424, 3453], [], [], [], [], []][[1, 4, 5, 7, 23, 45, 67, 78, 99, 334, 345], [], [], [23424, 3453], [], [345345], [], [], [], []][[1, 4, 5, 7, 23, 45, 67, 78, 99, 334, 345, 3453], [], [23424], [], [345345], [], [], [], [], []][[1, 4, 5, 7, 23, 45, 67, 78, 99, 334, 345, 3453, 23424], [], [], [345345], [], [], [], [], [], []]最后的结果是: [1, 4, 5, 7, 23, 45, 67, 78, 99, 334, 345, 3453, 23424, 345345]''' 实用排序算法要点总结 实用的基础排序算法有四种: 插入排序 : 在小数据量或者数据都较为有序的时候比起归并和快速排序有更佳的时间效率, 插入排序在这种情况下，只需要从头到尾扫描一遍，交换、移动少数元素即可；时间复杂度近乎 o(N)))。 所以插入排序经常可以当作是其他排序算法的子过程, 下面代码会有体现 快速排序 : 时间复杂度依赖数据打乱的程度 快排最差情形的时间复杂度是O(n2), 平均是O(nlogn) 就地快速排序使用的空间是O(1)的，也就是个常数级；而真正消耗空间的就是递归调用了，因为每次递归就要保持一些数据； 最优的情况下空间复杂度为：O(logn) ；每一次都平分数组的情况 最差的情况下空间复杂度为：O( n ) ；退化为冒泡排序的情况 选择基准的方式决定了两个分割后两个子序列的长度，进而对整个算法的效率产生决定性影响, 比如当如果一个有序递增序列, 每次选基准都选最后一个, 那肯定效率 很差了啊, 此时最差情形的时间复杂度是O(n2) 不稳定是因为等于pivot的num和pivot交换: 如果一个数num刚好跟pivot相等, 那partition完的时候, pivot要和partition index位置的数做交换, 如果这个数num刚好在partition index这个位置, 那这两个数就会发生交换, 然后肯定就不稳定了啊 举个例子：待排序数组: int a[] ={1, 2, 2, 3, 4, 5, 6};在快速排序的随机选择比较子(即pivot)阶段：若随机选择到了a[2]（即数组中的第二个2）为比较子，，而把大于等于比较子的数均放置在大数数组中，则a[1]（即数组中的第一个2）会到pivot的右边， 那么数组中的两个2非原序（这就是“不稳定”）。若随机选择到了a[1]为比较子，而把 小于等于 比较子的数均放置在小数数组中，则数组中的两个2顺序也非原序这就说明，quick sort是不稳定的。 归并排序 : 时间复杂度稳定但是占用2N的内存 归并的空间复杂度就是那个临时的数组和递归时压入栈的数据占用的空间：n + logn；所以空间复杂度为: O(n) 还有一种空间复杂度为O(1)的归并排序的自底向上的实现, 下文会讲 堆排序: 为什么在平均情况下快速排序比堆排序要优秀? 堆排序是渐进最优的比较排序算法，达到了O(nlgn)这一下界，而快排有一定的可能性会产生最坏划分，时间复杂度可能为O(n^2)，那为什么快排在实际使用中通常优于堆排序？ 虽然quick_sort会n^2（其实有稳定的nlgn的版本, 比如优化版的三路快排），但这毕竟很少出现。heap_sort大多数情况下比较次数都多于quick_sort，尽管大家都是nlogn。那就让倒霉蛋倒霉好了，大多数情况下快才是硬道理。 堆排比较的几乎都不是相邻元素，对cache极不友好，这才是很少被采用的原因。数学上的时间复杂度不代表实际运行时的情况.快排是分而治之，每次都在同一小段进行比较，最后越来约接近局部性。反观堆排，堆化过程中需要一直拿index的当前元素A和处于index*2 + 1 的左子元素B以及处于index*2 + 2 的右子元素C比较, 两个元素距离较远。(局部性原理是指CPU访问存储器时，无论是存取指令还是存取数据，所访问的存储单元都趋于聚集在一个较小的连续区域中。) 代码书写技巧: 归并和快排都是当left_index &gt;= right_index时, 停止递归 快排的partition过程分割index和遍历的初始index的选择: 普通快排: 12345678910# partition_index 在还没开始遍历之前时应该指向待遍历元素的最左边的那个元素的前一个位置# 在这里这种写法就是 `left_index`# 这才符合partition_index的定义:# partition_indexy指向小于pivot的那些元素的最后一个元素,# 即 less_than_pivots_last_elem_index# 因为还没找到比pivot小的元素之前, # partition_index是不应该指向任何待遍历的元素的partition_index = less_than_pivots_last_elem_index = left_indexi = left_index + 1 # 因为pivot_index取left_index了, 则我们从left_index+1开始遍历 三路快排: 123456789# lt_index 指向小于pivot的那些元素的最右边的一个元素,# lt_index 即 less_than_pivots_last_elem_index# 因为还没找到比pivot小的元素之前, # lt_index 是不应该指向任何待遍历的元素的, # gt_index 同理, gt_index指向大于pivot的那些元素的最左边的一个元素,lt_index = less_than_pivots_last_elem_index = left_indexgt_index = right_index + 1i = left_index + 1 # 因为pivot_index取left_index了, 则我们从left_index+1开始遍历 堆排序: 如果是对数组的[left_index, right_index]来排序, 且数组的首index为0的话, 则: 最后一个非叶子节点的index为left_index + (length/2 - 1) left_child_index = 2 * (pending_heapify_index-left_index) + 1 right_child_index = left_child_index + 1 是否原址: 原址: 插入排序、堆排序、快速排序 非原址: 归并排序 稳定性: 稳定: 插入排序、归并排序 不稳定: 堆排序、快速排序 内省排序: std的sort就是用的内省排序. 此算法首先从快速排序开始，当递归深度超过一定深度（深度为排序元素数量的对数值即logN, 快速排序在理想状态下，应当递归约 log n 次。因此，我们可以说，如果递归深度明显大于 log n，快速排序就掉进陷阱了。于是，我们可以将该阈值设置为 log n 的某一倍数，比如 2log n；一旦递归深度超过 2log n，就从快速排序切换到堆排序。）后转为堆排序。采用这个方法，内省排序既能在常规数据集上实现快速排序的高性能，又能在最坏情况下仍保持O(NlogN)的时间复杂度。不难归纳，这样的内省式排序，策略应该如下： 1. 在数据量足够大的情况使用快速排序； 2. 在快速排序掉入陷阱时，主动切换到堆排序； 3. 在快速排序和堆排序已经做到基本有序的情况下，或者数据量较小的情况下，主动切换到插入排序。 插入排序想象手上有几张牌， 现在你抽了一张牌， 然后需要从手上最右边的牌开始比较，然后插入到相应位置 通过不断的与前面已经排好序的元素比较并交换, 动画演示如下: python版本12345678910def insert_sort(arr, left_index, right_index): if not arr: return for i in xrange(left_index+1, right_index+1): # 从left_index+1开始, 也就是从第二个开始 for j in xrange(i, left_index, -1): if arr[j] &lt; arr[j-1]: # 注意保证j-1大于0 # 通过不断的与前面已经排好序的元素比较并交换, arr[j-1], arr[j] = arr[j], arr[j-1] else: break 与 cpp版本123456789101112void insert_sort(int* arr, int arr_len)&#123; for(int i = 1; i &lt; arr_len; ++i)&#123; for(int j = i; j &gt;= 0; --j)&#123; if(arr[j] &lt; arr[j-1])&#123; swap_elem(arr, j-1, j); &#125; else&#123; break; &#125; &#125; &#125;&#125; 插排优化因为基本的插入排序有太多交换操作了, 我们可以用直接赋值来优化 12345678910111213def insert_sort_optimized(arr, left_index, right_index): if not arr: return for i in xrange(left_index+1, right_index+1): # 从left_index+1开始, 也就是从第二个开始 temp_i_val = arr[i] j = i while j &gt;= left_index+1: if temp_i_val &lt; arr[j-1]: # 注意保证j-1大于0 arr[j] = arr[j-1] # 通过不断的与前面已经排好序的元素比较并直接赋值 j -= 1 else: break arr[j] = temp_i_val # 已经找到该插入的地方了, 直接赋值 归并排序归并排序用了分治的思想，有很多算法在结构上是递归的：为了解决一个给定的问题，算法要一次或多次地递归调用其自身来解决相关的子问题。这些算法通常采用分治策略（divide-and-conquier）：将原问题划分成n个规模较小而结构与原问题相似的子问题；递归地解决这些子问题，然后再合并其结果，就得到原问题的解。 分治模式在每一层递归上都有三个步骤： 分解（divide）：将原问题分解成一系列子问题； 解决（conquer）：递归地解各子问题。若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题的解。 归并的具体思路: 回到我们玩扑克牌的例子，假设桌上有两堆牌面朝上的牌，每堆都已排序，最小的牌在顶上。我们希望把这两堆牌合并成单一的排好序的输出堆，牌面朝下地放在桌上。我们的基本步骤包括在牌面朝上的两堆牌的顶上两张牌中选取较小的一张，将该牌从其堆中移开（该堆的顶上将显露一张新牌）并牌面朝下地将该牌放置到输出堆。重复这个步骤，直到一个输入堆为空，这时，我们只是拿起剩余的输入堆并牌面朝下地将该堆放置到输出堆。 动画演示: 归并排序的merge过程 python版本123456789101112131415161718192021222324252627def _merge(arr, left_index, mid_index, right_index): """ 归并的具体思路: 回到我们玩扑克牌的例子，假设桌上有两堆牌面朝上的牌，每堆都已**排序**，最小的牌在顶上。 我们希望把这两堆牌合并成单一的排好序的输出堆，牌面朝下地放在桌上。 我们的基本步骤包括在牌面朝上的两堆牌的顶上两张牌中选取较小的一张，将该牌从其堆中移开（该堆的顶上将 显露一张新牌）并牌面朝下地将该牌放置到输出堆。 重复这个步骤，直到一个输入堆为空，这时，我们只是拿起剩余的输入堆并牌面朝下地将该堆放置到输出堆。 """ # 注释掉下面这句是因为mid不一定等于(l+r)/2, 比如在`merge_sort_bottom_up()`就有可能 # mid_index = left_index + (right_index - left_index) / 2 temp_arr = [] i = left_index j = mid_index + 1 while i &lt;= mid_index and j &lt;= right_index: if arr[i] &lt;= arr[j]: temp_arr.append(arr[i]) i += 1 else: temp_arr.append(arr[j]) j += 1 # 这一步是模拟: 直到一个输入堆为空，这时，我们只是拿起剩余的输入堆并牌面朝下地将该堆放置到输出堆。 temp_arr.extend(arr[i:mid_index+1]) temp_arr.extend(arr[j:right_index+1]) # 注意右边界是小于等于right_index的 # 把归并好的数组数据放到原数组left_index到right_index的位置上去 for m in xrange(0, right_index-left_index+1): arr[left_index+m] = temp_arr[m] 与 cpp版本123456789101112131415161718192021222324252627282930void merge(int arr[], int left_i, int mid_i, int right_i)&#123; std::vector&lt;int&gt; temp_vec = std::vector&lt;int&gt;(); int i = left_i, j = mid_i + 1; while(i &lt;= mid_i &amp;&amp; j &lt;= right_i)&#123; if(arr[i] &lt; arr[j])&#123; temp_vec.push_back(arr[i]); ++i; &#125; else&#123; temp_vec.push_back(arr[j]); ++j; &#125; &#125; for(; i &lt;= mid_i; ++i) temp_vec.push_back(arr[i]); for(; j &lt;= right_i; ++j) temp_vec.push_back(arr[j]); for(int k = left_i; k &lt;= right_i; ++k) arr[k] = temp_vec[k-left_i];&#125;void merge_sort(int* arr, int left_index, int right_index)&#123; if(left_index &gt;= right_index)&#123; return; &#125; auto mid = left_index + (right_index - left_index) / 2; merge_sort(arr, left_index, mid); merge_sort(arr, mid+1, right_index); if (arr[mid] &gt; arr[mid+1]) merge(arr, left_index, mid, right_index);&#125; 归并自顶向下的实现123456789101112def merge_sort(arr, left_index, right_index): # 当 `merge_sort()` 递归到left_index等于right_index的时候, # 说明left_index, right_index已经相邻了, # 说明已经分解到底了, 左右都只剩下一个元素了, 所以此时应该return然后执行 `_merge()` 了 if not arr or left_index &gt;= right_index: return # 注意这里不能直接 `mid_index=(left_index+right_index)/2`, # 防止当left_index和right_index很大的时候他们之和溢出 mid_index = left_index + (right_index - left_index) / 2 merge_sort(arr, left_index, mid_index) merge_sort(arr, mid_index+1, right_index) _merge(arr, left_index, mid_index, right_index) 归并自顶向下的优化实现1234567891011121314151617181920212223242526def merge_sort_optimized(arr, left_index, right_index): # 当 `merge_sort()` 递归到left_index等于right_index的时候, # 说明left_index, right_index已经相邻了, # 说明已经分解到底了, 左右都只剩下一个元素了, 所以此时应该return然后执行 `_merge()` 了 if not arr or left_index &gt;= right_index: return+ # 优化1:+ # 数据量较小则使用插入排序+ if (right_index - left_index) &lt; 15:+ insert_sort_optimized(arr, left_index, right_index)+ return # 注意这里不能直接 `mid_index=(left_index+right_index)/2`, # 防止当left_index和right_index很大的时候他们之和溢出 mid_index = left_index + (right_index - left_index) / 2 merge_sort(arr, left_index, mid_index) merge_sort(arr, mid_index+1, right_index)+ # 优化2: + # 因为此时arr[mid_index]是左边的数组里最大的, 而arr[mid_index+1]是右边最小的,+ # 如果arr[mid_index] &lt;= arr[mid_index+1]则说明这一轮递归的arr的left到right已经是从小到大有序的了+ # 所以只在对于arr[mid_index] &gt; arr[mid_index+1]的情况,进行merge, + # 对于近乎有序的数组非常有效,但是对于一般情况,有一定的性能损失(因为多了这行代码判断大小)+ if arr[mid_index] &gt; arr[mid_index+1]: _merge(arr, left_index, mid_index, right_index) 归并自底向上的实现 12345678910111213141516171819202122def merge_sort_bottom_up(arr, left_index, right_index): if not arr or left_index &gt;= right_index: return arr_len = right_index - left_index + 1 size = 1 # 注意这里不是 `while size &lt;= arr_len/2`, # 否则比如arr_len=12, size为4的话, 只能把[0...7]和[8..11]的这两个区间的元素归并成有序, # size=8的话, 大于arr_len/2了, # 但只有size为8, 这样2倍size才能把arr全部归并, 所以应该`while size &lt; arr_len` while size &lt; arr_len: cur_left_index = left_index while cur_left_index &lt;= right_index-size: cur_mid_index = cur_left_index + size -1 possible_right_index = cur_left_index + 2*size -1 # possible_right_index有可能已经大于right_index了, 所以要min一下 cur_right_index = min(possible_right_index, right_index) # 归并从i位置开始的两倍size的一组数据 _merge(arr, cur_left_index, cur_mid_index, cur_right_index) cur_left_index += size * 2 # 每次归并完一组数据就i移动size的两倍 # print "size: %d" % size # print arr size *= 2 # size从1开始每次增加两倍 归并自底向上的优化实现12345678910111213141516171819202122232425262728293031323334353637383940def merge_sort_bottom_up_optimized(arr, left_index, right_index): if not arr or left_index &gt;= right_index: return+ # 优化1:+ # 先以size为16为一组数据来逐个对每组插入排序一遍+ size = 16+ cur_left_index = left_index+ while cur_left_index &lt; right_index:+ possible_right_index = cur_left_index + 2*size -1+ # possible_right_index有可能已经大于right_index了, 所以要min一下+ cur_right_index = min(possible_right_index, right_index)+ insert_sort(arr, cur_left_index, cur_right_index)+ cur_left_index += size # 右移到下一个size大小开头位置 arr_len = right_index - left_index + 1- size = 1 # 注意这里不是 `while size &lt;= arr_len/2`, # 比如arr_len=12, size为4的话, 只能把[0, 7]和[8, 11]的两个子数组归并成有序 # 那只有size为8, 这样2倍size才能把arr全部归并 # 但size=8的话, 大于arr_len/2了, 所以应该`while size &lt; arr_len` while size &lt; arr_len: cur_left_index = left_index while cur_left_index &lt;= right_index-size: cur_mid_index = cur_left_index + size -1 possible_right_index = cur_left_index + 2*size -1 # possible_right_index有可能已经大于right_index了, 所以要min一下 cur_right_index = min(possible_right_index, right_index) # 归并从i位置开始的两倍size的一组数据+ # 优化2: + # 因为此时arr[mid_index]左边的数组里最大的, 而arr[mid_index+1]是右边最小的,+ # 如果arr[mid_index] &lt;= arr[mid_index+1]则说明这一轮递归的arr的left到right已经是从小到大有序的了+ # 所以只在对于arr[mid_index] &gt; arr[mid_index+1]的情况,进行merge, + # 对于近乎有序的数组非常有效,但是对于一般情况,有一定的性能损失(因为多了这行代码判断大小)+ if arr[cur_mid_index] &gt; arr[cur_mid_index+1]: _merge(arr, cur_left_index, cur_mid_index, cur_right_index) cur_left_index += size * 2 # 每次归并完一组数据就i移动size的两倍 # print "size: %d" % size # print arr size *= 2 # size从1开始每次增加两倍 快速排序与归并排序一样， 快排也是用了分治的思想。 特别注意 : 快排的核心模块是Partition, 而Partition的复杂度为O(N). 你可以想象一个两副牌然后随意取出一张牌pivot，其他的所有牌都跟这张pivot牌比较， 大的放右边那一摞A，小的放左边B。接着再从左边这一摞B再随意取出一张牌pivot，其他的所有牌都跟这张pivot牌比较， 大的放右边那一摞，小的放左边，递归下去。A也重复上述步骤递归。 递归结束之后， 左边的都比右边的小， 而且是有序的。 动画演示: 快排效率很差的情况 对于分治算法，当每次划分时，算法若都能分成两个等长的子序列时，那么分治算法效率会达到最大。也就是说，基准的选择是很重要的。选择基准的方式决定了两个分割后两个子序列的长度，进而对整个算法的效率产生决定性影响所以当如果一个有序递增序列, 每次选基准都选最后一个, 那肯定效率很差了啊 普通快排注意初始index的位置:12345678910# partition_index 在还没开始遍历之前时应该指向待遍历元素的最左边的那个元素的前一个位置# 在这里这种写法就是 `left_index`# 这才符合partition_index的定义:# partition_index指向小于pivot的那些元素的最后一个元素,# 即 less_than_pivots_last_elem_index# 因为还没找到比pivot小的元素之前, # partition_index是不应该指向任何待遍历的元素的partition_index = less_than_pivots_last_elem_index = left_indexi = left_index + 1 # 因为pivot_index取left_index了, 则我们从left_index+1开始遍历 下面是原代码:python版本1234567891011121314151617181920212223242526272829303132def _partition(arr, left_index, right_index): # 选一个元素作为枢轴量, # 为了模拟上面这个动画演示, 这里我们选取最左边的元素 pivot_index = left_index pivot = arr[pivot_index] # partition_index 在还没开始遍历之前时应该指向待遍历元素的最左边的那个元素的前一个位置 # 在这里这种写法就是 `left_index` # 这才符合partition_index的定义: # partition_indexy指向小于pivot的那些元素的最后一个元素, # 即 less_than_pivots_last_elem_index # 因为还没找到比pivot小的元素之前, # partition_index是不应该指向任何待遍历的元素的 partition_index = less_than_pivots_last_elem_index = left_index i = left_index + 1 # 因为pivot_index取left_index了, 则我们从left_index+1开始遍历 while i &lt;= right_index: if arr[i] &lt; pivot: arr[i], arr[partition_index+1] = arr[partition_index+1], arr[i] partition_index += 1 i += 1 arr[pivot_index], arr[partition_index] = arr[partition_index], arr[pivot_index] return partition_indexdef quick_sort(arr, left_index, right_index): # 如果left等于right则说明已经partition到只有一个元素了, 可以直接return了 if not arr or left_index &gt;= right_index: return partition_index = _partition(arr, left_index, right_index) # 把partition_index左边的数据再递归快排一遍 quick_sort(arr, left_index, partition_index-1) quick_sort(arr, partition_index+1, right_index) 与 cpp版本1234567891011121314151617181920212223242526void swap_elem(int* arr, int index_a, int index_b)&#123; auto temp = arr[index_a]; arr[index_a] = arr[index_b]; arr[index_b] = temp;&#125;int partition(int arr[], int left_index, int right_index)&#123; int p_index = left_index; int left_end = p_index; for(int i = left_index+1; i &lt;= right_index; ++i)&#123; if(arr[i] &lt; arr[p_index])&#123; swap_elem(arr, i, left_end+1); left_end += 1; &#125; &#125; swap_elem(arr, left_end, p_index); return left_end;&#125;void quick_sort(int* arr, int left_index, int right_index)&#123; if(!arr || left_index &gt;= right_index) return; auto p_index = partition(arr, left_index, right_index); quick_sort(arr, left_index, p_index-1); quick_sort(arr, p_index+1, right_index);&#125; 普通快排的优化通过快排效率很差的情况, 我们知道快排在面对已经比较有序数组的时候效率如果固定选择某个位置的pivot则性能较差, 所以我们加上两种优化方式: 随机选pivot 小数组用插排 12345678910111213141516171819202122232425262728293031323334353637383940414243444546+ import randomdef _partition_optimized(arr, left_index, right_index): # 选一个元素作为枢轴量, # 为了模拟上面这个动画演示, 这里我们选取最左边的元素 pivot_index = left_index+ # 优化1:+ # 随机选一个元素和最左边的交换,+ # 配合下方的`pivot = arr[left_index]`就达到了随机选一个元素当pivot的效果+ rand_index = random.randint(left_index, right_index)+ arr[pivot_index], arr[rand_index] = arr[rand_index], arr[pivot_index] pivot = arr[pivot_index] # partition_index 在还没开始遍历之前时应该指向待遍历元素的最左边的那个元素的前一个位置 # 在这里这种写法就是 `left_index` # 这才符合partition_index的定义: # partition_indexy指向小于pivot的那些元素的最后一个元素, # 即 less_than_pivots_last_elem_index # 因为还没找到比pivot小的元素之前, # partition_index是不应该指向任何待遍历的元素的 partition_index = less_than_pivots_last_elem_index = left_index i = left_index + 1 # 因为pivot_index取left_index了, 则我们从left_index+1开始遍历 while i &lt;= right_index: if arr[i] &lt; pivot: arr[i], arr[partition_index+1] = arr[partition_index+1], arr[i] partition_index += 1 i += 1 arr[pivot_index], arr[partition_index] = arr[partition_index], arr[pivot_index] return partition_indexdef quick_sort_optimized(arr, left_index, right_index): # 如果left等于right则说明已经partition到只有一个元素了, 可以直接return了 if not arr or left_index &gt;= right_index: return+ # 优化2:+ # 小数组用插排+ if (right_index - left_index) &lt;= 15:+ insert_sort(arr, left_index, right_index)+ return partition_index = _partition(arr, left_index, right_index) # 把partition_index左边的数据再递归快排一遍 quick_sort(arr, left_index, partition_index-1) quick_sort(arr, partition_index+1, right_index) 解决普通快排有大量相同元素时的性能问题对于分治算法，当每次划分时，算法若都能分成两个等长的子序列时，那么分治算法效率会达到最大.当数组中有大量相同元素的时候, 不管怎么选pivot都很容易变成下面这种情况导致分成子序列的不平衡, 这将极大的影响时间复杂度, 最差的情况会退化成O(N2) 双路快排-初步解决有大量相同元素的性能问题所以产生了双路快排的方式, 他使用两个索引值（i、j）用来遍历我们的序列，将小于等于v的元素放在索引i所指向位置的左边，而将大于等于v的元素放在索引j所指向位置的右边, 通过下图我们可以看到当等于v的情况也会发生交换, 这就基本可以保证等于v的元素也可以较为均匀的放到左右两边 待改进的地方: 还是把等于v的元素加入到了待处理的数据中, 之后又去重复计算这些等于v的元素了, 为了排除这些已经等于v的元素, 所以产生了三路快排 三路快排-完全解决有大量相同元素的性能问题这是最经典的解决有大量重复元素的问题的快排方案, 被大多数系统所使用. 注意初始index的位置:12345678910pivot = arr[pivot_index]# lt_index 指向小于pivot的那些元素的最右边的一个元素,# lt_index 即 less_than_pivots_last_elem_index# 因为还没找到比pivot小的元素之前, # lt_index 是不应该指向任何待遍历的元素的, # gt_index 同理, gt_index指向大于pivot的那些元素的最左边的一个元素,lt_index = less_than_pivots_last_elem_index = left_indexgt_index = right_index + 1i = left_index + 1 # 因为pivot_index取left_index了, 则我们从left_index+1开始遍历 接下来是完整代码:cpp版本12345678910111213141516171819202122232425262728293031323334353637int* partition_3_ways(int arr[], int left_index, int right_index)&#123; int p_index = left_index; auto left_end = left_index; auto right_start = right_index + 1; for(int i=left_index + 1; i &lt; right_start;)&#123; if(arr[i] &lt; arr[p_index])&#123; swap_elem(arr, i, left_end + 1); left_end += 1; ++i; &#125; else if(arr[i] &gt; arr[p_index])&#123; swap_elem(arr, i, right_start - 1); right_start -= 1; // 注意!! 这个情况是不 `++i` 的 ! &#125; else&#123; ++i; &#125; &#125; swap_elem(arr, left_end, p_index); int* ret_arr = new int[2]; ret_arr[0] = left_end; ret_arr[1] = right_start; return ret_arr;&#125;void quick_sort_3_ways(int* arr, int left_index, int right_index)&#123; if(!arr || left_index &gt;= right_index) return; auto ret_arr = partition_3_ways(arr, left_index, right_index); auto left_end = ret_arr[0]; auto right_start = ret_arr[1]; delete[] ret_arr; quick_sort_3_ways(arr, left_index, left_end); quick_sort_3_ways(arr, right_start, right_index);&#125; 以及: python版本12345678910111213141516171819202122232425262728293031323334353637383940def quick_sort_3_ways(arr, left_index, right_index): # 如果left等于right则说明已经partition到只有一个元素了, 可以直接return了 if not arr or left_index &gt;= right_index: return if (right_index - left_index) &lt;= 15: insert_sort(arr, left_index, right_index) return # 选一个元素作为枢轴量, # 为了模拟上面这个动画演示, 这里我们选取最左边的元素 pivot_index = left_index # 随机选一个元素和最左边的交换, # 配合下方的`pivot = arr[left_index]`就达到了随机选一个元素当pivot的效果 rand_index = random.randint(left_index, right_index) arr[pivot_index], arr[rand_index] = arr[rand_index], arr[pivot_index] pivot = arr[pivot_index] # lt_index 指向小于pivot的那些元素的最右边的一个元素, # lt_index 即 less_than_pivots_last_elem_index # 因为还没找到比pivot小的元素之前, # lt_index 是不应该指向任何待遍历的元素的, # gt_index 同理, gt_index指向大于pivot的那些元素的最左边的一个元素, lt_index = less_than_pivots_last_elem_index = left_index gt_index = right_index + 1 i = left_index + 1 # 因为pivot_index取left_index了, 则我们从left_index+1开始遍历 while i &lt; gt_index: if arr[i] &lt; pivot: arr[i], arr[lt_index+1] = arr[lt_index+1], arr[i] lt_index += 1 i += 1 elif arr[i] &gt; pivot: # 注意!! 这个情况是不 `i += 1` 的 ! arr[i], arr[gt_index-1] = arr[gt_index-1], arr[i] gt_index -= 1 else: i += 1 arr[pivot_index], arr[lt_index] = arr[lt_index], arr[pivot_index] quick_sort_3_ways(arr, left_index, lt_index) quick_sort_3_ways(arr, gt_index, right_index) 堆排序最大堆的堆排序之后的数组是升序, 最小堆反之.堆排序 HeapSort 由 以下两部分组成 : 堆化 MaxHeapify 建堆 BuildMaxHeap TopK问题求一堆数组的最大的k个数 如果是求最大的k个数则用最小堆, 反之则用最大堆 算法的复杂度分析:由于使用了一个大小为 k 的堆，空间复杂度为 O(k)入堆和出堆操作的时间复杂度均为 O(logk)，每个元素都需要进行一次入堆操作，故算法的时间复杂度为 O(nlogk) 堆排序的复杂度 时间复杂度 : MaxHeapify : O(logN). BuildMaxHeap : O(N).看起来像是O(NlogN), 其实是O(N), 因为不同结点运行 MaxHeapify 的 时间和该结点的树高相关, 而大部分结点的高度都很小, &lt;&lt;算法导论&gt;&gt;中有相关证明 HeapSort : O(NlogN). 初始化堆 BuildMaxHeap 的时间复杂度为O(N); 之后因为每次交换结点然后从堆中去掉最后一个结点后都要重建堆 BuildMaxHeap (上述 HeapSort 函数代码中的倒数第三行 MaxHeapify(arr, 0, --length) 其实就是个重建堆的过程) , 重建堆 BuildMaxHeap 的时间复杂度为O(N), 而 length - 1 次调用了 MaxHeapify, MaxHeapify 的时间复杂度为O(lgN). 所以为 O(N + NlogN), 即为O(Nlogn) 空间复杂度 : O(1), 因为没有用辅助内存. 堆化注意: 以下演示图中的index是从1开始的, 方便我们看动图理解堆化过程, 我们下方代码的数组的index是从0开始的 注意 :在调用MaxHeapify的时候, 我们假定索引为index的元素的左子树和右子树都是最大堆, 不然你如果注意看的话, 你会发现上图中index为10的那个元素其实是没有计算到的, 因为我们假定以index=5为根节点的二叉树都是最大堆了, 所以无需计算他.那为何要作如此假设呢?因为要跟建堆 BuildMaxHeap 配合来完成堆排序, 而建堆 BuildMaxHeap是从下至上的. 动画演示如下, 比如要对17这个元素为父元素的所有子元素进行堆化: 如果是对数组的[left_index, right_index]来排序, 且数组的首index为0的话, 则: 最后一个非叶子节点的index为left_index + (length/2 - 1) left_child_index = 2 * (pending_heapify_index-left_index) + 1 right_child_index = left_child_index + 1这两个index的取得方式在下方代码有体现. 堆化递归写法递归写法更容易理解一些:12345678910111213141516171819# 递归版, 对 pending_heapify_index 元素执行堆化def _max_heapify_recursive(arr, pending_heapify_index, left_index, right_index): if pending_heapify_index &gt;= right_index: # 当满足此条件, 应该结束`_max_heapify_recursive`递归了 return left_child_index = 2 * (pending_heapify_index-left_index) + 1 right_child_index = left_child_index + 1 # 选出 pending_heapify_index 的左右孩子中最大的元素, # 并与 pending_heapify_index 元素交换 cur_max_index = pending_heapify_index if left_child_index &lt;= right_index and arr[cur_max_index] &lt; arr[left_child_index]: cur_max_index = left_child_index if right_child_index &lt;= right_index and arr[cur_max_index] &lt; arr[right_child_index]: cur_max_index = right_child_index # 若当前已经是最大元素了, 则停止递归, 如果不是则执行交换与继续递归 if cur_max_index != pending_heapify_index: arr[pending_heapify_index], arr[cur_max_index] = arr[cur_max_index], arr[pending_heapify_index] _max_heapify_recursive(arr, cur_max_index, left_index, right_index) # 继续 堆化 cur_max_index 的子元素 堆化迭代写法1234567891011121314151617181920# 迭代版, 对 pending_heapify_index 元素执行堆化def _max_heapify_iterative(arr, pending_heapify_index, left_index, right_index): left_child_index = 2 * (pending_heapify_index-left_index) + 1 while left_child_index &lt;= right_index: right_child_index = left_child_index + 1 # 选出 pending_heapify_index 的左右孩子中最大的元素, # 并与 pending_heapify_index 元素交换 cur_max_index = pending_heapify_index if left_child_index &lt;= right_index and arr[cur_max_index] &lt; arr[left_child_index]: cur_max_index = left_child_index if right_child_index &lt;= right_index and arr[cur_max_index] &lt; arr[right_child_index]: cur_max_index = right_child_index # 若当前已经是最大元素了, 则直接break, 如果不是则执行交换与继续新一轮的堆化循环 if cur_max_index != pending_heapify_index: arr[pending_heapify_index], arr[cur_max_index] = arr[cur_max_index], arr[pending_heapify_index] pending_heapify_index = cur_max_index left_child_index = 2 * (pending_heapify_index-left_index) + 1 else: break 迭代写法的话也可以使用赋值的方式取代不断的swap,该优化思想和我们之前对插入排序进行优化的思路是一致的, 此处这个优化代码就略了 建堆如果是对数组的[left_index, right_index]来排序, 且数组的首index为0的话, 则最后一个非叶子节点的index为left_index + (length/2 - 1), 我们对每一个不是叶结点的元素自底向上调用一次 Max_Heapify 就可以把一个大小为 length 的数组转换为最大堆. 注意: 为了方便我们看动图理解堆化过程, 以下动画演示图中的index是从1开始的, 而我们下方代码的数组的index是从0开始的 12345678910def _build_max_heap(arr, left_index, right_index): # 建堆, 从最后一个非叶结点开始, 自底向上堆化就建好了一个最大堆 root_index = left_index arr_len = right_index - left_index + 1 last_none_leaf_index = root_index + (arr_len/2 - 1) i = last_none_leaf_index while i &gt;= root_index: _max_heapify_recursive(arr, i, left_index, right_index) i -= 1 堆排序原址排序的具体实现 堆排序分两步: 建堆 重复以下两个操作: 把数组中的第一个元素(即根节点)也就是当前堆的最大元素逐个和数组后面的元素交换 对根节点做一次堆化操作 python版本12345678910111213def heap_sort(arr, left_index , right_index): if not arr or left_index &gt;= right_index or right_index &lt;= 0: return _build_max_heap(arr, left_index, right_index) # 把数组中的第一个元素(即根节点)也就是当前堆的最大元素逐个和数组后面的元素交换 # 交换后根节点已经违背最大堆性质了, 但其他的元素还是符合最大堆性质的 # 所以然后要对根节点做一次堆化操作 cur_right_index = right_index root_index = left_index while cur_right_index &gt;= root_index: arr[root_index], arr[cur_right_index] = arr[cur_right_index], arr[root_index] cur_right_index -= 1 _max_heapify_recursive(arr, root_index, left_index, cur_right_index) 与 cpp版本12345678910111213141516171819202122232425262728293031323334353637383940void heapify(int arr[], int p_i, int left_i, int right_i)&#123; if(p_i &gt; right_i) return; auto left_child_i = (p_i-left_i)*2 + 1; auto right_child_i = left_child_i + 1; auto max_child_i = p_i; if(left_child_i &lt;= right_i &amp;&amp; arr[left_child_i] &gt; arr[max_child_i]) max_child_i = left_child_i; if(right_child_i &lt;= right_i &amp;&amp; arr[right_child_i] &gt; arr[max_child_i]) max_child_i = right_child_i; if(max_child_i != p_i)&#123; swap_elem(arr, max_child_i, p_i); heapify(arr, max_child_i, left_i, right_i); &#125;&#125;void build_max_heap(int arr[], int left_i, int right_i)&#123; auto arr_len = right_i - left_i + 1; auto last_none_leaf_i = left_i + (arr_len / 2) - 1; while(last_none_leaf_i &gt;= left_i)&#123; heapify(arr, last_none_leaf_i, left_i, right_i); --last_none_leaf_i; &#125;&#125;void heap_sort(int* arr, int left_index, int right_index)&#123; if(!arr) return; build_max_heap(arr, left_index, right_index); auto cur_right_i = right_index; while (cur_right_i &gt;= left_index)&#123; swap_elem(arr, left_index, cur_right_i); --cur_right_i; heapify(arr, left_index, left_index, cur_right_i); &#125;&#125; 递归解题思路实际上，递归有两个显著的特征,终止条件和自身调用: 自身调用：原问题可以分解为子问题，子问题和原问题的求解方法是一致的，即都是调用自身的同一个函数。 终止条件：递归必须有一个终止的条件，即不能无限循环地调用本身。 递归调用可理解为入栈操作，而返回则为出栈操作。写递归算法的关键是要明确函数的「定义」是什么，然后相信这个定义，利用这个定义推导最终结果，绝不要试图跳入递归。我们千万不要跳进递归的细节里，你的脑袋才能压几个栈呀。 解决递归问题一般就三步曲，这个递归解题三板斧理解起来有点抽象，我们拿阶乘递归例子来喵喵吧~三部曲分别是： 定义函数功能 定义函数功能，就是说，你这个函数是干嘛的，做什么事情，换句话说，你要知道递归原问题是什么呀？比如你需要解决阶乘问题，定义的函数功能就是n的阶乘，如下： 1234//n的阶乘（n为大于0的自然数）int factorial (int n)&#123;&#125; 寻找递归终止条件 递归的一个典型特征就是必须有一个终止的条件，即不能无限循环地调用本身。所以，用递归思路去解决问题的时候，就需要寻找递归终止条件是什么。比如阶乘问题，当n=1的时候，不用再往下递归了，可以跳出循环啦，n=1就可以作为递归的终止条件，如下： 123456//n的阶乘（n为大于0的自然数）int factorial (int n)&#123; if(n==1)&#123; return 1; &#125;&#125; 找出递归结构, 或者递推函数的等价关系式 递归的「本义」，就是原问题可以拆为同类且更容易解决的子问题，即「原问题和子问题都可以用同一个函数关系表示。递推函数的等价关系式，这个步骤就等价于寻找原问题与子问题的关系，如何用一个公式把这个函数表达清楚」。阶乘的公式就可以表示为 f(n) = n * f(n-1), 因此，阶乘的递归程序代码就可以写成这样，如下： 123456int factorial (int n)&#123; if(n==1)&#123; return 1; &#125; return n * factorial(n-1);&#125; 「注意啦」，不是所有递推函数的等价关系都像阶乘这么简单，一下子就能推导出来。需要我们多接触，多积累，多思考，多练习递归题目滴~ 递归与二叉树递归，是使用计算机解决问题的一种重要的思考方式。而二叉树由于其天然的递归结构，使得基于二叉树的算法，均拥有着递归性质。使用二叉树，是研究学习递归算法的最佳入门方式。在这一章里，我们就来看一看二叉树中的递归算法。 二叉树递归技巧 如果采用前序遍历的递归形式解题, 则其实是从二叉树的顶部到底部来操作的, 脑海中得有这么一个想象, 从上到下访问每个结点之前做事 如果采用后序遍历的递归形式解题, 则其实是从二叉树的底部到顶部来操作的, 从下到上访问每个结点之后做事 一般很少用中序形式解题 lc236-LCA最近公共祖先问题lc236, 给出一棵二叉树的根节点，现在有这个二叉树的部分节点，要求这些节点最近的公共祖先 参考此处 参考 这道题目刷过的同学未必真正了解这里面回溯的过程，以及结果是如何一层一层传上去的。那么我给大家归纳如下三点： 求最小公共祖先，需要从底向上遍历，那么二叉树，只能通过后序遍历（即：回溯）实现从低向上的遍历方式。 在回溯的过程中，必然要遍历整颗二叉树，即使已经找到结果了，依然要把其他节点遍历完，因为要使用递归函数的返回值（也就是代码中的left和right）做逻辑判断。 要理解如果返回值left为空，right不为空为什么要返回right，为什么可以用返回right传给上一层结果。 可以说这里每一步，都是有难度的，都需要对二叉树，递归和回溯有一定的理解。 若 root 是 p,q 的 最近公共祖先 ，则只可能为以下情况之一： p 和 q 在 root 的子树中，且分列 root 的 异侧（即分别在左、右子树中）； p=root ，且 q 在 root 的左或右子树中； q=root ，且 p 在 root 的左或右子树中； 考虑通过递归对二叉树进行后序遍历，当遇到节点 p 或 q 时返回。从底至顶回溯，当节点 p,q 在节点 root 的异侧时，节点 root 即为最近公共祖先，则向上返回 root 。 递归解析： 终止条件： 当越过叶节点，则直接返回 null ； 当 root 等于 p,q ，则直接返回 root ； 递推工作： 开启递归左子节点，返回值记为 left ； 开启递归右子节点，返回值记为 right ； 返回值： 根据 left 和 right ，可展开为四种情况； 1. 当 left 和 right 同时为空 ：说明 root 的左 / 右子树中都不包含 p,q ，返回 null ； 2. 当 left 和 right 同时不为空 ：说明 p,q 分列在 当前 root 的 异侧 （分别在 左 / 右子树），因此 当前的root 为p/g最近公共祖先，返回 root ； 3. 当 left 为空 ，right 不为空 ：p,q 都不在 root 的左子树中，直接返回 right ,具体可分为两种情况： p,q 其中一个在 root 的 右子树 中，此时 right 指向 p（假设为 p ） p,q 两节点都在 root 的 右子树 中，此时的 right 指向 最近公共祖先节点 4. 当 left 不为空 ， right 为空 ：与情况 3. 同理； 代码如下:1234567891011121314151617181920212223242526class Solution_LCA(object): def lowestCommonAncestor(self, root, p, q): """ :type root: TreeNode :type p: TreeNode :type q: TreeNode :rtype: TreeNode """ if root == p or root == q: # 找到p或q了, 则返回p或q return root # 没找到p或q, 而且已经找到底, 越过叶子节点了, 则返回None if root is None: return None # 到 左子树 去找 left_child_find_res = self.lowestCommonAncestor(root.left, p, q) # 到 右子树 去找 right_child_find_res = self.lowestCommonAncestor(root.right, p, q) if not left_child_find_res: # 当 left 为空 ，right 不为空 ：p,q 都不在 root 的左子树中，直接返回 right return right_child_find_res if not right_child_find_res: return left_child_find_res # 当 left 和 right 同时不为空 ： # 说明 p,q 分列在 当前 root 的 异侧 （分别在 左 / 右子树）， # 因此 当前的root 为p/g最近公共祖先，返回 root ； return root lc106-后序中序求原二叉树 leetcode106题后序中序求原二叉树 参考: https://leetcode-cn.com/problems/construct-binary-tree-from-inorder-and-postorder-traversal/solution/ 首先来看题目给出的两个已知条件 中序遍历序列 和 后序遍历序列 根据这两种遍历的特性我们可以得出三个结论 在后序遍历序列中,最后一个元素为树的根节点 在中序遍历序列中,根节点的左边为左子树(设其长度为len_left), 根节点的右边为右子树 当前后序遍历序列中[postorder_left_index...len_left-1]为左子树的结点, 其他的除最后一个结点外都是右子树的结点 则代码如下:12345678910111213141516171819202122232425262728293031323334353637383940class Solution_build_bt(object): def buildTree(self, inorder, postorder): """ :type inorder: List[int] :type postorder: List[int] :rtype: TreeNode """ if not inorder or not postorder: return None def _proc_order_arr( inorder_left_index, inorder_right_index, postorder_left_index, postorder_right_index): if inorder_left_index &gt; inorder_right_index or \ postorder_left_index &gt; postorder_right_index: return None # 在后序遍历序列中,最后一个元素为树的根节点 root_val = postorder[postorder_right_index] root_inorder_index = inorder.index(root_val) _len_left_child = root_inorder_index-inorder_left_index root_node = TreeNode(root_val) # 在中序遍历序列中,根节点的左边为左子树(设其长度为len_left), 根节点的右边为右子树 # 当前后序遍历序列中`[postorder_left_index...len_left-1]`为左子树的结点, # 其他的除最后一个结点外都是右子树的结点 root_node.left = _proc_order_arr( inorder_left_index, root_inorder_index-1, postorder_left_index, postorder_left_index + (_len_left_child-1) ) root_node.right = _proc_order_arr( root_inorder_index+1, inorder_right_index, postorder_left_index+(_len_left_child), postorder_right_index-1 ) return root_node return _proc_order_arr(0, len(inorder)-1, 0, len(postorder)-1) lc112-path-sumleetcode112题 技巧: 首先要明确此递归函数的定义: 查看root是否为叶子结点并且root的val是否等于sum_num, 然后才开始写代码 123456789101112# 首先要明确此递归函数的定义: 查看root是否为叶子结点并且root的val是否等于sum_num,# 然后才开始写代码def has_path_sum(root, sum_num): if not root: return False # 判断是否为叶子 if not root.left and not root.left and root.val == sum_num: return True # 通过递归, 继续查看左右孩子节点是否有 sum_num-root.val if has_path_sum(root.left, sum_num-root.val): return True return has_path_sum(root.right, sum_num-root.val) lc257-binary-tree-pathslc257 12345678910111213141516def binary_tree_paths(root): result_arr = [] if not root: return result_arr if not root.left and not root.right: result_arr.append(str(root.val)) return result_arr # 获取左孩子的路径 left_bt_path_arr = binary_tree_paths(root.left) for _path in left_bt_path_arr: result_arr.append(str(root.val) + "-&gt;" + _path) # 获取右孩子的路径 right_bt_path_arr = binary_tree_paths(root.right) for _path in right_bt_path_arr: result_arr.append(str(root.val) + "-&gt;" + _path) return result_arr lc437-path-sum-3leetcode437题给出一颗二叉树以及一个数字sum, 判断在这棵二叉树上存在多少条路径, 其路径上的所有节点和为sum. 其中路径不一定要起始于根节点, 终止于叶子节点 路径可以从任意节点开始, 但是只能是向下走的 1234567891011121314151617181920212223def path_sum_3(root, sum_num): if not root: return 0 path_cnt = 0 # 先求包括node本身的情况, 此时这轮递归所说的node是代码中的root # 这种情况可以用类似于 path_sum问题 的 `has_path_sum`来实现 path_cnt += _get_path_sum_include_node(root, sum_num) # 再求不包括node本身的情况, # 直接计算左右孩子往下的路径中和为sum_num(注意不是sum_num-root.val)的路径数量 path_cnt += path_sum_3(root.left, sum_num) path_cnt += path_sum_3(root.right, sum_num) return path_cntdef _get_path_sum_include_node(node, sum_num): if not node: return 0 _path_cnt = 0 if node.val == sum_num: # node本身值等于sum_num也算一条路径 _path_cnt += 1 _path_cnt += _get_path_sum_include_node(node.left, sum_num-node.val) _path_cnt += _get_path_sum_include_node(node.right, sum_num-node.val) return _path_cnt 进阶-求path-sum-3的所有路径leetcode437题改一下, 改成:给出一颗二叉树以及一个数字sum, 请给出在这棵二叉树上的所有路径, 其路径上的所有节点和为sum. 其中路径不一定要起始于根节点, 终止于叶子节点 路径可以从任意节点开始, 但是只能是向下走的 根据本文lc437-path-sum-3的思路我们可以得到代码, 注意查看下方代码中的注释.1234567891011121314151617181920212223242526272829303132class Solution_sum_paths(object): def sum_paths(self, root, sum): if not root: return [] path_arr = [] # 先求包括node本身的情况, 此时这轮递归所说的node是代码中的root # 再求不包括node本身的情况, 左右孩子的情况, # 这样也就达到了把每个结点都当做是root然后向下寻找路径的目的 path_arr.extend(self._get_sum_paths(root, sum)) path_arr.extend(self.sum_paths(root.left, sum)) path_arr.extend(self.sum_paths(root.right, sum)) return path_arr def _get_sum_paths(self, cur_root, sum_num): if not cur_root: return [] path_str_arr = [] # if sum_num == 0: # pass # 不能这么写, 这么写的话, 拿不到之前的那个 cur_root 了 if sum_num - cur_root.val == 0: # 此时就已经找到了一个解 path_str_arr.append(str(cur_root.val)) return path_str_arr left_path_str_arr = self._get_sum_paths(cur_root.left, sum_num-cur_root.val) for _cur_path_str in left_path_str_arr: path_str_arr.append(str(cur_root.val) + "-&gt;" + _cur_path_str) right_path_str_arr = self._get_sum_paths(cur_root.right, sum_num-cur_root.val) for _cur_path_str in right_path_str_arr: path_str_arr.append(str(cur_root.val) + "-&gt;" + _cur_path_str) return path_str_arr lc114-二叉树展开为链表lc114给定一个二叉树，原地将它展开为一个单链表。例如，给定二叉树12345 1 / \ 2 5 / \ \3 4 6 将其展开为：12345678910111 \ 2 \ 3 \ 4 \ 5 \ 6 我们尝试给出这个函数的定义：给flatten函数输入一个节点root，那么以root为根的二叉树就会被拉平为一条链表。我们再梳理一下，如何按题目要求把一棵树拉平成一条链表？很简单，以下流程： 将root的左子树和右子树拉平。 将root的右子树连接到左子树下方，然后将整个左子树作为右子树。 你看，这就是递归的魅力，你说flatten函数是怎么把左右子树拉平的？不容易说清楚，但是只要知道flatten的定义如此，相信这个定义，让root做它该做的事情，然后flatten函数就会按照定义工作。另外注意递归框架是后序遍历，因为我们要从底到顶的来做拉平/连接操作。1234567891011121314151617181920212223class Solution_lc114(object): # [lc114](https://leetcode-cn.com/problems/flatten-binary-tree-to-linked-list/) def flatten(self, root): """ :type root: TreeNode :rtype: None Do not return anything, modify root in-place instead. """ if not root: return self.flatten(root.left) self.flatten(root.right) temp_left = root.left temp_right = root.right # 将左子树作为右子树 root.left = None root.right = temp_left root_r = root # 将左子树作为右子树 while (root_r.right): root_r = root_r.right root_r.right = temp_right 递归与回溯参考: https://leetcode-cn.com/problems/permutations/solution/hui-su-suan-fa-python-dai-ma-java-dai-ma-by-liweiw/ 回溯法 采用试错的思想，它尝试分步的去解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答的时候，它将取消上一步甚至是上几步的计算，再通过其它的可能的分步解答再次尝试寻找问题的答案。回溯法通常用最简单的递归方法来实现，在反复重复上述的步骤后可能出现两种情况： 找到一个可能存在的正确的答案； 在尝试了所有可能的分步方法后宣告该问题没有答案。 回溯法是解决很多算法问题的常见思想，甚至可以说是传统人工智能的基础方法。其本质依然是使用递归的方法在树形空间中寻找解。在这一章，我们来具体看一下将递归这种技术使用在非二叉树的结构中，从而认识回溯这一基础算法思想, 其实上一节的二叉树与递归也是回溯的思想, 不过我们通常把回溯这个名词用在表示递归查找解的问题上 比如下面这个树形问题电话号码字母组合, 如果n是一个固定的数比如为8, 其实我们可以使用8重循环来解决, 但是n是不固定了, 所以我们只能使用回溯法来解决, 回溯法是暴力解法的一个主要手段. 动态规划其实可以算是回溯法的基础上一种改进, 同时要发现一个递归结构, 以及其他的特点就可以用回溯法, 其实回溯法也可以剪枝来优化, 不用到达所有的叶子结点从而提升我们回溯法的运行效率. 回溯算法框架参考 废话不多说，直接上回溯算法框架。解决一个回溯问题，实际上就是一个决策树的遍历过程。你只需要思考 3 个问题： 路径: 也就是已经做出的选择。 选择列表: 供选择的列表 结束条件: 也就是到达决策树底层，无法再做选择的条件。 如果你不理解这三个词语的解释，没关系，我们后面会用「全排列」和「N 皇后问题」这两个经典的回溯算法问题来帮你理解这些词语是什么意思，现在你先留着印象。 代码方面，回溯算法的框架：12345678910result = []def backtrack(供选择的列表, 选择的路径中间状态): if 满足结束条件: result.add(选择的路径中间状态) return for 选择 in 供选择的列表: 做选择 backtrack(选择的路径中间状态, 供选择的列表) 撤销选择 其核心就是 for 循环里面的递归，在递归调用之前「做选择」，在递归调用之后「撤销选择」，特别简单。写 backtrack 函数时，需要维护走过的「路径」和当前可以做的「选择列表」，当触发「结束条件」时，将「路径」记入结果集。 排列问题合集排列问题代码模板和本文的lc46-经典全排列基本一致. lc46-经典全排列leetcode46题:给定一个整型数组, 其中的元素各不相同, 求返回这些元素的所有排列.如对于 [1, 2, 3], 则返回 [ [1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2], [3, 2, 1] ] 设计状态变量: 参考 首先这棵树除了根结点和叶子结点以外，每一个结点做的事情其实是一样的，即：在已经选择了一些数的前提下，在剩下的还没有选择的数中，依次选择一个数，这显然是一个 递归 结构； 递归的终止条件是： 一个排列中的数字已经选够了 ，因此我们需要一个变量来表示当前程序递归到第几层，我们把这个变量叫做 cnt ，每次往middle_state_container里添加元素cnt就加1, 当cnt等于全排列长度则递归终止. 当然也可以不用cnt, 每次直接if len(middle_state_container) == len(pending_proc_num_arr)也是可以的, 只是这样性能不高 布尔数组 used，初始化的时候都为 false 表示这些数还没有被选择，当我们选定一个数的时候，就将这个数组的相应位置设置为 true ，这样在考虑下一个位置的时候，就能够以 O(1)O(1) 的时间复杂度判断这个数是否被选择过，这是一种「以空间换时间」的思想。这些变量称为「状态变量」，它们表示了在求解一个问题的时候所处的阶段。需要根据问题的场景设计合适的状态变量。 注意查看下方代码中的 _generate_permutation, 排列问题基本都是这种代码写法模板. 123456789101112131415161718192021222324252627282930313233343536373839404142class Solution_lc46(object): # [lc46](https://leetcode-cn.com/problems/permutations/) def __init__(self): self._used = [] def permute(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ middle_arr = [] res_arr = [] self._used = [ False for _ in range(len(nums)) ] self._generate_permutation(nums, 0, res_arr, middle_arr) return res_arr def _generate_permutation( self, nums, cnt, res_arr, middle_arr): if cnt == len(nums): # 当cnt等于数字字符串长度的时候说明一轮已经递归到底了, # 则当前的 中间状态保存器 middle_state_container 则为一个解 # 此处需要深拷贝一下, 因为下方代码有个 `middle_state_container.pop(-1)` res_arr.append(copy.deepcopy(middle_arr)) return for i in range(len(nums)): if self._used[i]: # 如果本轮递归 used_num_set 已经有_single_num 了, # 说明当前排列 middle_state_container 中已经有 _single_num 了 # 那不应该再加入到这个排列中了 continue self._used[i] = True middle_arr.append(nums[i]) self._generate_permutation( nums, cnt+1, res_arr, middle_arr) # 本轮递归完毕后要清空相应记录的状态, 这就是回溯, # 递归本身会记录一些状态当退出的时候他会自动清除状态, # 那我们自己额外记录的状态, 比如 self._used_num_set 和 # middle_state_container 的状态应该自己手动清除 middle_arr.pop(-1) self._used[i] = False 进阶-lc47-全排列2lc47给定一个可包含重复数字的序列，返回所有不重复的全排列。示例:输入: [1,1,2]输出:12345[ [1,1,2], [1,2,1], [2,1,1]] 参考链接我们先对数组排序, 然后就方便做剪枝了 相较lc46, 代码diff如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution_lc47(object): # [lc47](https://leetcode-cn.com/problems/permutations-ii) def __init__(self): self._used = [] def permuteUnique(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ middle_arr = [] res_arr = []+ nums.sort() self._used = [ False for _ in range(len(nums)) ] self._generate_permutation(nums, 0, res_arr, middle_arr) return res_arr def _generate_permutation( self, nums, cnt, res_arr, middle_arr): if cnt == len(nums): # 当cnt等于数字字符串长度的时候说明一轮已经递归到底了, # 则当前的 中间状态保存器 middle_state_container 则为一个解 # 此处需要深拷贝一下, 因为下方代码有个 `middle_state_container.pop(-1)` res_arr.append(copy.deepcopy(middle_arr)) return for i in range(len(nums)): if self._used[i]: # 如果本轮递归 used_num_set 已经有_single_num 了, # 说明当前排列 middle_state_container 中已经有 _single_num 了 # 那不应该再加入到这个排列中了 continue+ # 剪枝条件：i &gt; 0 是为了保证 nums[i - 1] 有意义+ # 因为我们上对nums数组排序了, + # 所以可以写 `self._used[i-1] == False` 是因为+ # nums[i - 1] 在深度优先遍历的过程中刚刚被撤销选择+ if self._used[i-1] == False and (i &gt; 0 and nums[i] == nums[i-1]):+ continue self._used[i] = True middle_arr.append(nums[i]) self._generate_permutation( nums, cnt+1, res_arr, middle_arr) # 本轮递归完毕后要清空相应记录的状态, 这就是回溯, # 递归本身会记录一些状态当退出的时候他会自动清除状态, # 那我们自己额外记录的状态, 比如 self._used_num_set 和 # middle_state_container 的状态应该自己手动清除 middle_arr.pop(-1) self._used[i] = False 比狗-多数组且元素间有顺序要求的全排列不用管第一题, 我们做第二题,思路: 这类问题我们先把多个数组合并且保留各个元素对应原数组的index信息, 然后用合并后的数组做全排列并剪枝剪掉那些不符合顺序性要求的枝即可.代码如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Solution_bigo_thread_permute(object): def __init__(self): self._used = None self._thread_str_arr = [["A", "B", "C", "D"], ["E", "F", "G", "H"]] # self._thread_str_arr = [["A", "B"], ["E"]] def bigo_thread_permute(self): middle_arr = [] res_arr = [] # 方便精准的查询每个字母是否被使用以及 # 方便保证abcd和efgh各自的顺序性时剪枝 self._used = [ [False for _ in range(len(self._thread_str_arr[i])) ] for i in range(len(self._thread_str_arr)) ] _str_2_index_map = &#123;&#125; for i, _sub_arr in enumerate(self._thread_str_arr): for j, _str in enumerate(_sub_arr): # 存好str和他们的数组的index的对应关系 _str_2_index_map[_str] = [i, j] self._generate_permute(_str_2_index_map, 0, res_arr, middle_arr) return res_arr def _generate_permute(self, str_2_index_map, cnt, res_arr, middle_arr): if cnt == len(str_2_index_map): res_arr.append(copy.deepcopy(middle_arr)) return for _str, _index_list in str_2_index_map.iteritems(): i = _index_list[0] j = _index_list[1] # 剪枝: 为了保证abcd和efgh各自的顺序性, # 拿当前的j和used多维数组里i数组里的已经use的最大的max_j来作比较 # 如果小于等于则剪枝, # j大于max_j才能保证添加到middle_arr里的abcd和efgh各自的顺序性 if j &lt;= self._get_used_max_index_j(i): continue if self._used[i][j]: continue self._used[i][j] = True middle_arr.append(_str) self._generate_permute(str_2_index_map, cnt+1, res_arr, middle_arr) middle_arr.pop(-1) self._used[i][j] = False def _get_used_max_index_j(self, i): _max_index_j = -1 for _cur_index_j, _is_used in enumerate(self._used[i]): if _is_used: _max_index_j = _cur_index_j return _max_index_j 树形问题电话号码字母组合 递归关系式: digits 是数字字符串 s(digits) 是 digits 所能代表的字母字符串 则关系式如下: 1234s(digits[0...n-1]) = letter(digits[0]) + s(digits[1...n-1]) = letter(digits[0]) + letter(digits[1]) + s(digits[2...n-1]) = ... 这道题虽然叫字母组合问题, 但实际上是个排列问题.注意查看下方代码中的 _get_letter_combination, 排列问题基本都是这种代码写法模板. 实现代码如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950digits_map = &#123; "0": " ", "1": "", "2": "abc", "3": "def", "4": "ghi", "5": "jkl", "6": "mno", "7": "pqrs", "8": "tuv", "9": "wxyz",&#125;def letter_combinations_of_a_phone_number(digits_str): result_str_arr = [] if not digits_str: return result_str_arr assert "1" not in digits_str, "we dont proc 1" middle_state_container = [] _get_letter_combination(result_str_arr, digits_str, index=0, middle_state_container=middle_state_container) return result_str_arr def _get_letter_combination( result_str_arr, pending_proc_digits_str, index, middle_state_container): """ middle_state_container 中保存了 此时从 pending_proc_digits_str[0...index-1] 翻译得到的一个字母字符串 寻找和pending_proc_digits_str[index]匹配的字母, 获得pending_proc_digits_str[0...index]翻译得到的解 """ if index == len(pending_proc_digits_str): # 当index等于数字字符串长度的时候说明一轮已经递归到底了, # 则当前的 中间状态保存器 middle_state_container 则为一个解 # 此处需要深拷贝一下, 因为下方代码有个 `middle_state_container.pop(-1)` result_str_arr.append(copy.deepcopy(middle_state_container)) return # # 不处理1因为1对应的没字母 # while pending_proc_digits_str[index] == "1": # index += 1 # if index &gt;= len(pending_proc_digits_str): # return _cur_letters_str = digits_map[pending_proc_digits_str[index]] for _single_letter_str in _cur_letters_str: middle_state_container.append(_single_letter_str) _get_letter_combination( result_str_arr, pending_proc_digits_str, index+1, middle_state_container) middle_state_container.pop(-1) 组合问题合集组合问题代码模板和本文的lc77-经典组合问题基本一致. lc77-经典组合问题leetcode77题给出两个整数n和k, 求出1…n中k个数字的所有组合如n=4, k=2, 则结果为[ [1, 2], [1, 3], [1, 4], [2, 3], [2, 4], [3, 4] ] 123456789101112131415161718192021222324252627282930313233class Solution_lc77(object): def combine(self, n, k): """ :type n: int :type k: int :rtype: List[List[int]] """ result_arr = [] if k &lt;= 0 or k &gt; n or n &lt;= 0: return result_arr middle_state_container = [] self._generate_combinations(result_arr, n, k, 1, middle_state_container) return result_arr def _generate_combinations( self, result_arr, pending_proc_n, pending_prco_k, start_num, middle_state_container): """ 求解C(n,k), 当前已经找到的组合存储在 middle_state_container 中, 需要从start_num开始搜索新的元素 可以看出跟排列问题的代码模板很像, 只有终止递归条件和for循环的start_num不太一样 """ if len(middle_state_container) == pending_prco_k: result_arr.append(copy.deepcopy(middle_state_container)) return # 每次递归从start_num开始直到 pending_proc_n for _index in xrange(start_num, pending_proc_n+1): middle_state_container.append(_index) self._generate_combinations( result_arr, pending_proc_n, pending_prco_k, _index+1, middle_state_container) middle_state_container.pop(-1) 组合问题解决优化-剪枝从上面的 组合问题解题思路 中可以看出其实是没有必要计算 “取4” 的操作的,所以我们利用剪枝的思想, 把这部分优化掉, 代码如下:123456789101112131415161718192021222324252627def _generate_combinations_optimized( self, result_arr, pending_proc_n, pending_prco_k, start_num, middle_state_container): """ 求解C(n,k), 当前已经找到的组合存储在 middle_state_container 中, 需要从start_num开始搜索新的元素 可以看出跟排列问题的代码模板很像, 只有终止递归条件和for循环的start_num不太一样 """ if len(middle_state_container) == pending_prco_k: result_arr.append(copy.deepcopy(middle_state_container)) return- # 每次递归从start_num开始直到 pending_proc_n- for _cur_num in xrange(start_num, pending_proc_n+1):+ # 剪枝的思想, + # 还有k - middle_state_container.size()个空位,+ # 所以, [i...n] 中至少要有 k - middle_state_container.size() 个元素+ # i最多为 n - (k - middle_state_container.size()) + 1+ _cur_stop_num = pending_proc_n - (+ pending_prco_k - middle_state_container.size()) + 1+ # 每次递归从start_num开始直到 _cur_stop_num+ for _cur_num in xrange(start_num, _cur_stop_num+1): middle_state_container.append(_cur_num) self._generate_combinations( result_arr, pending_proc_n, pending_prco_k, _cur_num+1, middle_state_container) middle_state_container.pop(-1) lc39-组合总和lc39 给定一个无重复元素的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。candidates 中的数字可以无限制重复被选取。说明： 所有数字（包括 target）都是正整数。 解集不能包含重复的组合。 示例 1：输入：candidates = [2,3,6,7], target = 7,所求解集为：1234[ [7], [2,2,3]] 示例 2：输入：candidates = [2,3,5], target = 8,所求解集为：12345[ [2,2,2,2], [2,3,3], [3,5]] 参考以输入：candidates = [2, 3, 6, 7], target = 7 为例这棵树有 44 个叶子结点的值 00，对应的路径列表是 [[2, 2, 3], [2, 3, 2], [3, 2, 2], [7]]，而示例中给出的输出只有 [[7], [2, 2, 3]]。即：题目中要求每一个符合要求的解是 不计算顺序 的。下面我们分析为什么会产生重复。 针对具体例子分析重复路径产生的原因（难点）友情提示：这一部分我的描述是晦涩难懂的，建议大家先自己观察出现重复的原因，进而思考如何解决。产生重复的原因是：在每一个结点，做减法，展开分支的时候，由于题目中说 每一个元素可以重复使用，我们考虑了 所有的 候选数，因此出现了重复的列表。一种简单的去重方案是借助哈希表的天然去重的功能，但实际操作一下，就会发现并没有那么容易。可不可以在搜索的时候就去重呢？答案是可以的。遇到这一类相同元素不计算顺序的问题，我们在搜索的时候就需要 按某种顺序搜索。具体的做法是：每一次搜索的时候设置 下一轮搜索的起点 start_index, 请看下图。 即：从每一层的第 22 个结点开始，都不能再搜索产生同一层结点已经使用过的 candidate 里的元素 12345678910111213141516171819202122232425262728293031class Solution_lc39(object): def combinationSum(self, candidates, target): """ :type candidates: List[int] :type target: int :rtype: List[List[int]] """ if not candidates: return [] res_arr = [] middle_state_arr = [] self._generate_combinations(candidates, target, 0, res_arr, middle_state_arr) return res_arr def _generate_combinations( self, candidates_arr, cur_target_num, start_index, res_arr, middle_state_arr): if cur_target_num &lt; 0: return if cur_target_num == 0: res_arr.append(copy.deepcopy(middle_state_arr)) return # 这个cur_index是用来去除重复组合的 for cur_index in range(start_index, len(candidates_arr)): middle_state_arr.append(candidates_arr[cur_index]) cur_target_num -= candidates_arr[cur_index] self._generate_combinations( candidates_arr, cur_target_num, cur_index, res_arr, middle_state_arr) cur_target_num += candidates_arr[cur_index] middle_state_arr.pop(-1) 进阶-lc40-组合总和2lc40 如果candidates 中的每个数字在每个组合中只能使用一次呢?那应该改成:12345678910for cur_index in range(start_index, len(candidates_arr)): middle_state_arr.append(candidates_arr[cur_index]) cur_target_num -= candidates_arr[cur_index] self._generate_combinations( candidates_arr, cur_target_num,- cur_index,+ cur_index+1, res_arr, middle_state_arr) cur_target_num += candidates_arr[cur_index] middle_state_arr.pop(-1) 多个数组抽个数总和题目: 4 个数组，目标值 target，每个数组各找一个数，使得 4 个数和为 target，数组没有顺序，找到所有不重复的组合，要求时间复杂度 O(n^2) 123456789101112131415161718192021222324252627282930313233343536class Solution_multi_arr_sum(object): # 题目: 4 个数组，目标值 target，每个数组各找一个数，使得 4 个数和为 target， # 数组没有顺序，找到所有不重复的组合， # 要求时间复杂度 O(n^2) def multi_arr_sum(self, nums_arrs, target_sum_num): if not nums_arrs: return [] middle_arr = [] res_arr = [] self._generate_result( nums_arrs, target_sum_num, 0, res_arr, middle_arr) return res_arr def _generate_result( self, nums_arrs, target_sum_num, start_i_index, res_arr, middle_arr): if target_sum_num &lt; 0: return if len(middle_arr) == len(nums_arrs): if target_sum_num == 0: res_arr.append(copy.deepcopy(middle_arr)) return for i in range(start_i_index, len(nums_arrs)): for j in range(0, len(nums_arrs[i])): middle_arr.append(nums_arrs[i][j]) self._generate_result( nums_arrs, target_sum_num-nums_arrs[i][j], i+1, res_arr, middle_arr ) middle_arr.pop(-1)print "----------multi_arr_sum-------" print 'Solution_multi_arr_sum().multi_arr_sum([[1, 2], [3, 4], [5, 6, 9], [7, 8]], 18) :'print Solution_multi_arr_sum().multi_arr_sum([[1, 2], [3, 4], [5, 6, 9], [7, 8]], 18) 打印结果:123----------multi_arr_sum-------Solution_multi_arr_sum().multi_arr_sum([[1, 2], [3, 4], [5, 6, 9], [7, 8]], 18) :[[1, 3, 6, 8], [1, 4, 5, 8], [1, 4, 6, 7], [2, 3, 5, 8], [2, 3, 6, 7], [2, 4, 5, 7]] 排序组合总结什么时候使用 used 数组，什么时候使用 begin 变量有些朋友可能会疑惑什么时候使用 used 数组，什么时候使用 begin 变量。这里为大家简单总结一下： 排列问题，讲究顺序（即 [2, 2, 3] 与 [2, 3, 2] 视为不同列表时），需要记录哪些数字已经使用过，此时用 used 数组； 组合问题，不讲究顺序（即 [2, 2, 3] 与 [2, 3, 2] 视为相同列表时），需要按照某种顺序搜索，此时使用 begin 变量。 注意：具体问题应该具体分析， 理解算法的设计思想 是至关重要的，请不要死记硬背。 岛屿数量-经典floodfill问题lc200给你一个由 ‘1’（陆地）和 ‘0’（水）组成的的二维网格，请你计算网格中岛屿的数量。岛屿总是被水包围，并且每座岛屿只能由水平方向和/或竖直方向上相邻的陆地连接形成。此外，你可以假设该网格的四条边均被水包围。 示例 1：输入：grid = [ [“1”,”1”,”1”,”1”,”0”], [“1”,”1”,”0”,”1”,”0”], [“1”,”1”,”0”,”0”,”0”], [“0”,”0”,”0”,”0”,”0”]]输出：1如下图则有1个岛屿: 示例 2：输入：grid = [ [“1”,”1”,”0”,”0”,”0”], [“1”,”1”,”0”,”0”,”0”], [“0”,”0”,”1”,”0”,”0”], [“0”,”0”,”0”,”1”,”1”]]输出：3如下图则有3个岛屿: 此题也可以用并查集来解, 见本文的岛屿数量-并查集实战用dfs的解法代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution_number_of_islands(object): def __init__(self): self._visited_pos_set = set() # 方便搜索点的时候往上下左右搜 self._move_dir_arr = [(0, -1), (0, 1), (1, 0), [-1, 0]] def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if not grid: return 0 islands_cnt = 0 for x in xrange(0, len(grid)): assert (type(grid[x]) is list), ("x = %d" % x) for y in xrange(0, len(grid[x])): if tuple([x, y]) in self._visited_pos_set: continue if grid[x][y] != "1": continue self._dfs_islands(grid, x, y) islands_cnt += 1 # 一次搜索完成就算有一个岛屿了 return islands_cnt def _dfs_islands(self, grid, x, y): # x是纵坐标, y是横坐标 # print "x = %d" % x # print "y = %d" % y self._visited_pos_set.add(tuple([x, y])) # 上下左右四个方向搜索 for _move_dir in self._move_dir_arr: _new_x = x + _move_dir[0] _new_y = y + _move_dir[1] # 如果超出地图边界了, 注意 x是纵坐标, y是横坐标 if _new_x &gt;= len(grid) or _new_x &lt; 0 or \ _new_y &gt;= len(grid[0]) or _new_y &lt; 0: continue # 如果已经访问过了 if tuple([_new_x, _new_y]) in self._visited_pos_set: continue if grid[_new_x][_new_y] != "1": continue self._dfs_islands(grid, _new_x, _new_y) 经典N皇后问题… pending_fin 动态规划解题思路 首先，动态规划的穷举有点特别，因为这类问题存在「重叠子问题」，如果暴力穷举的话效率会极其低下，所以需要「备忘录」或者「DP table」来优化穷举过程，避免不必要的计算。 而且，动态规划问题一定会具备「最优子结构」，才能通过子问题的最值得到原问题的最值。 另外，虽然动态规划的核心思想就是穷举求最值，但是问题可以千变万化，穷举所有可行解其实并不是一件容易的事，只有列出正确的「状态转移方程」才能正确地穷举。 以上提到的重叠子问题、最优子结构、状态转移方程就是动态规划三要素。具体什么意思等会会举例详解，但是在实际的算法问题中，写出状态转移方程是最困难的. 动态规划算法就是将待求解问题分解成若干子问题，先求解子问题并保存子问题的答案避免重复计算，然后从这些子问题的解得到原问题的解。而如何断定一个问题是否可以用动态规划来解决，就需要掌握动态规划的两个基本要素: 重叠子问题性质 最优子结构性质 重叠子问题性质在用递归算法自顶向下解决一个问题时，每次产生的子问题并不总是新问题，有些子问题被反复计算多次。动态规划正是利用了这种子问题的重叠性质，对每个子问题只解一次，而后将其解保存到一个表格中，当再次需要解此子问题时，只是简单地用常数时间查看一下结果。保存重叠子问题的解（也就是 fib(3)）有以下两种方式： DP table（自底向上） 备忘录memo方法又称记忆化搜索（自顶向下） 最优子结构性质设计动态规划算法的第一步通常是要刻画最优解的结构。当问题的最优解包含了其子问题的最优解时，称该问题具有最优子结构性质 。问题的最优子结构性质提供了该问题可用动态规划求解的重要线索。 要符合 「最优⼦结构」，⼦问题间必须互相独⽴。啥叫相互独⽴？你肯定不想看数 学证明，我⽤⼀个直观的例⼦来讲解。 ⽐如说，你的原问题是考出最⾼的总成绩，那么你的⼦问题就是要把语⽂考 到最⾼，数学考到最⾼…… 为了每门课考到最⾼，你要把每门课相应的选 择题分数拿到最⾼，填空题分数拿到最⾼…… 当然，最终就是你每门课都 是满分，这就是最⾼的总成绩。 得到了正确的结果：最⾼的总成绩就是总分。因为这个过程符合最优⼦结 构，“每门科⽬考到最⾼”这些⼦问题是互相独⽴，互不⼲扰的。 但是，如果加⼀个条件：你的语⽂成绩和数学成绩会互相制约，此消彼⻓。 这样的话，显然你能考到的最⾼总成绩就达不到总分了，按刚才那个思路就 会得到错误的结果。因为⼦问题并不独⽴，语⽂数学成绩⽆法同时最优，所 以最优⼦结构被破坏。 解决动态规划问题步骤动态规划（Dynamic Programming，DP）是在多项式时间解决特定类型问题的一套方法论，且远远快于指数级别的蛮力法.解决动态规划问题三步法： 辨别是不是一个动态规划问题； 建立状态之间的关系, 构造状态转移方程 明确 base case 明确「状态」也就是原问题和子问题中会变化的变量。 明确「选择」, 也就是导致「状态」产生变化的行为。 定义 dp 数组 / 函数的含义 代码实现方式, 以下两种方式选其一: 为状态添加备忘录memo自顶向下用记忆化搜索的递归方式来写 用DP Table的动规方式来写 按上面的套路走，最后的结果就可以套这个框架：12345dp[0][0][...] = basefor 状态1 in 状态1的所有取值： for 状态2 in 状态2的所有取值： for ... dp[状态1][状态2][...] = 求最值(选择1，选择2...) 第一步-断定是否为动规问题一般情况下，需要求最优解的问题（最短路径问题，最长公共子序列，最大字段和等等，出现 最 字你就留意），在一定条件下对排列进行计数的计数问题（丑数问题）或某些概率问题都可以考虑用动态规划来解决。 所有的动态规划问题都满足重叠子问题性质，大多数经典的动态规划问题还满足最优子结构性质，当我们从一个给定的问题中发现了这些特性，就可以确定其可以用动态规划解决。 第二步-构造状态转移方程其步骤为: 明确 base case 明确「状态」也就是原问题和子问题中会变化的变量。 明确「选择」, 也就是导致「状态」产生变化的行为。 定义 dp 数组 / 函数的含义 DP 问题最重要的就是确定所有的状态和状态与状态之间的转移方程。确定状态转移方程是动态规划最难的部分，但也是最基础的，必须非常谨慎地选择状态，因为状态转移方程的确定取决于你对问题状态定义的选择。那么，状态到底是个什么鬼呢？「状态」 可以视为一组可以唯一标识给定问题中某个子问题解的参数，这组参数应尽可能的小，以减少状态空间的大小。 比如斐波那契数中，0 , 1, …, n 就可以视为参数，而通过这些参数定义出的 DP[0]，DP[1]，DP[2]，…，DP[n] 就是状态，而状态与状态之间的转移方程就是 DP(n) = DP(n-1) + DP(n-2) 。 再比如，经典的背包问题（Knapsack problem）中，状态通过 index 和 weight 两个参数来定义，即 DP[index][weight] 。DP[index][weight] 则表示当前从 0 到 index 的物品装入背包中可以获得的最大重量。因此，参数 index 和 weight 可以唯一确定背包问题的一个子问题的解。 所以，当确定给定的问题之后，首当其冲的就是确定问题的状态。动态规划算法就是将待求解问题分解成若干子问题，先求解子问题并保存子问题的答案避免重复计算，然后从这些子问题的解得到原问题的解。既然确定了一个一个的子问题的状态，接下来就是确定前一个状态到当前状态的转移关系式，也称状态转移方程。 构造状态转移方程是 DP 问题最难的部分，需要足够敏锐的直觉和观察力，而这两者都是要通过大量的练习来获得。我们用一个简单的问题来理解这个步骤: 凑零钱 第三步-用备忘录或者DP表来代码实现这个可以说是动态规划最简单的部分，我们仅需要存储子状态的解，以便下次使用子状态时直接查表从内存中获得。代码书写方式以下二者选其一: 为状态添加备忘录memo自顶向下用记忆化搜索的递归方式来写 用DP Table的动规方式来写. 备忘录memo VS DP表: 状态：DP Table 状态转移关系较难确定，备忘录状态转移关系较易确定。你可以理解为自顶向下推导较为容易，自底向上推导较难。比如 DP[n] = DP[n - 1] + DP[n - 3] + DP[n-5] 的确定。 代码：当约束条件较多的情况下，DP Table 较为复杂；备忘录代码相对容易实现和简单，仅需对递归代码进行改造。 效率：动态规划（DP Table）较快，我们可以直接从表中获取子状态的解；备忘录由于大量的递归调用和返回状态操作，速度较慢。 子问题的解：当所有的子问题的解都至少要被解一遍，自底向上的动态规划算法通常比自顶向下的备忘录方法快常数量级；当求解的问题的子问题空间中的部分子问题不需要计算，仅需求解部分子问题就可以解决原问题，此时备忘录方法要优于动态规划，因为备忘录自顶向下仅存储与原问题求解相关的子问题的解。 表空间：DP Table 依次填充所有子状态的解；而备忘录不必填充所有子问题的解，而是按需填充。 至于两个该如何选择，我想你的心中也有数了，建议按照解动态规划的四步骤依次求解，至于第四步，你个人喜欢用 DP Table 就用 DP Table ，喜欢备忘录就用备忘录。 理解动态规划-讲解凑零钱1lc322先看下题目：给你 k 种面值的硬币，面值分别为 c1, c2 ... ck，每种硬币的数量无限，再给一个总金额 amount，问你最少需要几枚硬币凑出这个金额，如果不可能凑出，算法返回 -1 。算法的函数签名如下： 1int coinChange(int[] coins, int amount); 比如说 k = 3，面值分别为 1，2，5，总金额 amount = 11。那么最少需要 3 枚硬币凑出，即 11 = 5 + 5 + 1。 你认为计算机应该如何解决这个问题？显然，就是把所有肯能的凑硬币方法都穷举出来，然后找找看最少需要多少枚硬币。 首先，这个问题是动态规划问题，因为它具有「最优子结构」的。 为什么说它符合最优子结构呢？比如你想求 amount = 11 时的最少硬币数（原问题），如果你知道凑出 amount = 10 的最少硬币数（子问题），你只需要把子问题的答案加一（再选一枚面值为 1 的硬币）就是原问题的答案。因为硬币的数量是没有限制的，所以子问题之间没有相互制约，是互相独立的。 分析状态转移那么，既然知道了这是个动态规划问题，就要思考如何列出正确的状态转移方程？ 1、确定 base case，这个很简单，显然目标金额 amount 为 0 时算法返回 0，因为不需要任何硬币就已经凑出目标金额了。 2、确定「状态」，也就是原问题和子问题中会变化的变量。由于硬币数量无限，硬币的面额也是题目给定的，只有目标金额会不断地向 base case 靠近，所以唯一的「状态」就是目标金额 amount。 3、确定「选择」，也就是导致「状态」产生变化的行为。目标金额为什么变化呢，因为你在选择硬币，你每选择一枚硬币，就相当于减少了目标金额。所以说所有硬币的面值，就是你的「选择」。 4、明确 dp 函数 / 数组的定义。我们这里讲的是自顶向下的解法，所以会有一个递归的 dp 函数，一般来说函数的参数就是状态转移中会变化的量，也就是上面说到的「状态」；函数的返回值就是题目要求我们计算的量。就本题来说，状态只有一个，即「目标金额」，题目要求我们计算凑出目标金额所需的最少硬币数量。所以我们可以这样定义 dp 函数： dp(n) 的定义：输入一个目标金额 n，返回凑出目标金额 n 的最少硬币数量。 搞清楚上面这几个关键点，解法的伪码就可以写出来了： 123456def coinChange(coins: List[int], amount: int): def dp(n): for coin in coins: res = min(res, 1 + dp(n - coin)) return res return dp(amount) 根据伪码，我们加上 base case 即可得到最终的答案。显然目标金额为 0 时，所需硬币数量为 0；当目标金额小于 0 时，无解，返回 -1： 暴力递归解法123456789101112def coinChange(coins: List[int], amount: int): def dp(n): if n == 0: return 0 if n &lt; 0: return -1 res = float('INF') for coin in coins: subproblem = dp(n - coin) if subproblem == -1: continue res = min(res, 1 + subproblem) return res if res != float('INF') else -1 return dp(amount) 至此，状态转移方程其实已经完成了，以上算法已经是暴力解法了，以上代码的数学形式就是状态转移方程：123dp[n] = min(dp[n-coin])+1, 当n &gt;0dp[n] = 0, 当n = 0;dp[n] = -1, 当n &lt; 0; 至此，这个问题其实就解决了，只不过需要消除一下重叠子问题 递归算法的时间复杂度分析：子问题总数 x 每个子问题的时间。 子问题总数为递归树节点个数，这个比较难看出来，是 O(n^k)，总之是指数级别的。每个子问题中含有一个 for 循环，复杂度为 O(k)。所以总时间复杂度为 O(k * n^k)，指数级别。 带备忘录的递归只需要稍加修改，就可以通过备忘录消除子问题：1234567891011121314151617def coinChange(coins: List[int], amount: int): memo = dict() def dp(n): if n in memo: return memo[n] if n == 0: return 0 if n &lt; 0: return -1 res = float('INF') for coin in coins: subproblem = dp(n - coin) if subproblem == -1: continue res = min(res, 1 + subproblem) memo[n] = res if res != float('INF') else -1 return memo[n] return dp(amount) 不画图了，很显然「备忘录」大大减小了子问题数目，完全消除了子问题的冗余，所以子问题总数不会超过金额数 n，即子问题数目为 O(n)。处理一个子问题的时间不变，仍是 O(k)，所以总的时间复杂度是 O(kn)。 dp数组的迭代解法参考 其实这个题还可以看成是一个恰好装满的完全背包问题, 见本文的这题的背包解法我们采用自下而上的方式进行思考。仍定义 F(i) 为组成金额 i 所需最少的硬币数量，假设在计算 F(i) 之前，我们已经计算出 F(0)−F(i−1) 的答案。 则 F(i) 对应的转移方程应为F(i) = min(F(i-Cj)) + 1 ​其中代表Cj的是第 j 枚硬币的面值，即我们枚举最后一枚硬币面额是 Cj，那么需要从 i-Cj这个金额的状态 F(i-Cj)转移过来，再算上枚举的这枚硬币数量 1 的贡献，由于要硬币数量最少，所以 F(i) 为前面能转移过来的状态的最小值加上枚举的硬币数量 1 。 举个例子：假设 coins = [1, 2, 5], amount = 11则，当 i==0 时无法用硬币组成，为 0 。当 i&lt;0 时，忽略 F(i) F(i) 最小硬币数量 F(0) 0 //金额为0不能由硬币组成 F(1) 1 //F(1)=min(F(1-1),F(1-2),F(1-5))+1=1 F(2) 1 //F(2)=min(F(2-1),F(2-2),F(2-5))+1=1 F(3) 2 //F(3)=min(F(3-1),F(3-2),F(3-5))+1=2 F(4) 2 //F(4)=min(F(4-1),F(4-2),F(4-5))+1=2 … … F(11) 3 //F(11)=min(F(11-1),F(11-2),F(11-5))+1=3 我们可以看到问题的答案是通过子问题的最优解得到的。 自底向上使用 dp table 来消除重叠子问题，关于「状态」「选择」和 base case 与之前没有区别，dp 数组的定义和刚才 dp 函数类似，也是把「状态」，也就是目标金额作为变量。不过 dp 函数体现在函数参数，而 dp 数组体现在数组索引： dp数组的定义：当目标金额为i时，至少需要dp[i]枚硬币凑出。 根据我们文章开头给出的动态规划代码框架可以写出如下解法：123456789class Solution: def coinChange(self, coins: List[int], amount: int) -&gt; int: dp = [float('inf')] * (amount + 1) dp[0] = 0 for coin in coins: for x in range(coin, amount + 1): dp[x] = min(dp[x], dp[x - coin] + 1) return dp[amount] if dp[amount] != float('inf') else -1 动态规划各种题型背包问题系列0-1背包问题注意: 「0-1 背包」问题是一类非常重要的动态规划问题这个题目中的物品不可以分割，要么装进包里，要么不装，不能说切成两块装一半。这也许就是 0-1 背包这个名词的来历。 最基本的背包问题就是 01 背包问题（01 knapsack problem）：一共有 N 件物品，第 i（i 从 1 开始）件物品的重量为 w[i]，价值为 v[i]。在总重量不超过背包承载上限 W 的情况下，能够装入背包的最大价值是多少？ 如果采用暴力穷举的方式，每件物品都存在装入和不装入两种情况，所以总的时间复杂度是 O(2^N)，这是不可接受的。而使用动态规划可以将复杂度降至 O(NW)。我们的目标是书包内物品的总价值，而变量是物品和书包的限重，所以我们可定义状态 dp:1dp[i][j]表示将前i件物品装进限重为j的背包可以获得的最大价值, 0&lt;=i&lt;=N, 0&lt;=j&lt;=W 那么我们可以将 dp[0][0…W] 初始化为 0，表示将前 0 个物品（即没有物品）装入书包的最大价值为 0。那么当 i &gt; 0 时dp[i][j]有两种情况： 不装入第 i 件物品，即dp[i−1][j]； 装入第 i 件物品（前提是能装下），即dp[i−1][j−w[i-1]] + v[i-1] 为什么是i-1注意上方的w[i-1]和v[i-1], 为什么是i-1呢?因为我们对dp[i][j]表示将前i件物品装进限重为j的背包可以获得的最大价值, 则i=0其实表示的是前0个物品并不是第0个物品, 所以实际对应weight数组和value数组的index应该为i-1 即状态转移方程为123dp[i][j] = max( dp[i−1][j], dp[i−1][j−w[i-1]+v[i-1]) # j &gt;= w[i-1] 所求的结果为dp[n][capacity] 根据状态转移方程得出以下动规代码:123456789101112131415161718192021222324252627282930313233343536373839def knapsack_dp(self, capacity, weight_arr, value_arr): if capacity == 0: return 0 assert(len(weight_arr) == len(value_arr)) n = len(weight_arr) # dp[i][j]表示将前i件物品装进限重为j的背包可以获得的最大价值, 0&lt;=i&lt;=N, 0&lt;=j&lt;=W # 那么我们可以将dp[0][0...W]初始化为0， # 表示将前0个物品（即没有物品）装入书包的最大价值为0。 # 那么当 i &gt; 0 时dp[i][j]有两种情况： # - 不装入第i件物品，即dp[i−1][j]； # - 装入第i件物品（前提是能装下），即dp[i−1][j−w[i-1]] + v[i-1]。 # 因为我们对dp[i][j]表示将前i件物品装进限重为j的背包可以获得的最大价值 # 则i=0其实表示的是0个物品, # 所以实际对应weight数组和value数组的index应该为i-1 # # 即状态转移方程为 # dp[i][j] = max(dp[i−1][j], dp[i−1][j−w[i-1]]+v[i-1]) // j &gt;= w[i-1] # 所求为dp[n][capacity] dp = [[0 for _ in xrange(capacity+1)] for _ in xrange(n+1)] # 动规是从底向上嘛, 先构建dp[0]的东西 for k in xrange(capacity+1): # 因为我们对dp[i][j]表示将前i件物品装进限重为j的背包可以获得的最大价值 # 则i=0其实表示的是0个物品, 所以 = 0 dp[0][k] = 0 for i in xrange(1, n+1): for j in xrange(capacity+1): # 因为我们对dp[i][j]表示将前i件物品装进限重为j的背包可以获得的最大价值 # 则i=0其实表示的是0个物品, # 所以实际对应weight数组和value数组的index应该为i-1 pack_args_index = i - 1 if j - weight_arr[pack_args_index] &lt; 0: dp[i][j] = dp[i-1][j] else: dp[i][j] = max( dp[i-1][j], value_arr[pack_args_index] + dp[i-1][j-weight_arr[pack_args_index]] ) return dp[n][capacity] 完全背包问题完全背包（unbounded knapsack problem）与 01 背包不同就是每种物品可以有无限多个：一共有 N 种物品，每种物品有无限多个，第 i（i 从 1 开始）种物品的重量为 w[i]，价值为 v[i]。在总重量不超过背包承载上限 W 的情况下，能够装入背包的最大价值是多少？ 我们的目标和变量和 01 背包没有区别，所以我们可定义与 01 背包问题几乎完全相同的状态 dp:1dp[i][j]表示将前i种物品装进限重为j的背包可以获得的最大价值, 0&lt;=i&lt;=N, 0&lt;=j&lt;=W 为什么完全背包是i而不是i-1我们注意!!!!! 完全背包问题的i指的是前i种, 而不是前i个, 这一点跟0-1背包是不同的, 0-1背包的i指的是前i个初始状态也是一样的，我们将 dp[0][0…W] 初始化为 0，表示将前 0 种物品（即没有物品）装入书包的最大价值为 0。那么当 i &gt; 0 时, 准备要放入第i种的某一个物品item_i_1时(注意, 是第i种的某一个, 第i种还可以有其他同种物品item_i_2, item_i_3…), 也有两种情况： 不装入第 i 种的当前这个物品item_i_1时，那只会有前i-1种商品了, 即dp[i−1][j] 装入第 i 种物品当前这个物品item_i_1时，此时和 0-1 背包不太一样，因为每种物品有无限个（但注意书包限重是有限的），所以此时不应该转移到dp[i−1][j−w[i-1]]而应该转移到dp[i][j−w[i-1]]，因为item_i_1是第i种物品的某一个物品, 所以应该放到dp[i]这个第i种物品的坑位里, 即装入item_i_1了之后还可以装入item_i_2, item_i_3…所以此时为: dp[i][j−w[i-1]] + v[i-1] 为什么是i-1 所以状态转移方程为123dp[i][j] = max( dp[i−1][j], dp[i][j−w[i-1]]+v[i-1]) # j &gt;= w[i-1] 可以看出这个状态转移方程与 0-1 背包问题唯一不同就是i的含义不同导致的 max 第二项不是 dp[i-1] 而是 dp[i]。 背包问题的其他形式 恰好装满 实战题目: lc416-分割等和子集-恰好装满0-1背包问题 lc322-凑金币1-恰好装满的完全背包问题 求方案总数 除了在给定每个物品的价值后求可得到的最大价值外，还有一类问题是问装满背包或将背包装至某一指定容量的方案总数。对于这类问题，需要将状态转移方程中的 max 改成 sum ，大体思路是不变的。例如若每件物品均是完全背包中的物品，转移方程即为 1dp[i][j] = sum(dp[i−1][j], dp[i][j−w[i-1]]) # j &gt;= w[i-1] 实战题目: lc518-凑零钱2-恰好装满完全背包问题 为什么是i-1 为什么完全背包是i而不是i-1 二维背包 前面讨论的背包容量都是一个量：重量。二维背包问题是指每个背包有两个限制条件（比如重量和体积限制），选择物品必须要满足这两个条件。此类问题的解法和一维背包问题不同就是dp数组要多开一维，其他和一维背包完全一样 实战题目: lc474-一和零-二维0-1背包 背包问题实战分割等和子集-恰好装满0-1背包问题leetcode416题给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。示例 1:输入: [1, 5, 11, 5]输出: true解释: 数组可以分割成 [1, 5, 5] 和 [11].示例 2:输入: [1, 2, 3, 5]输出: false解释: 数组不能分割成两个元素和相等的子集. 参考这其实是一个恰好装满0-1背包问题, 我们只要拿一定的数字填满所有数字和sum的一半, 剩余的数字一定等于sum/2, 则这个问题其实还是个背包问题, 只不过我们需要用一定的数字把这个背包填满, 对于第i个物品, 有两种情况: 我们用i-1就填满了背包, 则第i个就不需要用了 我们用了第i个才填满 状态定义：dp[i][j]表示对于容量为 j 的背包，若只是用前 i 个物品(前0个则表示没有物品)，每个数只能用一次，使得这些数的和恰好等于 j .(比如说，如果dp[4][9] = true，其含义为：对于容量为 9 的背包，若只是用前 4 个物品，可以有一种方法把背包恰好装满。)。 状态转移方程：很多时候，状态转移方程思考的角度是「分类讨论」，对于「0-1 背包问题」而言就是「当前考虑到的数字选与不选」。 不选择 nums[i]，则看前i-1个元素的是否能能和为j, 即 dp[i-1][j] 选择 nums[i]，看前i-1个元素的是否能能和为j-nums[i-1], 即dp[i-1][j- nums[i-1] 为什么是i-1则状态转移方程：dp[i][j] = dp[i - 1][j] or dp[i - 1][j - nums[i-1]] // j &gt;= nums[i-1] 一般写出状态转移方程以后，就需要考虑初始化条件。 初始化： dp[0][0] = True # 前0个则表示没有物品, 可以充满容量为0的背包 dp[0][c] = False # 前0个则表示没有物品, 是不可能充满容量c大于0的背包的 输出：dp[len][target]，这里 len 表示nums数组的长度，target 是数组的元素之和（必须是偶数）的一半。 则代码如下:123456789101112131415161718192021222324252627282930313233343536def canPartition(self, nums): """ :type nums: List[int] :rtype: bool 动规解法 """ # 特判：如果是奇数，就不符合要求 if not nums or len(nums) &lt; 2 or sum(nums) % 2 != 0: return False _bag_capcity = sum(nums) / 2 n = len(nums) # 状态定义： # dp[i][j]表示对于容量为 j 的背包，若只是用前 i 个物品(前0个则表示没有物品)， # 每个数只能用一次，使得这些数的和恰好等于 j . # (比如说，如果dp[4][9] = true，其含义为：对于容量为 9 的背包，若只是用前 4 个物品，可以有一种方法把背包恰好装满。)。 # 状态转移方程：很多时候，状态转移方程思考的角度是「分类讨论」，对于「0-1 背包问题」而言就是「当前考虑到的数字选与不选」。 # - 不选择 nums[i]，则看前i-1个元素的是否能能和为j, 即 `dp[i-1][j]` # - 选择 nums[i]，看前i-1个元素的是否能能和为`j-nums[i-1]`, 即`dp[i-1][j- nums[i-1]` # **注意上方的`nums[i-1]`, 为什么是i-1呢?** # 因为我们对dp[i][j]表示将前i件物品装进限重为j的背包可以获得的最大价值, 则i=0其实表示的是0个物品, # 所以实际对应nums数组的index应该为`i-1` # 则状态转移方程： # dp[i][j] = dp[i - 1][j] or dp[i - 1][j - nums[i-1]] // j &gt;= nums[i-1] dp = [[False for _ in range(_bag_capcity+1)] for _ in range(n+1)] dp[0][0] = True # 前0个则表示没有物品, 可以充满容量为0的背包 for c in range(1, _bag_capcity+1): dp[0][c] = False # 前0个则表示没有物品, 是不可能充满容量大于0的背包的 for i in range(1, n+1): for j in range(_bag_capcity+1): if j - nums[i-1] &lt; 0: dp[i][j] = dp[i-1][j] else: dp[i][j] = dp[i-1][j] or dp[i-1][j-nums[i-1]] return dp[n][_bag_capcity] 凑金币1-恰好装满的完全背包问题lc322先看下题目：给你 k 种面值的硬币，面值分别为 c1, c2 ... ck，每种硬币的数量无限，再给一个总金额 amount，问你最少需要几枚硬币凑出这个金额，如果不可能凑出，算法返回 -1 。算法的函数签名如下：1int coinChange(int[] coins, int amount); 比如说 k = 3，面值分别为 1，2，5，总金额 amount = 11。那么最少需要 3 枚硬币凑出，即 11 = 5 + 5 + 1。 如果我们将每种硬币看作是每种物品，面值金额看成是物品的重量，总金额是背包的总容量, 因为硬币无限, 这样此题就是是一个恰好装满的完全背包问题了。不过这里不是求最多装入多少价值而是求最少装满背包的数目，所以我们只需要将完全背包的转态转移方程中稍微改改即可: dp[i][j]定义为: 用前i种硬币可以抽一些硬币出来装满容量为j的背包的最少硬币数量 状态转移方程为: d[i][j] = min(dp[i-1][j], dp[i][j-coins[i-1]]+1) 为什么是i-1为什么完全背包是i而不是i-1 1234567891011121314151617181920212223242526272829def coinChange(self, coins, amount): """ :type coins: List[int] :type amount: int :rtype: int """ assert(coins) # 用背包的思路 if amount == 0: return 0 n = len(coins) # 如果我们将每种硬币看作是每种物品，面值金额看成是物品的重量，总金额是背包的总容量, 因为硬币无限, 这样此题就是是一个恰好装满的完全背包问题.了。不过这里不是求最多装入多少价值而是求最少装满背包的数目，所以我们只需要将[完全背包](#完全背包问题)的转态转移方程中稍微改改即可: # - dp[i][j]定义为: 用前i种硬币可以抽一些硬币出来装满容量为j的背包的最少硬币数量 # - 状态转移方程为: `d[i][j] = min(dp[i-1][j], dp[i][j-coins[i-1]])` # 因为之后要取min操作, 所以这里初始化为无穷大`float("inf")` dp = [ [ float("inf") for _ in range(amount+1) ] for _ in range(n+1) ] for k in range(n+1): dp[k][0] = 0 # 充满容量为0的背包, 最少的硬币个数为0 for i in range(n+1): for j in range(amount+1): if j - coins[i-1] &lt; 0: dp[i][j] = dp[i-1][j] else: dp[i][j] = min( dp[i-1][j], dp[i][j-coins[i-1]] + 1 ) return dp[n][amount] if dp[n][amount] != float("inf") else -1 凑零钱2-恰好装满完全背包问题lc518给定不同面额的硬币和一个总金额。写出函数来计算可以凑成总金额的硬币组合数。假设每一种面额的硬币有无限个。示例 1:输入: amount = 5, coins = [1, 2, 5]输出: 4解释: 有四种方式可以凑成总金额:5=55=2+2+15=2+1+1+15=1+1+1+1+1示例 2:输入: amount = 3, coins = [2]输出: 0解释: 只用面额2的硬币不能凑成总金额3。示例 3:输入: amount = 10, coins = [10]输出: 1 我们可以把这个问题转化为背包问题的描述形式：有一个背包，最大容量为amount，有一系列物品coins，每种物品的重量为coins[i]，每种物品的数量无限。请问有多少种方法，能够把背包恰好装满？这个问题和我们前面讲过的两个0-1背包问题，有一个最大的区别就是，每种物品的数量是无限的，这也就是传说中的「完全背包问题」，没啥高大上的，无非就是状态转移方程有一点变化而已。这是一个恰好装满完全背包问题 第一步要明确两点，「状态」和「选择」。 这部分都是背包问题的老套路了，我还是啰嗦一下吧：状态有两个，就是「背包的容量」和「可选择的物品」，选择就是「装进背包」或者「不装进背包」。 明白了状态和选择，动态规划问题基本上就解决了 第二步要明确dp数组的定义。 首先看看刚才找到的「状态」，有两个，也就是说我们需要一个二维dp数组。 dp[i][j]的定义如下： 从前i种物品里选取若干件物品，当背包容量为j时，有dp[i][j]种方法可以装满背包。 换句话说，翻译回我们题目的意思就是： 若使用coins中的前i种硬币的面值，若想凑出金额j，有dp[i][j]种凑法。 经过以上的定义，可以得到： base case 为 dp[0][..] = 0 因为如果不使用任何一种硬币，就无法凑出任何金额 dp[..][0] = 1 如果凑出的目标金额为 0，那么 “无为而治” (不用任何硬币)就是唯一的一种凑法。 我们最终想得到的答案就是dp[N][amount]，其中N为coins数组的大小。 第三步，根据「选择」，思考状态转移的逻辑。 如果你不把这第i种的某个物品装入背包，也就是说你不使用coins[i]这种面值的硬币，那么凑出面额j的方法数dp[i][j]应该等于dp[i-1][j]，继承之前的结果。 如果你把这第i种某个物品装入了背包，也就是说你使用coins[i]这种面值的硬币，那么dp[i][j]应该等于dp[i][j-coins[i-1]]。 首先由于i是从 1 开始的，所以coins的索引是i-1时表示第i种硬币的面值。dp[i][j-coins[i-1]]也不难理解，如果你决定使用这种面值的硬币，那么就应该关注如何凑出金额j - coins[i-1]。比如说，你想用面值为 2 的硬币凑出金额 5，那么如果你知道了凑出金额 3 的方法，再加上一枚面额为 2 的硬币，不就可以凑出 5 了嘛。综上就是两种选择，而我们想求的dp[i][j]是「共有多少种凑法」，所以dp[i][j]的值应该是以上两种选择的结果之和则状态转移方程为: dp[i][j] = dp[i-1][j] + dp[i][j-coins[i-1]] 1234567891011121314151617181920def change(self, amount, coins): """ :type amount: int :type coins: List[int] :rtype: int """ n = len(coins) # dp[i][j]`的定义如下： # 从前`i`种物品里选取若干件物品，当背包容量为`j`时，有`dp[i][j]`种方法可以装满背包。 dp = [ [ 0 for _ in range(amount+1) ] for _ in range(n+1) ] for k in range(n+1): # 如果凑出的目标金额为 0，那么 “无为而治” (不用任何硬币)就是唯一的一种凑法。 dp[k][0] = 1 for i in range(1, n+1): for j in range(1, amount+1): if j - coins[i-1] &lt; 0: dp[i][j] = dp[i-1][j] else: dp[i][j] = dp[i-1][j] + dp[i][j-coins[i-1]] return dp[n][amount] 一和零-二维0-1背包lc474给你一个二进制字符串数组 strs 和两个整数 m 和 n 。请你找出并返回 strs 的最大子集的大小，该子集中 最多 有 m 个 0 和 n 个 1 。如果 x 的所有元素也是 y 的元素，集合 x 是集合 y 的 子集 。示例 1：输入：strs = [“10”, “0001”, “111001”, “1”, “0”], m = 5, n = 3输出：4解释：最多有 5 个 0 和 3 个 1 的最大子集是 {“10”,”0001”,”1”,”0”} ，因此答案是 4 。其他满足题意但较小的子集包括 {“0001”,”1”} 和 {“10”,”1”,”0”} 。{“111001”} 不满足题意，因为它含 4 个 1 ，大于 n 的值 3 。示例 2：输入：strs = [“10”, “0”, “1”], m = 1, n = 1输出：2解释：最大的子集是 {“0”, “1”} ，所以答案是 2 。 pending_fini 股票利润最大系列stock总结我们先解决第四题, 然后: 第一题是只进行一次交易，相当于 k = 1； 第二题是不限交易次数，相当于 k = +infinity（正无穷）； 第三题是只进行 2 次交易，相当于 k = 2； 剩下两道也是不限交易次数，但是加了交易「冷冻期」和「手续费」的额外条件，其实就是第二题的变种，都很容易处理。 stock4-最通用的股票题lc188, 我们先看股票的第4个题, 这个题最后代表性, 答案也最通用. 给定一个整数数组 prices ，它的第 i 个元素 prices[i] 是一支给定的股票在第 i 天的价格。设计一个算法来计算你所能获取的最大利润。你最多可以完成 k 笔交易。注意: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。题中交易的含意是买入和卖出一支股票一次, 才称为一次交易示例 1：输入：k = 2, prices = [2,4,1]输出：2解释：在第 1 天 (股票价格 = 2) 的时候买入，在第 2 天 (股票价格 = 4) 的时候卖出，这笔交易所能获得利润 = 4-2 = 2 。示例 2：输入：k = 2, prices = [3,2,6,5,0,3]输出：7解释：在第 2 天 (股票价格 = 2) 的时候买入，在第 3 天 (股票价格 = 6) 的时候卖出, 这笔交易所能获得利润 = 6-2 = 4 。 随后，在第 5 天 (股票价格 = 0) 的时候买入，在第 6 天 (股票价格 = 3) 的时候卖出, 这笔交易所能获得利润 = 3-0 = 3 。 参考1 参考2 注意: 题中交易的含意是买入和卖出一支股票一次, 才称为一次交易但我们解题的时候可以把买入就当成一次交易会容易写代码一些,当然也可以定义dp为买了再卖才算一次交易, 只是代码难写一些, 而且初始化状态难弄一些, dp[i][k][0]为前i天最多可以完成k次交易时手中 无股票时 的最大利润 dp[i][k][1]为前i天最多可以完成k次交易时手中 有股票时 的最大利润 我们定义dp买入就算一次交易, 则:123456789101112131415# 前i天t次交易现在手上持有 = max(i-1天t次交易手上持有，i-1天t-1次交易手上不持有 - i天买入价格)dp[i][t][1] = max( dp[i-1][t][1], # 为什么是`prices[i-1]`呢? 因为这里的i是第i天, # 根据我们的dp定义, # 实际上第i=1天对应的是数组中的prices[0]的价格 # 我们dp对交易的定义是买入就算, 这里买入一张股票, 得减去`prices[i-1]` # 所以我们这里才`t-1`, 好理解一些 dp[i-1][t-1][0] - prices[i-1])# 前i天t次交易现在手上不持有 = max(i-1天t次交易手上不持有，i-1天t次交易手上持有 + i天卖出价格prices)dp[i][t][0] = max( dp[i-1][t][0], dp[i-1][t][1] + prices[i-1]) dp[0][t][0] 前0天(即还没开始之意)t次交易，手上不持有：可能的 0 dp[0][t][1] 前0天(即还没开始之意)t次交易，手上持有：不可能（前0天(即还没开始之意)没有股票，所以无法买入持有;持有说明至少进行了一次买入，买入就交易，因此这里不可能【不可能意思就是不能从这里转移】 dp[i][0][0] 前i天0次交易，手上不持有：0 dp[i][0][1] 前i天0次交易，手上持有：不可能（不交易手上不可能持有） 注意看下方代码的注释:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Solution_stock(object): def stock4_maxProfit(self, k, prices): """ :type k: int :type prices: List[int] :rtype: int """ if not prices or not k: return 0 n = len(prices) # 注意: 题中交易的含意是买入和卖出一支股票一次, 才称为一次交易 # 但我们解题的时候可以把买入就当成一次交易会容易写代码一些, # 当然也可以定义dp为买了再卖才算一次交易, 只是代码难写一些, 而且 # 初始化状态难弄一些 # dp[i][k][0]为前i天最多可以完成k次交易时手中 无股票时 的最大利润 # dp[i][k][1]为前i天最多可以完成k次交易时手中 有股票时 的最大利润 # 为什么下方要初始化为`n+1`呢? 因为我们要求的是第n天最多可以完成k次交易时手中无股票时的最大利润, # 而不是第n-1天, 注意我们下方说的第0天并不是数组意义的第1天. # 读者可能问为什么不是 dp[n - 1][K][1]？ # 因为 [1] 代表手上还持有股票，[0] 表示手上的股票已经卖出去了， # 很显然后者得到的利润一定大于前者。 dp = [ [ [ 0 for _ in range(2) ] for _ in range(k+1) ] for _ in range(n+1) ] for j in range(n+1): dp[j][0][0] = 0 # 前j天0次交易，手上不持有, 故为0 # 前j天0次交易，手上持有股票, 这是不可能的, # 我们dp对交易的定义是买入就算, 0次交易都没买入股票, 不可能持有股票 # 所以我们用负无穷来表示, 因为之后我们用max来取值, # 如果这里不这样初始化，而是初始化为0，那么我t次交易的无法去做max, # max它会取这个0,而不会去取那些负值 dp[j][0][1] = float("-inf") for t in range(k+1): # 前0天t次交易，手上持有股票, 这里所说的前0天不是数组的第1天, # 前0天是一个不存在的日子, 所以这是不可能的, # 所以我们用负无穷来表示, 因为之后我们用max来取值, # 如果这里不这样初始化，而是初始化为0，那么我t次交易的无法去做max, # max它会取这个0,而不会去取那些负值 dp[0][t][1] = float("-inf") dp[0][t][0] = 0 for i in range(1, n+1): for t in range(1, k+1): # i天t次交易现在手上持有 = max(i-1天t次交易手上持有，i-1天t-1次交易手上不持有 - i天买入价格) dp[i][t][1] = max( dp[i-1][t][1], # 为什么是`prices[i-1]`呢? 因为这里的i是第i天, # 根据我们的dp定义, # 实际上第i=1天对应的是数组中的prices[0]的价格 # 我们dp对交易的定义是买入就算, 这里买入一张股票, 得减去`prices[i-1]` # 所以我们这里才`t-1`, 好理解一些 dp[i-1][t-1][0] - prices[i-1] ) # i天t次交易现在手上不持有 = max(i-1天t次交易手上不持有，i-1天t次交易手上持有 + i天卖出价格prices) dp[i][t][0] = max( dp[i-1][t][0], dp[i-1][t][1] + prices[i-1] ) return dp[n][k][0] stock1lc121k=1 解法: 直接调用stock4的代码, 把k设置为1即可 stock2lc122k=无穷大 解法: 如果 k 为正无穷，那么就可以认为 k 和 k - 1 是一样的, k的约束已经没有作用了。所以dp数组可以去掉k这个维度.忽略下方注释中的t, 则:123456789101112131415# i天t次交易现在手上持有 = max(i-1天t次交易手上持有，i-1天t-1次交易手上不持有 - i天买入价格)dp[i][1] = max( dp[i-1][1], # 为什么是`prices[i-1]`呢? 因为这里的i是第i天, # 根据我们的dp定义, # 实际上第i=1天对应的是数组中的prices[0]的价格 # 我们dp对交易的定义是买入就算, 这里买入一张股票, 得减去`prices[i-1]` # 所以我们这里才`t-1`, 好理解一些 dp[i-1][0] - prices[i-1])# i天t次交易现在手上不持有 = max(i-1天t次交易手上不持有，i-1天t次交易手上持有 + i天卖出价格prices)dp[i][0] = max( dp[i-1][0], dp[i-1][1] + prices[i-1]) stock3lc123k=2 解法: 直接调用stock4的代码, 把k设置为2即可 stock5lc714 给定一个整数数组，其中第 i 个元素代表了第 i 天的股票价格 。​设计一个算法计算出最大利润。在满足以下约束条件下，你可以尽可能地完成更多的交易（多次买卖一支股票）: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 卖出股票后，你无法在第二天买入股票 (即冷冻期为 1 天)。示例:输入: [1,2,3,0,2]输出: 3解释: 对应的交易状态为: [买入, 卖出, 冷冻期, 买入, 卖出] 题目特点: 可无限次交易, 则k还是无穷大, 那么就可以认为 k 和 k - 1 是一样的, k的约束已经没有作用了。所以dp数组可以去掉k这个维度. 因为冷冻期的存在, 第i天如果手上有股票而且选择了要买股票的时候应该是从第i-2天开始状态转移, 注意下方代码中的dp[i-2][0] - prices[i-1] 则dp状态转移方程得改改(忽略下方注释中的t), 如下:123456789101112131415# i天t次交易现在手上持有 = max(i-1天t次交易手上持有，i-2天t-1次交易手上不持有 - i天买入价格)dp[i][1] = max( dp[i-1][1], # 为什么是`prices[i-2]`呢? 因为这里的i是第i天, # 根据我们的dp定义, # 实际上第i=1天对应的是数组中的prices[0]的价格 # 我们dp对交易的定义是买入就算, 这里买入第i天的一张股票, 得减去`prices[i-1]` # 所以我们这里才`t-1`, 好理解一些 dp[i-2][0] - prices[i-1])# i天t次交易现在手上不持有 = max(i-1天t次交易手上不持有，i-1天t次交易手上持有 + i天卖出价格prices)dp[i][0] = max( dp[i-1][0], dp[i-1][1] + prices[i-1]) stock6lc714你可以无限次地完成交易，但是你每笔交易都需要付手续费。 题目特点: 可无限次交易, 则k还是无穷大, 那么就可以认为 k 和 k - 1 是一样的, k的约束已经没有作用了。所以dp数组可以去掉k这个维度. 手续费的存在, 根据我们dp的定义, 我们定义购买即为一次交易, 那我们就在每次购买的时候加上这个手续费, 则第i天如果手上有股票而且选择了要买股票的时候应该加上手续费, 注意下方代码中的dp[i-1][0] - prices[i-1] - fee 则dp状态转移方程得改改(忽略下方注释中的t), 如下:123456789101112131415# i天t次交易现在手上持有 = max(i-1天t次交易手上持有，i-1天t-1次交易手上不持有 - i天买入价格)dp[i][1] = max( dp[i-1][1], # 为什么是`prices[i-1]`呢? 因为这里的i是第i天, # 根据我们的dp定义, # 实际上第i=1天对应的是数组中的prices[0]的价格 # 我们dp对交易的定义是买入就算, 这里买入一张股票, 得减去`prices[i-1]` # 所以我们这里才`t-1`, 好理解一些 dp[i-1][0] - prices[i-1] - fee)# i天t次交易现在手上不持有 = max(i-1天t次交易手上不持有，i-1天t次交易手上持有 + i天卖出价格prices)dp[i][0] = max( dp[i-1][0], dp[i-1][1] + prices[i-1]) 打家劫舍系列rob1leetcode198题你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。 示例 1：输入：[1,2,3,1]输出：4解释：偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2：输入：[2,7,9,3,1]输出：12解释：偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。 参考题目很容易理解，而且动态规划的特征很明显。我们前文 动态规划详解 做过总结，解决动态规划问题就是找「状态」和「选择」，仅此而已。假想你就是这个专业强盗，从左到右走过这一排房子，在每间房子前都有两种选择：抢或者不抢。 如果你抢了这间房子，那么你肯定不能抢相邻的下一间房子了，只能从下下间房子开始做选择。 如果你不抢这间房子，那么你可以走到下一间房子前，继续做选择。 当你走过了最后一间房子后，你就没得抢了，能抢到的钱显然是 0（base case）。以上的逻辑很简单吧，其实已经明确了「状态」和「选择」：你面前房子的索引就是状态，抢和不抢就是选择。在两个选项中选择偷窃总金额较大的选项，该选项对应的偷窃总金额即为从index开始偷到最后的房子能偷到的最高总金额.用 dp[i] 表示从index开始偷到最后的房子能偷到的最高总金额，那么就有如下的状态转移方程：dp[i] = max( dp[i+2] + nums[i], dp[i+1] )边界条件为： dp[n-1] = nums[n-1] , 最后一间房屋，则偷窃该房屋 dp[n-2] = max( nums[n-1], nums[n-2] ) , 最后两间房屋，选择其中金额较高的房屋进行偷窃 最终的答案即为 dp[n−1]，其中 n 是数组的长度递归memo写法:123456789101112131415def rob1_dp_memo(self, nums_arr): return self._do_rob1_dp_memo(nums_arr, 0)def _do_rob1_dp_memo(self, nums_arr, index): # 我们定义此函数为从index开始偷到最后的房子能偷到的最高总金额 if index &gt;= len(nums_arr): return 0 if index in self._memo: return self._memo[index] res = max( nums_arr[index] + self._do_rob1_dp_memo(nums_arr, index+2), self._do_rob1_dp_memo(nums_arr, index+1), ) self._memo[index] = res return res 迭代写法:12345678910def rob1_dp(self, nums_arr): if not nums_arr: return 0 n = len(nums_arr) dp = [0] * n dp[n-1] = nums_arr[n-1] dp[n-2] = max(nums_arr[n-1], nums_arr[n-2]) for i in range(n-3, -1, -1): dp[i] = max(dp[i+1], nums_arr[i]+dp[i+2]) return dp[0] rob1进阶-求出具体偷哪些房子的子序列还是用动规思路123456789101112131415def house_rob_detail_seq(self, nums_arr): if not nums_arr: return 0 n = len(nums_arr) # 根据上述思路, 我们用 dp[i] 表示从第 i 间房屋偷到最后一间能偷窃到的最 # 高总金额的房子数组子序列 dp = [ [] for _ in xrange(n) ] dp[n-1] = [nums_arr[n-1]] dp[n-2] = [max(nums_arr[n-1], nums_arr[n-2])] for i in range(n-3, -1, -1): if (nums_arr[i] + sum(dp[i+2])) &gt; sum(dp[i+1]): dp[i] = [nums_arr[i]] + dp[i+2] else: dp[i] = dp[i+1] return dp[0] rob2lc213这道题目和第一道描述基本一样，强盗依然不能抢劫相邻的房子，输入依然是一个数组，但是告诉你这些房子不是一排，而是围成了一个圈。 也就是说，现在第一间房子和最后一间房子也相当于是相邻的，不能同时抢。比如说输入数组nums=[2,3,2]，算法返回的结果应该是 3 而不是 4，因为开头和结尾不能同时被抢。 那就简单了啊，这三种情况，哪种的结果最大，就是最终答案呗！不过，其实我们不需要比较三种情况，只要比较情况二和情况三就行了，因为这两种情况对于房子的选择余地比情况一大呀，房子里的钱数都是非负数，所以选择余地大，最优决策结果肯定不会小。所以只需对之前的解法调用一下求个max值即可：123def rob2_dp(nums_arr): n = len(nums_arr) max(rob1_dp(nums_arr[0:n-1]), rob1_dp(nums_arr[1:n]) rob3lc337第三题的房子在二叉树的节点上，相连的两个房子不能同时被抢劫：示例 1:输入: [3,2,3,null,3,null,1]12345 3 / \2 3 \ \ 3 1 输出: 7解释: 小偷一晚能够盗取的最高金额 = 3 + 3 + 1 = 7.示例 2:输入: [3,4,5,1,3,null,1]12345 3 / \ 4 5 / \ \ 1 3 1 输出: 9解释: 小偷一晚能够盗取的最高金额 = 4 + 5 = 9. 整体的思路完全没变，还是做抢或者不抢的选择，取收益较大的选择。甚至我们可以直接按这个套路写出递归式的dp代码：123456789101112131415def rob3_dp(self, bt): # 此函数求出bt为根节点的最大价值 if not bt: return 0 if bt in memo: return memo[bt] # 抢, 然后去下下家 do_it = bt.val + \ (rob3_dp(bt.left.left) + rob3_dp(bt.left.right) if bt.left else 0) + \ (rob3_dp(bt.right.left) + rob3_dp(bt.right.left) if bt.right else 0) # 不抢, 然后去下家 not_do_it = rob3_dp(bt.left) + rob3_dp(bt.right) res = max(do_it, not_do_it) memo[bt] = res return res LIS问题-最长上升子序列leetcode300题LIS即longest-increasing-subsequence给定一个无序的整数数组，找到其中最长上升子序列的长度。示例:输入: [10,9,2,5,3,7,101,18]输出: 4解释: 最长的上升子序列是 [2,3,7,101]，它的长度是 4。说明:可能会有多种最长上升子序列的组合，你只需要输出对应的长度即可。你算法的时间复杂度应该为 O(n2) 。进阶: 你能将算法的时间复杂度降低到 O(n log n) 吗? 参考我们定义 dp[i] 为选取到第i个数字的时候的最长上升子序列的长度, 注意这里的定义, 第i个数字是一定要选取的则我们的状态转移方程为: dp[i] = max(dp[j]) + 1 , 其中 0 &lt;= j &lt; i 且 nums[j] &lt; nums[i] 即考虑往 dp[0…i−1] 中最长的上升子序列后面再加一个 nums[i]。由于 dp[j]dp[j] 代表 nums[0…j] 中以 nums[j] 结尾的最长上升子序列，所以如果能从 dp[j]dp[j] 这个状态转移过来，那么 nums[i] 必然要大于 nums[j]，才能将 nums[i] 放在 nums[j] 后面以形成更长的上升子序列。最后，整个数组的最长上升子序列即所有 dp[i]dp[i] 中的最大值。LIS =max(dp[i]), 其中 0 ≤ i &lt; n下图显示了该方法： 翻译成代码就是:1234567891011121314151617181920class Solution_LIS(object): def lengthOfLIS(self, nums): """ :type nums: List[int] :rtype: int """ if not nums: return 0 n = len(nums) # 我们定义 `dp[i]` 为选取到第i个数字的时候的最长上升子序列的长度, # 注意这里的定义, 第i个数字是一定要选取的. dp = [1] * n # 因为自己本身就是一个长度为1的上升子序列 dp[0] = 1 for i in xrange(1, n): for j in xrange(0, i): # 则我们的状态转移方程为: # `dp[i] = max(dp[j]) + 1 , 其中 0 &lt;= j &lt; i 且 nums[j] &lt; nums[i]` if nums[j] &lt; nums[i]: dp[i] = max(dp[i], dp[j] + 1) return max(dp) LIP问题-字跳一面-真正理解递推lc329, hard给定一个整数矩阵，找出最长递增路径的长度。对于每个单元格，你可以往上，下，左，右四个方向移动。 你不能在对角线方向上移动或移动到边界外（即不允许环绕）。示例 1:输入:123456nums = [ [9,9,4], [6,6,8], [2,1,1]] 输出: 4解释: 最长递增路径为 [1, 2, 6, 9]。示例 2:输入:123456nums = [ [3,4,5], [3,2,6], [2,2,1]] 输出: 4解释: 最长递增路径是 [3, 4, 5, 6]。注意不允许在对角线方向上移动。 由lc300-lis问题-最长上升子序列, 我们很容易得出 状态定义dp[i][j] 为选中 matrix[i][j] 的最长递增路径的长度, 注意这里的 matrix[i][j]是一定要选中的 dp[i][j] = 1, 都初始化为1, 因为根据状态定义, 即使相邻结点都小于自己, 那也至少为1 根据当前的元素和相邻上下左右的元素比较, 选出最大值再加1, 则为当前的dp, 故状态转移方程为: 123456dp[i][j] = 1 + max( dp[i-1][j] if i-1 &gt;= 0 and matrix[i-1][j] &lt; matrix[i][j] else 0, # 上 dp[i][j-1] if j-1 &gt;= 0 and matrix[i][j-1] &lt; matrix[i][j] else 0, # 左 dp[i+1][j] if i+1 &lt;= m-1 and matrix[i+1][j] &lt; matrix[i][j] else 0, # 下 dp[i][j+1] if j+1 &lt;= n-1 and matrix[i][j+1] &lt; matrix[i][j] else 0 # 右) 这个题目如果没有真正理解递推, 很容易写错: 以为直接拿着matrix就两重for循环遍历就完了.这样写是不对的, 因为要求的是递增路径, 所以我们得先根据matrix中每个元素值的大小按照从小到大排序,然后从最小值的元素开始遍历一步步由小到大递推到最大一个点,这样才算是考虑完全了, 这样才是从最小信息量的状态一点一点转移递推到大的状态的动态规划的过程. 代码如下:12345678910111213141516171819202122232425262728293031323334def longestIncreasingPath(self, matrix): """ :type matrix: List[List[int]] :rtype: int """ if not matrix: return 0 m = len(matrix) n = len(matrix[0]) # * 状态定义`dp[i][j]` 为选中 `matrix[i][j]` 的最长递增路径的长度, 注意这里的 `matrix[i][j]`是一定要选中的 # * `dp[i][j] = 1`, 都初始化为1, 因为根据状态定义, 即使相邻结点都小于自己, 那也至少为1 dp = [ [ 1 for _ in range(n) ] for _ in range(m) ] points_list = [] for i in range(m): for j in range(n): points_list.append([ matrix[i][j], i, j ]) # 这个题目**如果没有真正理解递推, 很容易写错: 以为直接拿着matrix就两重for循环遍历就完了.** # **这样写是不对的, 因为要求的是递增路径, 所以我们得先根据matrix中每个元素值的大小按照从小到大排序**, # 然后从最小值的元素开始遍历一步步由小到大递推到最大一个点, # 这样才算是考虑完全了, 这样才是从最小信息量的状态一点一点转移递推到大的状态的动态规划的过程. sorted_points_list = sorted(points_list, key=lambda x: x[0]) for val, i, j in sorted_points_list: dp[i][j] = 1 + max( dp[i-1][j] if i-1 &gt;= 0 and matrix[i-1][j] &lt; matrix[i][j] else 0, # 上 dp[i][j-1] if j-1 &gt;= 0 and matrix[i][j-1] &lt; matrix[i][j] else 0, # 左 dp[i+1][j] if i+1 &lt;= m-1 and matrix[i+1][j] &lt; matrix[i][j] else 0, # 下 dp[i][j+1] if j+1 &lt;= n-1 and matrix[i][j+1] &lt; matrix[i][j] else 0 # 右 ) max_path_len = 0 for i in range(m): for j in range(n): if dp[i][j] &gt; max_path_len: max_path_len = dp[i][j] return max_path_len LCS问题-最长公共子序列问题lc1143LCS即Longest-Common-Sequence给出两个字符串S1和S2, 求这两个字符串最长公共子序列的长度.比如: S1 = ABCD S2 = AEBD 则最长公共子序列为ABD, 其长度为3, 给定两个字符串 text1 和 text2，返回这两个字符串的最长公共子序列的长度。一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。例如，”ace” 是 “abcde” 的子序列，但 “aec” 不是 “abcde” 的子序列。两个字符串的「公共子序列」是这两个字符串所共同拥有的子序列。若这两个字符串没有公共子序列，则返回 0。则代码如下:123456789101112131415161718192021222324252627class Solution_LCS(object): def lengthOfLCS(self, str_arr): if not str_arr: return 0 assert len(str_arr) == 2 str_a = str_arr[0] str_b = str_arr[1] m = len(str_a) n = len(str_b) if m &lt; 1 or n &lt; 1: return 0 # 我们定义dp[i][j] 为 str_a[0...i] 和str_b[0...j]的最长子序列的长度 dp = [ [ 0 for _ in xrange(n) ] for _ in xrange(m) ] # 初始化最底层的基础数据 for k in xrange(n): dp[0][k] = 1 if str_a[0] == str_b[k] else 0 for h in xrange(m): dp[h][0] = 1 if str_a[h] == str_b[0] else 0 for i in xrange(1, m): for j in xrange(n): # 根据图中的状态转移方程得出, 有两种情况, 所以if一下 if str_a[i] == str_b[j]: dp[i][j] = dp[i-1][j-1] + 1 else: dp[i][j] = max(dp[i-1][j], dp[i][j-1] if j-1 &gt;= 0 else 0) return dp[m-1][n-1] 求LCS具体的是哪个子序列 思路1: 还是用动规来解 123456789101112131415161718192021222324252627282930def get_lcs_detail_seq_1(self, str_arr): if not str_arr: return "" assert len(str_arr) == 2 str_a = str_arr[0] str_b = str_arr[1] m = len(str_a) n = len(str_b) if m &lt; 1 or n &lt; 1: return "" # 我们定义dp[i][j] 为 str_a[0...i] 和str_b[0...j]的最长子序列 dp = [ [ "" for _ in xrange(n) ] for _ in xrange(m) ] # 初始化最底层的基础数据 for k in xrange(n): if str_a[0] == str_b[k]: for h in xrange(k, n): # k后面的也都要设置 dp[0][h] = str_a[0] for k in xrange(m): if str_a[k] == str_b[0]: for h in xrange(k, m): # k后面的也都要设置 dp[h][0] = str_b[0] for i in xrange(1, m): for j in xrange(1, n): # 根据图中的状态转移方程得出, 有两种情况, 所以if一下 if str_a[i] == str_b[j]: dp[i][j] = dp[i-1][j-1] + str_a[i] else: dp[i][j] = dp[i-1][j] if len(dp[i-1][j]) &gt; len(dp[i][j-1]) else dp[i][j-1] return dp[m-1][n-1] 思路2, 仅用来加深理解: 不管是LCS/LIS/0-1背包问题如果要求最优解的具体情况是哪种, 我们的思路就是要用dp解法求出整个dp数组之后, 然后根据dp的状态定义, 以及dp数组里具体存储了的信息反推回去. 从之前求lcs的代码以及上图中都可以看出, 从dp数组的末尾后面反推回去, 上一个公共字符所在的横纵index肯定在当前横纵index的左上. 那则对于LCS的具体解, 思路2的代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def get_lcs_detail_seq_2(self, str_arr): if not str_arr: return "" assert len(str_arr) == 2 str_a = str_arr[0] str_b = str_arr[1] m = len(str_a) n = len(str_b) if m &lt; 1 or n &lt; 1: return "" # 我们定义dp[i][j] 为 str_a[0...i] 和str_b[0...j]的最长子序列的长度 dp = [ [ 0 for _ in xrange(n) ] for _ in xrange(m) ] # 初始化最底层的基础数据 for k in xrange(n): if str_a[0] == str_b[k]: for h in xrange(k, n): # k后面的也都要置为1 dp[0][h] = 1 for k in xrange(m): if str_a[k] == str_b[0]: for h in xrange(k, m): # k后面的也都要置为1 dp[h][0] = 1 for i in xrange(1, m): for j in xrange(1, n): # 根据图中的状态转移方程得出, 有两种情况, 所以if一下 if str_a[i] == str_b[j]: dp[i][j] = dp[i-1][j-1] + 1 else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) # 不管是LCS/LIS/0-1背包问题如果要求最优解的具体情况是哪种, # 我们的思路就是要用dp解法求出整个dp数组之后, # 然后根据dp的状态定义, 以及dp数组里具体存储了的信息反推回去. # # 从之前求lcs的代码以及上图中都可以看出, # 从dp数组的末尾后面反推回去, # 上一个公共字符所在的横纵index肯定在当前横纵index的左上. p = len(str_a) - 1; q = len(str_b) - 1; _lcs_detail_seq = ""; while(p &gt;= 0 and q &gt;= 0): if( str_a[p] == str_b[q] ): _lcs_detail_seq = str_a[p] + _lcs_detail_seq; p -= 1; q -= 1; elif(p == 0): q -= 1; elif(q == 0): p -= 1; else: # 由dp数组图中可知, # 上一个公共字符所在的横纵index肯定在当前横纵index的左上. if(dp[p-1][q] &gt; dp[p][q-1]): # dp[p-1][q] 大, 则往左移动, p减一 # 这样才能才能找到最大公共子串的上一个公共字符嘛 p -= 1; else: # dp[p][q-1] 大, 则往上移动, q减一 # 这样才能才能找到最大公共子串的上一个公共字符嘛 q -= 1; return _lcs_detail_seq; 谷歌经典扔鸡蛋问题lc887题目是这样：你面前有一栋从 1 到N共N层的楼，然后给你K个鸡蛋（K至少为 1）。现在确定这栋楼存在楼层0 &lt;= F &lt;= N，在这层楼将鸡蛋扔下去，鸡蛋恰好没摔碎（高于F的楼层都会碎，低于F的楼层都不会碎）。现在问你，最坏情况下，你至少要扔几次鸡蛋，才能确定这个楼层F呢？ 举个例子来说明题意, 比如10层楼, 2个鸡蛋A和B, 最优解是 4 次. 这个4次是怎么算出来的呢? 随意举几种情况: 从5楼扔A, A碎了就扔另一个蛋B到0-4层, A没碎就继续扔B到6-10层 最好的情况是: A第一次扔没碎, 然后到第6层碎了, 只扔了2次 最坏: A第一次扔没碎, 然后到第10层都没碎, 扔了6次 从2楼/4/6/8/10扔A, A碎了就扔另一个蛋B, 比如在4楼A碎了就到3楼扔B 最好的情况是: A第一次扔就碎了, 然后B到1楼去扔, 只扔了2次 最坏: A一直扔到10楼才碎, 然后B到第9层去试试, 扔了6次 统计出来, 实际上最优的扔法是: A从第4楼/7/9/10这样扔 最好的情况是: A第一次扔就碎了, 然后B从1楼开始扔, 然后b在1楼就碎了, 只扔了2次 最坏: A一直扔到10楼才碎, 扔了4次 A第一次扔就碎了, 然后B从1楼开始扔, 然后b在3楼碎了, 扔了4次 如果题目没看懂, 建议看一下此视频的讲解.参考对动态规划问题，直接套我们以前多次强调的框架即可：这个问题有什么「状态」，有什么「选择」，然后穷举。 「状态」很明显，就是当前拥有的鸡蛋数K和需要测试的楼层数N。随着测试的进行，鸡蛋个数可能减少，楼层的搜索范围会减小，这就是状态的变化。 「选择」其实就是去选择哪层楼扔鸡蛋。回顾刚才的线性扫描和二分思路，二分查找每次选择到楼层区间的中间去扔鸡蛋，而线性扫描选择一层层向上测试。不同的选择会造成状态的转移。 现在明确了「状态」和「选择」，动态规划的基本思路就形成了：肯定是个二维的dp数组或者带有两个状态参数的dp函数来表示状态转移；外加一个 for 循环来遍历所有选择，择最优的选择更新结果 ：1234567# 当前状态为 (K 个鸡蛋，N 层楼)# 返回这个状态下的最优结果def dp(K, N): int res for 1 &lt;= i &lt;= N: res = min(res, 这次在第 i 层楼扔鸡蛋) return res 这段伪码还没有展示递归和状态转移，不过大致的算法框架已经完成了。我们在第i层楼扔了鸡蛋之后，可能出现两种情况：鸡蛋碎了，鸡蛋没碎。注意，这时候状态转移就来了： 如果鸡蛋碎了，那么鸡蛋的个数K应该减一，搜索的楼层区间应该从[1..N]变为[1..i-1]共i-1层楼； 如果鸡蛋没碎，那么鸡蛋的个数K不变，搜索的楼层区间应该从 [1..N]变为[i+1..N]共N-i层楼。 因为我们要求的是最坏情况下扔鸡蛋的次数，所以取决于哪种情况的dp结果更大, 所以要max一下：12345678910def dp(K, N): for 1 &lt;= i &lt;= N: # 最坏情况下的最少扔鸡蛋次数 res = min(res, max( dp(K - 1, i - 1), # 碎 dp(K, N - i) # 没碎 ) + 1 # 在第 i 楼扔了一次 ) return res 递归的 base case 很容易理解：当楼层数N等于 0 时，显然不需要扔鸡蛋；当鸡蛋数K为 1 时，显然只能线性扫描所有楼层：1234def dp(K, N): if K == 1: return N if N == 0: return 0 ... 至此，其实这道题就解决了！只要添加一个备忘录消除重叠子问题即可：1234567891011121314151617181920212223242526272829303132333435363738394041424344def superEggDrop(self, K, N): memo = dict() def dp(K, N): # base case if K == 1: return N if N == 0: return 0 # 避免重复计算 if (K, N) in memo: return memo[(K, N)] res = float('INF') # 穷举所有可能的选择 for i in range(1, N + 1): res = min(res, max( dp(K, N - i), dp(K - 1, i - 1) ) + 1 ) # 记入备忘录 memo[(K, N)] = res return res return dp(K, N)def superEggDrop_dp(self, K, N): """ :type K: int :type N: int :rtype: int """ # dp[k][n]: 表示为当前状态为 k 个鸡蛋，面对 n 层楼的 # 这个状态下最坏的情况的最少的扔鸡蛋的次数 dp = [ [ p for p in range(N+1) ] for _ in range(K+1) ] for t in range(2, K+1): for q in range(1, N+1): for m in range(1, q): dp[t][q] = min( dp[t][q], max( dp[t-1][m-1], # 碎了 dp[t][q-m], # 没碎 ) + 1 ) return dp[K][N] 这个算法的时间复杂度是多少呢？动态规划算法的时间复杂度就是子问题个数 × 函数本身的复杂度。函数本身的复杂度就是忽略递归部分的复杂度，这里dp函数中有一个 for 循环，所以函数本身的复杂度是 O(N)。子问题个数也就是不同状态组合的总数，显然是两个状态的乘积，也就是 O(KN)。所以算法的总时间复杂度是 O(K*N^2), 空间复杂度为子问题个数，即 O(KN)。 扔鸡蛋逆向思路2-推荐参考dp[2][3]表示：注意查看下方代码中的注释:1234567891011121314151617181920212223242526272829303132def superEggDrop2(self, K, N): """ :type K: int :type N: int :rtype: int * 鸡蛋掉落，鹰蛋（Leetcode 887）：（经典dp） * 有 K 个鸡蛋，有 N 层楼，用最少的操作次数 Q 检查出鸡蛋的质量。 * * 思路： * 本题应该逆向思维，若你有 K 个鸡蛋，你最多操作 Q 次，求 N 最大值。 * * dp[i][j] = dp[i][j-1] + dp[i-1][j-1] + 1; * 解释： * 0.dp[i][j]：如果你还剩 i 个蛋，且最多只能操作 j 次了，所能确定的最高楼层。 * 1.dp[i][j-1]：蛋没碎，因此该部分决定了所操作楼层的上面所能容纳的楼层最大值 * 2.dp[i-1][j-1]：蛋碎了，因此该部分决定了所操作楼层的下面所能容纳的楼层最大值 * 又因为第 j 次操作结果只和第 j-1 次操作结果相关，因此可以只用一维数组。此处略. * * 时复：O(K*根号(N)) """ # j 最多不会超过 N 次（线性扫描） # base case: # dp[0][..] = 0 # dp[..][0] = 0 dp = [ [ 0 for _ in range(N+1) ] for _ in range(K+1) ] j = 0 while dp[K][j] &lt; N: # 也就是给你K个鸡蛋，允许测试j次，最坏情况下最多能测试N层楼 j += 1 # 这个j为什么要减一而不是加一？之前定义得很清楚，这个j是一个允许的次数上界，而不是扔了几次。 for i in range(1, K+1): dp[i][j] = dp[i][j-1] + dp[i-1][j-1] + 1 return j 整数拆分这一小节, 我们开始讨论最优子结构: 通过求子问题的最优解, 可以获得原问题的最优解. leetcode343题给定一个正整数 n，将其拆分为至少两个正整数的和，并使这些整数的乘积最大化。 返回你可以获得的最大乘积。说明: 你可以假设 n 不小于 2 且不大于 58。 示例 1:输入: 2输出: 1解释: 2 = 1 + 1, 1 × 1 = 1。 示例 2:输入: 10输出: 36解释: 10 = 3 + 3 + 4, 3 × 3 × 4 = 36。 通过上图，我们很容易得到一个递归表达式：F(n) = max {i * F(n - i)}，i = 1，2，... ，n - 1上述表达式是表明n - i需要继续分解的情况，但如果i * (n - i)比F(n - i)要大，显然就不用再继续分解了。故我们还需要比较i * (n - i)与i * F(n - i)的大小关系。所以完整的表达式应该为：F(n) = max { i * F(n - i), i * (n - i)} , i = 1, 2, ... , n - 1基于此，就不难得到如下代码,而通过以下代码中的 integer_break_dp 方法中的注释, 可以很清晰的看出怎么从普通递归一点一点演进到动态规划的思路的!思路分析参考: https://leetcode-cn.com/problems/integer-break/solution/bao-li-sou-suo-ji-yi-hua-sou-suo-dong-tai-gui-hua-/ https://leetcode-cn.com/problems/integer-break/solution/ba-yi-ba-zhe-chong-ti-de-wai-tao-343-zheng-shu-cha/ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Solution_integer_break(object): def __init__(self): self._memo = &#123;&#125; # 将n进行分割(至少分割两部分), 可以获得的最大乘积 def integerBreak(self, n): """ :type n: int :rtype: int """ if n == 1: return 1 _res = -1 if n in self._memo: return self._memo[n] for i in xrange(1, n): # 计算`i + (n-i)` _res = max( _res, i * (n-i), # 有可能自己本身`i*(n-i)`就是最大的 i * self.integerBreak(n-i)) self._memo[n] = _res return _res def integer_break_dp(self, n): # 下面这种A思路是不行的: # dp[i]等价于 f(i)， # 那么上面针对 f(i) 写的递归公式对 dp[i] 也是适用的，我们拿来试试。 # 关键语句: # `res = max(res, i * (n - i), max(i * self.integerBreak(n - i)))` # 翻译过来就是：`dp[i] = max(_res, i * (n-i), i * dp[n-i])` # 则不难得出以下代码, 但因为 dp[n-i] 当前没求出来, 子问题没求出来, # 所以原问题也就求不出来了, 所以下面这三行代码是不行的 # _res = -1 # for i in xrange(1, n): # dp[i] = max(_res, i * (n-i), i * dp[n-i]) # 此时我们得下面这种B思路才行: # 我们用一层循环来生成上面这段A思路代码一系列的 n 值。 # 接着我们还要生成上面A思路代码中一系列的 i 值， # 注意到 n - i 是要大于 0 的， # 因此 i 只需要循环到 n - 1 即可。 # 由此不难翻译A思路得出以下代码(j代表n, k代表i): # dp[i] = max(dp[i], (i-j)*j, j* dp[i-j]) # j=1...i # dp[i] 表示将数字i分割(至少分割成两部分)后得到的最大乘积 dp = [ float("-inf") for _ in range(n+1) ] dp[0] = float("-inf") dp[1] = float("-inf") dp[2] = 1 for j in xrange(3, n+1): # 循环到n for k in xrange(1, j): # 循环到j-1即可 dp[j] = max(dp[j], k*(j-k), k*dp[j-k]) return dp[n] 双指针题型两个无序数组的公共元素集合思路: 两个集合求交集 1234567891011121314151617181920212223242526class Solution &#123;public: vector&lt;int&gt; intersection(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; unordered_set&lt;int&gt; set1, set2; for (auto&amp; num : nums1) &#123; set1.insert(num); &#125; for (auto&amp; num : nums2) &#123; set2.insert(num); &#125; return getIntersection(set1, set2); &#125; vector&lt;int&gt; getIntersection(unordered_set&lt;int&gt;&amp; set1, unordered_set&lt;int&gt;&amp; set2) &#123; if (set1.size() &gt; set2.size()) &#123; return getIntersection(set2, set1); &#125; vector&lt;int&gt; intersection; for (auto&amp; num : set1) &#123; if (set2.count(num)) &#123; intersection.push_back(num); &#125; &#125; return intersection; &#125;&#125;; 两个有序数组的公共元素集合lc349, easy 关键是怎么用上有序这个定语 如果两个数组是有序的，则可以使用双指针的方法得到两个数组的交集。 首先对两个数组进行排序，然后使用两个指针遍历两个数组。可以预见的是加入答案的数组的元素一定是递增的，为了保证加入元素的唯一性，我们需要额外记录变量 pre 表示上一次加入答案数组的元素。初始时，两个指针分别指向两个数组的头部。每次比较两个指针指向的两个数组中的数字，如果两个数字不相等，则将指向较小数字的指针右移一位，如果两个数字相等，且该数字不等于 pre ，将该数字添加到答案并更新 pre 变量，同时将两个指针都右移一位。当至少有一个指针超出数组范围时，遍历结束 参考 1234567891011121314151617181920212223242526class Solution &#123;public: vector&lt;int&gt; intersection(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; sort(nums1.begin(), nums1.end()); sort(nums2.begin(), nums2.end()); int length1 = nums1.size(), length2 = nums2.size(); int index1 = 0, index2 = 0; vector&lt;int&gt; intersection; while (index1 &lt; length1 &amp;&amp; index2 &lt; length2) &#123; int num1 = nums1[index1], num2 = nums2[index2]; if (num1 == num2) &#123; // 保证加入元素的唯一性 if (!intersection.size() || num1 != intersection.back()) &#123; intersection.push_back(num1); &#125; index1++; index2++; &#125; else if (num1 &lt; num2) &#123; index1++; &#125; else &#123; index2++; &#125; &#125; return intersection; &#125;&#125;; 最小覆盖子串-滑动窗口典型题目lc76, hard参考这里有一套滑动窗口算法的代码框架，我连在哪里做输出 debug 都给你写好了，以后遇到相关的问题，你就默写出来如下框架然后改三个地方就行，还不会出边界问题：123456789101112131415161718192021222324252627282930/* 滑动窗口算法框架 */void slidingWindow(string s, string t) &#123; unordered_map&lt;char, int&gt; need, window; for (char c : t) need[c]++; int left = 0, right = 0; int valid = 0; while (right &lt; s.size()) &#123; // c 是将移入窗口的字符 char c = s[right]; // 右移窗口 right++; // 进行窗口内数据的一系列更新 ... /*** debug 输出的位置 ***/ printf("window: [%d, %d)\n", left, right); /********************/ // 判断左侧窗口是否要收缩 while (window needs shrink) &#123; // d 是将移出窗口的字符 char d = s[left]; // 左移窗口 left++; // 进行窗口内数据的一系列更新 ... &#125; &#125;&#125; 其中两处…表示的更新窗口数据的地方，到时候你直接往里面填就行了。而且，这两个…处的操作分别是右移和左移窗口更新操作，等会你会发现它们操作是完全对称的。 我们继续就本题来谈滑动窗口算法的思路是这样： 我们在字符串S中使用双指针中的左右指针技巧，初始化left = right = 0，把索引左闭右开区间[left, right)称为一个「窗口」。 我们先不断地增加right指针扩大窗口[left, right)，直到窗口中的字符串符合要求（包含了T中的所有字符）。 此时，我们停止增加right，转而不断增加left指针缩小窗口[left, right)，直到窗口中的字符串不再符合要求（不包含T中的所有字符了）。同时，每次增加left，我们都要更新一轮结果。 重复第 2 和第 3 步，直到right到达字符串S的尽头。 这个思路其实也不难，第 2 步相当于在寻找一个「可行解」，然后第 3 步在优化这个「可行解」，最终找到最优解，也就是最短的覆盖子串。左右指针轮流前进，窗口大小增增减减，窗口不断向右滑动，这就是「滑动窗口」这个名字的来历。 下面画图理解一下，needs和window相当于计数器，分别记录T中字符出现次数和「窗口」中的相应字符的出现次数。初始状态： 增加right，直到窗口[left, right)包含了T中所有字符： 现在开始增加left，缩小窗口[left, right)。 直到窗口中的字符串不再符合要求，left不再继续移动。 之后重复上述过程，先移动right，再移动left…… 直到right指针到达字符串S的末端，算法结束。如果一个字符进入窗口，应该增加window计数器；如果一个字符将移出窗口的时候，应该减少window计数器；当valid满足need时应该收缩窗口；应该在收缩窗口的时候更新最终结果。下面是完整代码：123456789101112131415161718192021222324252627282930313233343536373839404142string minWindow(string s, string t) &#123; unordered_map&lt;char, int&gt; need, window; for (char c : t) need[c]++; int left = 0, right = 0; int valid = 0; // 记录最小覆盖子串的起始索引及长度 int start = 0, len = INT_MAX; while (right &lt; s.size()) &#123; // c 是将移入窗口的字符 char c = s[right]; // 右移窗口 right++; // 进行窗口内数据的一系列更新 if (need.count(c)) &#123; window[c]++; if (window[c] == need[c]) valid++; &#125; // 判断左侧窗口是否要收缩 while (valid == need.size()) &#123; // 在这里更新最小覆盖子串 if (right - left &lt; len) &#123; start = left; len = right - left; &#125; // d 是将移出窗口的字符 char d = s[left]; // 左移窗口 left++; // 进行窗口内数据的一系列更新 if (need.count(d)) &#123; if (window[d] == need[d]) valid--; window[d]--; &#125; &#125; &#125; // 返回最小覆盖子串 return len == INT_MAX ? "" : s.substr(start, len);&#125; n数之和问题-双指针从两端逼近参考 两数之和先讨论两数之和, 解决思路就是先排序然后用两个指针从首尾两端逼近为了防止结果重复, 指针应该向上图这样移动123456789101112131415161718192021vector&lt;vector&lt;int&gt;&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123; // nums 数组必须有序 sort(nums.begin(), nums.end()); int lo = 0, hi = nums.size() - 1; vector&lt;vector&lt;int&gt;&gt; res; while (lo &lt; hi) &#123; int sum = nums[lo] + nums[hi]; int left = nums[lo], right = nums[hi]; if (sum &lt; target) &#123; while (lo &lt; hi &amp;&amp; nums[lo] == left) lo++; &#125; else if (sum &gt; target) &#123; while (lo &lt; hi &amp;&amp; nums[hi] == right) hi--; &#125; else &#123; res.push_back(&#123;left, right&#125;); // 跳过所有重复的元素 while (lo &lt; hi &amp;&amp; nums[lo] == left) lo++; while (lo &lt; hi &amp;&amp; nums[hi] == right) hi--; &#125; &#125; return res;&#125; 三数之和确定了第一个数字之后，剩下的两个数字可以是什么呢？其实就是和为 target - nums[i]的两个数字呗, 此时threeSum函数的实现思路就出来了:先对数组排序, 然后遍历数组, 确定好第一个数字, 后面两个数字这两个数字用twoSum的双指针思路来求 四数之和先对数组排序, 然后遍历数组, 确定好第一个数字, 后面三个数字这两个数字用threeSum函数来求 n数之和以下代码看起来很长，实际上就是把之前的题目解法合并起来了，n == 2 时是 twoSum 的双指针解法，n &gt; 2 时就是穷举第一个数字，然后递归调用计算 (n-1)Sum，组装答案。 需要注意的是，调用这个 nSum 函数之前一定要先给 nums 数组排序，因为 nSum 是一个递归函数，如果在 nSum 函数里调用排序函数，那么每次递归都会进行没有必要的排序，效率会非常低。 12345678910111213141516171819202122232425262728293031323334353637383940/* 注意：调用这个函数之前一定要先给 nums 排序 */vector&lt;vector&lt;int&gt;&gt; nSumTarget( vector&lt;int&gt;&amp; nums, int n, int start, int target) &#123; int sz = nums.size(); vector&lt;vector&lt;int&gt;&gt; res; // 至少是 2Sum，且数组大小不应该小于 n if (n &lt; 2 || sz &lt; n) return res; // 2Sum 是 base case if (n == 2) &#123; // 双指针那一套操作 int lo = start, hi = sz - 1; while (lo &lt; hi) &#123; int sum = nums[lo] + nums[hi]; int left = nums[lo], right = nums[hi]; if (sum &lt; target) &#123; while (lo &lt; hi &amp;&amp; nums[lo] == left) lo++; &#125; else if (sum &gt; target) &#123; while (lo &lt; hi &amp;&amp; nums[hi] == right) hi--; &#125; else &#123; res.push_back(&#123;left, right&#125;); while (lo &lt; hi &amp;&amp; nums[lo] == left) lo++; while (lo &lt; hi &amp;&amp; nums[hi] == right) hi--; &#125; &#125; &#125; else &#123; // n &gt; 2 时，递归计算 (n-1)Sum 的结果 for (int i = start; i &lt; sz; i++) &#123; vector&lt;vector&lt;int&gt;&gt; sub = nSumTarget(nums, n - 1, i + 1, target - nums[i]); for (vector&lt;int&gt;&amp; arr : sub) &#123; // (n-1)Sum 加上 nums[i] 就是 nSum arr.push_back(nums[i]); res.push_back(arr); &#125; while (i &lt; sz - 1 &amp;&amp; nums[i] == nums[i + 1]) i++; &#125; &#125; return res;&#125; 其他类型经典题用Rand7实现Rand10lc470, medium已有方法 rand7 可生成 1 到 7 范围内的均匀随机整数，试写一个方法 rand10 生成 1 到 10 范围内的均匀随机整数。 参考 参考 为什么这道题不简单? 现在要从 rand7() 到 rand10()，也要求是等概率的，那只要我们把小的数映射到一个大的数就好办了，那首先想到的办法是乘个两倍试一试，每个 rand7() 它能生成数的范围是 1～7，rand 两次，那么数的范围就变为 2～14，哦，你可能发现没有 1 了，想要再减去个 1 来弥补，rand(7)+rand(7)-1，其实这样是错误的做法，因为对于数字 5 这种，你有两种组合方式 (2+3 or 3+2)，而对于 14，你只有一种组合方式(7+7)，它并不是等概率的，那么简单的加减法不能使用，因为它会使得概率不一致， 下面我们系统的来分析: Part1假设已知rand2()可以均匀的生成 [1,2] 的随机数，现在想均匀的生成 [1,4] 的随机数，该如何考虑？ 我想如果你也像我一样第一次接触这个问题，那么很可能会这么考虑——令两个rand2()相加，再做一些必要的边角处理。如下： 123456789101112rand2() + rand2() = ? ==&gt; [2,4] 1 + 1 = 2 1 + 2 = 3 2 + 1 = 3 2 + 2 = 4// 为了把生成随机数的范围规约成[1,n]，于是在上一步的结果后减1(rand2()-1) + rand2() = ? ==&gt; [1,3] 0 + 1 = 1 0 + 2 = 2 1 + 1 = 2 1 + 2 = 3 可以看到，使用这种方法处理的结果，最致命的点在于——其生成的结果不是等概率的。在这个简单的例子中，产生 2 的概率是 50%，而产生 1 和 3 的概率则分别是 25%。原因当然也很好理解，由于某些值会有多种组合，因此仅靠简单的相加处理会导致结果不是等概率的。 因此，我们需要考虑其他的方法了。 仔细观察上面的例子，我们尝试对 (rand2()-1) 这部分乘以 2，改动后如下： 12345(rand2()-1) × 2 + rand2() = ? ==&gt; [1,3] 0 + 1 = 1 0 + 2 = 2 2 + 1 = 3 2 + 2 = 4 神奇的事情发生了，奇怪的知识增加了。通过这样的处理，得到的结果恰是 [1,4] 的范围，并且每个数都是等概率取到的。因此，使用这种方法，可以通过rand2()实现rand4()。 也许这么处理只是我运气好，而不具有普适性？那就多来尝试几个例子。比如： 12(rand9()-1) × 7 + rand7() = result a b 为了表示方便，现将rand9()-1表示为 a，将rand7()表示为 b。计算过程表示成二维矩阵，如下： 可以看到，这个例子可以等概率的生成 [1,63] 范围的随机数。 规律再提炼一下，可以得到这样一个规律：1234已知 rand_N() 可以等概率的生成[1, N]范围的随机数那么：(rand_X() - 1) × Y + rand_Y() ==&gt; 可以等概率的生成[1, X * Y]范围的随机数即实现了 rand_XY() Part2那么想到通过rand4()来实现rand2()呢？这个就很简单了，已知rand4()会均匀产生 [1,4] 的随机数，通过取余，再加 1 就可以了。如下所示，结果也是等概率的。 12345rand4() % 2 + 1 = ? 1 % 2 + 1 = 2 2 % 2 + 1 = 1 3 % 2 + 1 = 2 4 % 2 + 1 = 1 事实上，只要rand_N()中 N 是 2 的倍数，就都可以用来实现rand2()，反之，若 N 不是 2 的倍数，则产生的结果不是等概率的。比如： 1234567891011121314rand6() % 2 + 1 = ? 1 % 2 + 1 = 2 2 % 2 + 1 = 1 3 % 2 + 1 = 2 4 % 2 + 1 = 1 5 % 2 + 1 = 2 6 % 2 + 1 = 1rand5() % 2 + 1 = ? 1 % 2 + 1 = 2 2 % 2 + 1 = 1 3 % 2 + 1 = 2 4 % 2 + 1 = 1 5 % 2 + 1 = 2 Part3ok，现在回到本题中。已知rand7()，要求通过rand7()来实现rand10()。 有了前面的分析，要实现rand10()，就需要先实现rand_N()，并且保证 N 大于 10 且是 10 的倍数。这样再通过rand_N() % 10 + 1 就可以得到 [1,10] 范围的随机数了。 而实现rand_N()，我们可以通过 part 1 中所讲的方法对rand7()进行改造，如下： 1(rand7()-1) × 7 + rand7() ==&gt; rand49() 但是这样实现的 N 不是 10 的倍数啊！这该怎么处理？这里就涉及到了 “拒绝采样” 的知识了，也就是说，如果某个采样结果不在要求的范围内，则丢弃它。基于上面的这些分析，再回头看下面的代码，想必是不难理解了。 1234567891011class Solution extends SolBase &#123; public int rand10() &#123; // 首先得到一个数 int num = (rand7() - 1) * 7 + rand7(); // 只要它还大于10，那就给我不断生成，因为我只要范围在1-10的，最后直接返回就可以了 while (num &gt; 10)&#123; num = (rand7() - 1) * 7 + rand7(); &#125; return num; &#125;&#125; 这样的一个问题是，我们的函数会得到 1～49之间的数，而我们只想得到 1～10 之间的数，这一部分占的比例太少了，简而言之，这样效率太低，太慢，可能要 while 循环很多次，那么解决思路就是舍弃一部分数，舍弃 41～49，因为是独立事件，我们生成的 1～40 之间的数它是等概率的，我们最后完全可以利用 1～40 之间的数来得到 1～10 之间的数。所以，我们的代码可以改成下面这样12345678class Solution extends SolBase &#123; public int rand10() &#123; while(true) &#123; int num = (rand7() - 1) * 7 + rand7(); // 等概率生成[1,49]范围的随机数 if(num &lt;= 40) return num % 10 + 1; // 拒绝采样，并返回[1,10]范围的随机数 &#125; &#125;&#125; Part4: 优化更进一步，这时候我们舍弃了 9 个数，舍弃的还是有点多，效率还是不高，怎么提高效率呢？那就是舍弃的数最好再少一点！因为这样能让 while 循环少转几次，那么对于大于 40 的随机数，别舍弃呀，利用这 9 个数，再利用那个公式操作一下： (大于40的随机数 - 40 - 1) * 7 + rand7() 这样我们可以得到 1−63 之间的随机数，只要舍弃 3 个即可，那对于这 3 个舍弃的，还可以再来一轮： (大于60的随机数 - 60 - 1) * 7 + rand7() 这样我们可以得到 1−21 之间的随机数，只要舍弃 1 个即可。 123456789101112131415161718192021/** * The rand7() API is already defined in the parent class SolBase. * public int rand7(); * @return a random integer in the range 1 to 7 */class Solution extends SolBase &#123; public int rand10() &#123; while (true)&#123; int num = (rand7() - 1) * 7 + rand7(); // 如果在40以内，那就直接返回 if(num &lt;= 40) return 1 + num % 10; // 说明刚才生成的在41-49之间，利用随机数再操作一遍 num = (num - 40 - 1) * 7 + rand7(); if(num &lt;= 60) return 1 + num % 10; // 说明刚才生成的在61-63之间，利用随机数再操作一遍 num = (num - 60 - 1) * 7 + rand7(); if(num &lt;= 20) return 1 + num % 10; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>Algo</tag>
        <tag>noodle</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cache和DB一致性]]></title>
    <url>%2F2018%2F09%2F25%2Fcache_db_consistency%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Cache Aside Pattern什么是 “Cache Aside Pattern”？答：旁路缓存方案的经验实践，这个实践又分读实践，写实践。 对于读请求先读 cache，再读 db如果，cache hit，则直接返回数据如果，cache miss，则访问 db，并将数据 set 回缓存 （1）先从 cache 中尝试 get 数据，结果 miss 了（2）再从 db 中读取数据，从库，读写分离（3）最后把数据 set 回 cache，方便下次读命中 对于写请求先操作数据库，再淘汰缓存（淘汰缓存，而不是更新缓存） 如上图：（1）第一步要操作数据库，第二步操作缓存（2）缓存，采用 delete 淘汰，而不是 set 更新 Cache Aside Pattern 为什么建议淘汰缓存，而不是更新缓存答：如果更新缓存，在并发写时，可能出现数据不一致。 如上图所示，如果采用 set 缓存。 在 1 和 2 两个并发写发生时，由于无法保证时序，此时不管先操作缓存还是先操作数据库，都可能出现：（1）请求 1 先操作数据库，请求 2 后操作数据库（2）请求 2 先 set 了缓存，请求 1 后 set 了缓存导致，数据库与缓存之间的数据不一致。所以，Cache Aside Pattern 建议，delete 缓存，而不是 set 缓存。 Cache Aside Pattern 为什么建议先操作数据库，再操作缓存？答：如果先操作缓存，在读写并发时，可能出现数据不一致。 如上图所示，如果先操作缓存。 在 1 和 2 并发读写发生时，由于无法保证时序，可能出现：（1）写请求淘汰了缓存（2）写请求操作了数据库（主从同步没有完成）（3）读请求读了缓存（cache miss）（4）读请求读了从库（读了一个旧数据）（5）读请求 set 回缓存（set 了一个旧数据）（6）数据库主从同步完成导致，数据库与缓存的数据不一致。 所以，Cache Aside Pattern 建议，先操作数据库，再操作缓存。 Cache Aside Pattern 方案存在什么问题？答：如果先操作数据库，再淘汰缓存，在原子性被破坏时：（1）修改数据库成功了（2）淘汰缓存失败了导致，数据库与缓存的数据不一致。 个人见解：这里个人觉得可以使用重试的方法，在淘汰缓存的时候，如果失败，则重试一定的次数。如果失败一定次数还不行，那就是其他原因了。比如说 redis 故障、内网出了问题。 关于这个问题，沈老师的解决方案是，使用先操作缓存（delete），再操作数据库。假如删除缓存成功，更新数据库失败了。缓存里没有数据，数据库里是之前的数据，数据没有不一致，对业务无影响。只是下一次读取，会多一次 cache miss。这里我觉得沈老师可能忽略了并发的问题，比如说以下情况：一个写请求过来，删除了缓存，准备更新数据库（还没更新完成）。然后一个读请求过来，缓存未命中，从数据库读取旧数据，再次放到缓存中，这时候，数据库更新完成了。此时的情况是，缓存中是旧数据，数据库里面是新数据，同样存在数据不一致的问题。如图： 主从同步延迟导致的缓存和数据不一致问题答：发生写请求后（不管是先操作 DB，还是先淘汰 Cache），在主从数据库同步完成之前，如果有读请求，都可能发生读 Cache Miss，读从库把旧数据存入缓存的情况。此时怎么办呢？ 数据库主从不一致先回顾下，无缓存时，数据库主从不一致问题。 如上图，发生的场景是，写后立刻读：（1）主库一个写请求（主从没同步完成）（2）从库接着一个读请求，读到了旧数据（3）最后，主从同步完成导致的结果是：主动同步完成之前，会读取到旧数据。 可以看到，主从不一致的影响时间很短，在主从同步完成后，就会读到新数据。 缓存与数据库不一致再看，引入缓存后，缓存和数据库不一致问题。 如上图，发生的场景也是，写后立刻读：（1+2）先一个写请求，淘汰缓存，写数据库 （3+4+5）接着立刻一个读请求，读缓存，cache miss，读从库，写缓存放入数据，以便后续的读能够 cache hit（主从同步没有完成，缓存中放入了旧数据） （6）最后，主从同步完成 导致的结果是：旧数据放入缓存，即使主从同步完成，后续仍然会从缓存一直读取到旧数据。 可以看到，加入缓存后，导致的不一致影响时间会很长，并且最终也不会达到一致。 问题分析可以看到，这里提到的缓存与数据库数据不一致，根本上是由数据库主从不一致引起的。当主库上发生写操作之后，从库 binlog 同步的时间间隔内，读请求，可能导致有旧数据入缓存。 思路：那能不能写操作记录下来，在主从时延的时间段内，读取修改过的数据的话，强制读主，并且更新缓存，这样子缓存内的数据就是最新。在主从时延过后，这部分数据继续读从库，从而继续利用从库提高读取能力。 不一致解决方案选择性读主 可以利用一个缓存记录必须读主的数据。 如上图，当写请求发生时：（1）写主库（2）将哪个库，哪个表，哪个主键三个信息拼装一个 key 设置到 cache 里，这条记录的超时时间，设置为 “主从同步时延”PS：key 的格式为 “db:table:PK”，假设主从延时为 1s，这个 key 的 cache 超时时间也为 1s。 如上图，当读请求发生时：这是要读哪个库，哪个表，哪个主键的数据呢，也将这三个信息拼装一个 key，到 cache 里去查询，如果，（1）cache 里有这个 key，说明 1s 内刚发生过写请求，数据库主从同步可能还没有完成，此时就应该去主库查询。并且把主库的数据 set 到缓存中，防止下一次 cahce miss。（2）cache 里没有这个 key，说明最近没有发生过写请求，此时就可以去从库查询 以此，保证读到的一定不是不一致的脏数据。 PS：如果系统可以接收短时间的不一致，建议建议定时更新缓存就可以了。避免系统过于复杂。]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>noodle</tag>
        <tag>DB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个轻量级的kcp会话实现]]></title>
    <url>%2F2018%2F08%2F09%2Fkcpp_intro%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[kcpp 轻量级的kcp会话实现-kcppkcpp真正实现了只需要包含一个头文件再随意写几行代码就可以用上kcp, 而无需烦心如何组织代码来适配kcp 只需包含 kcpp.h 这一个头文件即可 只需调用 KcpSession::Send 和 KcpSession::Recv 和 KcpSession::Update 即可完成UDP的链接状态管理、会话控制、 RUDP协议调度 Features single-header-only session implementation dynamic redundancy two-channel reliable unreliable kcpp Examples realtime-server : A realtime dedicated game server ( FPS / MOBA ). 一个实时的专用游戏服务器. realtime-server-ue4-demo : A UE4 State Synchronization demo for realtime-server. 为realtime-server而写的一个UE4状态同步demo, Video Preview 视频演示 TestKcppServer.cpp TestKcppClient.cpp kcpp Usagethe main loop was supposed as: 12345678910111213141516171819202122232425Game.Init()// kcpp initkcpp::KcpSession myKcpSess( KcpSession::RoleTypeE, std::bind(udp_output, _1, _2), std::bind(udp_input), std::bind(timer));while (!isGameOver) myKcpSess.Update() while (myKcpSess.Recv(data, len)) if (len &gt; 0) Game.HandleRecvData(data, len) else if (len &lt; 0) Game.HandleRecvError(len); if (myKcpSess.CheckCanSend()) myKcpSess.Send(data, len) else Game.HandleCanNotSendForNow() Game.Logic() Game.Render() The Recv/Send/Update functions of kcpp are guaranteed to be non-blocking.Please read TestKcppClient.cpp and TestKcppServer.cpp for some basic usage. kcp源码注释本项目还附了一个注释版的kcp源码 ikcp.h 和 ikcp.c， 算是另一种的 kcp详解, 方便自己学习也为大家更快的上手, 原始代码来自： https://github.com/skywind3000/kcp , 感谢 skywind3000 带来 这么短小精悍的好项目 注 : 项目中使用 tab 缩进且设置了tab = 2 space 几乎每个段落都有注释, 且关键数据结构还带有图解, 比如 : 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950...//// kcp发送的数据包设计了自己的包结构，包头一共24bytes，包含了一些必要的信息，具体内容和大小如下：// // |&lt;------------ 4 bytes ------------&gt;|// +--------+--------+--------+--------+// | conv | conv：Conversation, 会话序号，用于标识收发数据包是否一致// +--------+--------+--------+--------+ cmd: Command, 指令类型，代表这个Segment的类型// | cmd | frg | wnd | frg: Fragment, 分段序号，分段从大到小，0代表数据包接收完毕// +--------+--------+--------+--------+ wnd: Window, 窗口大小// | ts | ts: Timestamp, 发送的时间戳// +--------+--------+--------+--------+// | sn | sn: Sequence Number, Segment序号// +--------+--------+--------+--------+// | una | una: Unacknowledged, 当前未收到的序号，// +--------+--------+--------+--------+ 即代表这个序号之前的包均收到// | len | len: Length, 后续数据的长度// +--------+--------+--------+--------+//...//---------------------------------------------------------------------// ...// rcv_queue 接收消息的队列, rcv_queue的数据是连续的，rcv_buf可能是间隔的// nrcv_que // 接收队列rcv_queue中的Segment数量, 需要小于 rcv_wnd// rcv_queue 如下图所示// +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+// ... | 2 | 3 | 4 | ............................................... // +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+// ^ ^ ^ // | | | // rcv_nxt rcv_nxt + nrcv_que rcv_nxt + rcv_wnd //// snd_buf 发送消息的缓存// snd_buf 如下图所示// +---+---+---+---+---+---+---+---+---+---+---+---+---+// ... | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | ...........// +---+---+---+---+---+---+---+---+---+---+---+---+---+// ^ ^ ^// | | |// snd_una snd_nxt snd_una + snd_wnd ////// rcv_buf 接收消息的缓存// rcv_buf 如下图所示, rcv_queue的数据是连续的，rcv_buf可能是间隔的// +---+---+---+---+---+---+---+---+---+---+---+---+---+// ... | 2 | 4 | 6 | 7 | 8 | 9 | ...........// +---+---+---+---+---+---+---+---+---+---+---+---+---+ //... 在注释的过程中， 除了少量空格和换行以及一处有无符号比较的调整(为保证高警告级别可编译过)外 :if ((IUINT32)count &gt;= IKCP_WND_RCV) return -2;没有对原始代码进行任何其他改动， 最大程度地保证了代码的“原汁原味”。 QQ群因为 KCP 官方群已经满了, 可以加群 496687140]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>KCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件描述符FD与Inode]]></title>
    <url>%2F2018%2F08%2F06%2Ffd_inode%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[FD 文件描述符 Linux 系统中，把一切都看做是文件，当进程打开现有文件或创建新文件时，内核向进程返回一个文件描述符，文件描述符就是内核为了高效管理已被打开的文件所创建的索引，用来指向被打开的文件，所有执行 I/O 操作的系统调用都会通过文件描述符。 文件描述符、文件、进程间的关系我们可以通过 linux 的几个基本的 I/O 操作函数来理解什么是文件描述符。 123456fd = open(pathname, flags, mode)// 返回了该文件的fdrlen = read(fd, buf, count)// IO操作均需要传入该文件的fd值wlen = write(fd, buf, count)status = close(fd) 每当进程用open（）函数打开一个文件，内核便会返回该文件的文件描述符（一个非负的整形值），此后所有对该文件的操作，都会以返回的 fd 文件描述符为参数。 文件描述符可以理解为进程文件描述表这个表的索引，或者把文件描述表看做一个数组的话，文件描述符可以看做是数组的下标。当需要进行 I/O 操作的时候，会传入 fd 作为参数，先从进程文件描述符表查找该 fd 对应的那个条目，取出对应的那个已经打开的文件的句柄，根据文件句柄指向，去系统 fd 表中查找到该文件指向的 inode，从而定位到该文件的真正位置，从而进行 I/O 操作。 我们仔细看这张图, 在进程A中，文件描述符1和30都指向了同一个打开的文件句柄（标号23）。这可能是通过调用dup()、dup2()、fcntl()或者对同一个文件多次调用了open()函数而形成的。 进程A的文件描述符2和进程B的文件描述符2都指向了同一个打开的文件句柄（标号73）。这种情形可能是在调用fork()后出现的（即，进程A、B是父子进程关系），或者当某进程通过UNIX域套接字将一个打开的文件描述符传递给另一个进程时，也会发生。再者是不同的进程独自去调用open函数打开了同一个文件，此时进程内部的描述符正好分配到与其他进程打开该文件的描述符一样。此外，进程A的描述符0和进程B的描述符3分别指向不同的打开文件句柄，但这些句柄均指向i-node表的相同条目（1976），换言之，指向同一个文件。发生这种情况是因为每个进程各自对同一个文件发起了open()调用。同一个进程两次打开同一个文件，也会发生类似情况。 小结: 由于进程级文件描述符表的存在，不同的进程中会出现相同的文件描述符，它们可能指向同一个文件，也可能指向不同的文件 两个不同的文件描述符，若指向同一个打开文件句柄，将共享同一文件偏移量。因此，如果通过其中一个文件描述符来修改文件偏移量（由调用read()、write()或lseek()所致），那么从另一个描述符中也会观察到变化，无论这两个文件描述符是否属于不同进程，还是同一个进程，情况都是如此。 要获取和修改打开的文件标志（例如：O_APPEND、O_NONBLOCK和O_ASYNC），可执行fcntl()的F_GETFL和F_SETFL操作，其对作用域的约束与上一条颇为类似。 文件描述符标志（即，close-on-exec）为进程和文件描述符所私有。对这一标志的修改将不会影响同一进程或不同进程中的其他文件描述符 每个文件描述符会与一个打开的文件相对应 不同的文件描述符也可能指向同一个文件 相同的文件可以被不同的进程打开，也可以在同一个进程被多次打开 系统为维护文件描述符建立了三个表 进程级的文件描述符表 系统级的文件描述符表 文件系统的 i-node 表 (inode 见下文) 进程级别的文件描述表linux 内核会为每一个进程创建一个task_truct结构体来维护进程信息，称之为 进程描述符，task_truct结构体中的指针指向一个名称为file_struct的结构体，该结构体即 进程级别的文件描述表。 1struct files_struct *files 它的每一个条目记录的是单个文件描述符的相关信息 fd 控制标志，前内核仅定义了一个，即 close-on-exec 文件描述符所打开的文件句柄的引用【注 2】 [注释 2]：文件句柄这里可以理解为文件名，或者文件的全路径名，因为 linux 文件系统文件名和文件是独立的，以此与 inode 区分 系统级别的文件描述符表内核对系统中所有打开的文件维护了一个描述符表，也被称之为 【打开文件表】，表格中的每一项被称之为 【打开文件句柄】，一个【打开文件句柄】 描述了一个打开文件的全部信息。主要包括： 当前文件偏移量（调用 read() 和 write() 时更新，或使用 lseek() 直接修改） 打开文件时所使用的状态标识（即，open() 的 flags 参数） 文件访问模式（如调用 open() 时所设置的只读模式、只写模式或读写模式） 与信号驱动相关的设置 对该文件 i-node 对象的引用 文件类型（例如：常规文件、套接字或 FIFO）和访问权限 一个指针，指向该文件所持有的锁列表 文件的各种属性，包括文件大小以及与不同类型操作相关的时间戳 Inode表每个文件系统会为存储于其上的所有文件 (包括目录) 维护一个 i-node 表，单个 i-node 包含以下信息： 文件类型 (file type)，可以是常规文件、目录、套接字或 FIFO 访问权限 文件锁列表 (file locks) 文件大小 … i-node 存储在磁盘设备上，内核在内存中维护了一个副本，这里的 i-node 表为后者。副本除了原有信息，还包括：引用计数 (从打开文件描述体)、所在设备号以及一些临时属性，例如文件锁。 注：进程 A 的 fd 表中，左边 fd0，fd1，fd2… 就是各个文件描述符，它是 fd 表的索引，fd 不是表里那个 fd flags！这里不要搞混淆了，fd flags 目前只有一个取值。 在进程 A 中，文件描述符 1 和 30 都指向了同一个打开的文件句柄（标号 23）。这可能是通过调用 dup()、dup2()、fcntl() 或者对同一个文件多次调用了 open() 函数而形成的。 dup（），也称之为文件描述符复制函数，在某些场景下非常有用，比如：标准输入 / 输出重定向。在 shell 下，完成这个操作非常简单，大部分人都会，但是极少人思考过背后的原理。大概描述一下需要的几个步骤，以标准输出 (文件描述符为 1) 重定向为例： 打开目标文件，返回文件描述符 n； 关闭文件描述符 1； 调用 dup 将文件描述符 n 复制到 1； 关闭文件描述符 n； 进程 A 的文件描述符 2 和进程 B 的文件描述符 2 都指向了同一个打开的文件句柄（标号 73）。这种情形可能是在调用 fork() 后出现的（即，进程 A、B 是父子进程关系）【注 3】，或者当某进程通过 UNIX 域套接字将一个打开的文件描述符传递给另一个进程时，也会发生。再者是不同的进程独自去调用 open 函数打开了同一个文件，此时进程内部的描述符正好分配到与其他进程打开该文件的描述符一样。 注 3： 子进程会继承父进程的文件描述符表，也就是子进程继承父进程打开的文件 这句话的由来。 此外，进程 A 的描述符 0 和进程 B 的描述符 3 分别指向不同的打开文件句柄，但这些句柄均指向 i-node 表的相同条目（1976），换言之，指向同一个文件。发生这种情况是因为每个进程各自对同一个文件发起了 open() 调用。同一个进程两次打开同一个文件，也会发生类似情况。 文件描述符数目限制 有资源的地方就有战争，“文件描述符”也是一种资源，系统中的每个进程都需要有 “文件描述符” 才能进行改变世界的宏图霸业。世界需要秩序，于是就有了 “文件描述符限制” 的规定。 如下表： 永久修改用户级限制时有三种设置类型： soft 指的是当前系统生效的设置值 hard 指的是系统中所能设定的最大值 “-” 指的是同时设置了 soft 和 hard 的值 inode理解 inode，要从文件储存说起。文件储存在硬盘上，硬盘的最小存储单位叫做” 扇区”（Sector）。每个扇区储存 512 字节（相当于 0.5KB）。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个” 块”（block）。这种由多个扇区组成的” 块”，是文件存取的最小单位。” 块” 的大小，最常见的是 4KB，即连续八个 sector 组成一个 block。 文件数据都储存在” 块” 中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做 inode，中文译名为” 索引节点”。 每一个文件都有对应的 inode，里面包含了与该文件有关的一些信息。 inode 的内容inode 包含文件的元信息，具体来说有以下内容： 文件的字节数 文件拥有者的 User ID 文件的 Group ID 文件的读、写、执行权限 文件的时间戳，共有三个：ctime 指 inode 上一次变动的时间，mtime 指文件内容上一次变动的时间，atime 指文件上一次打开的时间。 链接数，即有多少文件名指向这个 inode 文件数据 block 的位置 可以用stat命令，查看某个文件的 inode 信息：stat 233.txt 总之，除了文件名以外的所有文件信息，都存在 inode 之中。至于为什么没有文件名，下文会有详细解释。 inode 的大小inode 也会消耗硬盘空间，所以硬盘格式化的时候，操作系统自动将硬盘分成两个区域。一个是数据区，存放文件数据；另一个是 inode 区（inode table），存放 inode 所包含的信息。 每个 inode 节点的大小，一般是 128 字节或 256 字节。inode 节点的总数，在格式化时就给定，一般是每 1KB 或每 2KB 就设置一个 inode。假定在一块 1GB 的硬盘中，每个 inode 节点的大小为 128 字节，每 1KB 就设置一个 inode，那么 inode table 的大小就会达到 128MB，占整块硬盘的 12.8%。 查看每个硬盘分区的 inode 总数和已经使用的数量，可以使用 df 命令。df -i查看每个 inode 节点的大小，可以用如下命令：sudo dumpe2fs -h /dev/hda | grep &quot;Inode size&quot;由于每个文件都必须有一个 inode，因此有可能发生 inode 已经用光，但是硬盘还未存满的情况。这时，就无法在硬盘上创建新文件。 inode 号码每个 inode 都有一个号码，操作系统用 inode 号码来识别不同的文件。 这里值得重复一遍，Unix/Linux 系统内部不使用文件名，而使用 inode 号码来识别文件。对于系统来说，文件名只是 inode 号码便于识别的别称或者绰号。 表面上，用户通过文件名，打开文件。实际上，系统内部这个过程分成三步：首先，系统找到这个文件名对应的 inode 号码；其次，通过 inode 号码，获取 inode 信息；最后，根据 inode 信息，找到文件数据所在的 block，读出数据。 使用 ls -i 命令，可以看到文件名对应的 inode 号码：ls -i 233.txt 目录文件Unix/Linux 系统中，目录（directory）也是一种文件。打开目录，实际上就是打开目录文件。目录文件的结构非常简单，就是一系列目录项（dirent）的列表。每个目录项，由两部分组成: 所包含文件的文件名， 该文件名对应的 inode 号码。 ls 命令只列出目录文件中的所有文件名：ls /etcls -i 命令列出整个目录文件，即文件名和 inode 号码：ls -i /etc如果要查看文件的详细信息，就必须根据 inode 号码，访问 inode 节点，读取信息。ls -l 命令列出文件的详细信息。ls -l /etc理解了上面这些知识，就能理解目录的权限。目录文件的读权限（r）和写权限（w），都是针对目录文件本身。由于目录文件内只有文件名和 inode 号码，所以如果只有读权限，只能获取文件名，无法获取其他信息，因为其他信息都储存在 inode 节点中，而读取 inode 节点内的信息需要目录文件的执行权限（x）。 链接 硬链接一般情况下，文件名和 inode 号码是” 一一对应” 关系，每个 inode 号码对应一个文件名。但是，Unix/Linux 系统允许，多个文件名指向同一个 inode 号码。 这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为” 硬链接”（hard link）。ln 命令可以创建硬链接:ln 源文件 目标文件运行上面这条命令以后，源文件与目标文件的 inode 号码相同，都指向同一个 inode。 inode 信息中有一项叫做i_nlink”链接数”，记录指向该 inode 的文件名总数，这时就会增加 1。 反过来，删除一个文件名，就会使得 inode 节点中的i_nlink”链接数” 减 1。当这个值减到 0，表明没有文件名指向这个 inode，系统就会回收这个 inode 号码，以及其所对应 block 区域。 这里顺便说一下目录文件的i_nlink”链接数”。创建目录时，默认会生成两个目录项：”.” 和”..”。前者的 inode 号码就是当前目录的 inode 号码，等同于当前目录的” 硬链接”；后者的 inode 号码就是当前目录的父目录的 inode 号码，等同于父目录的” 硬链接”。所以，任何一个目录的” 硬链接” 总数，总是等于 2 加上它的子目录总数（含隐藏目录）。 简单说，硬链接就是一个 inode 号对应多个文件名。就是同一个文件使用了多个别名（上图中 hard link 就是 file 的一个别名，他们有共同的 inode）。 由于硬链接是有着相同 inode 号仅文件名不同的文件，因此硬链接存在以下几点特性： 文件有相同的 inode 及 data block； 只能对已存在的文件进行创建； 不能交叉文件系统进行硬链接的创建； 不能对目录进行创建，只可对文件创建； 删除一个硬链接文件并不影响其他有相同 inode 号的文件, 只是相应的链接计数器（link count)减1 软链接(又称符号链接，即 soft link 或 symbolic link） 软链接与硬链接不同，若文件用户数据块中存放的内容是另一文件的路径名的指向，则该文件就是软连接。软链接就是一个普通文件，只是数据块内容有点特殊。软链接有着自己的 inode 号以及用户数据块。（见图2）软连接可以指向目录，而且软连接所指向的目录可以位于不同的文件系统中。 ln -s 命令可以创建软链接。 ln -s 源文文件或目录 目标文件或目录 软链接特性： 软链接有自己的文件属性及权限等； 可对不存在的文件或目录创建软链接； 软链接可交叉文件系统； 软链接可对文件或目录创建； 创建软链接时，链接计数 i_nlink 不会增加； 删除软链接并不影响被指向的文件，但若被指向的原文件被删除，则相关软连接被称为死链接或悬挂的软链接（即 dangling link，若被指向路径文件被重新创建，死链接可恢复为正常的软链接）。 文件 A 和文件 B 的 inode 号码虽然不一样，但是文件 A 的内容是文件 B 的路径。读取文件 A 时，系统会自动将访问者导向文件 B。因此，无论打开哪一个文件，最终读取的都是文件 B。这时，文件 A 就称为文件 B 的” 软链接”（soft link）或者” 符号链接（symbolic link）。 这意味着，文件 A 依赖于文件 B 而存在，如果删除了文件 B，打开文件 A 就会报错：”No such file or directory”。这是软链接与硬链接最大的不同：文件 A 指向文件 B 的文件名，而不是文件 B 的 inode 号码，文件 B 的 inode” 链接数” 不会因此发生变化。 inode 的特殊作用由于 inode 号码与文件名分离，这种机制导致了一些 Unix/Linux 系统特有的现象。 有时，文件名包含特殊字符，无法正常删除。这时，直接删除 inode 节点，就能起到删除文件的作用。 移动文件或重命名文件，只是改变文件名，不影响 inode 号码。 打开一个文件以后，系统就以 inode 号码来识别这个文件，不再考虑文件名。因此，通常来说，系统无法从 inode 号码得知文件名。 第 3 点使得软件更新变得简单，可以在不关闭软件的情况下进行更新，不需要重启。因为系统通过 inode 号码，识别运行中的文件，不通过文件名。更新的时候，新版文件以同样的文件名，生成一个新的 inode，不会影响到运行中的文件。等到下一次运行这个软件的时候，文件名就自动指向新版文件，旧版文件的 inode 则被回收。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[noodle_plan]]></title>
    <url>%2F2018%2F08%2F06%2Fnoodle_plan%2F</url>

    <encrypted>1</encrypted>

    <content type="text"><![CDATA[pending_fini 看看其他内存库的优化实现,,如tcmalloc啥的 算法类 线段树 字典树 lc高频 热门 kmp lc 174 地下城游戏问题 矩阵相关的算法题 lfu, 代码太长, 而且用到了不常用的数据结构, 目测不太会考 微信社招面经的面/笔试题 字节跳动百度拼多多面笔试题 . . . Linux定时器实现原理时间轮定时器-低分辨率实现Linux 2.6.16之前，内核只支持低精度时钟，内核定时器的工作方式： 系统启动后，会读取时钟源设备(RTC,HPET，PIT…)，初始化当前系统时间。 内核会根据HZ(系统定时器频率，节拍率)参数值，设置时钟事件设备，启动tick(节拍)中断。HZ表示1秒种产生多少个时钟硬件中断，tick就表示连续两个中断的间隔时间。 设置时钟事件设备后，时钟事件设备会定时产生一个tick中断，触发时钟中断处理函数，更新系统时钟,并检测timer wheel，进行超时事件的处理。 在上面工作方式下，Linux 2.6.16 之前，内核软件定时器采用timer wheel多级时间轮的实现机制，维护操作系统的所有定时事件。timer wheel的触发是基于系统tick周期性中断。所以说这之前，linux只能支持ms级别的时钟，随着时钟源硬件设备的精度提高和软件高精度计时的需求，有了高精度时钟的内核设计。 所谓低分辨率定时器，是指这种定时器的计时单位基于jiffies值的计数，也就是说，它的精度只有1HZ，假如你的内核配置的HZ是1000，那意味着系统中的低分辨率定时器的精度就是1ms。早期的内核版本中，内核并不支持高精度定时器，理所当然只能使用这种低分辨率定时器, 后来随着时钟源硬件设备的精度提高和软件高精度计时的需求，才有了高精度时钟的内核设计 时间轮算法思想多级时间轮, 插入/删除/execute复杂度都O(1) 算法思想: 把定时器分为 5 个桶，每桶的粒度分别表示为：1 jiffies，256 jiffies，256*64 jiffies，256*64*64 jiffies，256*64*64*64 jiffies，每桶bucket中的slot的数量分别为：256，64，64，64，64，能表示的范围为 2^32 这好几个bucket, 其中一个bucket叫near是差不多要触发的定时器范围是[0, 0x100), 和几个定时时长比较久的bucket: [0x100, 0x4000)以及[0x4000, 0x100000)以及[0x100000, 0x4000000) tick: 每次tick都检查jiffies是否已经又经过一轮 TVR_MASK(255) 了, 经过了一轮index就又等于0, 然后就去后面的bucket[0][INDEX(0)]里去拿定时器迁移到near里(这个INDEX(0)宏其实是拿到jiffies_的第9到14位的值), 如果INDEX(0)也等于0, 则说明bucket[0]也轮转迁移了一圈了, 接着就需要去bucket[1]里拿定时器迁移到bucket[0]里, 后面INDEX(1)和INDEX(2)对应的bucket调整都以此类推, 这就跟水表一样, 小表转一圈需要调整中表, 中表转一圈则要调整大表差不多 为啥可以直接把这个bucket[0][INDEX(0)]里的定时器直接迁移到near里呢? 因为在插入的时候就是这么哈希的, 举个比较简单的不准确但是可以说明原理的例子, 假如 near里是存最近60秒过期的定时器, bucket[0][0]存的是60到120过期的, bucket[0][1]的是120到180过期的, 则jiffies等于60的时候就要把bucket[0][0]迁移到near里, jiffies等于120的时候bucket[0][1]迁移到near里… 类似于linux的时间轮实现: 假设curr_time=0x12345678，那么下一个检查的时刻为0x12345679。如果tv1.bucket[0x79]上链表非空，则下一个检查时刻tv1.bucket[0x79]上的定时器节点超时。如果curr_time到了0x12345700，低8位为空，说明有进位产生，这时移出9～14位对应的定时器链表(即正好对应着tv2轮)，把tv2.bucket[此时9-14位的值]所对应的timer链表迁移到tv1来，这就完成了一次进位迁移操作。同样地，当curr_time的第9-14位为0时，这表明tv2轮对tv3轮有进位发生，将curr_time第14-19位的值作为下标，移出tv3中对应的定时器链表，然后将它们迁移到tv2去。tv4,tv5依次类推。之所以能够根据curr_time来检查超时链，是因为tv1~tv5轮的度量范围正好依次覆盖了整型的32位：tv1(1-8位)，tv2(9-14位)，tv3(15-20位)，tv4(21-26位)，tv5(27-32位)；而curr_time计数的递增中，低位向高位的进位正是低级时间轮转圈带动高级时间轮走动的过程。 插入: 有好几个bucket, 然后用类似于取模哈希的思想先判断还有多久过期的区间, 然后根据过期时间expire取他相应的位放入相应的桶里的某个slot的定时器链表TimerList里即可, 参考下方代码, 如果expire已经超过了桶能表示的最大值MAX_TVAL了, 那就直接对MAX_TVAL+当前时间哈希放在最后一个桶的某个槽里, tick的时候会逐渐把他往前迁移的 excute: near_ 里面的定时器因为都已经在 addTimerNode 根据expire哈希安插好了, 所以这里 jiffies_ &amp; TVR_MASK 出来的index是几, 那就直接从near_里取出来执行就完事了,见下方代码 删除: 因为插入的时候还专门另外有个哈希表来保存定时器id和定时器的映射关系, 所有删除的时候就直接根据传入的定时器id来找到定时器本身然后把他标记为已删除, 然后在excute的时候会找到near_[index]这个定时器链表TimerList移除 删除: 惰性删除, 只是标记相关node为被canceled, 然后excute的时候再freeNode tickless: 不嫌麻烦还可以每次从 timer 集合里面选择最先要超时的事件，计算还有多长时间就会超时，作为 select wait 的值，每次都不一样，每次都基本精确，同时不会占用多余 cpu，这叫 tickless，Linux 的 3.x以上版本也支持 tickless 的模式来驱动各种系统级时钟，号称更省电更精确，不过需要你手动打开，FreeBSD 9 以后也引入了 tickless。 时间轮有什么缺点虽然大部分时间里，时间轮可以实现O(1)时间复杂度，但是当有进位发生时，不可预测的O(N)定时器级联迁移时间，这对于低分辨率定时器来说问题不大，可是它大大地影响了定时器的精度； 时间轮核心代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697void WheelTimer::addTimerNode(TimerNode* node)&#123; int64_t expires = node-&gt;expire; uint64_t idx = (uint64_t)(expires - jiffies_); TimerList* list = nullptr; if (idx &lt; TVR_SIZE) // [0, 0x100) &#123; int i = expires &amp; TVR_MASK; // 因为只关心后8位(即TVR_BITS=8) list = &amp;near_[i]; &#125; else if(idx &lt; (1 &lt;&lt; (TVR_BITS + TVN_BITS))) // [0x100, 0x4000) &#123; // 因为不关心后8位(即TVR_BITS=8)的数, 所以直接 expires &gt;&gt; TVR_BITS 了 // 又因为 TimerList buckets_[WHEEL_BUCKETS][TVN_SIZE] 的第二维为 TVN_SIZE, 所以要 &amp; TVN_MASK int i = (expires &gt;&gt; TVR_BITS) &amp; TVN_MASK; list = &amp;buckets_[0][i]; &#125; else if(idx &lt; (1 &lt;&lt; (TVR_BITS + 2 * TVN_BITS))) // [0x4000, 0x100000) &#123; ...&#125;// #define INDEX(N) ( ( jiffies_ &gt;&gt; (8 + (N) * 6) ) &amp; 1111 11)#define INDEX(N) ((jiffies_ &gt;&gt; (TVR_BITS + (N) * TVN_BITS)) &amp; TVN_MASK)// cascades all vectors and executes all expired timerint WheelTimer::tick()&#123; int fired = 0; // 每次tick都检查是否已经又经过一轮 TVR_MASK(255) 了, // 经过了一轮index就又等于0, 然后就去后面的bucket里找是否有需要调整到near的定时器 // 就跟水表一样, 小表转一圈需要调整中表, 中表转一圈则要调整大表 int index = jiffies_ &amp; TVR_MASK; if(index == 0) // cascade timers &#123; if(cascade(0, INDEX(0)) &amp;&amp; cascade(1, INDEX(1)) &amp;&amp; cascade(2, INDEX(2)) ) cascade(3, INDEX(3)); &#125; jiffies_++; fired += execute(index); return fired;&#125;int WheelTimer::execute()&#123; int fired = 0; // near 里面的定时器因为都已经在 addTimerNode 根据expire里哈希安插好了, // 所以这里 jiffies_ &amp; TVR_MASK 出来的index是几, 那就直接从near_里取出来执行就完事了 int index = jiffies_ &amp; TVR_MASK; TimerList expired; near_[index].swap(expired); // swap list for (auto node : expired) &#123; if (!node-&gt;canceled &amp;&amp; node-&gt;cb) &#123; //printf("wheel node %d triggered at %lld of jiffies %lld\n", node-&gt;id, current_, jiffies_); node-&gt;cb(); size_--; fired++; &#125; ref_.erase(node-&gt;id); freeNode(node); &#125; return fired; // cascade all the timers at bucket of index up one level bool WheelTimer::cascade(int bucket, int index)&#123; // swap list TimerList list; buckets_[bucket][index].swap(list); for(auto&amp; node : list)&#123; if(node-&gt;id &gt; 0)&#123; addTimerNode(node); // 把各个定时器往前推, 比如条件达成就挪到this-&gt;near_里去 &#125; &#125; // 如INDEX(N), 当N=0, 因为进入本函数之前, jiffies_ &amp; TVR_MASK 是为 0 的, // 说明 jiffies_ 8位以前的高位绝对有不为0的位, // jiffies右移8位然后跟TVN_MASK(即63, 即二进制111111, 六位)做且操作之后的结果 index == 0 , // 则说明jiffies大于N=0的这个bucket区间了, 还需要调整下一个区间(即 N+1 这个bucket区间), // 就跟水表一样, 小表转一圈需要调整中表, 中表转一圈则要调整大表 return index == 0; &#125; // Do lazy cancellation, so we can effectively use vector as container of timer nodes bool WheelTimer::Cancel(int id) &#123; TimerNode* node = ref_[id]; if (node != nullptr) &#123; node-&gt;canceled = true; size_--; return true; &#125; return false; &#125; 红黑树定时器-高精度实现而随着内核的不断演进，大牛们已经对这种低分辨率定时器的精度不再满足，而且，硬件也在不断地发展，系统中的定时器硬件的精度也越来越高，这也给高分辨率定时器的出现创造了条件。内核从2.6.16开始加入了高精度定时器架构。它可以为我们提供纳秒级的定时精度，以满足对精确时间有迫切需求的应用程序或内核驱动，例如多媒体应用，音频设备的驱动程序等等。 当前内核同时存在新旧timer wheel 和hrtimer两套timer的实现，内核启动后会进行从低精度模式到高精度时钟模式的切换. 与时间轮的区别Linux 2.6.16 ，内核支持了高精度的时钟，内核采用新的定时器hrtimer，其实现逻辑和Linux 2.6.16 之前定时器逻辑区别： hrtimer采用红黑树进行高精度定时器的管理，而不是时间轮； 高精度时钟定时器不在依赖系统的tick中断，而是基于时钟硬件的事件触发。 旧内核的定时器实现依赖于系统定时器硬件定期的tick，基于该tick，内核会扫描timer wheel处理超时事件，会更新jiffies，wall time(墙上时间，现实时间)，process的使用时间等等工作。 新的内核不再会直接支持周期性的tick，新内核定时器框架采用了基于高精度时钟硬件的下次中断触发，而不是以前的周期性触发。新内核实现了hrtimer(high resolution timer)于事件触发。 hrtimer的工作原理我们知道，低分辨率定时器使用5个链表数组来组织timer_list结构，形成了著名的时间轮概念，对于高分辨率定时器，我们期望组织它们的数据结构至少具备以下条件： 稳定而且快速的查找能力； 快速地插入和删除定时器的能力； 排序功能； 内核的开发者考察了多种数据结构，例如基数树、哈希表等等，最终他们选择了红黑树（rbtree）来组织hrtimer，红黑树已经以库的形式存在于内核中，并被成功地使用在内存管理子系统和文件系统中，随着系统的运行，hrtimer不停地被创建和销毁，新的hrtimer按顺序被插入到红黑树中，树的最左边的节点就是最快到期的定时器，内核用一个hrtimer结构来表示一个高精度定时器 通过将高精度时钟硬件的下次中断触发时间设置为红黑树中最早到期的Timer 的时间，时钟到期后从红黑树中得到下一个 Timer 的到期时间，并设置硬件，如此循环反复。 如何在高精度模式下模拟tick当系统切换到高精度模式后，tick_device被高精度定时器系统接管，不再定期地产生tick事件，我们知道，到目前的版本为止（V3.4），内核还没有彻底废除jiffies机制，系统还是依赖定期到来的tick事件，供进程调度系统和时间更新等操作，大量存在的低精度定时器也仍然依赖于jiffies的计数，所以，尽管tick_device被接管，高精度定时器系统还是要想办法继续提供定期的tick事件。为了达到这一目的，内核使用了一个取巧的办法：既然高精度模式已经启用，可以定义一个hrtimer，把它的到期时间设定为一个jiffy的时间，当这个hrtimer到期时，在这个hrtimer的到期回调函数中，进行和原来的tick_device同样的操作，然后把该hrtimer的到期时间顺延一个jiffy周期，如此反复循环，完美地模拟了原有tick_device的功能。 Linux文件系统详细的可以查看本博客的这篇文章哈文件描述符FD与Inodefd数目大小的限制可以改变, 参考 文件描述符限制 系统目录结构 Linux 系统目录结构 登录系统后，在当前命令窗口下输入命令： ls / 你会看到如下图所示: 树状目录结构： 以下是对这些目录的解释： /bin： bin 是 Binaries (二进制文件) 的缩写, 这个目录存放着最经常使用的命令。 以下是对这些目录的解释： /bin： bin 是 Binaries (二进制文件) 的缩写, 这个目录存放着最经常使用的命令。 /boot： 这里存放的是启动 Linux 时使用的一些核心文件，包括一些连接文件以及镜像文件。 /dev ： dev 是 Device(设备) 的缩写, 该目录下存放的是 Linux 的外部设备，在 Linux 中访问设备的方式和访问文件的方式是相同的。 /etc： etc 是 Etcetera(等等) 的缩写, 这个目录用来存放所有的系统管理所需要的配置文件和子目录。 /home： 用户的主目录，在 Linux 中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的，如上图中的 alice、bob 和 eve。 /lib： lib 是 Library(库) 的缩写这个目录里存放着系统最基本的动态连接共享库，其作用类似于 Windows 里的 DLL 文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found： 这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /media： linux 系统会自动识别一些设备，例如 U 盘、光驱等等，当识别后，Linux 会把识别的设备挂载到这个目录下。 /mnt： 系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在 /mnt/ 上，然后进入该目录就可以查看光驱里的内容了。 /opt： opt 是 optional(可选) 的缩写，这是给主机额外安装软件所摆放的目录。比如你安装一个 ORACLE 数据库则就可以放到这个目录下。默认是空的。 /proc： proc 是 Processes(进程) 的缩写，/proc 是一种伪文件系统（也即虚拟文件系统），存储的是当前内核运行状态的一系列特殊文件，这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。 这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的 ping 命令，使别人无法 ping 你的机器： 1echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all /root： 该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin： s 就是 Super User 的意思，是 Superuser Binaries (超级用户的二进制文件) 的缩写，这里存放的是系统管理员使用的系统管理程序。 /selinux： 这个目录是 Redhat/CentOS 所特有的目录，Selinux 是一个安全机制，类似于 windows 的防火墙，但是这套机制比较复杂，这个目录就是存放 selinux 相关的文件的。 /srv： 该目录存放一些服务启动之后需要提取的数据。 /sys： 这是 Linux2.6 内核的一个很大的变化。该目录下安装了 2.6 内核中新出现的一个文件系统 sysfs 。 sysfs 文件系统集成了下面 3 种文件系统的信息：针对进程信息的 proc 文件系统、针对设备的 devfs 文件系统以及针对伪终端的 devpts 文件系统。 该文件系统是内核设备树的一个直观反映。 当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。 /tmp： tmp 是 temporary(临时) 的缩写这个目录是用来存放一些临时文件的。 /usr： usr 是 unix shared resources(共享资源) 的缩写，这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于 windows 下的 program files 目录。 /usr/bin： 系统用户使用的应用程序。 /usr/sbin： 超级用户使用的比较高级的管理程序和系统守护程序。 /usr/src： 内核源代码默认的放置目录。 /var： var 是 variable(变量) 的缩写，这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 /run： 是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。 在 Linux 系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。/etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。/bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在 /bin/ls 目录下的。值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除 root 外的通用户），而 / sbin, /usr/sbin 则是给 root 使用的指令。/var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在 /var/log 目录下，另外 mail 的预设放置也是在这里。 inode硬盘的最小存储单位是扇区(Sector)，块(block)由多个扇区组成。文件数据存储在块中。块的最常见的大小是 4kb，约为 8 个连续的扇区组成（每个扇区存储 512 字节）。一个文件可能会占用多个 block，但是一个块只能存放一个文件。 虽然，我们将文件存储在了块(block)中，但是我们还需要一个空间来存储文件的 元信息 metadata ：如某个文件被分成几块、每一块在的地址、文件拥有者，创建时间，权限，大小等。这种 存储文件元信息的区域就叫 inode，译为索引节点：i（index）+node。 每个文件都有一个 inode，存储文件的元信息。 可以使用 stat 命令可以查看文件的 inode 信息。每个 inode 都有一个号码，Linux/Unix 操作系统不使用文件名来区分文件，而是使用 inode 号码区分不同的文件。 简单来说：inode 就是用来维护某个文件被分成几块、每一块在的地址、文件拥有者，创建时间，权限，大小等信息。简单总结一下： inode ：记录文件的属性信息，可以使用 stat 命令查看 inode 信息。 block ：实际文件的内容，如果一个文件大于一个块时候，那么将占用多个 block，但是一个块只能存放一个文件。（因为数据是由 inode 指向的，如果有两个文件的数据存放在同一个块中，就会乱套了） 软链接与硬链接详细的可参考: https://blog.csdn.net/yangxjsun/article/details/79681229 硬链接普通链接一般就是指硬链接, 硬链接是新的目录条目，其引用系统中的现有文件。文件系统中的每一文件默认具有一个硬链接。为节省空间，可以不复制文件，而创建引用同一文件的新硬链接。新硬链接如果在与现有硬链接相同的目录中创建，则需要有不同的文件名，否则需要在不同的目录中。指向同一文件的所有硬链接具有相同的权限、连接数、用户/组所有权、时间戳以及文件内容。指向同一文件内容的硬链接需要在相同的文件系统中。简单说，硬链接就是一个 inode 号对应多个文件名。就是同一个文件使用了多个别名（上图中 hard link 就是 file 的一个别名，他们有共同的 inode）。 由于硬链接是有着相同 inode 号仅文件名不同的文件，因此硬链接存在以下几点特性： 文件有相同的 inode 及 data block； 只能对已存在的文件进行创建； 不能交叉文件系统进行硬链接的创建； 不能对目录进行创建，只可对文件创建； 删除一个硬链接文件并不影响其他有相同 inode 号的文件, 只是相应的链接计数器（link count)减1 软链接(又称符号链接，即 soft link 或 symbolic link） 软链接与硬链接不同，若文件用户数据块中存放的内容是另一文件的路径名的指向，则该文件就是软连接。软链接就是一个普通文件，只是数据块内容有点特殊。软链接有着自己的 inode 号以及用户数据块。（见图2）软连接可以指向目录，而且软连接所指向的目录可以位于不同的文件系统中。 软链接特性： 软链接有自己的文件属性及权限等； 可对不存在的文件或目录创建软链接； 软链接可交叉文件系统； 软链接可对文件或目录创建； 创建软链接时，链接计数 i_nlink 不会增加； 删除软链接并不影响被指向的文件，但若被指向的原文件被删除，则相关软连接被称为死链接或悬挂的软链接（即 dangling link，若被指向路径文件被重新创建，死链接可恢复为正常的软链接）。 Linux 为什么多进程能够读写正在删除的文件参考进程表_文件表_inode_vnode Linux中多进程环境下，打开同一个文件，当一个进程进行读写操作，如果另外一个进程删除了这个文件，那么读写该文件的进程会发生什么呢? 因为文件被删除了，读写进程发生异常? 正在读写的进程仍然正常读写，好像没有发生什么？ 学操作系统原理的时候，我们知道，linux是通过link的数量来控制文件删除，只有当一个文件不存在任何link的时候，这个文件才会被删除。 而每个文件都会有2个link计数器: i_count: i_count的意义是当前使用者的数量，也就是打开文件进程的个数。 i_nlink: i_nlink的意义是介质连接的数量. 或者可以理解为 i_count是内存引用计数器，i_nlink是硬盘引用计数器。再换句话说，当文件被某个进程引用时，i_count 就会增加；当创建文件的硬连接的时候，i_nlink 就会增加。 对于 rm 而言，就是减少 i_nlink。这里就出现一个问题，如果一个文件正在被某个进程调用，而用户却执行 rm 操作把文件删除了，会出现什么结果呢？ 当用户执行 rm 操作后，ls 或者其他文件管理命令不再能够找到这个文件，但是进程却依然在继续正常执行，依然能够从文件中正确的读取内容。这是因为，rm 操作只是将 i_nlink 置为 0 了；由于文件被进程引用的缘故，i_count 不为 0，所以系统没有真正删除这个文件。i_nlink 是文件删除的充分条件，而 i_count 才是文件删除的必要条件。 基于以上只是，大家猜一下，如果在一个进程在打开文件写日志的时候，手动或者另外一个进程将这个日志删除，会发生什么情况？ 是的，数据库并没有停掉。虽然日志文件被删除了，但是有一个进程已经打开了那个文件，所以向那个文件中的写操作仍然会成功，数据仍然会提交。 文件操作偏移lseeklseek的函数用于设置文件偏移量。 每个打开的文件都有一个与其相关联的“当前文件偏移量”（当前文件偏移量）。它通常是一个非负整数，用以度量从文件开始处计算的字节数。通常，读写操作都从当前文件偏移量处开始，并使偏移量增加所读写的字节数。按系统默认的情况，当打开一个文件时，除非制定O_APPEND选项，否则该偏移量被设置为0。 文件空洞我们知道lseek()系统调用可以改变文件的偏移量，但如果程序调用使得文件偏移量跨越了文件结尾，然后再执行I/O操作，将会发生什么情况？ read()调用将会返回0，表示文件结尾。令人惊讶的是，write()函数可以在文件结尾后的任意位置写入数据。在这种情况下，对该文件的下一次写将延长该文件，并在文件中构成一个空洞，这一点是允许的。从原来的文件结尾到新写入数据间的这段空间被成为文件空洞。调用write后文件结尾的位置已经发生变化。 文件空洞不占用任何磁盘空间，直到后续某个时点，在文件空洞中写入了数据，文件系统才会为之分配磁盘块。空洞的存在意味着一个文件名义上的大小可能要比其占用的磁盘存储总量要大（有时大出许多）。向文件空洞中写入字节，内核需要为其分配存储单元，即使文件大小不变，系统的可用磁盘空间也将减少。这种情况并不常见，但也需要了解。 实际中的空洞文件会在哪里用到呢?常见的场景有: 一是在下载电影的时候,发现刚开始下载,文件的大小就已经到几百M了. 二是在创建虚拟机的磁盘镜像的时候,你创建了一个100G的磁盘镜像,但是其实装起来系统之后,开始也不过只占用了3,4G的磁盘空间,如果一开始把100G都分配出去的话,无疑是很大的浪费. 空洞文件方法对多线程共同操作文件是及其有用的。有时候我们创建一个很大的文件(比如视频文件)，如果从头开始依次构建时间很长。有一种思路就是将文件分为多段，然后多线程来操作每个线程负责其中一段的写入。（就像修100公里的高速公路，分成20个段来修，每个段就只负责5公里，就可以大大提高效率）。 习题Linux下两个进程可以同时打开同一个文件，这时如下描述错误的是(答案是4)： 两个进程中分别产生生成两个独立的fd 两个进程可以任意对文件进行读写操作，操作系统并不保证写的原子性 进程可以通过系统调用对文件加锁，从而实现对文件内容的保护 任何一个进程删除该文件时，另外一个进程会立即出现读写失败 两个进程可以分别读取文件的不同部分而不会相互影响 一个进程对文件长度和内容的修改另外一个进程可以立即感知 proc文件夹参考: https://www.cnblogs.com/liushui-sky/p/9354536.html 下面是作者系统（RHEL5.3）上运行的一个PID为2674的进程saslauthd的相关文件，其中有些文件是每个进程都会具有的，后文会对这些常见文件做出说明。123456789101112131415161718192021222324252627[root@rhel5 ~]# ll /proc/2674total 0dr-xr-xr-x 2 root root 0 Feb 8 17:15 attr-r-------- 1 root root 0 Feb 8 17:14 auxv-r--r--r-- 1 root root 0 Feb 8 17:09 cmdline-rw-r--r-- 1 root root 0 Feb 8 17:14 coredump_filter-r--r--r-- 1 root root 0 Feb 8 17:14 cpusetlrwxrwxrwx 1 root root 0 Feb 8 17:14 cwd -&gt; /var/run/saslauthd-r-------- 1 root root 0 Feb 8 17:14 environlrwxrwxrwx 1 root root 0 Feb 8 17:09 exe -&gt; /usr/sbin/saslauthddr-x------ 2 root root 0 Feb 8 17:15 fd-r-------- 1 root root 0 Feb 8 17:14 limits-rw-r--r-- 1 root root 0 Feb 8 17:14 loginuid-r--r--r-- 1 root root 0 Feb 8 17:14 maps-rw------- 1 root root 0 Feb 8 17:14 mem-r--r--r-- 1 root root 0 Feb 8 17:14 mounts-r-------- 1 root root 0 Feb 8 17:14 mountstats-rw-r--r-- 1 root root 0 Feb 8 17:14 oom_adj-r--r--r-- 1 root root 0 Feb 8 17:14 oom_scorelrwxrwxrwx 1 root root 0 Feb 8 17:14 root -&gt; /-r--r--r-- 1 root root 0 Feb 8 17:14 schedstat-r-------- 1 root root 0 Feb 8 17:14 smaps-r--r--r-- 1 root root 0 Feb 8 17:09 stat-r--r--r-- 1 root root 0 Feb 8 17:14 statm-r--r--r-- 1 root root 0 Feb 8 17:10 statusdr-xr-xr-x 3 root root 0 Feb 8 17:15 task-r--r--r-- 1 root root 0 Feb 8 17:14 wchan cmdline — 启动当前进程的完整命令，但僵尸进程目录中的此文件不包含任何信息； 12[root@rhel5 ~]# more /proc/2674/cmdline /usr/sbin/saslauthd cwd — 指向当前进程运行目录的一个符号链接； environ — 当前进程的环境变量列表，彼此间用空字符（NULL）隔开；变量用大写字母表示，其值用小写字母表示； 12[root@rhel5 ~]# more /proc/2674/environ TERM=linuxauthd exe — 指向启动当前进程的可执行文件（完整路径）的符号链接，通过/proc/N/exe可以启动当前进程的一个拷贝； fd — 这是个目录，包含当前进程打开的每一个文件的文件描述符（file descriptor），这些文件描述符是指向实际文件的一个符号链接； 123456789[root@rhel5 ~]# ll /proc/2674/fdtotal 0lrwx------ 1 root root 64 Feb 8 17:17 0 -&gt; /dev/nulllrwx------ 1 root root 64 Feb 8 17:17 1 -&gt; /dev/nulllrwx------ 1 root root 64 Feb 8 17:17 2 -&gt; /dev/nulllrwx------ 1 root root 64 Feb 8 17:17 3 -&gt; socket:[7990]lrwx------ 1 root root 64 Feb 8 17:17 4 -&gt; /var/run/saslauthd/saslauthd.pidlrwx------ 1 root root 64 Feb 8 17:17 5 -&gt; socket:[7991]lrwx------ 1 root root 64 Feb 8 17:17 6 -&gt; /var/run/saslauthd/mux.accept limits — 当前进程所使用的每一个受限资源的软限制、硬限制和管理单元；此文件仅可由实际启动当前进程的UID用户读取；（2.6.24以后的内核版本支持此功能）； maps — 当前进程关联到的每个可执行文件和库文件在内存中的映射区域及其访问权限所组成的列表； 123456[root@rhel5 ~]# cat /proc/2674/maps 00110000-00239000 r-xp 00000000 08:02 130647 /lib/libcrypto.so.0.9.8e00239000-0024c000 rwxp 00129000 08:02 130647 /lib/libcrypto.so.0.9.8e0024c000-00250000 rwxp 0024c000 00:00 0 00250000-00252000 r-xp 00000000 08:02 130462 /lib/libdl-2.5.so00252000-00253000 r-xp 00001000 08:02 130462 /lib/libdl-2.5.so mem — 当前进程所占用的内存空间，由open、read和lseek等系统调用使用，不能被用户读取； root — 指向当前进程运行根目录的符号链接；在Unix和Linux系统上，通常采用chroot命令使每个进程运行于独立的根目录； stat — 当前进程的状态信息，包含一系统格式化后的数据列，可读性差，通常由ps命令使用； statm — 当前进程占用内存的状态信息，通常以“页面”（page）表示； status — 与stat所提供信息类似，但可读性较好，如下所示，每行表示一个属性信息；其详细介绍请参见 proc的man手册页； 123456789101112131415161718[root@rhel5 ~]# more /proc/2674/status Name: saslauthdState: S (sleeping)SleepAVG: 0%Tgid: 2674Pid: 2674PPid: 1TracerPid: 0Uid: 0 0 0 0Gid: 0 0 0 0FDSize: 32Groups:VmPeak: 5576 kBVmSize: 5572 kBVmLck: 0 kBVmHWM: 696 kBVmRSS: 696 kB………… task — 目录文件，包含由当前进程所运行的每一个线程的相关信息，每个线程的相关信息文件均保存在一个由线程号（tid）命名的目录中，这类似于其内容类似于每个进程目录中的内容；（内核2.6版本以后支持此功能） Linux进程管理读者-写者问题定义： 允许多个进程同时对数据进行读操作，但是不允许读和写以及写和写操作同时发生。 解决方案： 读者优先 读进程只要看到有其他读进程正在访问文件，就可以继续作读访问；写进程必须等待所有读进程都不访问时才能写文件，即使写进程可能比一些读进程更早提出申请。 读者写者公平竞争，老实排队 因为读者优先的方案如果在读访问非常频繁的场合，有可能造成写进程一直无法访问文件的局面….为了避免这种情况的产生，读者写者请求都老实排队， 排到谁就执行谁， 不准读者插队 写者优先 如果有写者申请写文件，那么在申请之前已经开始读取文件的可以继续读取，但是如果再有读者申请读取文件，则不能够读取，只有在所有的写者写完之后才可以读取 哲学家就餐问题 5 个沉默寡言的哲学家围坐在圆桌前，每人面前一盘意面。叉子放在哲学家之间的桌面上。（5 个哲学家，5 根叉子） 所有的哲学家都只会在思考和进餐两种行为间交替。哲学家只有同时拿到左边和右边的叉子才能吃到面，而同一根叉子在同一时间只能被一个哲学家使用。每个哲学家吃完面后都需要把叉子放回桌面以供其他哲学家吃面。只要条件允许，哲学家可以拿起左边或者右边的叉子，但在没有同时拿到左右叉子时不能进食。 设计一个进餐规则（并行算法）使得每个哲学家都不会挨饿；也就是说，在没有人知道别人什么时候想吃东西或思考的情况下，每个哲学家都可以在吃饭和思考之间一直交替下去。 显而易见，如果不小心处理会有死锁现象， 比如：当每个科学家都同时拿起了左边的筷子时候死锁发生了，都想拿自己右边的筷子，但是科学家每个人左手都不松手。导致都吃不了饭 参考解决方案： 规定奇数号科学家先拿左边的筷子，然后拿右边的筷子。偶数号科学家先拿右边的筷子，然后那左边的筷子。导致0，1科学家竞争1号筷子，2，3科学家竞争3号筷子。四号科学家无人竞争。最后总有一个科学家能获得两只筷子。 仅当科学家左右两只筷子都能用的时候，才允许他进餐，代码里的用trylock来实现 至多允许四个哲学家同时去拿左边的筷子，最终保证至少有一个科学家能进餐，并且用完之后释放筷子，从而使更多的哲学家能够拿到筷子。 活锁在某种情形下，轮询（忙等待）可用于进入临界区或存取资源。采用这一策略的主要原因是，相比所做的工作而言，互斥的时间很短而挂起等待的时间开销很大。考虑一个原语，通过该原语，调用进程测试一个互斥信号量，然后或者得到该信号量或者返回失败信息。 现在假设有一对进程使用两种资源。每个进程需要两种资源，它们利用轮询原语enter_region去尝试取得必要的锁，如果尝试失败，则该进程继续尝试。如果进程A先运行并得到资源1，然后进程2运行并得到资源2，以后不管哪一个进程运行，都不会有任何进展，但是哪一个进程也没有被阻塞。结果是两个进程总是一再消耗完分配给它们的CPU配额，但是没有进展也没有阻塞。因此，没有出现死锁现象（因为没有进程阻塞），但是从现象上看好像死锁发生了，这就是活锁（livelock）。 死锁参考 必要条件 (口诀互占不还？233): 互斥：每个资源要么已经分配给了一个进程，要么就是可用的。 占有和等待：已经得到了某个资源的进程可以再请求新的资源。 不可抢占：已经分配给一个进程的资源不能强制性地被抢占，它只能被占有它的进程显式地释放。 环路等待：有两个或者两个以上的进程组成一条环路，该环路中的每个进程都在等待下一个进程所占有的资源。 死锁处理方法大纲主要有以下四种方法： 鸵鸟策略 死锁检测与死锁恢复 死锁预防 死锁避免 鸵鸟策略把头埋在沙子里，假装根本没发生问题。因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任务措施的方案会获得更高的性能。当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。大多数操作系统，包括 Unix，Linux 和 Windows，处理死锁问题的办法仅仅是忽略它。 死锁检测与死锁恢复不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。 每种类型一个资源的死锁检测 上图为资源分配图，其中方框表示资源，圆圈表示进程。资源指向进程表示该资源已经分配给该进程，进程指向资源表示进程请求获取该资源。 图 a 可以抽取出环，如图 b，它满足了环路等待条件，因此会发生死锁。 每种类型一个资源的死锁检测算法是通过检测有向图是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有向图存在环，也就是检测到死锁的发生。（当然也可以用拓扑排序思路来检测哈） 每种类型多个资源的死锁检测 上图中，有三个进程四个资源，每个数据代表的含义如下： E 向量：资源总量 A 向量：资源剩余量 C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量 R 矩阵：每个进程请求的资源数量 进程 P1 和 P2 所请求的资源都得不到满足，只有进程 P3 可以，让 P3 执行，之后释放 P3 拥有的资源，此时 A = (2 2 2 0)。P2 可以执行，执行后释放 P2 拥有的资源，A = (4 2 2 1) 。P1 也可以执行。所有进程都可以顺利执行，没有死锁。 算法总结如下： 每个进程最开始时都不被标记，执行过程有可能被标记。当算法结束时，任何没有被标记的进程都是死锁进程。 寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。 如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。 如果没有这样一个进程，算法终止。 死锁恢复 利用抢占恢复 利用回滚恢复 通过杀死进程恢复 死锁预防在程序运行之前预防发生死锁。 破坏互斥条件 例如假脱机打印机技术允许若干个进程同时输出，唯一真正请求物理打印机的进程是打印机守护进程。 破坏占有和等待条件 一种实现方式是规定所有进程在开始执行前请求所需要的全部资源。 破坏不可抢占条件 破坏环路等待 给资源统一编号，进程只能按编号顺序来请求资源。 死锁避免在程序运行时避免发生死锁。避免死锁的主要算法是基于一个安全状态的概念。在描述算法前，我们先讨论有关安全的概念。 安全状态的检测 图 a 的第二列 Has 表示已拥有的资源数，第三列 Max 表示总共需要的资源数，Free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。 安全状态的定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的。 安全状态的检测与死锁的检测类似，因为安全状态必须要求不能发生死锁。下面的银行家算法与死锁检测算法非常类似，可以结合着做参考对比。 单个资源的银行家算法Dijkstra（1965）提出了一种能够避免死锁的调度算法，称为银行家算法（banker’s algorithm），一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。 客户们各自做自己的生意，在某些时刻需要贷款（相当于请求资源）。在某一时刻，具体情况如图b所示。这个状态是安全的，由于保留着2个单位，银行家能够拖延除了C以外的其他请求。因而可以让C先完成，然后释放C所占的4个单位资源。有了这4个单位资源，银行家就可以给D或B分配所需的贷款单位，以此类推。 考虑假如向B提供了另一个他所请求的贷款单位，如图b所示，那么我们就有如图c所示的状态，该状态是不安全的。如果忽然所有的客户都请求最大的限额，而银行家无法满足其中任何一个的要求，那么就会产生死锁。不安全状态并不一定引起死锁，由于客户不一定需要其最大贷款额度，但银行家不敢抱这种侥幸心理。 银行家算法就是对每一个请求进行检查，检查如果满足这一请求是否会达到安全状态。若是，那么就满足该请求；若否，那么就推迟对这一请求的满足。为了看状态是否安全，银行家看他是否有足够的资源满足某一个客户。如果可以，那么这笔投资认为是能够收回的，并且接着检查最接近最大限额的一个客户，以此类推。如果所有投资最终都被收回，那么该状态是安全的，最初的请求可以批准。 上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。 多个资源的银行家算法 可以把银行家算法进行推广以处理多个资源 上图中有五个进程，四个资源。左边的图表示已经分配的资源，右边的图表示还需要分配的资源。最右边的 E、P 以及 A 分别表示：总资源、已分配资源以及可用资源，注意这三个为向量，而不是具体数值，例如 A=(1020)，表示 4 个资源分别还剩下 1/0/2/0。 检查一个状态是否安全的算法如下： 查找右边的矩阵是否存在一行小于等于向量 A。如果不存在这样的行，那么系统将会发生死锁，状态是不安全的。 假若找到这样一行，将该进程标记为终止，并将其已分配资源加到 A 中。 重复以上两步，直到所有进程都标记为终止，则状态时安全的。 如果一个状态不是安全的，需要拒绝进入这个状态。 linux进程调度参考 https://juejin.im/post/6844903568613310477 在Linux中，线程和进程一视同仁，所以讲到进程调度，也包含了线程调度。 调度分两种: 非抢占式多任务 除非任务自己结束，否则将会一直执行。 抢占式多任务（Linux用的是这种) 这种情况下，由调度程序来决定什么时候停止一个进程的运行，这个强制的挂起动作即为抢占。采用抢占式多任务的基础是使用时间片轮转机制来为每个进程分配可以运行的时间单位。 Linux有两种不同的进程优先级范围: 使用nice值：越大的nice值意味着更低的优先级。 (-19 ~ 20之间) 实时优先级：可配置(通过实时调度API)，越高意味着进程优先级越高。 任何实时的进程优先级都高于普通的进程，因此上面的两种优先级范围处于互不相交的范畴。 时间片：Linux中并不是以固定的时间值(如10ms)来分配时间片的，而是将处理器的使用比作为“时间片”划分给进程。这样，进程所获得的实际CPU时间就和系统的负载密切相关。 Linux内核有两个调度类： CFS(完全公平调度器Completely Fair Scheduler) 实时调度类。 公平调度CFS举个例子来区分Unix调度和CFS, 有两个运行的优先级相同的进程: 在Unix中可能是每个各执行5ms，执行期间完全占用处理器，但在“理想情况”下，应该是，能够在10ms内同时运行两个进程，每个占用处理器一半的能力。 CFS的做法是：CFS 调度程序并不采用严格规则来为一个优先级分配某个长度的时间片, 在所有可运行进程的总数上计算出一个进程应该运行的时间，nice值不再作为时间片分配的标准，而是用于处理计算获得的处理器使用权重。 现在我们来看一个简单的例子，假设我们的系统只有两个进程在运行，一个是文本编辑器（I/O消耗型），另一个是视频解码器（处理器消耗型）。理想的情况下，文本编辑器应该得到更多的处理器时间，至少当它需要处理器时，处理器应该立刻被分配给它（这样才能完成用户的交互），这也就意味着当文本编辑器被唤醒的时候，它应该抢占视频解码程序。按照普通的情况，OS应该分配给文本编辑器更大的优先级和更多的时间片，但在Linux中，这两个进程都是普通进程，他们具有相同的nice值，因此它们将得到相同的处理器使用比（50%）。但实际的运行过程中会发生什么呢？CFS将能够注意到，文本编辑器使用的处理器时间比分配给它的要少得多（因为大多时间在等待I/O），这种情况下，要实现所有进程“公平”地分享处理器，就会让文本编辑器在需要运行时立刻抢占视频解码器（每次都是如此）。 实时调度Linux还实现了 POS1X实时调度扩展。这些扩展允许应用程序精确地控制如何分配CPU给进程。运作在两个实时调度策略 SCHED RR （循环） SCHED FIFO （先入先出） 下的进程的优先级总是高于运作在非实时策略下的进程。实时进程优先级的取值范围为1 （低）〜99（高）。只有进程处于可运行状态，那么优先级更高的进程就会完全将优先级低的进程排除在CPU之外。 运作在SCHED_FIFO策略下的进程会互斥地访问CPU直到它执行终止或自动释放CPU或被进入可运行状态的优先级更高的进程抢占。 类似的规则同样适用于SCHED RR策略, 但在该策略下，如果存在多个进程运行于同样的优先级下，那么CPU就会以循环的方式被这些进程共享。 实时调度采用 SCHED_FIFO 或 SCHED_RR 实时策略来调度的任何任务，与普通（非实时的）任务相比，具有更高的优先级。 Linux 采用两个单独的优先级范围，一个用于实时任务，另一个用于正常任务。实时任务分配的静态优先级为 0〜99，而正常任务分配的优先级为 100〜139。 这两个值域合并成为一个全局的优先级方案，其中较低数值表明较高的优先级。正常任务，根据它们的nice值，分配一个优先级；这里 -20 的nice值映射到优先级 100，而 +19 的nice值映射到 139。下图显示了这个方案。 linux轻量级进程LWP对于Linux操作系统而言，它对Thread的实现方式比较特殊。在Linux内核中，其实是没有线程的概念的，它把所有的线程当做标准的进程来实现，也就是说Linux内核，并没有为线程提供任何特殊的调度语义，也没有为线程实现特定的数据结构。取而代之的是，线程只是一个与其他进程共享某些资源的进程。每一个线程拥有一个唯一的task_struct结构，Linux内核它仅仅把线程当做一个正常的进程，或者说是轻量级进程，LWP(Lightweight processes)。 Linux线程与进程的区别，主要体现在资源共享、调度、性能几个方面，首先看一下资源共享方面。上面也提到，线程其实是共享了某一个进程的资源，这些资源包括： 内存地址空间 进程基础信息 大部分数据 打开的文件 信号处理 当前工作目录 用户和用户组属性 … 哪些是线程独自拥有的呢？ 线程ID 一系列的寄存器 栈的局部变量和返回地址 错误码 errno 信号掩码 优先级 … 这里说一个黑科技，线程拥有独立的调用栈，除了栈之外共享了其他所有的段segment。但是由于线程间共享了内存，也就是说一个线程，理论上是可以访问到其他线程的调用栈的，可以用一个指针变量，去访问其他线程的局部栈帧，以访问其他线程的局部变量。 LWP如何创建出来那么Linux中线程是如何创建出来的呢？上面也提到，在Linux中线程是一种资源共享的方式，可以在创建进程的时候，指定某些资源是从其他进程共享的，从而在概念上创建了一个线程。在Linux中，可以通过clone系统调用来创建一个进程，它的函数签名如下：12#include &lt;sched.h&gt;int clone(int (*fn)(void *), void *child_stack, int flags, void *arg, ...); 我们在使用clone创建进程的过程中，可以指明相应的参数，来决定共享某些资源，比如:1clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0); 这个clone系统调用的行为类似于fork，不过新创建出来的进程，它的内存地址、文件系统资源、打开的文件描述符和信号处理器，都是共享父进程的。换句话说，这个新创建出来的进程，也被叫做Linux Thread。从这个例子中，也可以看出Linux中，线程其实是进程实现资源共享的一种方式。 在内核中，clone调用经过参数传递和解释后会调用do_fork，这个核内函数同时也是fork、vfork系统调用的最终实现：1int do_fork(unsigned long clone_flags, unsigned long stack_start, struct pt_regs* regs,unsigned long stack_size); 在do_fork中，不同的clone_flags将导致不同的行为（共享不同的资源），下面列举几个flag的作用。 CLONE_VM 如果do_fork时指定了CLONE_VM开关，创建的轻量级进程的内存空间将会和父进程指向同一个地址，即创建的轻量级进程将与父进程共享内存地址空间。 CLONE_FS 如果do_fork时指定了CLONE_FS开关，对于轻量级进程则会与父进程共享相同的所在文件系统的根目录和当前目录信息。也就是说，轻量级进程没有独立的文件系统相关的信息，进程中任何一个线程改变当前目录、根目录等信息都将直接影响到其他线程。 CLONE_FILES 如果do_fork时指定了CLONE_FILES开关，创建的轻量级进程与父进程将会共享已经打开的文件。这一共享使得任何线程都能访问进程所维护的打开文件，对它们的操作会直接反映到进程中的其他线程。 CLONE_SIGHAND 如果do_fork时指定了CLONE_FILES开关，轻量级进程与父进程将会共享对信号的处理方式。也就是说，子进程与父进程的信号处理方式完全相同，而且可以相互更改。 尽管linux支持轻量级进程，但并不能说它就支持内核线程，因为linux的”线程”和”进程”实际上处于一个调度层次，共享一个进程标识符空间，这种限制使得不可能在linux上实现完全意义上的POSIX线程机制，因此众多的linux线程库实现尝试都只能尽可能实现POSIX的绝大部分语义，并在功能上尽可能逼近。 多核CPU是否能同时执行多个进程？多核的作用就是每个CPU可以调度不同的任务“并行”执行。注意，这里说的是“并行”，而不是“并发”，所以问题的回答是“能”。 第二个问题，“同时最多执行几个进程“?这里你想描述的“同时”的意思，是某一个特定时刻吗？如果是，很明显，在某一特定时刻，每个核只能调度一个任务执行，所以有多少个核最多就可以调度多少个进程（或者说成线程比较准确些）。但在一段时间之内，每个核可以“并发”调度多个任务执行。如何“并发”，这就是由不同操作系统的进程调度策略规定的了，比如常见Linux的CFS调度算法和Windows的抢占式调度算法。 创建守护进程的步骤(两fork一set, u工文dev)最关键的三步骤: 调用fork，然后使父进程exit。虽然子进程继承了父进程的进程组ID，但获得了一个新的进程ID，这就保证了子进程不是一个进程组的组长进程。这是下面将要进行的setsid调用的先决条件。 调用setsid创建一个新会话。使调用进程：(a)成为新会话的首进程，(b)成为一个新进程组的组长进程．(c)没有控制终端。也可概括为 : 开启一个新会话并释放它与控制终端之间的所有关联关系 再次fork并杀掉首进程.这样就确保了子进程不是一个会话首进程， 根据linux中获取终端的规则（只有会话首进程才能请求一个控制终端）， 这样进程永远不会重新请求一个控制终端 1234567 会 话 / | \ / | \ / | \ 前台进程组 后台进程组1 后台进程组2 ... / | \ / | \ / | \进程1 进程2 ... 进程3 进程4 ... ... 进程组进程组就是一系列相互关联的进程集合，系统中的每一个进程也必须从属于某一个进程组；每个进程组中都会有一个唯一的 ID(process group id)，简称 PGID；PGID 一般等同于进程组的创建进程的 Process ID，而这个进进程一般也会被称为进程组先导 (process group leader) 会话会话（session）是一个若干进程组的集合，同样的，系统中每一个进程组也都必须从属于某一个会话；一个会话只拥有最多一个控制终端（也可以没有），该终端为会话中所有进程组中的进程所共用。一个会话中前台进程组只会有一个，只有其中的进程才可以和控制终端进行交互；除了前台进程组外的进程组，都是后台进程组；和进程组先导类似，会话中也有会话先导 (session leader) 的概念，用来表示建立起到控制终端连接的进程。在拥有控制终端的会话中，session leader 也被称为控制进程(controlling process)，一般来说控制进程也就是登入系统的 shell 进程(login shell)； 杀死进程组或会话中的所有进程我们可以使用该 PGID，通过 kill 命令向整个进程组发送信号： kill -SIGTERM -- -19701 我们用一个负数 -19701 向进程组发送信号。如果我们传递的是一个正数，这个数将被视为进程 ID 用于终止进程。如果我们传递的是一个负数，它被视为 PGID，用于终止整个进程组。负数来自系统调用的直接定义。 杀死会话中的所有进程与之完全不同。有些系统没有会话 ID 的概念。即使是具有会话 ID 的系统，例如 Linux，也没有提供系统调用来终止会话中的所有进程。你需要遍历 /proc 输出的进程树，收集所有的 SID，然后一一终止进程。Pgrep 实现了遍历、收集并通过会话 ID 杀死进程的算法。使用以下命令： pkill -s &lt;SID&gt; SIGHUPSIGHUP 会在以下 3 种情况下被发送给相应的进程： 终端关闭时，该信号被发送到 session 首进程以及作为 job 提交的进程（即用 &amp; 符号提交的进程）； session 首进程退出时，该信号被发送到该 session 中的前台进程组中的每一个进程； 若父进程退出导致进程组成为孤儿进程组，且该进程组中有进程处于停止状态（收到 SIGSTOP 或 SIGTSTP 信号），该信号会被发送到该进程组中的每一个进程。 例如：在我们登录 Linux 时，系统会分配给登录用户一个终端 (Session)。在这个终端运行的所有程序，包括前台进程组和后台进程组，一般都属于这个 Session。当用户退出 Linux 登录时，前台进程组和后台有对终端输出的进程将会收到 SIGHUP 信号。这个信号的默认操作为终止进程，因此前台进程组和后台有终端输出的进程就会中止。此外，对于与终端脱离关系的守护进程，正常情况下是永远都收不到这个信号的, 所以可以人为的发SIGHUP信号给她用于通知它做一些想要的自定义的操作, 比较常见的如重新读取配置文件操作。 比如 xinetd 超级服务程序。 SIGCHLD与僵死进程SIGCHLD信号,子进程结束时, 父进程会收到这个信号。如果父进程没有处理这个信号，也没有等待(waitpid)子进程，子进程虽然终止，但是还会在内核进程表中占有表项，这时的子进程称为僵尸进程。这种情况我们应该捕捉它，或者wait它派生的子进程，或者父进程先终止，这时子进程变成孤儿进程的终止自动由init进程 来接管 孤儿进程与僵尸进程 孤儿进程: 就是没有父进程的进程。当然创建的时候肯定是要先创建父进程了，当父进程退出时，它的子进程们（一个或者多个）就成了孤儿进程了。父进程退出后，子进程被一个进程 ID 为 1 的进程领养的。还挺好这个结果，至少还是有人管的，被暖到了～ 进程 id 为 1 的进程是 init 进程，每当有孤儿进程出现时，init 进程就会收养它并成为它的父进程，来照顾它以孤儿进程以后的生活。 危害: 因为孤儿进程会被 init 进程接管，所以孤儿进程是没有危害的。 僵尸进程: 和孤儿进程相反的是，这次是子进程先退出，而父进程又没有去处理回收释放子进程的资源，这个时候子进程就成了僵尸进程。 危害: 资源上是占用不了什么资源。但是通常系统的进程数量都是有限制的，如果有大量的僵尸进程占用进程号，导致新的进程无法创建，这个危害类似于占个坑，不办事 处理: 直接kill -9僵尸进程的话一般是kill不掉的, 只能ps之后查出他的父进程的pid然后去kill他的父进程的pid SIGPIPE在网络编程中，SIGPIPE 这个信号是很常见的。当往一个写端关闭的管道或 socket 连接中连续写入数据时会引发 SIGPIPE 信号, 引发 SIGPIPE 信号的写操作将设置 errno 为 EPIPE。在 TCP 通信中，当通信的双方中的一方 close 一个连接时，若另一方接着发数据，根据 TCP 协议的规定，会收到一个 RST 响应报文，若再往这个服务器发送数据时，系统会发出一个 SIGPIPE 信号给进程，告诉进程这个连接已经断开了，不能再写入数据。 因为 SIGPIPE 信号的默认行为是结束进程，而我们绝对不希望因为写操作的错误而导致程序退出，尤其是作为服务器程序来说就更恶劣了。所以我们应该对这种信号加以处理，在这里，介绍处理 SIGPIPE 信号的方式： 一般给 SIGPIPE 设置 SIG_IGN 信号处理函数，忽略该信号: signal(SIGPIPE, SIG_IGN); 前文说过，引发 SIGPIPE 信号的写操作将设置 errno 为 EPIPE,。所以，第二次往关闭的 socket 中写入数据时, 会返回 - 1, 同时 errno 置为 EPIPE. 这样，便能知道对端已经关闭，然后进行相应处理，而不会导致整个进程退出. 内核态与用户态的区别 内核态：cpu可以访问内存的所有数据，包括外围设备，例如硬盘，网卡，cpu也可以将自己从一个程序切换到另一个程序。 用户态：只能受限的访问内存，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取。 从用户态到内核态切换可以通过三种方式： 系统调用: 其实系统调用本身就是中断，但是软件中断，跟硬中断不同。 异常：如果当前进程运行在用户态，如果这个时候发生了异常事件，就会触发切换。例如：缺页异常。 外设中断：当外设完成用户的请求时，会向CPU发送中断信号。 Linux网络编程 I/O模式 水平触发 边缘触发 epoll ✓ ✓ select/poll ✓ 信号驱动 ✓ select一个常见的select例子(一个回射服务器)如下:回射服务器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/* include fig01 */#include "unp.h"intmain(int argc, char **argv)&#123; int i, maxi, maxfd, listenfd, connfd, sockfd; int nready, client[FD_SETSIZE]; ssize_t n; fd_set rset, allset; char buf[MAXLINE]; socklen_t clilen; struct sockaddr_in cliaddr, servaddr; listenfd = Socket(AF_INET, SOCK_STREAM, 0); bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); Bind(listenfd, (SA *) &amp;servaddr, sizeof(servaddr)); Listen(listenfd, LISTENQ); maxfd = listenfd; /* initialize */ maxi = -1; /* index into client[] array */ for (i = 0; i &lt; FD_SETSIZE; i++) client[i] = -1; /* -1 indicates available entry */ FD_ZERO(&amp;allset); FD_SET(listenfd, &amp;allset);/* end fig01 *//* include fig02 */ for ( ; ; ) &#123; rset = allset; /* structure assignment */ nready = Select(maxfd+1, &amp;rset, NULL, NULL, NULL); if (FD_ISSET(listenfd, &amp;rset)) &#123; /* new client connection */ clilen = sizeof(cliaddr); connfd = Accept(listenfd, (SA *) &amp;cliaddr, &amp;clilen);#ifdef NOTDEF printf("new client: %s, port %d\n", Inet_ntop(AF_INET, &amp;cliaddr.sin_addr, 4, NULL), ntohs(cliaddr.sin_port));#endif for (i = 0; i &lt; FD_SETSIZE; i++) if (client[i] &lt; 0) &#123; client[i] = connfd; /* save descriptor */ break; &#125; if (i == FD_SETSIZE) err_quit("too many clients"); FD_SET(connfd, &amp;allset); /* add new descriptor to set */ if (connfd &gt; maxfd) maxfd = connfd; /* for select */ if (i &gt; maxi) maxi = i; /* max index in client[] array */ if (--nready &lt;= 0) continue; /* no more readable descriptors */ &#125; for (i = 0; i &lt;= maxi; i++) &#123; /* check all clients for data */ if ( (sockfd = client[i]) &lt; 0) continue; if (FD_ISSET(sockfd, &amp;rset)) &#123; if ( (n = Read(sockfd, buf, MAXLINE)) == 0) &#123; /*4connection closed by client */ Close(sockfd); FD_CLR(sockfd, &amp;allset); client[i] = -1; &#125; else Writen(sockfd, buf, n); if (--nready &lt;= 0) break; /* no more readable descriptors */ &#125; &#125; &#125;&#125;/* end fig02 */ 参考select poll epoll的区别 可以看出select的缺点如下: (遍)select返回的是含有整个句柄的数组，应用程序需要遍历整个数组才能发现哪些句柄发生了事件； fd_set 使用数组实现，数组大小使用 FD_SETSIZE 定义，所以只能监听少于 FD_SETSIZE 数量的描述符。FD_SETSIZE 大小默认为 1024，因此默认只能监听少于 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译 (内)内核/用户空间内存拷贝问题，每次调用select都需要将全部描述符从应用进程缓冲区复制到内核缓冲区 (数)单个进程能够监视的文件描述符的数量存在最大限制，通常是1024，当然可以更改数量，但由于select采用轮询的方式扫描文件描述符，文件描述符数量越多，性能越差； select的触发方式是水平触发，应用程序如果没有完成对一个已经就绪的文件描述符进行IO，那么之后再次select调用还是会将这些文件描述符通知进程。 相比于select模型，poll使用链表保存文件描述符，因此没有了监视文件数量的限制，但其他三个缺点依然存在。 epoll一个常见的epoll使用例子:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/*************************************************************************\* Copyright (C) Michael Kerrisk, 2017. ** ** This program is free software. You may use, modify, and redistribute it ** under the terms of the GNU General Public License as published by the ** Free Software Foundation, either version 3 or (at your option) any ** later version. This program is distributed without any warranty. See ** the file COPYING.gpl-v3 for details. *\*************************************************************************//* Listing 63-5 */#include &lt;sys/epoll.h&gt;#include &lt;fcntl.h&gt;#include "tlpi_hdr.h"#define MAX_BUF 1000 /* Maximum bytes fetched by a single read() */#define MAX_EVENTS 5 /* Maximum number of events to be returned from a single epoll_wait() call */intmain(int argc, char *argv[])&#123; int epfd, ready, fd, s, j, numOpenFds; struct epoll_event ev; struct epoll_event evlist[MAX_EVENTS]; char buf[MAX_BUF]; if (argc &lt; 2 || strcmp(argv[1], "--help") == 0) usageErr("%s file...\n", argv[0]); epfd = epoll_create(argc - 1); if (epfd == -1) errExit("epoll_create"); /* Open each file on command line, and add it to the "interest list" for the epoll instance */ for (j = 1; j &lt; argc; j++) &#123; fd = open(argv[j], O_RDONLY); if (fd == -1) errExit("open"); printf("Opened \"%s\" on fd %d\n", argv[j], fd); ev.events = EPOLLIN; /* Only interested in input events */ ev.data.fd = fd; if (epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &amp;ev) == -1) errExit("epoll_ctl"); &#125; numOpenFds = argc - 1; while (numOpenFds &gt; 0) &#123; /* Fetch up to MAX_EVENTS items from the ready list of the epoll instance */ printf("About to epoll_wait()\n"); ready = epoll_wait(epfd, evlist, MAX_EVENTS, -1); if (ready == -1) &#123; if (errno == EINTR) continue; /* Restart if interrupted by signal */ else errExit("epoll_wait"); &#125; printf("Ready: %d\n", ready); /* Deal with returned list of events */ for (j = 0; j &lt; ready; j++) &#123; printf(" fd=%d; events: %s%s%s\n", evlist[j].data.fd, (evlist[j].events &amp; EPOLLIN) ? "EPOLLIN " : "", (evlist[j].events &amp; EPOLLHUP) ? "EPOLLHUP " : "", (evlist[j].events &amp; EPOLLERR) ? "EPOLLERR " : ""); if (evlist[j].events &amp; EPOLLIN) &#123; s = read(evlist[j].data.fd, buf, MAX_BUF); if (s == -1) errExit("read"); printf(" read %d bytes: %.*s\n", s, s, buf); &#125; else if (evlist[j].events &amp; (EPOLLHUP | EPOLLERR)) &#123; /* After the epoll_wait(), EPOLLIN and EPOLLHUP may both have been set. But we'll only get here, and thus close the file descriptor, if EPOLLIN was not set. This ensures that all outstanding input (possibly more than MAX_BUF bytes) is consumed (by further loop iterations) before the file descriptor is closed. */ printf(" closing fd %d\n", evlist[j].data.fd); // 关闭一个文件描述符会自动的将其从所有的 epoll 实例的兴趣列表中移除 if (close(evlist[j].data.fd) == -1) errExit("close"); numOpenFds--; &#125; &#125; &#125; printf("All file descriptors closed; bye\n"); exit(EXIT_SUCCESS);&#125; epoll的设计和实现select完全不同。epoll把原先的select/poll调用分成了3个部分： 调用epoll_create()建立一个epoll对象（在epoll文件系统中为这个句柄对象分配资源） 调用epoll_ctl向epoll对象中添加这100万个连接的套接字 调用epoll_wait收集发生的事件的连接 总结: epoll_ctl 用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait 便可以得到事件完成的描述符。 从上面的描述可以看出，epoll 只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，并且进程不需要通过轮询来获得事件完成的描述符。 epoll 仅适用于 Linux OS。 epoll 比 select 和 poll 更加灵活而且没有描述符数量限制。 水平触发与边缘触发的区别默认情况下 epoll 提供的是水平触发通知.要使用边缘触发通知，我们在调用epoll_ctl()时在ev．events字段中指定EPOLLET标志. 例如 : 12345struct epoll_event ev;ev.data.fd = fd;ev.events = EPOLLIN | EPOLLET;if (epoll_ctl(epfd, EPOLL_CTL_ADD, fd, ev) == -1) errExit("epoll_ctl"); 我们通过一个例子来说明epoll的水平触发和边缘触发通知之间的区别。假设我们使用epoll来监视一个套接字上的输入（EPOLLIN），接下来会发生如下的事件。 套接字上有输入到来。 我们调用一次epoll_wait()。无论我们采用的是水平触发还是边缘触发通知，该调用都会告诉我们套接字已经处于就绪态了。 再次调用epoll_wait()。 如果我们采用的是水平触发通知，那么第二个epoll_wait()调用将告诉我们套接字处于就绪态。 而如果我们采用边缘触发通知，那么第二个epoll_wait()调用将阻塞，因为自从上一次调用epoll_wait()以来并没有新的输入到来。边缘触发通知通常和非阻塞的文件描述符结合使用。因而，采用epoll的边缘触发通知机制的程序基本框架如下: 1. 让所有待监视的文件描述符都成为非阻塞的。 2. 通过epoll_ctl()构建epoll的兴趣列表。 3. 通过epoll_wait()取得处于就绪态的描述符列表。 4. 针对每一个处于就绪态的文件描述符，不断进行I/O处理直到相关的系统调用( 例如read()、write()，recv()、send()或accept() )返回EAGAIN或EWOULDBLOCK错误。 水平触发需要处理的问题使用linux epoll模型，水平触发模式（Level-Triggered）；当socket可写时，会不停的触发socket可写的事件，如何处理？ 第一种最普通的方式： 当需要向socket写数据时，将该socket加入到epoll模型（epoll_ctl）；等待可写事件。 接收到socket可写事件后，调用write()或send()发送数据。。。 当数据全部写完后， 将socket描述符移出epoll模型。 这种方式的缺点是： 即使发送很少的数据，也要将socket加入、移出epoll模型。有一定的操作代价。 第二种方式，（是本人的改进方案， 叫做directly-write） 向socket写数据时，不将socket加入到epoll模型；而是直接调用send()发送； 只有当或send()返回错误码EAGAIN（系统缓存满），才将socket加入到epoll模型，等待可写事件后(表明系统缓冲区有空间可以写了)，再发送数据。 全部数据发送完毕，再移出epoll模型。 这种方案的优点： 当用户数据比较少时，不需要epool的事件处理。 在高压力的情况下，性能怎么样呢？ 对一次性直接写成功、失败的次数进行统计。如果成功次数远大于失败的次数， 说明性能良好。（如果失败次数远大于成功的次数，则关闭这种直接写的操作，改用第一种方案。同时在日志里记录警告） 在我自己的应用系统中，实验结果数据证明该方案的性能良好。 事实上，网络数据可分为两种到达/发送情况： 一是分散的数据包， 例如每间隔40ms左右，发送/接收3-5个 MTU（或更小，这样就没超过默认的8K系统缓存）。 二是连续的数据包， 例如每间隔1s左右，连续发送/接收 20个 MTU（或更多）。 第三种方式： 使用Edge-Triggered（边沿触发），这样socket有可写事件，只会触发一次。 可以在应用层做好标记。以避免频繁的调用 epoll_ctl( EPOLL_CTL_ADD, EPOLL_CTL_MOD)。 这种方式是epoll 的 man 手册里推荐的方式， 性能最高。但如果处理不当容易出错，事件驱动停止。 epoll实现细节epoll的高效就在于，当我们调用epoll_ctl往里塞入百万个句柄时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的句柄给我们用户。这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。 而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已，如何能不高效？！ 那么，这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。 如此，一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。 最后看看epoll独有的两种模式LT和ET。无论是LT和ET模式，都适用于以上所说的流程。区别是，LT模式下，只要一个句柄上的事件一次没有处理完，会在以后调用epoll_wait时次次返回这个句柄，而ET模式仅在第一次返回。 这件事怎么做到的呢？当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表，最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了。所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的。 select 和 epoll的区别select函数，必须得清楚select跟linux特有的epoll的区别， 有三点(遍内数)： 遍历 ： 每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大；当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd, 每次只需要简单的从列表里取出就行了 内存拷贝 ： select，poll每次调用都要把fd集合从用户态往内核态拷贝一次; epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次 数量限制 ： select默认只支持1024个；epoll并没有最大数目限制 非阻塞的connect和accept 非阻塞connect为啥要用?怎么用? 为啥要用: 因为connect是比较耗时的, 所以我们希望可以在connecting的时候并行的做点其他的事 怎么用: 调用非阻塞connect之后会立马返回EINPROCESS错误, 然后我们去epoll注册一个可写事件, 等待此套接字可写我们判断一下如果不是socket发生异常错误则即为connect连上了 非阻塞accept有啥用, 怎么用?为啥要用? 为啥要用: 如果调用阻塞accept，这样如果在select检测到有连接请求，但在调用accept之前，这个请求断开了，然后调用accept的时候就会阻塞在哪里，除非这时有另外一个连接请求，如果没有，则一直被阻塞在accept调用上, 无法处理任何其他已就绪的描述符。 怎么用: 我们去epoll注册一个监听套接字的fd可读事件, 等待此套接字的fd可写我们判断一下如果不是socket发生异常错误则即为准备好了一个新连接 注意 : 当socket异常错误的时候socket是可读并可写的, 所以在非阻塞connect(判断是否可写)/accept(判断是否可读)的时候要特别注意这种情况, 要用getsockopt函数, 使用SO_ERROR选项来检查处理. 阻塞和非阻塞的send和recv和sendto和recvfrom注意: 首先需要说明的是，不管阻塞还是非阻塞，在发送时都会将数据从应用进程缓冲区拷贝到内核套接字发送缓冲区（UDP并没有实际存在这个内核套接字发送缓冲区, UDP的套接字缓冲区大小仅仅是可写到该套接字UDP数据包的大小上限, TCP/UDP都可以用SO_SNDBUF选项来更改该内核缓冲区大小）。 发送, 我们发送选用send（这里特指TCP）以及sendto（这里特指UDP）来描述 阻塞 在阻塞模式下send操作将会等待所有数据均被拷贝到发送缓冲区后才会返回。阻塞的send操作返回的发送大小，必然是你参数中的发送长度的大小。 在阻塞模式下的sendto操作不会阻塞。 关于这一点的原因在于：UDP并没有真正的发送缓冲区，它所做的只是将应用缓冲区拷贝给下层协议栈，在此过程中加上UDP头，IP头，所以实际不存在阻塞。 非阻塞 在非阻塞模式下send操作调用会立即返回。关于立即返回大家都不会有异议。还是拿阻塞send的那个例子来看，当缓冲区只有192字节，但是却需要发送2000字节时，此时调用立即返回，并得到返回值为192。从中可以看到，非阻塞send仅仅是尽自己的能力向缓冲区拷贝尽可能多的数据，因此在非阻塞下send才有可能返回比你参数中的发送长度小的值。如果缓冲区没有任何空间时呢？这时肯定也是立即返回，但是你会得到WSAEWOULDBLOCK/EWOULDBLOCK 的错误，此时表示你无法拷贝任何数据到缓冲区，你最好休息一下再尝试发送。 在非阻塞模式下sendto操作 不会阻塞（与阻塞一致，不作说明）。 接收, 接收选用recv（这里特指TCP）以及recvfrom（这里特指UDP）来描述 阻塞 在阻塞模式下recv，recvfrom操作将会阻塞 到缓冲区里有至少一个字节（TCP）或者一个完整UDP数据报才返回。 在没有数据到来时，对它们的调用都将处于睡眠状态，不会返回。 非阻塞 在非阻塞模式下recv，recvfrom操作将会立即返回。 如果缓冲区有任何一个字节数据（TCP）或者一个完整UDP数据报，它们将会返回接收到的数据大小。而如果没有任何数据则返回错误 WSAEWOULDBLOCK/EWOULDBLOCK。 reuseaddr和reuseport reuseaddr的作用? 参考 https://zhuanlan.zhihu.com/p/35367402 主要是用于绑定TIME_WAIT状态的地址: 一个非常现实的问题是，假如一个systemd托管的service异常退出了，留下了TIME_WAIT状态的socket，那么systemd将会尝试重启这个service。但是因为端口被占用，会导致启动失败，造成两分钟的服务空档期，systemd也可能在这期间放弃重启服务。但是在设置了SO_REUSEADDR以后，处于TIME_WAIT状态的地址也可以被绑定，就杜绝了这个问题。因为TIME_WAIT其实本身就是半死状态，虽然这样重用TIME_WAIT可能会造成不可预料的副作用，但是在现实中问题很少发生，所以也忽略了它的副作用 reuseport有啥用? SO_REUSEPORT使用场景：linux kernel 3.9 引入了最新的SO_REUSEPORT选项，使得多进程或者多线程创建多个绑定同一个ip:port的监听socket，提高服务器的接收链接的并发能力,程序的扩展性更好；此时需要设置SO_REUSEPORT（注意所有进程都要设置才生效）。 1setsockopt(listenfd, SOL_SOCKET, SO_REUSEPORT,(const void *)&amp;reuse , sizeof(int)); 目的：每一个进程有一个独立的监听socket，并且bind相同的ip:port，独立的listen()和accept()；提高接收连接的能力。（例如nginx多进程同时监听同一个ip:port）解决的问题： 避免了应用层多线程或者进程监听同一ip:port的“惊群效应”。 内核层面实现负载均衡，保证每个进程或者线程接收均衡的连接数。 只有effective-user-id相同的服务器进程才能监听同一ip:port （安全性考虑） Linux内存管理为什么需要虚拟内存虚拟内存的目的是为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存。 为了更好的管理内存，操作系统将内存抽象成地址空间。每个程序拥有自己的地址空间，这个地址空间被分割成多个块，每一块称为一页。这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内存中。当程序引用到不在物理内存中的页时，由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令。 从上面的描述中可以看出，虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存，也就是说一个程序不需要全部调入内存就可以运行，这使得有限的内存运行大程序成为可能。例如有一台计算机可以产生 16 位地址，那么一个程序的地址空间范围是 0~64K。该计算机只有 32KB 的物理内存，虚拟内存技术允许该计算机运行一个 64K 大小的程序。 MMU工作原理内存管理单元（MMU）管理着地址空间和物理内存的转换，其中的页表（Page table）存储着页（程序地址空间）和页框（物理内存空间）的映射表。 一个虚拟地址分成两个部分: 一部分存储页面号， 一部分存储偏移量。 上图的页表存放着 16 个页，这 16 个页需要用 4 个比特位来进行索引定位。例如对于虚拟地址（0010 000000000100），前 4 位是存储页面号 2，读取表项内容为（110 1），页表项最后一位表示是否存在于内存中，1 表示存在。后 12 位存储偏移量。这个页对应的页框的地址为 （110 000000000100）。 主机字节序主机字节序又叫 CPU 字节序，其不是由操作系统决定的，而是由 CPU 指令集架构决定的。主机字节序分为两种： 记忆技巧: 低序地址存了高序字节就叫大端, 反之就小端 大端字节序（Big Endian）：高序字节存储在低位地址，低序字节存储在高位地址 小端字节序（Little Endian）：低序字节存储在低位地址, 高序字节存储在高位地址，目前主要是Intel/AMD/ARM在用 存储方式:32 位整数 0x12345678 是从起始位置为 0x00 的地址开始存放，则： 内存地址 0x00 0x01 0x02 0x03 大端 12 34 56 78 小端 78 56 34 12 12345int i = 0x12345678;if (*(char*)(&amp;i) == 0x12) cout &lt;&lt; "大端" &lt;&lt; endl;else cout &lt;&lt; "小端" &lt;&lt; endl; 网络字节序网络字节顺序是 TCP/IP 中规定好的一种数据表示格式，它与具体的 CPU 类型、操作系统等无关，从而可以保证数据在不同主机之间传输时能够被正确解释。 网络字节顺序采用：大端（Big Endian）排列方式。 Linux虚拟地址空间如何分布Linux 使用虚拟地址空间，大大增加了进程的寻址空间，由低地址到高地址(下图中从下到上即为从低到高)分别为(口诀: 文初堆栈)： 文本段(只读段)：该部分空间只能读，不可写；(包括：代码段、rodata 段(C常量字符串和#define定义的常量) ) 数据段(初始化数据段与未初始化数据段)：保存初始化了的与未初始化的全局变量、静态变量的空间； 堆 ：就是平时所说的动态内存， malloc/new 大部分都来源于此。其中堆顶的位置可通过函数 brk 和 sbrk 进行动态调整。 文件映射区域 ：如动态库、共享内存等映射物理空间的内存，一般是 mmap 函数所分配的虚拟地址空间。 栈：用于维护函数调用的上下文空间，一般为 8M ，可通过 ulimit –s 查看。 内核虚拟空间：用户代码不可见的内存区域，由内核管理(页表就存放在内核虚拟空间)。上图是 32 位系统典型的虚拟地址空间分布(来自《深入理解计算机系统》)。 brk函数先了解：brk()和sbrk()函数12int brk( const void *addr )void* sbrk ( intptr_t incr ); 这两个函数的作用主要是扩展heap的上界brk。第一个函数的参数为设置的新的brk上界地址，如果成功返回0，失败返回-1。第二个函数的参数为需要申请的内存的大小，然后返回heap新的上界brk地址。如果sbrk的参数为0，则返回的为原来的brk地址。 mmap虚拟内存系统通过将虚拟内存分割为称作虚拟页 (Virtual Page，VP) 大小固定的块，一般情况下，每个虚拟页的大小默认是 4096 字节。同样的，物理内存也被分割为物理页(Physical Page，PP)，也为 4096 字节。 在 LINUX 中我们可以使用 mmap 用来在进程虚拟内存地址空间中分配地址空间，创建和物理内存的映射关系。 映射关系可以分为两种 文件映射 磁盘文件映射进程的虚拟地址空间，使用文件内容初始化物理内存。 匿名映射 一个匿名映射没有对应的文件. 相反, 这种映射的分页会初始化全为 0 的内存空间 而对于映射关系是否共享又分为 私有映射 (MAP_PRIVATE, 也称作写时复制映射) 多进程间数据共享，修改不反应到磁盘实际文件，是一个 copy-on-write（写时复制）的映射方式。 当一个进程试图修改一个分页的内容时, 内核首先会为该进程创建一个新分页并将需要修改的分页中的内容复制到新分页中 共享映射 (MAP_SHARED) 多进程间数据共享，修改反应到磁盘实际文件中。 因此总结起来有 4 种组合, 他们的用途如下: 私有文件映射 多个进程使用同样的物理内存页进行初始化，但是各个进程对内存文件的修改不会共享，也不会反应到物理文件中 私有匿名映射 mmap 会创建一个新的映射，各个进程不共享，这种使用主要用于分配内存 (malloc 分配大内存会调用 mmap)。 例如开辟新进程时，会为每个进程分配虚拟的地址空间，这些虚拟地址映射的物理内存空间各个进程间读的时候共享，写的时候会 copy-on-write。 共享文件映射 可以让多个无关进程通过虚拟内存技术共享同样的物理内存空间，对内存文件 的修改会反应到实际物理文件中，他也是进程间通信 (IPC) 的一种机制。 共享匿名映射 这种机制在进行 fork 的时候不会采用写时复制，父子进程完全共享同样的物理内存页，这也就实现了父子进程通信 (IPC), 但只有相关进程之间才可以这么做 这里值得注意的是，mmap 只是在虚拟内存分配了地址空间，只有在第一次访问虚拟内存的时候才分配物理内存。在 mmap 之后，并没有在将文件内容加载到物理页上，只是在虚拟内存中分配了地址空间。当进程在访问这段地址时，通过查找页表，发现虚拟内存对应的页没有在物理内存中缓存，则产生 “缺页”，由内核的缺页异常处理程序处理，将文件对应内容，以页为单位 (4096) 加载到物理内存，注意是只加载缺页，但也会受操作系统一些调度策略影响，加载的比所需的多。 12void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);int munmap(void *addr, size_t length); 这里要注意的是fd参数，fd为映射的文件描述符，如果是匿名映射，可以设为-1； mmap函数第一种用法是映射磁盘文件到内存中；而malloc使用的是mmap函数的第二种用法，即匿名映射，匿名映射不映射磁盘文件，而是向映射区申请一块内存。 munmap函数是用于释放内存，第一个参数为内存首地址，第二个参数为内存的长度。接下来看下mmap函数的参数。 由于brk/sbrk/mmap属于系统调用，如果每次申请内存，都调用这三个函数中的一个，那么每次都要产生系统调用开销（即cpu从用户态切换到内核态的上下文切换，这里要保存用户态数据，等会还要切换回用户态），这是非常影响性能的；其次，这样申请的内存容易产生碎片，因为堆是从低地址到高地址，如果低地址的内存没有被释放，高地址的内存就不能被回收。 malloc和free原理 malloc: 当申请小内存的时，malloc使用sbrk分配内存 当申请大内存时，使用mmap函数申请内存 但是这只是分配了虚拟内存，还没有映射到物理内存，当访问申请的内存时，才会因为缺页异常，内核分配物理内存。 将所有空闲内存块连成链表，每个节点记录空闲内存块的地址、大小等信息 分配内存时，找到大小合适的块，切成两份，一分给用户，一份放回空闲链表 free时，直接把内存块返还给链表 解决外部碎片：将能够合并的内存块进行合并 malloc函数的实质体现在：它有一个将可用的内存块连接为一个长长的列表的所谓空闲链表。调用malloc函数时，它沿连接表寻找一个大到足以满足用户请求所需要的内存块。然后，将该内存块一分为二（一块的大小与用户请求的大小相等，另一块的大小就是剩下的字节）。接下来，将分配给用户的那块内存传给用户，并将剩下的那块（如果有的话）返回到连接表上。 这里注意，malloc找到的内存块大小一定是会大于等于我们需要的内存大小，下面会提到如果所有的内存块都比要求的小会怎么办？ 调用free函数时，它将用户释放的内存块连接到空闲链上。到最后，空闲链会被切成很多的小内存片段，如果这时用户申请一个大的内存片段，那么空闲链上可能没有可以满足用户要求的片段了。于是，malloc函数请求延时，并开始在空闲链上翻箱倒柜地检查各内存片段，对它们进行整理，将相邻的小空闲块合并成较大的内存块。 在对内存块进行了 free 调用之后，我们需要做的是诸如将它们标记为未被使用的等事情，并且，在调用 malloc 时，我们要能够定位未被使用的内存块。因此， malloc返回的每块内存的起始处首先要有这个结构： 内存控制块结构定义1234struct mem_control_block &#123; int is_available; int size;&#125;; 现在，您可能会认为当程序调用 malloc 时这会引发问题 —— 它们如何知道这个结构？答案是它们不必知道；在返回指针之前，我们会将其移动到这个结构之后，把它隐藏起来。这使得返回的指针指向没有用于任何其他用途的内存。那样，从调用程序的角度来看，它们所得到的全部是空闲的、开放的内存。然后，当通过 free() 将该指针传递回来时，我们只需要倒退几个内存字节就可以再次找到这个结构。 关于 malloc 获得虚存空间的实现，与 glibc 的版本有关，但大体逻辑是： 若分配内存小于 128k ，调用 sbrk() ，将堆顶指针向高地址移动，获得新的虚存空间。 若分配内存大于 128k ，调用 mmap() ，在文件映射区域中分配匿名虚存空间。 接着： VSZ为虚拟内存 RSS为物理内存 VSZ 并不是每次 malloc 后都增长，是与上一节说的堆顶没发生变化有关，因为可重用堆顶内剩余的空间，这样的 malloc 是很轻量快速的。 但如果 VSZ 发生变化，基本与分配内存量相当，因为 VSZ 是计算虚拟地址空间总大小。 RSS 的增量很少，是因为 malloc 分配的内存并不就马上分配实际存储空间，只有第一次使用，如第一次 memset 后才会分配。 由于每个物理内存页面大小是 4k ，不管 memset 其中的 1k 还是 5k 、 7k ，实际占用物理内存总是 4k 的倍数。所以 RSS 的增量总是 4k 的倍数。 因此，不是 malloc 后就马上占用实际内存，而是第一次使用时发现虚存对应的物理页面未分配，产生缺页中断，才真正分配物理页面，同时更新进程页面的映射关系。这也是 Linux 虚拟内存管理的核心概念之一。 vmalloc和kmalloc和malloc的区别 kmalloc和vmalloc是分配的是内核的内存,malloc分配的是用户的内存 kmalloc保证分配的内存在物理上是连续的,vmalloc保证的是在虚拟地址空间上的连续,malloc不保证任何东西(这点是自己猜测的,不一定正确) kmalloc能分配的大小有限,vmalloc和malloc能分配的大小相对较大 内存只有在要被DMA访问的时候才需要物理上连续 vmalloc比kmalloc要慢 对于提供了MMU（存储管理器，辅助操作系统进行内存管理，提供虚实地址转换等硬件支持）的处理器而言，Linux提供了复杂的存储管理系统，使得进程所能访问的内存达到4GB。 进程的4GB内存空间被人为的分为两个部分–用户空间与内核空间。用户空间地址分布从0到3GB(PAGE_OFFSET，在0x86中它等于0xC0000000)，3GB到4GB为内核空间。 内核空间中，从3G到vmalloc_start这段地址是物理内存映射区域（该区域中包含了内核镜像、物理页框表mem_map等等），比如我们使用 的 VMware虚拟系统内存是160M，那么3G～3G+160M这片内存就应该映射物理内存。在物理内存映射区之后，就是vmalloc区域。对于 160M的系统而言，vmalloc_start位置应在3G+160M附近（在物理内存映射区与vmalloc_start期间还存在一个8M的gap 来防止跃界），vmalloc_end的位置接近4G(最后位置系统会保留一片128k大小的区域用于专用页面映射) 一般情况下，只有硬件设备才需要物理地址连续的内存，因为硬件设备往往存在于MMU之外，根本不了解虚拟地址；但为了性能上的考虑，内核中一般使用kmalloc(),而只有在需要获得大块内存时才使用vmalloc，例如当模块被动态加载到内核当中时，就把模块装载到由vmalloc（）分配的内存上。 kmalloc: kmalloc申请的是较小的连续的物理内存，内存物理地址上连续，虚拟地址上也是连续的，使用的是内存分配器slab的一小片。申请的内存位于物理内存的映射区域。其真正的物理地址只相差一个固定的偏移。而且不对获得空间清零。可以查看slab分配器 kzalloc: 用kzalloc申请内存的时候， 效果等同于先是用 kmalloc() 申请空间 , 然后用 memset() 来初始化 ,所有申请的元素都被初始化为 0. vmalloc: vmalloc用于申请较大的内存空间，虚拟内存是连续。申请的内存的则位于vmalloc_start～vmalloc_end之间，与物理地址没有简单的转换关系，虽然在逻辑上它们也是连续的，但是在物理上它们不要求连续。 malloc: malloc分配的是用户的内存。除非被阻塞否则他执行的速度非常快，而且不对获得空间清零。 Buddy（伙伴）分配算法参考: https://zhuanlan.zhihu.com/p/149581303 伙伴系统用于管理物理页，主要目的在于维护可用的连续物理空间，避免外部碎片。所有关于内存分配的操作都会与其打交道，buddy是物理内存的管理的门户 Linux 内核引入了伙伴系统算法（Buddy system），什么意思呢？就是把相同大小的页框块用链表串起来，页框块就像手拉手的好伙伴，也是这个算法名字的由来。 具体的，所有的空闲页框分组为11个块链表，每个块链表分别包含大小为1，2，4，8，16，32，64，128，256，512和1024个连续页框的页框块。最大可以申请1024个连续页框，对应4MB大小的连续内存。 伙伴系统:因为任何正整数都可以由 2^n 的和组成，所以总能找到合适大小的内存块分配出去，减少了外部碎片产生 。 分配实例:比如：我需要申请4个页框，但是长度为4个连续页框块链表没有空闲的页框块，伙伴系统会从连续8个页框块的链表获取一个，并将其拆分为两个连续4个页框块，取其中一个，另外一个放入连续4个页框块的空闲链表中。释放的时候会检查，释放的这几个页框前后的页框是否空闲，能否组成下一级长度的块。 Slab分配器伙伴系统和slab不是二选一的关系，slab 内存分配器是对伙伴分配算法的补充 slab的目的在于避免内部碎片。从buddy系统获取的内存至少是一个页，也就是4K，如果仅仅需要8字节的内存，显然巨大的内部碎片无法容忍。 slab从buddy系统申请空间，将较大的连续内存拆分成一系列较小的内存块。 用户申请空间时从slab中获取大小最相近的小块内存，这样可以有效减少内部碎片。在slab最大的块为8K，slab中所有块在物理上也是连续的。 上面说的用于内存分配的slab是通用的slab，主要用于支持kmalloc分配内存。 slab还有一个作用就是用作对象池，针对经常分配和回收的对象比如task_struct，可以分配一个slab对象池对其优化。这种slab是独立于通用的内存分配slab的，在内核中有很多这样的针对特定对象的slab。 在内核中想要分配一段连续的内存，首先向slab系统申请，如果不满足（超过两个页面，也就是8K），直接向buddy系统申请。如果还不满足（超过4M，也就是1024个页面），将无法获取到连续的物理地址。可以通过vmalloc获取虚拟地址空间连续，但物理地址不连续的更大的内存空间。 malloc是用户态使用的内存分配接口，最终还是向buddy申请内存，因为buddy系统是管理物理内存的门户。申请到大块内存后，再像slab一样对其进行细分维护，根据用户需要返回相应内存的指针。 fork内存语义 共享代码段, 子指向父 : 父子进程共享同一代码段, 子进程的页表项指向父进程相同的物理内存页(即数据段/堆段/栈段的各页) 写时复制(copy-on-write) : 内核会捕获所有父进程或子进程针对这些页面(即数据段/堆段/栈段的各页)的修改企图, 并为将要修改的页面创建拷贝, 将新的页面拷贝分配给遭内核捕获的进程, 从此父/子进程可以分别修改各自的页拷贝, 不再相互影响. 虽然fork创建的子进程不需要拷贝父进程的物理内存空间, 但是会复制父进程的空间内存页表. 例如对于10GB的redis进程, 需要复制约20MB的内存页表, 因为此fork操作耗时跟进程总内存量息息相关 零(CPU)拷贝参考 https://juejin.im/post/6844903949359644680 “先从简单开始，实现下这个场景：从一个文件中读出数据并将数据传到另一台服务器上？”大概伪代码如下:12File.read(file, buf, len);Socket.send(socket, buf, len); 可以看出, 这样效率是很低的. 下图分别对应传统 I/O 操作的数据读写流程，整个过程涉及 2 次 CPU 拷贝、2 次 DMA 拷贝总共 4 次拷贝，以及 4 次上下文切换，下面简单地阐述一下相关的概念。 上下文切换：当用户程序向内核发起系统调用时，CPU 将用户进程从用户态切换到内核态；当系统调用返回时，CPU 将用户进程从内核态切换回用户态。 CPU拷贝：由 CPU 直接处理数据的传送，数据拷贝时会一直占用 CPU 的资源。 DMA拷贝：由 CPU 向DMA磁盘控制器下达指令，让 DMA 控制器来处理数据的传送，数据传送完毕再把信息反馈给 CPU，从而减轻了 CPU 资源的占有率。 传统读操作当应用程序执行 read 系统调用读取一块数据的时候，如果这块数据已经存在于用户进程的页内存中，就直接从内存中读取数据；如果数据不存在，则先将数据从磁盘加载数据到内核空间的读缓存（read buffer）中，再从读缓存拷贝到用户进程的页内存中。1read(file_fd, tmp_buf, len); 复制代码基于传统的 I/O 读取方式，read 系统调用会触发 2 次上下文切换，1 次 DMA 拷贝和 1 次 CPU 拷贝，发起数据读取的流程如下： 用户进程通过 read() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。 CPU利用DMA控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。 CPU将读缓冲区（read buffer）中的数据拷贝到用户空间（user space）的用户缓冲区（user buffer）。 上下文从内核态（kernel space）切换回用户态（user space），read 调用执行返回。 传统写操作当应用程序准备好数据，执行 write 系统调用发送网络数据时，先将数据从用户空间的页缓存拷贝到内核空间的网络缓冲区（socket buffer）中，然后再将写缓存中的数据拷贝到网卡设备完成数据发送。1write(socket_fd, tmp_buf, len); 复制代码基于传统的 I/O 写入方式，write() 系统调用会触发 2 次上下文切换，1 次 CPU 拷贝和 1 次 DMA 拷贝，用户程序发送网络数据的流程如下： 用户进程通过 write() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。 CPU 将用户缓冲区（user buffer）中的数据拷贝到内核空间（kernel space）的网络缓冲区（socket buffer）。 CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。 上下文从内核态（kernel space）切换回用户态（user space），write 系统调用执行返回。 sendfilesendfile 系统调用在 Linux 内核版本 2.1 中被引入，目的是简化通过网络在两个通道之间进行的数据传输过程。sendfile 系统调用的引入，不仅减少了 CPU 拷贝的次数，还减少了上下文切换的次数，它的伪代码如下：1sendfile(socket_fd, file_fd, len); 复制代码通过 sendfile 系统调用，数据可以直接在内核空间内部进行 I/O 传输，从而省去了数据在用户空间和内核空间之间的来回拷贝。与 mmap 内存映射方式不同的是， sendfile 调用中 I/O 数据对用户空间是完全不可见的。也就是说，这是一次完全意义上的数据传输过程。 基于 sendfile 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝，用户程序读写数据的流程如下： 用户进程通过 sendfile() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。 CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。 CPU 将读缓冲区（read buffer）中的数据拷贝到的网络缓冲区（socket buffer）。 CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。 上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。 相比较于 mmap 内存映射的方式，sendfile 少了 2 次上下文切换，但是仍然有 1 次 CPU 拷贝操作。sendfile 存在的问题是用户程序不能对数据进行修改，而只是单纯地完成了一次数据传输过程。 “这样确实改善了很多，但还没达到零拷贝的要求（还有一次cpu参与的拷贝），还有其它黑技术？”“对的，如果底层网络接口卡支持收集(gather)操作的话，就可以进一步的优化。”“怎么说？”“继续看下一小节” sendfile + DMA gather copyLinux 2.4 版本的内核对 sendfile 系统调用进行修改，如果底层网络接口卡支持收集(gather)操作的话, 为 DMA 拷贝引入了 gather 操作。它将内核空间（kernel space）的读缓冲区（read buffer）中对应的数据描述信息（内存地址、地址偏移量）记录到相应的网络缓冲区（ socket buffer）中，由 DMA 根据内存地址、地址偏移量将数据批量地从读缓冲区（read buffer）拷贝到网卡设备中，这样就省去了内核空间中仅剩的 1 次 CPU 拷贝操作，sendfile 的伪代码如下：1sendfile(socket_fd, file_fd, len); 复制代码在硬件的支持下，sendfile 拷贝方式不再从内核缓冲区的数据拷贝到 socket 缓冲区，取而代之的仅仅是缓冲区文件描述符和数据长度的拷贝，这样 DMA 引擎直接利用 gather 操作将页缓存中数据打包发送到网络中即可，本质就是和虚拟内存映射的思路类似。 基于 sendfile + DMA gather copy 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换、0 次 CPU 拷贝以及 2 次 DMA 拷贝，用户程序读写数据的流程如下： 用户进程通过 sendfile() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。 CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。 CPU 把读缓冲区（read buffer）的文件描述符（file descriptor）和数据长度拷贝到网络缓冲区（socket buffer）。 基于已拷贝的文件描述符（file descriptor）和数据长度，CPU 利用 DMA 控制器的 gather/scatter 操作直接批量地将数据从内核的读缓冲区（read buffer）拷贝到网卡进行数据传输。 上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。 sendfile + DMA gather copy 拷贝方式同样存在用户程序不能对数据进行修改的问题，而且本身需要硬件的支持，它只适用于将数据从文件拷贝到 socket 套接字上的传输过程。 TCP包头长度 20个字节 三次握手 如果第三次握手的ack丢失了咋办当客户端收到服务端的SYNACK应答后，其状态变为ESTABLISHED，并会发送ACK包给服务端，准备发送数据了。如果此时ACK在网络中丢失（如上图所示），过了超时计时器后，那么服务端会重新发送SYNACK包，重传次数根据/proc/sys/net/ipv4/tcp_synack_retries来指定，默认是5次。如果重传指定次数到了后，仍然未收到ACK应答，那么一段时间后，Server自动关闭这个连接。 问题就在这里，客户端已经认为连接建立，而服务端则可能处在SYN-RCVD或者CLOSED，接下来我们需要考虑这两种情况下服务端的应答： 服务端处于CLOSED，当接收到连接已经关闭的请求时，服务端会返回RST 报文，客户端接收到后就会关闭连接，如果需要的话则会重连，那么那就是另一个三次握手了。 服务端处于SYN-RCVD，此时如果接收到正常的ACK 报文，那么很好，连接恢复，继续传输数据；如果接收到写入数据等请求呢？注意了，此时写入数据等请求也是带着ACK 报文的，实际上也能恢复连接，使服务器恢复到ESTABLISHED状态，继续传输数据。 SYN-Flood与SYN-Cookie所谓SYN-Flood(SYN 洪泛攻击)，就是利用SYNACK 报文的时候，服务器会为客户端请求分配缓存，那么黑客（攻击者），就可以使用一批虚假的ip向服务器大量地发建立TCP 连接的请求，服务器为这些虚假ip分配了缓存后，处在SYN_RCVD状态，存放在半连接队列中；另外，服务器发送的请求又不可能得到回复（ip都是假的，能回复就有鬼了），只能不断地重发请求，直到达到设定的时间/次数后，才会关闭。 服务器不断为这些半开连接分配资源，导致服务器的连接资源被消耗殆尽，不过所幸，我们可以使用SYN Cookie进行稍微的防御一下。 所谓的SYN Cookie防御系统，与前面接收到SYN 报文就分配缓存不同，此时暂不分配资源；同时利用SYN 报文的源和目的地IP和端口，以及服务器存储的一个秘密数，使用它们进行散列，得到server_isn作为服务端的初始 TCP 序号，也就是所谓的SYN cookie, 然后将SYNACK 报文中发送给客户端，接下来就是对ACK 报文进行判断，如果其返回的ack里的确认号正好等于server_isn + 1，说明这是一个合法的ACK，那么服务器才会为其生成一个具有套接字的全开的连接。(有点类似于JWT那一套机制哈) 缺点: 增加了密码学运算, 增大了cpu消耗 因为没有保存半连接状态, 所以无法存储一些比如大窗口/sack等信息 四次挥手 timewait的意义 2msl之后网络中的数据分节全部消失, 防止影响到复用了原端口ip的新连接 如果b没收到最后一个ack, b就会重发fin, a如果不维护一个timewait却收到了一个fin会感觉莫名其妙然后响应一个rst, 然后b就会解释为一个错误 timewait和closewait太多咋办 timewait太多咋办? net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭； net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。 net.ipv4.tcp_fin_timeout这个时间可以减少在异常情况下服务器从FIN-WAIT-2转到TIME_WAIT的时间。 closewait太多咋办? 解决方案只有: 查代码. 因为如果一直保持在CLOSE_WAIT状态，那么只有一种情况，就是在对方关闭连接之后服务器程序自己没有进一步发出fin信号。换句话说，就是在对方连接关闭之后，程序里没有检测到，或者由于什么逻辑bug导致服务端没有主动发起close, 或者程序压根就忘记了这个时候需要关闭连接，于是这个资源就一直被程序占着。 tcp拥塞控制 快速重传: 报文段1成功接收并被确认ACK 2，接收端的期待序号为2，当报文段2丢失，报文段3失序到来，与接收端的期望不匹配，接收端重复发送冗余ACK 2。这样，如果在超时重传定时器溢出之前，接收到连续的三个重复冗余ACK（其实是收到4个同样的ACK，第一个是正常的，后三个才是冗余的），发送端便知晓哪个报文段在传输过程中丢失了，于是重发该报文段，不需要等待超时重传定时器溢出，大大提高了效率。这便是快速重传机制。 快速恢复 慢启动 拥塞避免 tcp滑动窗口 每个TCP连接的两端都维护一组窗口：发送窗口结构（send window structure）和接收窗口结构（receive window structure）。TCP以字节为单位维护其窗口结构。TCP头部中的窗口大小字段相对ACK号有一个字节的偏移量。发送端计算其可用窗口，即它可以立即发送的数据量。可用窗口（允许发送但还未发送）计算值为提供窗口（即由接收端通告的窗口）大小减去在传（已发送但未得到确认）的数据量。图中P1、P2、P3分别记录了窗口的左边界、下次发送的序列号、右边界。 如上图所示， 随着发送端接收到返回的数据ACK，滑动窗口也随之右移。发送端根据接收端返回的ACK可以得到两个重要的信息：一是接收端期望收到的下一个字节序号；二是当前的窗口大小（再结合发送端已有的其他信息可以得出还能发送多少字节数据）。 需要注意的是：发送窗口的左边界只能右移，因为它控制的是已发送并受到确认的数据，具有累积性，不能返回；右边界可以右移也可以左移（能左移的右边界会带来一些缺陷，下文会讲到）。 接收端也维护一个窗口结构，但比发送窗口简单（只有左边界和右边界）。该窗口结构记录了已接收并确认的数据，以及它能够接收的最大序列号，该窗口能保证接收数据的正确性（避免存储重复的已接收和确认的数据，以及避免存储不应接收的数据）。由于TCP的累积ACK特性，只有当到达数据序列号等于左边界时，窗口才能向前滑动。 零窗口与TCP持续计时器 Zero Window 上图，我们可以看到一个处理缓慢的Server（接收端）是怎么把Client（发送端）的TCP Sliding Window给降成0的。此时，你一定会问，如果Window变成0了，TCP会怎么样？是不是发送端就不发数据了？是的，发送端就不发数据了，你可以想像成“Window Closed”，那你一定还会问，如果发送端不发数据了，接收方一会儿Window size 可用了，怎么通知发送端呢？ 解决这个问题，TCP使用了Zero Window Probe技术，缩写为ZWP，也就是说，client在server窗口变成0后，会发ZWP的包给server，让server来告诉client此时server的Window尺寸，一般这个值会设置成3次，第次大约30-60秒（不同的实现可能会不一样）。如果3次过后还是0的话，有的TCP实现就会发RST把链接断了。 Nagle算法与CORK算法区别 cork算法: 所谓的CORK就是塞子的意思，形象地理解就是用CORK将连接塞住，使得数据先不发出去，等到拔去塞子后再发出去。设置该选项后，内核会尽力把小数据包拼接成一个大的数据包（一个MTU）再发送出去，当然若一定时间后（一般为200ms，该值尚待确认），内核仍然没有组合成一个MTU时也必须发送现有的数据（不可能让数据一直等待吧）。 Nagle算法: 基本定义是任意时刻，最多只能有一个未被确认的小段。 所谓“小段”，指的是小于MSS尺寸的数据块，所谓“未被确认”，是指一个数据块发送出去后，没有收到对方发送的ACK确认该数据已收到。 默认情况下，发送数据采用Nagle 算法。这样虽然提高了网络吞吐量，但是实时性却降低了，在一些交互性很强的应用程序来说是不允许的，使用TCP_NODELAY选项可以禁止Nagle 算法。此时，应用程序向内核递交的每个数据包都会立即发送出去。需要注意的是，虽然禁止了Nagle 算法，但网络的传输仍然受到TCP确认延迟机制的影响。 异同点:Nagle算法和CORK算法非常类似，但是它们的着眼点不一样，Nagle算法主要避免网络因为太多的小包（协议头的比例非常之大）而拥塞，而CORK算法则是为了提高网络的利用率，使得总体上协议头占用的比例尽可能的小。如此看来这二者在避免发送小包上是一致的，在用户控制的层面上，Nagle算法完全不受用户socket的控制，你只能简单的设置TCP_NODELAY而禁用它，CORK算法同样也是通过设置或者清除TCP_CORK使能或者禁用之，然而Nagle算法关心的是网络拥塞问题，只要所有的ACK回来则发包，而CORK算法却可以关心内容，在前后数据包发送间隔很短的前提下（很重要，否则内核会帮你将分散的包发出），即使你是分散发送多个小数据包，你也可以通过使能CORK算法将这些内容拼接在一个包内，如果此时用Nagle算法的话，则可能做不到这一点。 ACK延迟确认机制接收方在收到数据后，并不会立即回复ACK,而是延迟一定时间。一般ACK延迟发送的时间为200ms，但这个200ms并非收到数据后需要延迟的时间。系统有一个固定的定时器每隔200ms会来检查是否需要发送ACK包。这样做有两个目的。 这样做的目的是ACK是可以合并的，也就是指如果连续收到两个TCP包，并不一定需要ACK两次，只要回复最终的ACK就可以了，可以降低网络流量。 如果接收方有数据要发送，那么就会在发送数据的TCP数据包里，带上ACK信息。这样做，可以避免大量的ACK以一个单独的TCP包发送，减少了网络流量。 HTTP与HTTPS下面实例是一点典型的使用GET来传递数据的实例,客户端请求：1234GET /hello.txt HTTP/1.1User-Agent: curl/7.16.3 libcurl/7.16.3 OpenSSL/0.9.7l zlib/1.2.3Host: www.example.comAccept-Language: en, mi 服务端响应:123456789HTTP/1.1 200 OKDate: Mon, 27 Jul 2009 12:28:53 GMTServer: ApacheLast-Modified: Wed, 22 Jul 2009 19:15:56 GMTETag: &quot;34aa387-d-1568eb00&quot;Accept-Ranges: bytesContent-Length: 51Vary: Accept-EncodingContent-Type: text/plain 输出结果：1Hello World! My payload includes a trailing CRLF. 客户端请求消息客户端发送一个HTTP请求到服务器的请求消息包括以下格式: 请求行（request line） 请求头部（header） 空行 请求数据 由四个部分组成，下图给出了请求报文的一般格式。 服务器响应消息HTTP响应也由四个部分组成，分别是: 状态行 消息报头 空行 响应正文 httpsHTTPS 协议（HyperText Transfer Protocol over Secure Socket Layer）：一般理解为HTTP+SSL/TLS，通过 SSL证书来验证服务器的身份，并为浏览器和服务器之间的通信进行加密。 那么SSL/TLS又是什么？ SSL（Secure Socket Layer，安全套接字层）：1994年为 Netscape 所研发，SSL 协议位于 TCP/IP 协议与各种应用层协议之间，为数据通讯提供安全支持。 TLS（Transport Layer Security，传输层安全）：其前身是 SSL，它最初的几个版本（SSL 1.0、SSL 2.0、SSL 3.0）由网景公司开发，1999年从 3.1 开始被 IETF 标准化并改名，发展至今已经有 TLS 1.0、TLS 1.1、TLS 1.2 三个版本。SSL3.0和TLS1.0由于存在安全漏洞，已经很少被使用到。TLS 1.3 改动会比较大，目前还在草案阶段，目前使用最广泛的是TLS 1.1、TLS 1.2。 https 不是一种新的协议，只是 http 的通信接口部分使用了 ssl 和 tsl 协议替代，加入了加密、证书、完整性保护的功能，下面解释一下加密和证书，如下图所示 对称加密也叫共享密钥加密, 加密和解密公用一套秘钥，这样就会产生问题，已共享秘钥加密方式必须将秘钥传送给对方，但如果通信被监听，那么秘钥可能会被泄漏产生危险。常见对称加密算法有des, aes 非对称加密也叫公开秘钥加密, 使用一种非对称加密的算法，使用一对非对称的秘钥，一把叫做公有秘钥，一把叫做私有秘钥，在加密的时候，通信的一方使用公有秘钥进行加密，通信的另一方使用私有秘钥进行解密，利用这种方式不需要发送私有秘钥，也就不存在泄漏的风险了。常见非对称加密算法有rsa https 加密方式因为公开秘钥加密的方式比共享秘钥加密的方式钥消耗 cpu 资源，https 采取了混合加密的方式，来结合两者的优点。 在秘钥交换阶段使用公开加密的方式，之后建立连接后使用共享秘钥加密方式进行加密，如下图。 为什么要使用证书因为公开加密还存在一些问题就是无法证明公开秘钥的正确性(有可能被黑客中间替换成了黑客自己的公钥, 然后黑客伪装成服务器/客户端做中间转发)，为了解决这个问题，https 采取了有数字证实认证机构和其相关机构颁发的公开秘钥证书，通信过程如下图所示。 解释一下上图的步骤： 服务器将自己的公开秘钥传到数字证书认证机构 数字证书认证机构使用自己的秘钥来对传来的服务器公钥进行加密，并颁发数字证书 服务器将传回的公钥证书发送给客户端，客户端使用数字机构颁发的公开秘钥来验证证书的有效性，以及公开秘钥的真实性 证书签名是先将证书信息（证书机构名称、有效期、拥有者、拥有者公钥）进行hash，再用CA的私有密钥对hash值加密而生成的。 所以拦截者虽然可以拦截并篡改证书信息（主要是拥有者和拥有者的公钥），但是由于拦截者没有CA的私钥，所以无法生成正确的签名，从而导致客户端拿到签名后，用CA公有密钥对证书签名解密后值与用证书计算出来的实际hash值不一样，从而得不到客户端信任。(其实这个ca公钥和私钥也就是非对称加密的思想了) 客户端使用服务器的公开秘钥进行消息加密，后发送给服务器。 服务器使用私有秘钥进行解密。 浏览器在安装的时候会内置可信的数字证书机构的公开秘钥，如下图所示。 这就是为什么我们使用自己生成的证书的时候会产生安全警告的原因。 再附一张 https 的具体通信步骤和图解。 cookie服务器发送的响应报文包含 Set-Cookie 首部字段，客户端得到响应报文后把 Cookie 内容保存到浏览器中。1234HTTP/1.0 200 OKContent-type: text/htmlSet-Cookie: yummy_cookie=chocoSet-Cookie: tasty_cookie=strawberry 客户端之后对同一个服务器发送请求时，会从浏览器中取出 Cookie 信息并通过 Cookie 请求首部字段发送给服务器。123GET /sample_page.html HTTP/1.1Host: www.example.orgCookie: yummy_cookie=choco; tasty_cookie=strawberry Domain 标识指定了哪些主机可以接受 Cookie。如果不指定，默认为当前文档的主机（不包含子域名）。如果指定了 Domain，则一般包含子域名。例如，如果设置 Domain=mozilla.org，则 Cookie 也包含在子域名中（如 developer.mozilla.org）。 Path 标识指定了主机下的哪些路径可以接受 Cookie（该 URL 路径必须存在于请求 URL 中）。以字符 %x2F (“/“) 作为路径分隔符，子路径也会被匹配。例如，设置 Path=/docs，则以下地址都会匹配： /docs /docs/Web/ /docs/Web/HTTP session如何保存较好 一个用户的 Session 信息如果存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器，由于服务器没有用户的 Session 信息，那么该用户就需要重新进行登录等操作..有什么好的解决方案呢?Session Server使用一个单独的服务器存储 Session 数据，可以使用传统的 MySQL，也使用 Redis 或者 Memcached 这种内存型数据库。 优点： 为了使得大型网站具有伸缩性，集群中的应用服务器通常需要保持无状态，那么应用服务器不能存储用户的会话信息。Session Server 将用户的会话信息单独进行存储，从而保证了应用服务器的无状态。 缺点： 需要去实现存取 Session 的代码 cookie和session和token的区别 由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识具体的用户，这个机制就是Session.典型的场景比如购物车，当你点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用户创建了特定的Session，用用于标识这个用户，并且跟踪用户，这样才知道购物车里面有几本书。这个Session是保存在服务端的，有一个唯一标识。在服务端保存Session的方法很多，内存、数据库、文件都有。集群的时候也要考虑Session的转移，在大型的网站，一般会有专门的Session服务器集群，用来保存用户会话，这个时候 Session 信息都是放在内存的，使用一些缓存服务比如Memcached之类的来放 Session。 思考一下服务端如何识别特定的客户？这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端，需要在 Cookie 里面记录一个Session ID，以后每次请求把这个会话ID发送到服务器，我就知道你是谁了。有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。 Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。所以，总结一下：Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中；Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。 为什么需要token来替代session机制? 因为session的存储对服务器说是一个巨大的开销， 严重的限制了服务器扩展能力， 比如说我用两个机器组成了一个集群， 小 F 通过机器 A 登录了系统， 那 session id 会保存在机器 A 上， 假设小 F 的下一次请求被转发到机器 B 怎么办？ 机器 B 可没有小 F 的 session id 啊。有时候会采用一点小伎俩： session sticky ， 就是让小 F 的请求一直粘连在机器 A 上， 但是这也不管用， 要是机器 A 挂掉了， 还得转到机器 B 去。 接下来我们介绍事实上的token标准JWT JWTsessionId 的方式本质是把用户状态信息维护在 server 端，token 的方式就是把用户的状态信息加密成一串 token 传给前端，然后每次发请求时把 token 带上，传回给服务器端；服务器端收到请求之后，解析 token 并且验证相关信息(用jwt的header里的加密方式然后根据自己的不公开的密钥把jwt中的payload用加密一下得到一个签名 s, 然后用s对比看看是不是跟jwt里的signature相等, 相等则说明token对了)； 备注: 对于数据校验，专门的消息认证码生成算法, HMAC - 一种使用单向散列函数构造消息认证码的方法，其过程是不可逆的、唯一确定的，并且使用密钥来生成认证码，其目的是防止数据在传输过程中被篡改或伪造。将原始数据与认证码一起传输，数据接收端将原始数据使用相同密钥和相同算法再次生成认证码，与原有认证码进行比对，校验数据的合法性。 所以跟第一种登录方式最本质的区别是：通过解析 token 的计算时间换取了 session 的存储空间 业界通用的加密方式是 jwt, jwt 的具体格式如图：简单的介绍一下 jwt，它主要由 3 部分组成：1234567891011121314151617181920header 头部&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;&#125;payload 负载&#123; &quot;sub&quot;: &quot;1234567890&quot;, &quot;name&quot;: &quot;John Doe&quot;, &quot;iat&quot;: 1516239022, &quot;exp&quot;: 1555341649998&#125;signature 签名&#123; HMACSHA256( base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload), your-256-bit-secret ) secret base64 encoded&#125; header header 里面描述加密算法和 token 的类型，类型一般都是 JWT； payload 里面放的是用户的信息，也就是第一种登录方式中需要维护在服务器端 session 中的信息； signature 是对前两部分的签名，也可以理解为加密；实现需要一个密钥（secret），这个 secret 只有服务器才知道，然后使用 header 里面的算法按照如下方法来加密： 1234HMACSHA256(base64UrlEncode(header) + "." +base64UrlEncode(payload),secret) 总之，最后的 jwt = base64url(header) + &quot;.&quot; + base64url(payload) + &quot;.&quot; + signaturejwt 可以放在 response 中返回，也可以放在 cookie 中返回，这都是具体的返回方式，并不重要。客户端发起请求时，官方推荐放在 HTTP header 中：1Authorization: Bearer &lt;token&gt; 这样子确实也可以解决 cookie 跨域(比如移动平台上对cookie支持不好)的问题，不过具体放在哪儿还是根据业务场景来定，并没有一定之规。 jwt过期了如何刷新前面讲的 Token，都是 Access Token，也就是访问资源接口时所需要的 Token，还有另外一种 Token，Refresh Token，通常情况下，Refresh Token 的有效期会比较长，而 Access Token 的有效期比较短，当 Access Token 由于过期而失效时，使用 Refresh Token 就可以获取到新的 Access Token，如果 Refresh Token 也失效了，用户就只能重新登录了。 在 JWT 的实践中，引入 Refresh Token，将会话管理流程改进如下: 客户端使用用户名密码进行认证 服务端生成有效时间较短的 Access Token（例如 10 分钟），和有效时间较长的 Refresh Token（例如 7 天） 客户端访问需要认证的接口时，携带 Access Token 如果 Access Token 没有过期，服务端鉴权后返回给客户端需要的数据 如果携带 Access Token 访问需要认证的接口时鉴权失败（例如返回 401 错误），则客户端使用 Refresh Token 向刷新接口申请新的 Access Token 如果 Refresh Token 没有过期，服务端向客户端下发新的 Access Token 客户端使用新的 Access Token 访问需要认证的接口 常见的HTTP相应状态码总之：(一般标准用法是这样用哈, 但是真的写代码的时候其实跟get/post/put一样, 想怎么用全看自己, 前后端开发人员协商好就行) 1XX：消息 2XX：成功 3XX：重定向 4XX：请求错误 5XX、6XX：服务器错误 常见状态代码、状态描述的说明如下: 200 OK:请求已成功，请求所希望的响应头或数据体将随此响应返回。实际的响应将取决于所使用的请求方法。在GET请求中，响应将包含与请求的资源相对应的实体。在POST请求中，响应将包含描述或操作结果的实体。[7] 301 Moved Permanently:被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个URI之一。如果可能，拥有链接编辑功能的客户端应当自动把请求的地址修改为从服务器反馈回来的地址。[19]除非额外指定，否则这个响应也是可缓存的。新的永久性的URI应当在响应的Location域中返回。除非这是一个HEAD请求，否则响应的实体中应当包含指向新的URI的超链接及简短说明。如果这不是一个GET或者HEAD请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。注意：对于某些使用HTTP/1.0协议的浏览器，当它们发送的POST请求得到了一个301响应的话，接下来的重定向请求将会变成GET方式。 400 Bad Request由于明显的客户端错误（例如，格式错误的请求语法，太大的大小，无效的请求消息或欺骗性路由请求），服务器不能或不会处理该请求。[31] 401 Unauthorized（RFC 7235）参见：HTTP基本认证、HTTP摘要认证类似于403 Forbidden，401语义即“未认证”，即用户没有必要的凭据。[32]该状态码表示当前请求需要用户验证。该响应必须包含一个适用于被请求资源的WWW-Authenticate信息头用以询问用户信息。客户端可以重复提交一个包含恰当的Authorization头信息的请求。[33]如果当前请求已经包含了Authorization证书，那么401响应代表着服务器验证已经拒绝了那些证书。如果401响应包含了与前一个响应相同的身份验证询问，且浏览器已经至少尝试了一次验证，那么浏览器应当向用户展示响应中包含的实体信息，因为这个实体信息中可能包含了相关诊断信息。注意：当网站（通常是网站域名）禁止IP地址时，有些网站状态码显示的401，表示该特定地址被拒绝访问网站。 403 Forbidden主条目：HTTP 403服务器已经理解请求，但是拒绝执行它。与401响应不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。如果这不是一个HEAD请求，而且服务器希望能够讲清楚为何请求不能被执行，那么就应该在实体内描述拒绝的原因。当然服务器也可以返回一个404响应，假如它不希望让客户端获得任何信息。 404 Not Found主条目：HTTP 404请求失败，请求所希望得到的资源未被在服务器上发现，但允许用户的后续请求。[35]没有信息能够告诉用户这个状况到底是暂时的还是永久的。假如服务器知道情况的话，应当使用410状态码来告知旧资源因为某些内部的配置机制问题，已经永久的不可用，而且没有任何可以跳转的地址。404这个状态码被广泛应用于当服务器不想揭示到底为何请求被拒绝或者没有其他适合的响应可用的情况下。 500 Internal Server Error通用错误消息，服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。没有给出具体错误信息。[59] 503 Service Unavailable由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是暂时的，并且将在一段时间以后恢复。[62]如果能够预计延迟时间，那么响应中可以包含一个Retry-After头用以标明这个延迟时间。如果没有给出这个Retry-After信息，那么客户端应当以处理500响应的方式处理它。 get和post的本质区别从设计初衷上来说，GET 用来实现从服务端取数据，POST 用来实现向服务端提出请求对数据做某些修改，也因此如果你向nginx用post请求静态文件，nginx会直接返回 405 not allowed，但是服务端毕竟是人实现的，你可以让 POST 做 GET 相同的事情 get请求的参数一般放在url中，但是浏览器和服务器程序对url长度还是有限制的。post请求的参数一般放在body，你硬要放到url中也可以。 在RESTful风格中，get用于从服务器获获取数据，而post用于创建数据 Connection: keep-alive在早期的HTTP/1.0中，每次http请求都要创建一个连接，而创建连接的过程需要消耗资源和时间，为了减少资源消耗，缩短响应时间，就需要重用连接。在后来的HTTP/1.0中以及HTTP/1.1中，引入了重用连接的机制，就是在http请求头中加入Connection: keep-alive来告诉对方这个请求响应完成后不要关闭，下一次咱们还用这个请求继续交流。协议规定HTTP/1.0如果想要保持长连接，需要在请求头中加上Connection: keep-alive，而HTTP/1.1默认是支持长连接的，有没有这个请求头都行。 要实现长连接很简单，只要客户端和服务端都保持这个http长连接即可。但问题的关键在于保持长连接后，浏览器如何知道服务器已经响应完成？在使用短连接的时候，服务器完成响应后即关闭http连接，这样浏览器就能知道已接收到全部的响应，同时也关闭连接（TCP连接是双向的）。 在使用长连接的时候，响应完成后服务器是不能关闭连接的，那么它就要在响应头中加上特殊标志告诉浏览器已响应完成。一般情况下这个特殊标志就是Content-Length，来指明响应体的数据大小，比如Content-Length: 120表示响应体内容有120个字节，这样浏览器接收到120个字节的响应体后就知道了已经响应完成。 由于Content-Length字段必须真实反映响应体长度，但实际应用中，有些时候响应体长度并没那么好获得，例如响应体来自于网络文件，或者由动态语言生成。这时候要想准确获取长度，只能先开一个足够大的内存空间，等内容全部生成好再计算。但这样做一方面需要更大的内存开销，另一方面也会让客户端等更久。这时候Transfer-Encoding: chunked响应头就派上用场了，该响应头表示响应体内容用的是分块传输，此时服务器可以将数据一块一块地分块响应给浏览器而不必一次性全部响应，待浏览器接收到全部分块后就表示响应结束。 具体格式如下: 如果一个HTTP消息（包括客户端发送的请求消息或服务器返回的应答消息）的Transfer-Encoding消息头的值为chunked，那么，消息体由数量未定的块组成，并以最后一个大小为0的块为结束。 每一个非空的块都以该块包含数据的字节数（字节数以十六进制表示）开始，跟随一个CRLF （回车及换行），然后是数据本身，最后块CRLF结束。在一些实现中，块大小和CRLF之间填充有白空格（0x20） 最后一块是单行，由块大小（0），一些可选的填充白空格，以及CRLF。最后一块不再包含任何数据，但是可以发送可选的尾部，包括消息头字段。 消息最后以CRLF结尾。 以分块传输一段文本内容：“人的一生总是在追求自由的一生 So easy”来说明分块传输的过程，如下图所示: url编码urlencode是什么RFC3986文档规定，Url中只允许包含英文字母（a-zA-Z）、数字（0-9）、-_.~4个特殊字符以及所有保留字符。那如何对Url中的非法字符进行编码呢? Url编码通常也被称为百分号编码（Url Encoding，also known as percent-encoding），是因为它的编码方式非常简单，使用%百分号加上两位的字符——0123456789ABCDEF——代表一个字节的十六进制形式。Url编码默认使用的字符集是US-ASCII。例如a在US-ASCII码中对应的字节是0x61，那么Url编码之后得到的就是%61，我们在地址栏上输入http://g.cn/search?q=%61%62%63， 实际上就等同于在google上搜索abc了。又如@符号在ASCII字符集中对应的字节为0x40，经过Url编码之后得到的是%40。 对于非ASCII字符，需要使用ASCII字符集的超集进行编码得到相应的字节，然后对每个字节执行百分号编码。对于Unicode字符，RFC文档建议使用utf-8对其进行编码得到相应的字节，然后对每个字节执行百分号编码。如”中文”使用UTF-8字符集得到的字节为0xE4 0xB8 0xAD 0xE6 0x96 0x87，经过Url编码之后得到”%E4%B8%AD%E6%96%87”。 如果某个字节对应着ASCII字符集中的某个非保留字符，则此字节无需使用百分号表示。例如”Url编码”，使用UTF-8编码得到的字节是0x55 0x72 0x6C 0xE7 0xBC 0x96 0xE7 0xA0 0x81，由于前三个字节对应着ASCII中的非保留字符”Url”，因此这三个字节可以用非保留字符”Url”表示。最终的Url编码可以简化成”Url%E7%BC%96%E7%A0%81” ，当然，如果你用”%55%72%6C%E7%BC%96%E7%A0%81”也是可以的。 很多HTTP监视工具或者浏览器地址栏等在显示Url的时候会自动将Url进行一次解码（使用UTF-8字符集），这就是为什么当你在Firefox中访问Google搜索中文的时候，地址栏显示的Url包含中文的缘故。但实际上发送给服务端的原始Url还是经过编码的。 MySQL参考网址 https://chenjiabing666.github.io/2020/04/20/Mysql%E6%9C%80%E5%85%A8%E9%9D%A2%E8%AF%95%E6%8C%87%E5%8D%97/ https://blog.csdn.net/qq_41011723/article/details/105953813 https://blog.csdn.net/qq_41011723/article/details/106028153 MySQL、MongoDB、Redis 数据库之间的区别 mysql一条语句的执行过程速记: 连/分/优/执/存 char和varchar的区别是什么 char(n) ：固定长度类型，比如订阅 char(10)，当你输入”abc”三个字符的时候，它们占的空间还是 10 个字节，其他 7 个是空字节。 chat 优点：效率高；缺点：占用空间；适用场景：存储密码的 md5 值，固定长度的，使用 char 非常合适。 varchar(n) ：可变长度，存储的值是每个值占用的字节再加上一个用来记录其长度的字节的长度。 所以，从空间上考虑 varcahr 比较合适；从效率上考虑 char 比较合适，二者使用需要权衡。 redo log与binlog与undo log的区别参考 https://www.cnblogs.com/Java3y/p/12453755.html , 写的非常好也可参考 https://www.jianshu.com/p/68d5557c65be redo logredo log 存在于InnoDB 引擎中，InnoDB引擎是以插件形式引入Mysql的，redo log的引入主要是为了实现Mysql的crash-safe能力。实际上Mysql的基本存储结构是页(记录都存在页里边)，所以MySQL是先把这条记录所在的页找到，然后把该页加载到内存中，将对应记录进行修改。现在就可能存在一个问题：如果在内存中把数据改了，还没来得及落磁盘，而此时的数据库挂了怎么办？显然这次更改就丢了。 如果每个请求都需要将数据立马落磁盘之后，那速度会很慢，MySQL可能也顶不住。所以MySQL是怎么做的呢？MySQL引入了redo log，内存写完了，然后会写一份redo log，这份redo log记载着这次在某个页上做了什么修改.其实写redo log的时候，也会有buffer，是先写buffer，再真正落到磁盘中的。至于从buffer什么时候落磁盘，会有配置供我们配置。 写redo log也是需要写磁盘的，但它的好处就是顺序IO（我们都知道顺序IO比随机IO快非常多）。 所以，redo log的存在为了：当我们修改的时候，写完内存了，但数据还没真正写到磁盘的时候。此时我们的数据库挂了，我们可以根据redo log来对数据进行恢复。因为redo log是顺序IO，所以写入的速度很快，并且redo log记载的是物理变化（xxxx页做了xxx修改），文件的体积很小，恢复速度很快 binlogbinlog记录了数据库表结构和表数据变更，比如update/delete/insert/truncate/create。它不会记录select（因为这没有对表没有进行变更）binlog我们可以简单理解为：存储着每条变更的SQL语句 undo logundo log主要有两个作用： 回滚 多版本控制(MVCC) 在数据修改的时候，不仅记录了redo log，还记录undo log，如果因为某些原因导致事务失败或回滚了，可以用undo log进行回滚undo log主要存储的也是逻辑日志，比如我们要insert一条数据了，那undo log会记录的一条对应的delete日志。我们要update一条记录时，它会记录一条对应相反的update记录。 这也应该容易理解，毕竟回滚嘛，跟需要修改的操作相反就好，这样就能达到回滚的目的。因为支持回滚操作，所以我们就能保证：“一个事务包含多个操作，这些操作要么全部执行，要么全都不执行”。【原子性】 因为undo log存储着修改之前的数据，相当于一个前版本，MVCC实现的是读写不阻塞，读的时候只要返回前一个版本的数据就行了。 undolog和binlog和redolog不同之处总结 参考 https://www.jianshu.com/p/68d5557c65be redo log: 只存在于innodb引擎中 物理格式的日志，记录的是物理数据页面的修改的信息（数据库中每个页的修改），面向的是表空间、数据文件、数据页、偏移量等。 undo log 逻辑格式的日志，在执行undo的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，与redo log不同。 binlog 逻辑格式的日志，可以简单认为就是执行过的事务中的sql语句。 但又不完全是sql语句这么简单，而是包括了执行的sql语句（增删改）反向的信息。比如delete操作的话，就对应着delete本身和其反向的insert/update操作的话，就对应着update执行前后的版本的信息；insert操作则对应着delete和insert本身的信息。 因此可以基于binlog做到闪回功能。 binlog可以作为恢复数据使用，主从复制搭建，redo log作为异常宕机或者介质故障后的数据恢复使用。 redo log是在InnoDB存储引擎层产生，而binlog是MySQL数据库的上层产生的，并且binlog日志不仅仅针对INNODB存储引擎，MySQL数据库中的任何存储引擎对于数据库的更改都会产生binlog日志。 两种日志记录的内容形式不同。MySQL的binlog是逻辑日志，可以简单认为记录的就是sql语句。而innodb存储引擎层面的redo日志是物理日志, 是数据页面的修改之后的物理记录。 关于事务提交时，redo log和binlog的写入顺序，为了保证主从复制时候的主从一致（当然也包括使用binlog进行基于时间点还原的情况），是要严格一致的，MySQL通过两阶段提交过程来完成事务的一致性的，也即redo log和binlog的一致性的，理论上是先写redo log，再写binlog，两个日志都提交成功（刷入磁盘），事务才算真正的完成。因此redo日志的写盘，并不一定是随着事务的提交才写入redo日志文件的，而是随着事务的开始，逐步开始的。那么当我执行一条 update 语句时，redo log 和 binlog 是在什么时候被写入的呢？这就有了我们常说的「两阶段提交」： 写入：redo log（prepare） 写入：binlog 写入：redo log（commit） 两种日志与记录写入磁盘的时间点不同，binlog日志只在事务提交完成后进行一次写入。而innodb存储引擎的redo日志在事务进行中不断地被写入，并日志不是随事务提交的顺序进行写入的。 binlog日志仅在事务提交时记录，并且对于每一个事务，仅在事务提交时记录，并且对于每一个事务，仅包含对应事务的一个日志。而对于innodb存储引擎的redo日志，由于其记录是物理操作日志，因此每个事务对应多个日志条目，并且事务的redo日志写入是并发的，并非在事务提交时写入，其在文件中记录的顺序并非是事务开始的顺序。 binlog不是循环使用，在写满或者重启之后，会生成新的binlog文件，redo log是循环使用。 binlog 日志是 master 推的还是 salve 来拉的？slave来拉的, 因为每一个slave都是完全独立的个体，所以slave完全依据自己的节奏去处理同步， 二阶段提交redo log 保证的是数据库的 crash-safe 能力。采用的策略就是常说的“两阶段提交”。 一条update的SQL语句是按照这样的流程来执行的：将数据页加载到内存 → 修改数据 → 更新数据 → 写redo log（状态为prepare） → 写binlog → 提交事务(数据写入成功后将redo log状态改为commit) 只有当两个日志都提交成功（刷入磁盘），事务才算真正的完成。一旦发生系统故障（不管是宕机、断电、重启等等），都可以配套使用 redo log 与 binlog 做数据修复。 两阶段提交机制的必要性 binlog 存在于Mysql Server层中，主要用于数据恢复；当数据被误删时，可以通过上一次的全量备份数据加上某段时间的binlog将数据恢复到指定的某个时间点的数据。 redo log 存在于InnoDB 引擎中，InnoDB引擎是以插件形式引入Mysql的，redo log的引入主要是为了实现Mysql的crash-safe能力。 假设redo log和binlog分别提交，可能会造成用日志恢复出来的数据和原来数据不一致的情况。 假设先写redo log再写binlog，即redo log没有prepare阶段，写完直接置为commit状态，然后再写binlog。那么如果写完redo log后Mysql宕机了，重启后系统自动用redo log 恢复出来的数据就会比binlog记录的数据多出一些数据，这就会造成磁盘上数据库数据页和binlog的不一致，下次需要用到binlog恢复误删的数据时，就会发现恢复后的数据和原来的数据不一致。 假设先写binlog再写redolog。如果写完binlog后Mysql宕机了，那么binlog上的记录就会比磁盘上数据页的记录多出一些数据出来，下次用binlog恢复数据，就会发现恢复后的数据和原来的数据不一致。 由此可见，redo log和binlog的两阶段提交是非常必要的。 索引 聚集索引(也叫聚簇索引)是啥 聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据 非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因。 外键是啥: 比如在students表中，通过class_id的字段，可以把数据与另一张表关联起来，这种列称为外键(一般不用外键, 因为会降低数据库性能) mysql 索引在什么情况下会失效 https://database.51cto.com/art/201912/607742.htm 查询条件包含or，可能导致索引失效 如何字段类型是字符串，where时一定用引号括起来，否则索引失效 like通配符可能导致索引失效。 联合索引，查询时的条件列不是联合索引中的第一个列，索引失效。 在索引列上使用mysql的内置函数，索引失效 对索引列运算（如，+、-、*、/），索引失效。 索引字段上使用（！= 或者 &lt; &gt;，not in）时，可能会导致索引失效。 索引字段上使用is null， is not null，可能导致索引失效。 左连接查询或者右连接查询查询关联的字段编码格式不一样，可能导致索引失效。 mysql估计使用全表扫描要比使用索引快,则不使用索引。 mysql 的索引模型:在MySQL中使用较多的索引有Hash索引，B+树索引等，而我们经常使用的InnoDB存储引擎的默认索引实现为：B+树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 为什么说B类树更适合数据库索引为什么说B类树更适合数据库索引 mysql全文索引 pending_finmysql 有那些存储引擎，有哪些区别 innodb是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ Next-Key Locking 防止幻影读。主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。 MyISAM类型不支持事务处理等高级处理，而InnoDB类型支持。 MyISAM类型的表强调的是性能，其执行速度比InnoDB类型更快，但是不提供事务支持，而InnoDB提供事务支持以及外部键等高级数据库功能。 现在一般都是选用InnoDB了，InnoDB支持行锁, 而MyISAM的全表锁，myisam的读写串行问题，并发效率锁表，效率低，MyISAM对于读写密集型应用一般是不会去选用的 memory引擎一般用于临时表, 使用表级锁，没有事务机制, 虽然内存访问快，但如果频繁的读写，表级锁会成为瓶颈, 且内存昂贵..满了就亏了 InnoDB是聚集索引，使用B+Tree作为索引结构，数据文件是和（主键）索引绑在一起的（表数据文件本身就是按B+Tree组织的一个索引结构），必须要有主键，通过主键索引效率很高。MyISAM是非聚集索引，也是使用B+Tree作为索引结构，索引和数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 综上所述, 如果表的读操作远远多于写操作时，并且不需要事务的支持的，可以将 MyIASM 作为数据库引擎的首选 mysql 主从同步分哪几个过程复制的基本过程如下： 从节点上的I/O 线程连接主节点，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容； 主节点接收到来自从节点的I/O请求后，通过负责复制的I/O线程根据请求信息读取指定日志指定位置之后的日志信息，返回给从节点。返回信息中除了日志所包含的信息之外，还包括本次返回的信息的bin-log file 的以及bin-log position；从节点的I/O线程接收到内容后，将接收到的日志内容更新到本机的relay log中，并将读取到的binary log文件名和位置保存到master-info 文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log 的哪个位置开始往后的日志内容，请发给我”； Slave 的 SQL线程检测到relay-log 中新增加了内容后，会将relay-log的内容解析成在主节点上实际执行过的操作，并在本数据库中执行。 主从同步延迟与同步数据丢失问题主库将变更写binlog日志，然后从库连接到主库之后，从库有一个IO线程，将主库的binlog日志拷贝到自己本地，写入一个中继日志中。接着从库中有一个SQL线程会从中继日志读取binlog，然后执行binlog日志中的内容，也就是在自己本地再次执行一遍SQL，这样就可以保证自己跟主库的数据是一样的。 这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行SQL的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。所以mysql实际上在这一块有两个机制: 一个是半同步复制，用来解决主库数据丢失问题 一个是并行复制，用来解决主从同步延时问题(实在解决不了只能强制读主库)。 半同步复制（Semisynchronous replication） 逻辑上: 是介于全同步复制与全异步复制之间的一种，主库只需要等待至少一个从库节点收到并且 Flush Binlog 到 Relay Log 文件即可，主库不需要等待所有从库给主库反馈。同时，这里只是一个收到的反馈，而不是已经完全完成并且提交的反馈，如此，节省了很多时间。 技术上: 介于异步复制和全同步复制之间，主库在执行完客户端提交的事务后不是立刻返回给客户端，而是等待至少一个从库接收到并写到relay log中才返回给客户端。相对于异步复制，半同步复制提高了数据的安全性，同时它也造成了一定程度的延迟，这个延迟最少是一个TCP/IP往返的时间。所以，半同步复制最好在低延时的网络中使用。 库并行复制所谓并行复制，指的是从库开启多个线程，并行读取relay log中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 异步复制（Asynchronous replication） 逻辑上: MySQL默认的复制即是异步的，主库在执行完客户端提交的事务后会立即将结果返给给客户端，并不关心从库是否已经接收并处理，这样就会有一个问题，主如果crash掉了，此时主上已经提交的事务可能并没有传到从库上，如果此时，强行将从提升为主，可能导致新主上的数据不完整。 技术上: 主库将事务 Binlog 事件写入到 Binlog 文件中，此时主库只会通知一下 Dump 线程发送这些新的 Binlog，然后主库就会继续处理提交操作，而此时不会保证这些 Binlog 传到任何一个从库节点上。 全同步复制（Fully synchronous replication） 逻辑上: 指当主库执行完一个事务，所有的从库都执行了该事务才返回给客户端。因为需要等待所有从库执行完该事务才能返回，所以全同步复制的性能必然会收到严重的影响。 技术上: 当主库提交事务之后，所有的从库节点必须收到、APPLY并且提交这些事务，然后主库线程才能继续做后续操作。但缺点是，主库完成一个事务的时间会被拉长，性能降低。 乐观锁与悲观锁的区别？ 悲观锁：认为数据随时会被修改，因此每次读取数据之前都会上锁，防止其它事务读取或修改数据；应用于数据更新比较频繁的场景； 乐观锁：操作数据时不会上锁，但是更新时会判断在此期间有没有别的事务更新这个数据，若被更新过，则失败重试；适用于读多写少的场景。 乐观锁怎么实现: 加版本号 cas 实现事务采取了哪些技术以及思想？ ★ a原子性：使用 undo log ，从而达到回滚 ★ d持久性：使用 redo log，从而达到故障后恢复 ★ i隔离性：使用锁以及MVCC,运用的优化思想有读写分离，读读并行，读写并行 ★ c一致性：通过回滚，以及恢复，和在并发环境下的隔离做到一致性。 mysql四个事务隔离级别四个隔离级别的区别以及每个级别可能产生的问题以及实现原理MySQL 的事务隔离是在 MySQL. ini 配置文件里添加的，在文件的最后添加：transaction-isolation = REPEATABLE-READ可用的配置值：READ-UNCOMMITTED、READ-COMMITTED、REPEATABLE-READ、SERIALIZABLE。 MySQL的事务隔离级别一共有四个，分别是 读未提交 读已提交 可重复读 可串行化 MySQL的隔离级别的作用就是让事务之间互相隔离，互不影响，这样可以保证事务的一致性。 隔离级别比较：可串行化&gt;可重复读&gt;读已提交&gt;读未提交 隔离级别对性能的影响比较：可串行化&gt;可重复读&gt;读已提交&gt;读未提交 由此看出，隔离级别越高，所需要消耗的MySQL性能越大（如事务并发严重性），为了平衡二者，一般建议设置的隔离级别为可重复读，MySQL默认的隔离级别也是可重复读。 事务并发可能出现的情况 脏读（Dirty Read） 一个事务读到了另一个未提交事务修改过的数据 会话B开启一个事务，把id=1的name为武汉市修改成温州市，此时另外一个会话A也开启一个事务，读取id=1的name，此时的查询结果为温州市，会话B的事务最后回滚了刚才修改的记录，这样会话A读到的数据是不存在的，这个现象就是脏读。（脏读只在读未提交隔离级别才会出现） 不可重复读（Non-Repeatable Read） 一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值。（不可重复读在读未提交和读已提交隔离级别都可能会出现） 会话A开启一个事务，查询id=1的结果，此时查询的结果name为武汉市。接着会话B把id=1的name修改为温州市（隐式事务，因为此时的autocommit为1，每条SQL语句执行完自动提交），此时会话A的事务再一次查询id=1的结果，读取的结果name为温州市。会话B再此修改id=1的name为杭州市，会话A的事务再次查询id=1，结果name的值为杭州市，这种现象就是不可重复读。 幻读（Phantom） 一个事务先根据某些条件查询出一些记录，之后另一个事务又向表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能把另一个事务插入的记录也读出来。（幻读在读未提交、读已提交、可重复读隔离级别都可能会出现） 会话A开启一个事务，查询id&gt;0的记录，此时会查到name=武汉市的记录。接着会话B插入一条name=温州市的数据（隐式事务，因为此时的autocommit为1，每条SQL语句执行完自动提交），这时会话A的事务再以刚才的查询条件（id&gt;0）再一次查询，此时会出现两条记录（name为武汉市和温州市的记录），这种现象就是幻读。 各个隔离级别的详细说明 读未提交（READ UNCOMMITTED） 在读未提交隔离级别下，事务A可以读取到事务B修改过但未提交的数据。 可能发生脏读、不可重复读和幻读问题，一般很少使用此隔离级别。 读已提交（READ COMMITTED） 在读已提交隔离级别下，事务B只能在事务A修改过并且已提交后才能读取到事务B修改的数据。 读已提交隔离级别解决了脏读的问题，但可能发生不可重复读和幻读问题，一般很少使用此隔离级别。 可重复读（REPEATABLE READ） 在可重复读隔离级别下，事务B只能在事务A修改过数据并提交后，自己也提交事务后，才能读取到事务B修改的数据。 可重复读隔离级别解决了脏读和不可重复读的问题，但可能发生幻读问题。 提问：为什么上了写锁（写操作），别的事务还可以读操作？因为InnoDB有MVCC机制（多版本并发控制），可以使用快照读，而不会被阻塞。 可串行化（SERIALIZABLE） 各种问题（脏读、不可重复读、幻读）都不会发生，通过加锁实现（读锁和写锁）。 mvcc是啥mvcc必看文章: mysql mvcc实现原理 https://chenjiayang.me/2019/06/22/mysql-innodb-mvcc/ MVCC (Multiversion Concurrency Control) 中文全程叫多版本并发控制，是现代数据库（包括 MySQL、Oracle、PostgreSQL 等）引擎实现中常用的处理读写冲突的手段，目的在于提高数据库高并发场景下的吞吐性能。MVCC 的每一个写操作都会创建一个新版本的数据，读操作会从有限多个版本的数据中挑选一个最合适（要么是最新版本，要么是指定版本）的结果直接返回 。通过这种方式，我们就不需要关注读写操作之间的数据冲突 每条记录在更新的时候都会同时记录一条回滚操作（回滚操作日志undo log）。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。即通过回滚（rollback操作），可以回到前一个状态的值。InnoDB 为了解决这个问题，设计了 ReadView（可读视图）的概念. 如此一来不同的事务在并发过程中，SELECT 操作可以不加锁而是通过 MVCC 机制读取指定的版本历史记录，并通过一些手段保证保证读取的记录值符合事务所处的隔离级别，从而解决并发场景下的读写冲突。 mysql把每个操作都定义成一个事务，每开启一个事务，系统的事务版本号自动递增。每行记录都有两个隐藏列：创建版本号和删除版本号 ReadView 系统版本号 SYS_ID：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。 事务版本号 TRX_ID ：事务开始时的系统版本号。 MVCC 维护了一个 ReadView 结构，主要包含了当前系统未提交的事务列表 TRX_IDs {TRX_ID_1, TRX_ID_2, …}，还有该列表的最小值 TRX_ID_MIN 和 TRX_ID_MAX。 在进行 SELECT 操作时，根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系，从而判断数据行快照是否可以使用： TRX_ID &lt; TRX_ID_MIN，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此可以使用。 TRX_ID &gt; TRX_ID_MAX，表示该数据行快照是在事务启动之后被更改的，因此不可使用。 TRX_ID_MIN &lt;= TRX_ID &lt;= TRX_ID_MAX，需要根据隔离级别再进行判断： 提交读：如果 TRX_ID 在 TRX_IDs 列表中，表示该数据行快照对应的事务还未提交，则该快照不可使用。否则表示已经提交，可以使用。 可重复读：都不可以使用。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题。 在数据行快照不可使用的情况下，需要沿着 Undo Log 的回滚指针 ROLL_PTR 找到下一个快照，再进行上面的判断。 mysql在可重复读RR的隔离级别下如何避免幻读的参考: next-key锁: mysql 排它锁之行锁、间隙锁、后码锁 mvcc: mysql mvcc实现原理 https://blog.csdn.net/DILIGENT203/article/details/100751755 知识点: Record Lock：行锁, 锁直接加在索引记录上面，锁住的是key。当需要对表中的某条数据进行写操作（insert、update、delete、select for update）时，需要先获取记录的排他锁（X锁），这个就称为行锁。 Gap Lock：间隙锁，锁定索引记录间隙，确保索引记录的间隙不变。间隙锁是针对事务隔离级别为可重复读或以上级别而设计的。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。例如当一个事务执行语句SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE;，则其它事务就不能在 t.c 中插入 15。 Next-Key Lock = Record Lock + Gap Lock ， 它是 Record Locks 和 Gap Locks 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个前开后闭区间，例如一个索引包含以下值：10, 11, 13, and 20，那么就需要锁定以下区间： 12345(-∞, 10](10, 11](11, 13](13, 20](20, +∞) 默认情况下，InnoDB工作在可重复读隔离级别下，并且会以Next-Key Lock的方式对数据行进行加锁，这样可以有效防止幻读的发生。Next-Key Lock是行锁和间隙锁的组合，当InnoDB扫描索引记录的时候，会首先对索引记录加上Record Lock，再对索引记录两边的间隙加上间隙锁（Gap Lock）。加上间隙锁之后，其他事务就不能在这个间隙修改或者插入记录。 快照读：简单的select操作，属于快照读，不加锁。(当然，也有例外，下面会分析) select * from table where ?; 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。 select * from table where ? lock in share mode; select * from table where ? for update; insert into table values (…); update table set ? where ?; delete from table where ?; 在MySQL中普通的select称为快照读，不需要锁，而insert、update、delete、select for update则称为当前读，需要给数据加锁，幻读中的“读”即是针对当前读。RR事务隔离级别允许存在幻读，但InnoDB RR级别却通过Gap锁避免了幻读 mysql如何实现避免幻读: 在快照读读情况下，mysql通过mvcc来避免幻读。 在当前读读情况下，mysql通过next-key lock来避免幻读。 正确理解InnoDB引擎RR隔离级别解决了幻读这件事Mysql官方给出的幻读解释是：只要在一个事务中，第二次select多出了row就算幻读。 先看问题:a事务先select，b事务insert确实会加一个gap锁，但是如果b事务commit，这个gap锁就会释放（释放后a事务可以随意dml操作），a事务再select出来的结果在MVCC下还和第一次select一样，接着a事务不加条件地update，这个update会作用在所有行上（包括b事务新加的），a事务再次select就会出现b事务中的新行，并且这个新行已经被update修改了，实测在RR级别下确实如此。 如果这样理解的话，Mysql的RR级别确实防不住幻读, 但是我们不能向上面这样理解, 我们得如下理解: select * from t where a=1;属于快照读 select * from t where a=1 lock in share mode;属于当前读 不能把快照读和当前读得到的结果不一样这种情况认为是幻读，这是两种不同的使用。所以mysql的rr级别是解决了幻读的。 如上面问题所说，T1 select 之后 update，会将 T2 中 insert 的数据一起更新，那么认为多出来一行，所以防不住幻读。看着说法无懈可击，但是其实是错误的，InnoDB 中设置了快照读和当前读两种模式，如果只有快照读，那么自然没有幻读问题，但是如果将语句提升到当前读，那么 T1 在 select 的时候需要用如下语法： select * from t for update (lock in share mode) 进入当前读，那么自然没有 T2 可以插入数据这一回事儿了。 Redisredis 数据结构有哪些？分别怎么实现的？ String: 全是整数的时候用整数编码int 当有字符串的时候用简单动态字符串sds编码 HashTable: 元素比较少或者元素比较短的时候用压缩表ziplist(key1|val1|key2|val2|…这样存储), 其他时候就用字典ht Set: 元素全是整数的时候用整数集合编码(一种特殊的编码, 会使用各种规则来利用位空间, 来节省内存), 其他时候用字典ht编码(键为Set的元素, 值都为Null) List: 元素比较少或者元素比较短的时候用压缩表ziplist, 其他时候就用双端列表LinkedList编码 ZSet: 参考 http://redisbook.com/preview/object/sorted_set.html 参考 https://redisbook.readthedocs.io/en/latest/datatype/sorted_set.html 元素比较少或者元素比较短的时候用压缩表ziplist(member1|score1|member2|score2|…, 按照score从小到大排列), 其他时候就用跳跃表SkipList编码, 这个编码里包含一个字典结构和一个跳表结构, 但这两种数据结构都会通过指针来共享相同元素的成员和分值， 所以同时使用跳跃表和字典来保存集合元素不会产生任何重复成员或者分值， 也不会因此而浪费额外的内存: 字典用于快速查找, 如ZScore查询member成员的 score 值, 或者快速确定是否有某个member 跳表用于zrank/zrange等 zset各种问题为什么zset用跳表不用红黑树现在我们看看，对于这个问题，Redis的作者 @antirez 是怎么说的： There are a few reasons: They are not very memory intensive. It’s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees. A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees. They are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code. 可参考: 本博客文章跳表总结: 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。 从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。 从算法实现难度上来比较，skiplist比平衡树要简单得多。 zset是怎么支持查询排名的跳表怎么支持查询排名的 延时队列用redis怎么做用zset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者轮询zset用zrangebyscore指令获取N秒之前的数据轮询进行处理。 ZSET做排行榜时要实现分数相同时按时间顺序排序怎么实现说了一个将 score 拆成高 32 位和低 32 位，高 32 位存分数，低 32 位存时间的方法。 哈希表渐进式rehash 当以下条件中的任意一个被满足时， 程序会自动开始对哈希表执行扩展操作： 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 1 ； 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 5 ； 根据 BGSAVE 命令或 BGREWRITEAOF 命令是否正在执行， 服务器执行扩展操作所需的负载因子并不相同， 这是因为在执行 BGSAVE 命令或 BGREWRITEAOF 命令的过程中， Redis 需要创建当前服务器进程的子进程， 所以在子进程存在期间， 服务器会提高执行扩展操作所需的负载因子， 从而尽可能地避免在子进程存在期间进行哈希表扩展操作， 这可以避免不必要的内存写入操作， 最大限度地节约内存。 另一方面， 当哈希表的负载因子小于 0.1 时， 程序自动开始对哈希表执行收缩操作。 以下是哈希表渐进式 rehash 的详细步骤： 为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始。 在 rehash 进行期间， 每次对字典执行删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一。 随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成。 渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。 因为在进行渐进式 rehash 的过程中， 字典会同时使用 ht[0] 和 ht[1] 两个哈希表， 所以在渐进式 rehash 进行期间， 字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行： 比如说， 要在字典里面查找一个键的话， 程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找， 诸如此类。 另外， 在渐进式 rehash 执行期间， 新添加到字典的键值对一律会被保存到 ht[1] 里面， 而 ht[0] 则不再进行任何添加操作： 这一措施保证了 ht[0] 包含的键值对数量会只减不增， 并随着 rehash 操作的执行而最终变成空表。 redis 持久化有哪几种方式，怎么选？ 混合持久化 原因: 重启 Redis 时，我们很少使用 rdb 来恢复内存状态，因为会丢失大量数据。如果使用 AOF 日志重放，性能则相对 rdb 来说要慢很多，这样在 Redis 实例很大的情况下，启动的时候需要花费很长的时间。 原理: 混合持久化同样也是通过bgrewriteaof完成的，不同的是当开启混合持久化时，fork出的子进程先将共享的内存副本全量的以RDB方式写入aof文件，然后在将aof_rewrite_buf重写缓冲区的增量命令以AOF方式写入到文件，写入完成后通知主进程更新统计信息，并将新的含有RDB格式和AOF格式的AOF文件替换旧的的AOF文件。 简单的说：新的AOF文件前半段是RDB格式的全量数据后半段是AOF格式的增量数据， rdb 优势: RDB文件紧凑，全量备份，非常适合用于进行备份和灾难恢复。 生成RDB文件的时候，redis主进程会fork()一个子进程来处理所有保存工作，主进程不需要进行任何磁盘IO操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 劣势: 当进行快照持久化时，会开启一个子进程专门负责快照持久化，子进程会拥有父进程的内存数据，父进程修改内存子进程不会反应出来，所以在快照持久化期间修改的数据不会被保存，可能丢失数据。 aof 优势: AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台刷盘线程执行一次fsync操作(这种一秒刷盘一次的策略, 可能会造成追加阻塞: 当硬盘资源繁忙时，即主线程发现距离上次fsync时间超过2秒, 为了数据安全性, 主线程会阻塞直到后台刷盘线程执行fsync操作完成)，保证最多丢失1秒钟的数据。所以这也是redis重启优先加载aof的理由 AOF日志文件没有任何磁盘寻址的开销，写入性能非常高，文件不容易破损。 AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。 AOF日志文件的命令通过可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据 劣势: 对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大 AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的 bgsave流程说一下 子进程创建RDB文件, 根据父进程内存生成临时快照文件, 完成后对原有RDB文件进行原子替换. 然后子进程发送信号给父进程表示完成 aof流程说一下以及aof追加阻塞是啥 追加阻塞: AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台刷盘线程执行一次fsync操作(这种一秒刷盘一次的策略, 可能会造成追加阻塞: 当硬盘资源繁忙时，即主线程发现距离上次fsync时间超过2秒, 为了数据安全性, 主线程会阻塞直到后台刷盘线程执行fsync操作完成)，保证最多丢失1秒钟的数据。 AOF重写的实现 所谓的“重写”其实是一个有歧义的词语, AOF重写并不需要对原有AOF文件进行任何的读取，写入，分析等操作，这个功能是通过读取服务器当前的数据库状态来实现的。 如当前列表键list在数据库中的值就为[&quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;]。要使用尽量少的命令来记录list键的状态，最简单的方式不是去读取和分析现有AOF文件的内容，，而是直接读取list键在数据库中的当前值，然后用一条RPUSH list &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot;代替前面的6条命令 AOF 重写程序可以很好地完成创建一个新 AOF 文件的任务， 但是， 在执行这个程序的时候， 调用者线程会被阻塞。很明显， 作为一种辅佐性的维护手段， Redis 不希望 AOF 重写造成服务器无法处理请求， 所以 Redis 决定将 AOF 重写程序放到（后台）子进程里执行， 这样处理的最大好处是： 子进程进行 AOF 重写期间，主进程可以继续处理命令请求。 子进程带有主进程的数据副本，使用子进程而不是线程，可以在避免锁的情况下，保证数据的安全性。 不过， 使用子进程也有一个问题需要解决： 因为子进程在进行 AOF 重写期间， 主进程还需要继续处理命令， 而新的命令可能对现有的数据进行修改， 这会让当前数据库的数据和重写后的 AOF 文件中的数据不一致。为了解决这个问题， Redis 增加了一个 AOF 重写缓存， 这个缓存在 fork 出子进程之后开始启用， Redis 主进程在接到新的写命令之后， 除了会将这个写命令的协议内容追加到现有的 AOF 文件之外， 还会追加到这个重写缓存中, 换言之， 当子进程在执行 AOF 重写时， 主进程需要执行以下三个工作： 处理命令请求。 将写命令追加到现有的 AOF 文件中。 将写命令追加到 AOF 重写缓存中。 当子进程完成 AOF 重写之后， 它会向父进程发送一个完成信号， 父进程在接到完成信号之后， 会调用一个信号处理函数， 并完成以下工作： 将 AOF 重写缓存中的内容全部写入到新 AOF 文件中。 对新的 AOF 文件进行改名rename，覆盖原有的 AOF 文件。这就是aof的原子替换. 在整个 AOF 后台重写过程中， 只有最后的写入缓存和改名操作会造成主进程阻塞， 在其他时候， AOF 后台重写都不会对主进程造成阻塞， 这将 AOF 重写对性能造成的影响降到了最低。以上就是 AOF 后台重写， 也即是 BGREWRITEAOF 命令的工作原理。 如何做rdb和aof的原子替换的比如想要将temp文件原子替换origin文件, 则直接rename tmp文件到origin文件即可实现.rename通过来说, 直接修改 file system metadata, 如inode信息. 在posix标准里, rename实现是原子的, 即: rename成功, 原文件名 指向 temp 文件; 原文件内容被删除. rename失败, 原文件名 仍指向原来的文件内容. redis 主从同步是怎样的过程？ 从redis发出sync要求 主redis开始bgsave(并且一边开启指令buffer来存储bgsave过程中的写指令们记为cmd) 主redis把bgsave生成的rdb发给从redis 把cmd发送给从redis 从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令 总结:主从刚刚连接的时候，进行全量同步；全量同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。redis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。 redis key 的过期策略Redis键的过期策略，是有定期删除+惰性删除两种。 定期好理解，默认100ms就 随机 抽一些设置了过期时间的key，去检查是否过期，过期了就删了。 惰性删除，查询时再判断是否过期，过期就删除键不返回值。 内存淘汰机制当新增数据发现内存达到限制时，Redis触发内存淘汰机制。 lru lfu(least frequency used, redis 4新增) random ttl redis的LRU算法说一下 普通的LRU算法: 一般是用哈希表+双向链表来实现的: 基于 HashMap 和 双向链表实现 LRU 的整体的设计思路是，可以使用 HashMap 存储 key，这样可以做到 save 和 get key的时间都是 O(1)，而 HashMap 的 Value 指向双向链表实现的 LRU 的 Node 节点. 其核心操作的步骤是: save(key, value):首先在 HashMap 找到 Key 对应的节点，如果节点存在，更新节点的值，并把这个节点移动队头。如果不存在，需要构造新的节点，并且尝试把节点塞到队头，如果LRU空间不足，则通过 tail 淘汰掉队尾的节点，同时在 HashMap 中移除 Key。 get(key):通过 HashMap 找到 LRU 链表节点，因为根据LRU 原理，这个节点是最新访问的，所以要把节点插入到队头，然后返回缓存的值。 Redis的LRU实现: 如果按照HashMap和双向链表实现，需要额外的存储存放 next 和 prev 指针，牺牲比较大的存储空间，显然是不划算的。所以Redis采用了一个近似的做法，就是定时每隔一段时间就随机取出若干个key，然后按照访问时间排序后，淘汰掉最不经常使用的. Redis 3.0之后又改善了算法的性能，会提供一个待淘汰候选key的pool，里面默认有16个key，按照空闲时间排好序。更新时从Redis键空间随机选择N个key，分别计算它们的空闲时间 idle，key只会在pool不满或者空闲时间大于pool里最小的时，才会进入pool，然后从pool中选择空闲时间最大的key淘汰掉。 redis哨兵 Redis Sentinel是Redis的高可用实现方案：故障发现、故障自动转移、配置中心 客户端通知。 Redis Sentinel从Redis 2.8版本开始才正式生产可用，之前版本生产不可用。 尽可能在不同物理机上部署Redis Sentinel所有节点。 Redis Sentinel中的Sentinel节点个数应该为大于等于3且最好为奇数。 Redis Sentinel中的数据节点与普通数据节点没有区别。 哨兵是一个配置提供者，而不是代理。在引入哨兵之后，客户端会先连接哨兵，再获取到主节点之后，客户端会和主节点直接通信。如果发生了故障转移，哨兵会通知到客户端。所以这也需要客户端的实现对哨兵的显式支持。 Redis Sentinel通过三个定时任务实现了Sentinel节点对于主节点、从节点、其余 Sentinel节点的监控。 Redis Sentinel在对节点做失败判定时分为主观下线和客观下线。 Redis Sentinel实现读写分离高可用可以依赖Sentinel节点的消息通知，获取Redis 数据节点的状态变化。 用文字描述一下故障切换（failover）的过程: 假设主服务器宕机，哨兵1先检测到这个结果，系统并不会马上进行failover过程，仅仅是哨兵1主观的认为主服务器不可用，这个现象成为主观下线。 当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，就对这个主节点故障达成一致, 这个过程称为客观下线。 这样对于客户端而言，一切都是透明的。然后通过raft算法从哨兵中选出一个哨兵来执行故障转移 redis集群redis集群是一个由多个主从节点群组成的分布式服务器群，它具有复制、高可用和分片特性。Redis集群不需要sentinel哨兵也能完成节点移除和故障转移的功能。需要将每个节点设置成集群模式，这种集群模式没有中心节点，可水平扩展，据官方文档称可以线性扩展到上万个节点(官方推荐不超过1000个节点)。redis集群的性能和高可用性均优于之前版本的哨兵模式，且集群配置非常简单。集群模式有以下几个特点： 由多个Redis服务器组成的分布式网络服务集群； 集群之中有多个Master主节点，每一个主节点都可读可写； 节点之间会互相通信，两两相连, 采用gossip协议来通信； Redis集群无中心节点。 集群的伸缩本质是: 槽数据在节点中的移动 优点在哨兵模式中，仍然只有一个Master节点。当并发写请求较大时，哨兵模式并不能缓解写压力。 我们知道只有主节点才具有写能力，那如果在一个集群中，能够配置多个主节点，缓解写压力，redis-cluster集群模式能达到此类要求。 在Redis-Cluster集群中，可以给每一个主节点添加从节点，主节点和从节点直接遵循主从模型的特性。当用户需要处理更多读请求的时候，添加从节点开启read-only来读写分离可以扩展系统的读性能。 缺点 Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的行为。 不能用redis事务机制(不过就算不用redis集群一般也不推荐用redis的事务, 毕竟假事务无法回滚嘛, 比如multi之后那些在 EXEC 命令执行之后所产生的错误， 并没有对它们进行特别处理： 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。)。因为一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。 故障转移Redis集群的主节点内置了类似Redis Sentinel的节点故障检测和自动故障转移功能，当集群中的某个主节点下线时，集群中的其他在线主节点会注意到这一点，并对已下线的主节点进行故障转移。集群进行故障转移的方法和Redis Sentinel进行故障转移的方法基本一样(也有主观下线和客观下线)，不同的是，在集群里面，故障转移的过程是: 在集群内广播选举消息 集群中其他在线的持有槽的主节点投票到故障主节点的从节点们 被选出来的从节点变成主节点 所以集群不必另外使用Redis Sentinel。 集群分片策略常见的集群分片算法有： 一般哈希算法 一致性哈希算法 Hash Slot算法 Redis采用的是Hash Slot 一般哈希算法计算方式：hash(key)%N缺点：如果增加一个redis，映射公式变成了 hash(key)%(N+1)​ 如果一个redis宕机了，映射公式变成了 hash(key)%(N-1)​ 在以上两种情况下，几乎所有的缓存都失效了。 一致性哈希算法先构造出一个长度为2^32整数环，根据节点名称的hash值（分布在[0,2^32-1]）放到这个环上。现在要存放资源，根据资源的Key的Hash值（也是分布在[0,2^32-1]），在环上顺时针的找到离它最近的一个节点，就建立了资源和节点的映射关系。 优点：一个节点宕机时，上面的数据转移到顺时针的下一个节点中，新增一个节点时，也只需要将部分数据迁移到这个节点中，对其他节点的影响很小 缺点： 由于数据在环上分布不均，可能存在某个节点存储的数据比较多，那么当他宕机的时候，会导致大量数据涌入下一个节点中，把另一个节点打挂了，然后所有节点都挂了 在增减节点时需要增加一倍或减去一半节点才能保证数据和负载的均衡 改进：引进了虚拟节点的概念，想象在这个环上有很多“虚拟节点”，数据的存储是沿着环的顺时针方向找一个虚拟节点，每个虚拟节点都会关联到一个真实节点 HashSlot算法Redis采用的是Hash Slot分片算法，用来计算key存储位置的。集群将整个数据库分为16384个槽位slot，所有key-value数据都存储在这些slot中的某一个上。一个slot槽位可以存放多个数据，key的槽位计算公式为：slot_number=CRC16(key)%16384，其中CRC16为16位的循环冗余校验和函数。客户端可能会挑选任意一个redis实例去发送命令，每个redis实例接收到命令，都会计算key对应的hash slot，如果在本地就在本地处理，否则返回moved给客户端，让客户端进行重定向到对应的节点执行命令(实现得好一点的smart客户端会缓存键-slot-节点的映射关系来获得性能提升). 那为什么是16384个槽呢? ps:CRC16算法产生的hash值有16bit，该算法可以产生2^16-=65536个值。换句话说，值是分布在0~65535之间。那作者在做mod运算的时候，为什么不mod65536，而选择mod16384？作者解答 在redis节点发送心跳包时需要把所有的槽放到这个心跳包里，以便让节点知道当前集群信息，16384=16k，在发送心跳包时使用char进行bitmap压缩后是16384÷8÷1024=2kb，也就是说使用2k的空间创建了16k的槽数。 虽然使用CRC16算法最多可以分配65535（2^16-1）个槽位，65535=65k，当槽位为65536时，这块的大小是: 65536÷8÷1024=8kb，也就是说需要需要8k的心跳包，作者认为这样做不太值得； Cache和DB如何一致详细的请参考: https://segmentfault.com/a/1190000015804406本博客也有一份: Cache和DB一致性 总结: 使用cache aside pattern 对于读请求 先读 cache，再读 db 如果，cache hit，则直接返回数据 如果，cache miss，则访问 db，并将数据 set 回缓存 对于写请求 先操作数据库，再淘汰缓存（淘汰缓存，而不是更新缓存, 如果更新缓存，在并发写时，可能出现数据不一致。） Cache Aside Pattern 方案存在什么问题？ 问题1: 如果先写数据库，再淘汰缓存，在原子性被破坏时： 修改数据库成功了 淘汰缓存失败了导致，数据库与缓存的数据不一致。 如何解决问题1? 在淘汰缓存的时候，如果失败，则重试一定的次数。如果失败一定次数还不行，那就是其他原因了。比如说 redis 故障、内网出了问题。 问题2: 主从同步延迟导致的缓存和数据不一致问题 问题: 发生写请求后（不管是先操作 DB，还是先淘汰 Cache），在主从数据库同步完成之前，如果有读请求，都可能发生读 Cache Miss，读从库把旧数据存入缓存的情况。此时怎么办呢？ 解决思路: 在主从时延的时间段内，读取修改过的数据的话，强制读主，并且更新缓存，这样子缓存内的数据就是最新。在主从时延过后，这部分数据继续读从库，从而继续利用从库提高读取能力。 具体解决方案: 写请求发生的时候: 将哪个库，哪个表，哪个主键三个信息拼装一个 key 设置到 cache 里，这条记录的超时时间，设置为 “主从同步时延”, PS：key 的格式为 “db:table:PK”，假设主从延时为 1s，这个 key 的 cache 超时时间也为 1s。 当读请求发生时：这是要读哪个库，哪个表，哪个主键的数据呢，也将这三个信息拼装一个 key，到 cache 里去查询，如果， （1）cache 里有这个 key，说明 1s 内刚发生过写请求，数据库主从同步可能还没有完成，此时就应该去主库查询。并且把主库的数据 set 到缓存中，防止下一次 cache miss。 （2）cache 里没有这个 key，说明最近没有发生过写请求，此时就可以去从库查询 缓存雪崩是啥?咋处理?是指大面积的缓存失效，打崩了DB. 如果缓存挂掉，所有的请求会压到数据库，如果未提前做容量预估，可能会把数据库压垮（在缓存恢复之前，数据库可能一直都起不来），导致系统整体不可服务。又或者打个比方, 如果所有首页的Key失效时间都是12小时，中午12点刷新的，我零点有个秒杀活动大量用户涌入，假设当时每秒 6000 个请求，本来缓存在可以扛住每秒 5000 个请求，但是缓存当时所有的Key都失效了。此时 1 秒 6000 个请求全部落数据库，数据库必然扛不住. 处理方案: key随机过期 key永不过期, 比如开个单独线程去定时更新缓存 高可用, 如果Redis是集群部署，将热点数据均匀分布在不同的Redis库中也能避免全部失效的问题 隔离服务, 限流降级 缓存穿透是啥?咋处理?是指缓存和数据库中都没有的数据，而用户不断发起请求，严重会击垮数据库 我们数据库的 id 都是1开始自增上去的，如发起为id值为 -1 的数据或 id 为特别大不存在的数据。这时的用户很可能是攻击者，攻击会导致数据库压力过大，严重会击垮数据库。 处理方案: 缓存穿透我会在接口层增加校验，比如用户鉴权校验，参数做校验，不合法的参数直接代码Return，比如：id 做基础校验，id &lt;=0的直接拦截等。 布隆过滤器, 把存在的key提前存放好在布隆过滤器中, 当查询的时候快速判断出你这个Key是否在数据库中不存在或可能存在, 不存在则直接return. 原理如下: 如果我们要映射一个值到布隆过滤器中，我们需要使用多个不同的哈希函数生成多个哈希值，并对每个生成的哈希值指向的 bit 位置 1，例如针对值 “baidu” 和三个不同的哈希函数分别生成了哈希值 1、4、7，则有 Ok，我们现在再存一个值 “tencent”，如果哈希函数返回 3、4、8 的话，图继续变为： 值得注意的是，4 这个 bit 位由于两个值的哈希函数都返回了这个 bit 位，因此它被覆盖了。现在我们如果想查询 “dianping” 这个值是否存在，哈希函数返回了 1、5、8三个值，结果我们发现 5 这个 bit 位上的值为 0，说明没有任何一个值映射到这个 bit 位上，因此我们可以很确定地说 “dianping” 这个值不存在。而当我们需要查询 “baidu” 这个值是否存在的话，那么哈希函数必然会返回 1、4、7，然后我们检查发现这三个 bit 位上的值均为 1，那么我们可以说 “baidu” 存在了么？答案是不可以，只能是 “baidu” 这个值可能存在。 缓存击穿是啥?咋处理?是指持续的大并发的访问一个热点数据, 当这个Key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库 这个跟缓存雪崩有点像，但是又有一点不一样，缓存雪崩是因为大面积的缓存失效，打崩了DB，而缓存击穿不同的是缓存击穿是指一个Key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个Key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个完好无损的桶上凿开了一个洞。 处理方案: key永不过期, 比如开个单独线程去定时更新缓存 互斥锁, 在key失效的瞬间, 只允许一个查询操作的线程A去查询数据库并重建缓存并上互斥锁, 其他的查询操作线程全部等待线程A操作完了再从缓存里取数据 etcd etcd 参考 https://wingsxdu.com/post/database/etcd/#gsc.tab=0 raft 参考 https://www.jianshu.com/p/5aed73b288f7 重点参考 https://segmentfault.com/a/1190000022248118 etcd 是一个 Go 语言编写的分布式、高可用的强一致性键值存储系统，用于提供可靠的分布式键值(key-value)存储、配置共享和服务发现等功能。 etcd可以用于存储关键数据和实现分布式调度，它在现代化的集群运行中能够起到关键性的作用。 Raft用于保证分布式数据的一致性。动画演示Raft Raft选主过程 动画演示Raft选主前提知识: Election timeout选举周期: The election timeout is the amount of time a follower waits until becoming a candidate. heartbeat timeout心跳时间间隔 选主的具体流程如下: 假设三个节点的集群，三个节点上均运行 一个随机选举周期timer（每个 Timer 持续时间是随机的, 一般是150~300ms），Raft算法使用随机 Timer 来初始化 Leader 选举流程，第一个节点率先完成了 Timer，那它就要变成candidate，然后带着自身的数据版本信息发起vote 随后它就会向其他两个节点发送成为 Leader 的请求，其他follower节点接收到请求后会以投票回应然后第一个节点是否被选举为 Leader。 在每一任期内，最多允许一个节点被选举为leader 投票后就会重置一下选举周期timer, 重新计时 检查是否candidate的数据版本比自己要新, 如果比自己旧, 那就会无情拒绝, 如果有都变成了candidate的多个节点，follower们采取哪个candidate先来先投票的策略。 在一个任期内，一个节点只能投一票 如果超过半数的follower都认为他是合适做领导的，那么恭喜，新的leader产生了. 成为 Leader 后，该节点会以固定心跳时间间隔heartbeat timeout向其他节点发送通知确保自己仍是Leader，follower收到了心跳则会重置一下选举周期timer, 重新计时。 有些情况下当 Follower 们收不到 Leader 的通知后，比如说 Leader 节点宕机或者失去了连接，则其他节点当选举周期timer到期后就会会重复之前选举过程选举出新的 Leader。 如果多个candidate同时发起了选举: 只有follower能投票且只能投一次(投了a就不能投b了), candidate B 是不能给candidate A 投票的 如果其中一个candidate拿到了超过半数的选票也可以成为leader 如果所有candidate都没有获得大多数选票时(很可能发生这种情况, 因为在一个任期内，一个节点只能投一票, 投了A就不能再投B了)，则所有节点还是还是继续走一个随机选举周期timer的选举流程, 等待下一次某个节点的选举周期timer触发则term加1进行下一任期的选举 数据读写流程如何保证一致性的 Raft的log entry的复制流程动画 etcd写请求流程写请求的过程简要总结: 在etcd-raft实现中，所有的写请求都会由leader执行并将请求日志同步到follower节点，且若follower节点收到客户端的写请求，则是把写请求转发给leader。 待该 写请求 日志被复制到集群半数以上的节点时，该 写请求 日志会被 Leader 节点确认为己提交，Leader 会回复客户端写请求操作成功 为什么说即使完成了一次写请求流程也有可能读到过期数据? etcd又是如何解决此问题的?(见此etcd线性一致性读) 前提知识: consensus共识在实现机制上属于复制状态机(Replicated State Machine)的范畴，复制状态机是一种很有效的容错技术，基于复制日志来实现，每个 Server 存储着一份包含命令序列的日志文件，状态机会按顺序执行这些命令。因为日志中的命令和顺序都相同，因此所有节点会得到相同的数据。因此保证系统一致性就简化为保证操作日志的一致，这种复制日志的方式被大量运用，如 GSF、HDFS、ZooKeeper和 etcd 都是这种机制。 raft中的日志(log entry)并不是系统Debug日志，而是序列化后的command，这些Command复制到各个节点后，通过序列化内容的解析出命令后，在各个节点上执行并返回操作结果从而实现复制状态机 原因: etcd 就完成了一次写操作仅仅代表写请求日志操作成功完毕了，写操作成功仅仅意味着日志达成了一致（已经落盘）,并不意味着这条日志被应用到状态机(状态机 apply 日志的行为在大多数 Raft 算法的实际实现中都是异步的, raft不管具体状态机如何实现, 他只规定了日志的复制流程算法标准)，而并不能确保当前状态机也已经 apply 了日志。所以此时读取状态机并不能准确反应数据的状态，很可能会读到过期数据。 Leader 应用了这条日志也不意味着所有的 Follower 都应用了这条日志，每个节点会独立地决定应用日志的时机，这中间可能存在着一定的延迟。虽然 etcd 应用日志的过程是异步的，但是这种批处理策略能够一次批量写入多条日志，可以提升节点的 I/O 性能。 etcd 有额外的机制解决这个问题。这部分内容会在下文的etcd线性一致性读介绍到。 etcd线性一致性读etcd 两种方案来保证线性一致性读: ReadIndex 方案 ReadIndex是etcd-raft的默认方案。 虽然状态机应用日志的行为是异步的，但是已经提交的日志都满足线性一致，那么只要等待这些日志都应用到状态机中再执行查询，读请求也可以满足线性一致。 ReadIndex机制的执行流程如下： 从 leader节点读: 1. 读操作执行前记录『此时』集群的 CommittedIndex(commitIndex即为log中最后一个被提交的index值. 因为此时自己就是leader, 所以从 leader 获取到的 commited index 就作为此次读请求的 ReadIndex)，记为ReadIndex； 2. 向 Follower 发送心跳消息，如果超过法定人数的节点响应了心跳消息，那么就能保证 当前Leader 节点还是leader, 主要是防止本leader是已经隔离的了小集群里的leader，这样就确保了 Leader 节点的数据都是最新的 3. 等待状态机『至少』应用到ReadIndex，即 AppliedIndex &gt;= ReadIndex； 4. 执行读请求，将结果返回给 Client。 从 follower 节点读: Follower 先向 Leader 询问 readIndex，Leader 收到 Follower 的请求后依然要通过 上述的第2步骤广播心跳确认自己 Leader 的身份，然后返回当前的 commitIndex 作为 readIndex，Follower 拿到 readIndex 后，等待本地的 applyIndex 大于等于 readIndex 后，即可读取状态机中的数据返回。 LeaseRead 方案 etcd-raft不推荐采用此方案 基本的思路是 Leader 取一个比 Election Timeout选举周期小的租期，在租期不会发生选举，确保 Leader 不会变，所以可以跳过 ReadIndex 的第二步，也就降低了延时。 LeaseRead 与 ReadIndex 类似，但更进一步，不仅省去了 Log，还省去了网络交互。它可以大幅提升读的吞吐也能显著降低延时。 缺陷: LeaseRead 的正确性和时间挂钩，因此时间的实现至关重要，如果漂移严重，这套机制就会有问题。LeaseRead 的正确性和时间的实现挂钩，由于不同主机的 CPU 时钟有误差，所以也有可能读取到过期的数据。再次强调，这依赖于机器的时钟飘移速率，换言之，若各机器之间的时钟差别过大，则此种基于lease的机制就可能出现问题 etcd存在脑裂情况吗etcd不存在脑裂情况. 众所周知 etcd 使用 Raft 协议来解决数据一致性问题。一个 Raft Group 只能有一个 Leader 存在，如果一旦发生网络分区，Leader 只会在多数派一边被选举出来，而少数派则全部处于 Follower 或 Candidate 状态，所以一个长期运行的集群是不存在脑裂问题的。etcd 官方文档也明确了这一点： The majority side becomes the available cluster and the minority side is unavailable; there is no “split-brain” in etcd. 但是有一种特殊情况，假如旧的 Leader 和集群其他节点出现了网络分区，其他节点选出了新的 Leader，但是旧 Leader 并没有感知到新的 Leader，那么此时集群可能会出现一个短暂的「双 Leader」状态。这种情况并不能称之为脑裂，原因如下： 这种情况并不能称之为脑裂，原因有二： 这不是一个长期运行状态，维持时间不会超过一个投票周期 etcd网络分区时 如果leader在少数派 此时多数派会有follower选举周期timer触发则任期term增加, 并且会选出多数派新leader, 此时少数派leader会检查法定人数是否大于节点数量一半, 检查确认后则少数派集群进入是不可用状态，全部变为 Follower 或 Candidate 状态, 不支持raft请求，只支持非一致性读请求。一旦网络分区清除，少数派因为任期term较小, 则这边会自动承认来自多数这边的 leader 并同步多数派的数据状态。 如果在leader在多数派, 则一切照旧 网络分区时 etcd 也有 ReadIndex、LeaseRead 机制来解决这种状态下的数据一致性问题 新Leader会无条件提交旧Leader日志吗其实这里有 4 种情况： Leader 复制给少数节点，然后宕机 Leader 复制给多数节点，然后宕机 Leader 复制给多数节点，本地提交成功，返回客户端成功，然后宕机 场景 1-2 压根没有给客户端承诺，所以是新 Leader 不会立即 commit 前任 Leader 的日志；场景 3 承诺了客户端，无论如何日志是不允许丢的，所以新 Leader 一定会 commit 日志。 etcd可以偶数个部署吗可以, 但非常不建议. 偶数个节点的集群非但不能提升容错能力，反而会带来资源的浪费并可能使选举的时间变长。同时在奇数个集群的情况下，即使产生网络分区也能保证始终有一方占据大多数的节点，进而选举出新的 Leader 来保证集群的可用。而偶数个节点则可能会出现对半分的场景，这样任意一方都无法选举出 Leader，导致集群的不可用。 不能直接read返回吗疑问：为什么 read 请求到达 leader 之后需要获取最新的 commit index，然后再等到 applied index &gt;= commit index 之后再 read 数据返回 ？不能直接 read 返回吗 ？ 答案：不可以，因为那样不满足 linearizable read，因为要想满足 linearizable read 那么必须保证，已经被 read 到的数据，那么后面的 read （非并发的 read） 都应该能 read 到，也就是不会出现 read 到旧数据。我们已 commit index 为依据去 read，可以保证 read request 到达 leader 越晚，其 commit index 必然越大，也就是 r1 arrive time &lt;= r2 arrive time，那么 r1 commit index &lt;= r2 commit index，那么 r2 read 到的数据就至少和 r1 一样新，从而保证了 linearizable read。 相反，直接 read 返回，相当于是记录下当前 applied index，但是 applied index 并不是严格的单调的往上增加的，例如集群 {A，B，C}开始 A 为 leader，这个时候 applied index 为 a1，然后 A 挂了，B 重新选举为了 leader，这个时候 B 的 applied index 为 a2，那么我们并不能保证 a2 &gt;= a1，因为很有可能，B 由于日志复制延迟导致日志虽然复制过去了（保证拥有最新的日志，能选为 leader），但是还没来得及 apply，那么如果一个 read 来到 B，可能就会 read 到旧数据，出现 read 的新旧数据反转，从而不满足线性一致性。 etcd架构及解析 从 etcd 的架构图中我们可以看到，etcd 主要分为四个部分。 HTTP Server： 用于处理用户发送的 API 请求以及其它 etcd 节点的同步与心跳信息请求。 Store： 用于处理 etcd 支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是 etcd 对用户提供的大多数 API 功能的具体实现。 Raft： Raft 强一致性算法的具体实现，是 etcd 的核心。 WAL： Write Ahead Log（预写式日志），是 etcd 的数据存储方式。除了在内存中存有所有数据的状态以及节点的索引以外，etcd 就通过 WAL 进行持久化存储。WAL 中，所有的数据提交前都会事先记录日志。 Snapshot 是为了防止数据过多而进行的状态快照；Entry 表示存储的具体日志内容。 通常，一个用户的请求发送过来，会经由 HTTP Server 转发给 Store 进行具体的事务处理，如果涉及到节点的修改，则交给 Raft 模块进行状态的变更、日志的记录，然后再同步给别的 etcd 节点以确认数据提交，最后进行数据的提交，再次同步。 etcd的使用场景 服务发现 负载均衡 分布式锁 分布式队列 上面说到etcd可以很容易的实现分布式锁, 锁服务有两种使用方式，一是保持独占，二是控制时序。通过控制时序，即所有想要获得锁的用户都会被安排执行，但是获得锁的顺序也是全局唯一的，同时决定了执行顺序, 就可以实现分布式队列。etcd 为此也提供了一套 API（自动创建有序键），对一个目录建值时指定为POST动作，这样 etcd 会自动在目录下生成一个当前最大的值为键，存储这个新的值（客户端编号）。同时还可以使用 API 按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可以是代表客户端的编号。 etcd概念术语 Raft： etcd所采用的保证分布式系统强一致性的算法。 Node： 一个Raft状态机实例。 Member： 一个etcd实例。它管理着一个Node，并且可以为客户端请求提供服务。 Cluster： 由多个Member构成可以协同工作的etcd集群。 Peer： 对同一个etcd集群中另外一个Member的称呼。 Client： 向etcd集群发送HTTP请求的客户端。 WAL： 预写式日志，etcd用于持久化存储的日志格式。 snapshot： etcd防止WAL文件过多而设置的快照，存储etcd数据状态。 Proxy： etcd的一种模式，为etcd集群提供反向代理服务。 Leader： Raft算法中通过竞选而产生的处理所有数据提交的节点。 Follower： 竞选失败的节点作为Raft中的从属节点，为算法提供强一致性保证。 Candidate： 当Follower超过一定时间接收不到Leader的心跳时转变为Candidate开始竞选。 Term： 某个节点成为Leader到下一次竞选时间，称为一个Term。 Index： 数据项编号。Raft中通过Term和Index来定位数据。 Entry: 表示存储的具体日志内容。 etcd数据存储etcd 的存储分为内存存储和持久化（硬盘）存储两部分，内存中的存储除了顺序化的记录下所有用户对节点数据变更的记录外，还会对用户数据进行索引、建堆等方便查询的操作。而持久化则使用预写式日志（WAL：Write Ahead Log）进行记录存储。 在 WAL 的体系中，所有的数据在提交之前都会进行日志记录。在 etcd 的持久化存储目录中，有两个子目录。一个是 WAL，存储着所有事务的变化记录；另一个则是 snapshot，用于存储某一个时刻 etcd 所有目录的数据。通过 WAL 和 snapshot 相结合的方式，etcd 可以有效的进行数据存储和节点故障恢复等操作。 WAL（Write Ahead Log）最大的作用是记录了整个数据变化的全部历程。在 etcd 中，所有数据的修改在提交前，都要先写入到 WAL 中。使用 WAL 进行数据的存储使得 etcd 拥有两个重要功能: 故障快速恢复： 当你的数据遭到破坏时，就可以通过执行所有 WAL 中记录的修改操作，快速从最原始的数据恢复到数据损坏前的状态。 数据回滚（undo）/ 重做（redo）：因为所有的修改操作都被记录在 WAL 中，需要回滚或重做，只需要方向或正向执行日志中的操作即可。 WAL的缺陷: WAL 是一种 Append Only 的日志文件，只会在文件结尾不断地添加新日志，这样做可以避免大量随机 I/O 带来的性能损失，但是随着程序的运行，节点需要处理客户端和集群中其他节点发来的大量请求，相应的 WAL 日志量也会不断增加，这会占用大量的磁盘空间。当节点宕机后，如果要恢复其状态，则需要从头读取全部的 WAL 日志文件，这显然是非常耗时的。 WAL的缺陷的解决方案: 快照, 为了解决WAL的这个缺陷，etcd 会定期创建快照，将整个节点的状态进行序列化，然后写入稳定的快照文件中，在该快照文件之前的日志记录就可以全部丢弃掉。在恢复节点状态时会先加载快照文件，使用快照数据将节点恢复到对应的状态，之后从 WAL 文件读取快照之后的数据，将节点恢复到正确的状态。 etcd 的快照有两种: 一种是用于存储某一时刻 etcd 的所有数据的数据快照， 另一种是用于集群中较慢节点追赶数据的 RPC 快照。 分布式系统分布式系统的就准备: CAP理论 BASE理论 分布式事务 分布式锁 限流 熔断 一致性选举算法 主从架构 集群架构 异地多活 负载均衡 分层架构 微服务, 服务治理 … 共识consensus 准确的翻译是共识，即多个提议者达成共识的过程，例如 Paxos，Raft 就是共识算法，paxos 是一种共识理论，分布式系统是他的场景，一致性是他的目标。 一致性（Consistency）的含义比共识（consensus）要宽泛，一致性指的是多个副本对外呈现的状态。包括顺序一致性、线性一致性、最终一致性等。而共识特指达成一致的过程，但注意，共识并不意味着实现了一致性，一些情况下他是做不到的。 一致性的类别提到分布式架构就一定绕不开 “一致性” 问题，而 “一致性” 其实又包含了数据一致性和事务一致性两种情况，本节主要讨论数据一致性（事务一致性指 ACID）。复制是导致出现数据一致性问题的唯一原因。 关于强和弱的定义，可以参考剑桥大学的 slide. Strong consistency – ensures that only consistent state can be seen: All replicas return the same value when queried for the attribute of an object * All replicas return the same value when queried for the attribute of an object. This may be achieved at a cost – high latency. Weak consistency – for when the “fast access” requirement dominates: update some replica, e.g. the closest or some designated replica the updated replica sends up date messages to all other replicas. different replicas can return different values for the queried attribute of the object the value should be returned, or “not known”, with a timestamp in the long term all updates must propagate to all replicas ……. 一致性的详细分类: 强一致性 强一致性集群中，对任何一个节点发起请求都会得到相同的回复，但可能会产生相对高的延迟 线性一致性Linearizability consistency, 也叫原子一致性, 大多数时候我们说强一致性其实是指线性一致性, 两个要求： 任何一次读都能读到某个数据的最近一次写的数据。 系统中的所有进程，看到的操作顺序，都和全局时钟下的顺序一致。 顺序一致性Sequential consistency, 比线性一致性稍弱, 但也算是强一致性的一种, 两个要求： 任何一次读都能读到某个数据的最近一次写的数据。 系统的所有进程的顺序一致，而且是合理的。即不需要和全局时钟下的顺序一致，错的话一起错，对的话一起对。 弱一致性 弱一致性具有更低的响应延迟，但可能会回复过期的数据, 导致各个节点拿到的数据不一致。 最终一致性(Eventual consistency) 含义: 系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型。 因果一致性(Causal consistency) 含义: 如果一系列写入按某个逻辑顺序发生，那么任何人读取这些写入时，会看见它们以正确的逻辑顺序出现。 实现: 一种方案是应用保证将问题和对应的回答写入相同的分区 读写一致性: 含义: 它可以保证，如果用户刷新页面，他们总会看到自己刚提交的任何更新。它不会对其他用户的写入做出承诺，其他用户的更新可能稍等才会看到，但它保证用户自己提交的数据能马上被自己看到。 单调读: 含义: 如果先前读取到较新的数据，后续读取不会得到更旧的数据. 实现: 实现单调读取的一种方式是确保每个用户总是从同一个节点进行读取（不同的用户可以从不同的节点读取），比如可以基于用户 ID 的哈希值来选择节点，而不是随机选择节点。 线性一致性etcd 读写都做了线性一致，即 etcd 是标准的强一致性保证。 线性一致性又被称为强一致性、严格一致性、原子一致性。是程序能实现的最高的一致性模型，也是分布式系统用户最期望的一致性。CAP 中的 C 一般就指它顺序一致性中进程只关心大家认同的顺序一样就行，不需要与全局时钟一致，线性就更严格，从这种偏序（partial order）要达到全序（total order） 要求是： 任何一次读都能读到某个数据的最近一次写的数据。 系统中的所有进程，看到的操作顺序，都与全局时钟下的顺序一致。 顺序一致性两个要求： 任何一次读都能读到某个数据的最近一次写的数据。 系统的所有进程的顺序一致，而且是合理的。即不需要和全局时钟下的顺序一致，错的话一起错，对的话一起对。 举例说明1下面的图满足了顺序一致，但不满足线性一致。 x 和 y 的初始值为 0 Write(x,4) 代表写入 x=4，Read(y,2) 为读取 y =2 从图上看，进程 P1，P2 的一致性并没有冲突。因为从这两个进程的角度来看，顺序应该是这样的：1Write(y,2), Read(x,0), Write(x,4), Read(y,2) 这个顺序对于两个进程内部的读写顺序都是合理的，只是这个顺序与全局时钟下看到的顺序并不一样。在全局时钟的观点来看，P2 进程对变量 X 的读操作在 P1 进程对变量 X 的写操作之后，然而 P2 读出来的却是旧的数据 0 举例说明2假设我们有个分布式 KV 系统，以下是四个进程 对其的操作顺序和结果:-- 表示持续的时间，因为一次写入或者读取，客户端从发起到响应是有时间的，发起早的客户端，不一定拿到数据就早，有可能因为网络延迟反而会更晚。情况 1：1234A: --W(x,1)----------------------B: --W(x,2)----------------------C: -R(x,1)- --R(x,2)-D: -R(x,1)- --R(x,2)-- 情况 2：1234A: --W(x,1)----------------------B: --W(x,2)----------------------C: -R(x,2)- --R(x,1)-D: -R(x,2)- --R(x,1)-- 上面情况 1 和 2 都是满足顺序一致性的，C 和 D 拿的顺序都是 1-2，或 2-1，只要 CD 的顺序一致，就是满足顺序一致性。只是从全局看来，情况 1 更真实，情况 2 就显得” 错误 “了，因为情况 2 是这样的顺序1B W(x,2) -&gt; A W(x,1) -&gt; C R(x,2) -&gt; D R(x,2) -&gt; C R(x,1) -&gt; D R(x,1) 不过一致性不保证正确性，所以这仍然是一个顺序一致。再加一种情况 3：情况 3：1234A: --W(x,1)----------------------B: --W(x,2)----------------------C: -R(x,2)- --R(x,1)-D: -R(x,1)- --R(x,2)-- 情况 3 就不属于顺序一致了，因为 C 和 D 两个进程的读取顺序不同了。 举例说明3这也是顺序一致的, 但是可能不满足产品经理要求. 从时间轴上可以看到，B0 发生在 A0 之前，读取到的 x 值为 0。B2 发生在 A0 之后，读取到的 x 值为 1。而读操作 B1，C0，C1 与写操作 A0 在时间轴上有重叠，因此他们可能读取到旧的值 0，也可能读取到新的值 1。注意，C1 发生在 B1 之后（二者在时间轴上没有重叠），但是 B1 看到 x 的新值，C1 反而看到的是旧值。即对用户来说，x 的值发生了回跳。 CAP理论一个分布式系统不可能同时满足以下三个基本需求，最多只能同时满足其中两项: 一致性（C：Consistency, CAP的C指的是强一致性） 在分布式环境下，一致性是指数据在多个副本之间能否保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。 对于一个将数据副本分布在不同分布式节点上的系统来说，如果对第一个节点的数据进行了更新操作并且更新成功后，却没有使得第二个节点上的数据得到相应的更新，于是在对第二个节点的数据进行读取操作时，获取的依然是老数据（或称为脏数据），这就是典型的分布式数据不一致的情况。在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都可以读取到其最新的值，那么这样的系统就被认为具有强一致性。 可用性（A：Availability）可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。这里的重点是 “有限时间内” 和 “返回结果”。 “有限时间内” 是指，对于用户的一个操作请求，系统必须能够在指定的时间内返回对应的处理结果，如果超过了这个时间范围，那么系统就被认为是不可用的。另外，”有限的时间内” 是指系统设计之初就设计好的运行指标，通常不同系统之间有很大的不同，无论如何，对于用户请求，系统必须存在一个合理的响应时间，否则用户便会对系统感到失望。 “返回结果” 是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确地反映出队请求的处理结果，即成功或失败，而不是一个让用户感到困惑的返回结果。 分区容错性（P：Partition tolerance）系统应该能持续提供服务，即使系统内部有消息丢失（分区）。 网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络）中，由于一些特殊的原因导致这些子网络出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。 需要注意的是，组成一个分布式系统的每个节点的加入与退出都可以看作是一个特殊的网络分区。 cap通俗而精准的解释一个分布式系统里面，节点组成的网络本来应该是连通的。然而可能因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。数据就散布在了这些不连通的区域中。这就叫分区。 当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。 提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项就可能分布到各个区里。容忍性就提高了。 然而，要把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。 cap的一些组合例子既然一个分布式系统无法同时满足一致性、可用性、分区容错性三个特点，所以我们就需要抛弃一个： 选择 说明 CA 放弃分区容错性，加强一致性和可用性，其实就是传统的单机数据库的选择 AP 放弃一致性（这里说的一致性是强一致性），追求分区容错性和可用性，这是很多分布式系统设计时的选择，例如很多 NoSQL 系统就是如此 CP 放弃可用性，追求一致性和分区容错性，基本不会选择，网络问题会直接让整个系统不可用, 例如 zookeeper和 etcd都是cp的 需要明确的一点是，对于一个分布式系统而言，分区容错性是一个最基本的要求。因为既然是一个分布式系统，那么分布式系统中的组件必然需要被部署到不同的节点，否则也就无所谓分布式系统了，因此必然出现子网络。而对于分布式系统而言，网络问题又是一个必定会出现的异常情况，因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。因此系统架构师往往需要把精力花在如何根据业务特点在 C（一致性）和 A（可用性）之间寻求平衡。 BASE 理论BASE 是 Basically Available（基本可用） Soft state（软状态, 即中间状态) Eventually consistent（最终一致性） 三个短语的缩写。BASE 理论是对 CAP 中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结， 是基于 CAP 定理逐步演化而来的。BASE 理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。接下来看一下 BASE 中的三要素： 基本可用 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。注意，这绝不等价于系统不可用。比如： 响应时间上的损失。正常情况下，一个在线搜索引擎需要在 0.5 秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了 1~2 秒 系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 软状态 软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 最终一致性 最终一致性强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 总的来说，BASE 理论面向的是大型高可用可扩展的分布式系统，和传统的事物 ACID 特性是相反的，它完全不同于 ACID 的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID 特性和 BASE 理论往往又会结合在一起。 服务治理服务治理主要包括: 服务注册发现 限流 监控 网关 负载均衡 日志采集 链路追踪 详细如下图: 分布式锁 主要有: etcd/zookeeper(严谨) redis(遭到质疑, 极限情况有可能有问题, 但因为性能较高且极限情况不容易发生, 也有人用) 分布式锁过期时间到了但业务没执行完怎么办注册一个定时任务，每隔一定时间就去延长锁超时时间 基于etcd的分布式锁因为 etcd 使用 Raft 算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。 保持独占即所有获取锁的用户最终只有一个可以得到。etcd 为此提供了一套实现分布式锁原子操作 CAS（CompareAndSwap）的 API。通过设置prevExist值，可以保证在多个节点同时去创建某个目录时，只有一个成功。而创建成功的用户就可以认为是获得了锁。 基于redis的分布式锁 利用setnx+expire命令 (错误的做法): setnx和expire是分开的两步操作，不具有原子性 使用Lua脚本（包含setnx和expire两条指令） 使用 set key value [EX seconds][PX milliseconds][NX|XX] 命令 (正确做法, 接下来介绍这种) Redis 在 2.6.12 版本开始，为 SET 命令增加一系列选项： 1SET key value\[EX seconds\]\[PX milliseconds\]\[NX|XX\] EX seconds: 设定过期时间，单位为秒 PX milliseconds: 设定过期时间，单位为毫秒 NX: 仅当 key 不存在时设置值 XX: 仅当 key 存在时设置值 value 必须要具有唯一性，我们可以用 UUID 来做，设置随机字符串保证唯一性，至于为什么要保证唯一性？假如 value 不是随机字符串，而是一个固定值，那么就可能存在下面的问题： 1. 客户端 1 获取锁成功2. 客户端 1 在某个操作上阻塞了太长时间3. 设置的 key 过期了，锁自动释放了4. 客户端 2 获取到了对应同一个资源的锁5. 客户端 1 从阻塞中恢复过来，因为 value 值一样，所以执行释放锁操作时就会释放掉客户端 2 持有的锁，这样就会造成问题 所以通常来说，在释放锁时，我们需要对 value 进行验证 释放锁时需要验证 value 值，也就是说我们在获取锁的时候需要设置一个 value，不能直接用 del key 这种粗暴的方式，因为直接 del key 任何客户端都可以进行解锁了，所以解锁时，我们需要判断锁是否是自己的，基于 value 值来判断, 这里使用 Lua 脚本的方式，尽量保证原子性。 致命缺陷:使用 set key value [EX seconds][PX milliseconds][NX|XX] 命令 看上去很 OK，实际上在有 Redis 主从结构的时候也会出现问题，比如说 A 客户端在 Redis 的 master 节点上拿到了锁，但是这个加锁的 key 还没有同步到 slave 节点，master 故障，发生故障转移，一个 slave 节点升级为 master 节点，B 客户端也可以获取同个 key 的锁，但客户端 A 也已经拿到锁了，这就导致多个客户端都拿到锁。 RedLock参考: https://zhuanlan.zhihu.com/p/100140241#RedLock 使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。 尝试从 N 个互相独立 Redis 实例获取锁； 计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从大多数（N / 2 + 1）实例上获取了锁，才认为获取锁成功； 如果获取锁失败，就到每个实例上释放锁 分布式锁的高并发优化先说一个超卖问题的情景, 假设订单系统部署两台机器上，不同的用户都要同时买10台iphone，分别发了一个请求给订单系统。接着每个订单系统实例都去数据库里查了一下，当前iphone库存是12台。俩大兄弟一看，乐了，12台库存大于了要买的10台数量啊！于是乎，每个订单系统实例都发送SQL到数据库里下单，然后扣减了10个库存，其中一个将库存从12台扣减为2台，另外一个将库存从2台扣减为-8台。现在完了，库存出现了负数！泪奔啊，没有20台iphone发给两个用户啊！这可如何是好。 用分布式锁如何解决库存超卖问题: 只有一个订单系统实例可以成功加分布式锁，然后只有他一个实例可以查库存、判断库存是否充足、下单扣减库存，接着释放锁。释放锁之后，另外一个订单系统实例才能加锁，接着查库存，一下发现库存只有2台了，库存不足，无法购买，下单失败。不会将库存扣减为-8的。 分布式锁的方案在高并发场景下有什么问题？ 分布式锁一旦加了之后，对同一个商品的下单请求，会导致所有客户端都必须对同一个商品的库存锁key进行加锁。比如，对iphone这个商品的下单，都必对“iphone_stock”这个锁key来加锁。这样会导致对同一个商品的下单请求，就必须串行化，一个接一个的处理,假设加锁之后，释放锁之前，查库存 -&gt; 创建订单 -&gt; 扣减库存，这个过程性能很高吧，算他全过程20毫秒，这应该不错了。那么1秒是1000毫秒，只能容纳50个对这个商品的请求依次串行完成处理。效率低下. 假如下单时，用分布式锁来防止库存超卖，但是是每秒上千订单的高并发场景，如何对分布式锁进行高并发优化来应对这个场景？解决方案: 分段加锁 其实说出来也很简单，相信很多人看过java里的ConcurrentHashMap的源码和底层原理，应该知道里面的核心思路，就是分段加锁！ 在某些情况下我们可以将锁分解技术进一步扩展为一组独立对象上的锁进行分解，这成为分段锁。其实说的简单一点就是：容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 比如：在ConcurrentHashMap中使用了一个包含16个锁的数组，每个锁保护所有散列桶的1/16，其中第N个散列桶由第（N mod 16）个锁来保护。假设使用合理的散列算法使关键字能够均匀的分部，那么这大约能使对锁的请求减少到越来的1/16。也正是这项技术使得ConcurrentHashMap支持多达16个并发的写入线程。 假如你现在iphone有1000个库存，那么你完全可以给拆成20个库存段，要是你愿意，可以在数据库的表里建20个库存字段，比如stock_01，stock_02，类似这样的，也可以在redis之类的地方放20个库存key。总之，就是把你的1000件库存给他拆开，每个库存段是50件库存，比如stock_01对应50件库存，stock_02对应50件库存。接着，每秒1000个请求过来了，好！此时其实可以是自己写一个简单的随机算法，每个请求都是随机在20个分段库存里，选择一个进行加锁。这样就好了，同时可以有最多20个下单请求一起执行，每个下单请求锁了一个库存分段，然后在业务逻辑里面，就对数据库或者是Redis中的那个分段库存进行操作即可，包括查库存 -&gt; 判断库存是否充足 -&gt; 扣减库存。 有一个坑大家一定要注意：如果某个下单请求，咔嚓加锁，然后发现这个分段库存里的库存不足了，此时咋办？这时你得自动释放锁，然后立马换下一个分段库存，再次尝试加锁后尝试处理。这个过程一定要实现。 分布式事务解决方案参考: https://www.cnblogs.com/mayundalao/p/11798502.html https://zhuanlan.zhihu.com/p/88226625 https://xiaomi-info.github.io/2020/01/02/distributed-transaction/ https://zhuanlan.zhihu.com/p/183753774 事务有两种: 刚性事务：遵循ACID原则，强一致性。 柔性事务：遵循BASE理论，最终一致性；与刚性事务不同，柔性事务允许一定时间内，不同节点的数据不一致，但要求最终一致。 二阶段提交2PC 大致的流程： 第一阶段（prepare）：事务管理器向所有本地资源管理器发起请求，询问是否是 ready 状态，所有参与者都将本事务能否成功的信息反馈发给协调者； 第二阶段 (commit/rollback)：事务管理器根据所有本地资源管理器的反馈，通知所有本地资源管理器，步调一致地在所有分支上提交或者回滚。 缺点: 同步阻塞：当参与事务者存在占用公共资源的情况，其中一个占用了资源，其他事务参与者就只能阻塞等待资源释放，处于阻塞状态。 单点故障：一旦事务管理器出现故障，整个系统不可用 数据不一致：在阶段二，如果事务管理器只发送了部分 commit 消息，此时网络发生异常，那么只有部分参与者接收到 commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。 不确定性：当协事务管理器发送 commit 之后，并且此时只有一个参与者收到了 commit，那么当该参与者与事务管理器同时宕机之后，重新选举的事务管理器无法确定该条消息是否提交成功。 实战:目前支付宝使用两阶段提交思想实现了分布式事务服务 (Distributed Transaction Service, DTS) ，它是一个分布式事务框架，用来保障在大规模分布式环境下事务的最终一致性。具体可参考支付宝官方文档：https://tech.antfin.com/docs/2/46887 TCC关于 TCC（Try-Confirm-Cancel）的概念，最早是由 Pat Helland 于 2007 年发表的一篇名为《Life beyond Distributed Transactions:an Apostate’s Opinion》的论文提出。 TCC 事务机制相比于上面介绍的 XA，解决了其几个缺点： 解决了协调者单点，由主业务方发起并完成这个业务活动。业务活动管理器也变成多点，引入集群。 同步阻塞：引入超时，超时后进行补偿，并且不会锁定整个资源，将资源转换为业务逻辑形式，粒度变小。 数据一致性，有了补偿机制之后，由业务活动管理器控制一致性 TCC(Try Confirm Cancel) Try 阶段：尝试执行，完成所有业务检查（一致性）, 预留必须业务资源（准隔离性） Confirm 阶段：确认执行真正执行业务，不作任何业务检查，只使用 Try 阶段预留的业务资源，Confirm 操作满足幂等性。要求具备幂等设计，Confirm 失败后需要进行重试。 Cancel 阶段：取消执行，释放 Try 阶段预留的业务资源 Cancel 操作满足幂等性 Cancel 阶段的异常和 Confirm 阶段异常处理方案基本上一致。 在 Try 阶段，是对业务系统进行检查及资源预览，比如订单和存储操作，需要检查库存剩余数量是否够用，并进行预留，预留操作的话就是新建一个可用库存数量字段，Try 阶段操作是对这个可用库存数量进行操作。基于 TCC 实现分布式事务，会将原来只需要一个接口就可以实现的逻辑拆分为 Try、Confirm、Cancel 三个接口，所以代码实现复杂度相对较高。 缺点:TCC 需要事务接口提供 try, confirm, cancel 三个接口，提高了编程的复杂性。依赖于业务方来配合提供这样的接口，推行难度大，所以一般不推荐使用这种方式。 实战:一般来说和钱相关的支付、交易等相关的场景，也可以用TCC，严格严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性! 本地消息表本地消息表这个方案最初是 ebay 架构师 Dan Pritchett 在 2008 年发表给 ACM 的文章。该方案中会有消息生产者与消费者两个角色，假设系统 A 是消息生产者，系统 B 是消息消费者，其大致流程如下： 当系统 A 被其他系统调用发生数据库表更操作，首先会更新数据库的业务表，其次会往相同数据库的消息表中插入一条数据，两个操作发生在同一个事务中, 如果本步骤发生操作失败, 则直接事务回滚 系统 A 的脚本定期轮询本地消息往 mq 中写入一条消息，如果消息发送失败会进行重试 系统 B 消费 mq 中的消息，并处理业务逻辑。如果本地事务处理失败，会在继续消费 mq 中的消息进行重试，如果业务上的失败，可以通知系统 A 进行回滚操作 本地消息表实现的条件： 消费者与生成者的接口都要支持幂等 生产者需要额外的创建消息表 需要提供补偿逻辑，如果消费者业务失败，需要生产者支持回滚操作 此方案的核心是将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。人工重试更多的是应用于支付场景，通过对账系统对事后问题的处理。 缺点:最大的问题就在于严重依赖于数据库的消息表来管理事务,这个会导致高并发场景无力,难以扩展,一般很少用 实战:跨行转账可通过该方案实现。用户 A 向用户 B 发起转账，首先系统会扣掉用户 A 账户中的金额，将该转账消息写入消息表中，如果事务执行失败则转账失败，如果转账成功，系统中会有定时轮询消息表，往 mq 中写入转账消息，失败重试。mq 消息会被实时消费并往用户 B 中账户增加转账金额，执行失败会不断重试。 小米海外商城用户订单数据状态变更，会将变更状态记录消息表中，脚本将订单状态消息写入 mq，最终消费 mq 给用户发送邮件、短信、push 等。 可靠消息最终一致性大致流程如下： A 系统先向 mq 发送一条 prepare 消息，如果 prepare 消息发送失败，则直接取消操作 如果消息发送成功，则执行本地事务 如果本地事务执行成功，则向 mq 发送一条 confirm 消息，如果发送失败，则发送回滚消息 B 系统定期消费 mq 中的 confirm 消息，执行本地事务，并发送 ack 消息。如果 B 系统中的本地事务失败，会一直不断重试，如果是业务失败，会向 A 系统发起回滚请求 mq 会定期轮询所有 prepared 消息调用系统 A 提供的接口查询消息的处理情况，如果该 prepare 消息本地事务处理成功，则重新发送 confirm 消息，否则直接回滚该消息 该方案与本地消息最大的不同是去掉了本地消息表，其次本地消息表依赖消息表重试写入 mq 这一步由本方案中的轮询 prepare 消息状态来重试或者回滚该消息替代。其实现条件与容错方案基本一致。 实战:目前市面上实现该方案的只有阿里的 RocketMq。 尽最大努力通知最大努力通知其实就是定期校对, 是最简单的一种柔性事务，适用于一些最终一致性时间敏感度低的业务，且被动方处理结果 不影响主动方的处理结果。 业务活动的主动方，在完成业务处理之后，向业务活动的被动方发送消息，允许消息丢失。主动方可以设置时间阶梯型通知规则，在通知失败后按规则重复通知，直到通知N次后不再通知。主动方提供校对查询接口给被动方按需校对查询，用于恢复丢失的业务消息。业务活动的被动方如果正常接收了数据，就正常返回响应，并结束事务。如果被动方没有正常接收，根据定时策略，向业务活动主动方查询，恢复丢失的业务消息 最大努力通知方案的特点: 用到的服务模式：可查询操作、幂等操作。 被动方的处理结果不影响主动方的处理结果；适用于对业务最终一致性的时间敏感度低的系统, 比如适合跨企业的系统间的操作，或者企业内部比较独立的系统间的操作，比如银行通知、商户通知等； 这个方案的大致意思就是： 系统 A 本地事务执行完之后，发送个消息到 MQ； 会有个专门消费 MQ 的服务 notify_service(即最大努力通知服务) ，这个服务会消费 MQ 并调用系统 B 的接口； 要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么 notify_service (即最大努力通知服务)就定时尝试重新调用系统 B, 反复 N 次，最后还是不行就放弃。 实战:小米海外商城目前除了支付回调外，最常用的场景是订单数据同步。例如系统 A、B 进行数据同步，当系统 A 发生订单数据变更，先将数据变更消息写入小米 notify 系统（作用等同 mq），然后 notify 系统异步处理该消息来调用系统 B 提供的接口并进行重试到最大次数。 负载均衡算法有哪些 轮询法 将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器，而不关心服务器实际的连接数和当前的系统负载。 随机法 通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。由概率统计理论可以得知，随着客户端调用服务端的次数增多，其实际效果越来越接近于平均分配调用量到后端的每一台服务器，也就是轮询的结果。 源地址哈希法 源地址哈希的思想是根据获取客户端的 IP 地址，通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。采用源地址哈希法进行负载均衡，同一 IP 地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问。 加权轮询法 不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此它们的抗压能力也不相同。给配置高、负载低的机器配置更高的权重，让其处理更多的请；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载，加权轮询能很好地处理这一问题，并将请求顺序且按照权重分配到后端。加权轮询算法的结果，就是要生成一个服务器序列。每当有请求到来时，就依次从该序列中取出下一个服务器用于处理该请求。比如针对c权重4, b权重2, a权重1的例子，加权轮询算法会生成序列{c, c, b, c, a, b, c}也有可能是{a, a, a, a, a, b, c}, 有可能不均匀, 前五个请求都会分配给服务器a。在Nginx源码中，实现了一种叫做平滑的加权轮询（smooth weighted round-robin balancing）的算法，它生成的序列更加均匀。比如前面的例子，它生成的序列为{ a, a, b, a, c, a, a}，转发给后端a的5个请求现在分散开来，不再是连续的。这样，每收到7个客户端的请求，会把其中的1个转发给后端a，把其中的2个转发给后端b，把其中的4个转发给后端c。收到的第8个请求，重新从该序列的头部开始轮询。 普通加权轮询法 平滑加权轮询法 加权随机法 与加权轮询法一样，加权随机法也根据后端机器的配置，系统的负载分配不同的权重。不同的是，它是按照权重随机请求后端服务器，而非顺序。 最小连接数法 最小连接数算法比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，它是根据后端服务器当前的连接情况，动态地选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器。 负载均衡的平滑加权轮询算法怎么实现当我们需要把一份数据发送到一个Set中的任意机器的时候，很容易想到的一个问题是，如何挑Set中的机器作为数据的接收方？显然算法需要符合以下要求： 支持加权，以便在机器故障时可以降低其权重 在加权的前提下，尽可能地把请求平摊到每台机器上 第一点很好理解，而第二点的意思是，比如说我们现在有a, b, c三个选择，权重分别是5, 1, 1，我们希望输出的结果是类似于a, a, b, a, c, a, a，而不是a, a, a, a, a, b, c。 从Github上面可以看到，Nginx以前也是使用和LVS类似的算法，并在某一次提交中修改为当前的算法，该算法大致思想如下：123456789101112131415161718192021222324252627282930313233343536Upstream: smooth weighted round-robin balancing.For edge case weights like &#123; 5, 1, 1 &#125; we now produce &#123; a, a, b, a, c, a, a &#125;sequence instead of &#123; c, b, a, a, a, a, a &#125; produced previously.Algorithm is as follows: on each peer selection we increase current_weightof each eligible peer by its weight, select peer with greatest current_weightand reduce its current_weight by total number of weight points distributedamong peers.In case of &#123; 5, 1, 1 &#125; weights this gives the following sequence ofcurrent_weight&apos;s: a b c 0 0 0 (initial state) 5 1 1 (a selected) -2 1 1 3 2 2 (a selected) -4 2 2 1 3 3 (b selected) 1 -4 3 6 -3 4 (a selected) -1 -3 4 4 -2 5 (c selected) 4 -2 -2 9 -1 -1 (a selected) 2 -1 -1 7 0 0 (a selected) 0 0 0 该算法除了有权重weight，还引入了另一个变量current_weight，在每一次遍历中会把current_weight加上weight的值，并选择current_weight最大的元素，对于被选择的元素，再把current_weight减去所有权重之和。 假设有 N 台服务器 S = {S0, S1, S2, …, Sn}，默认权重为 W = {W0, W1, W2, …, Wn}，当前权重为 CW = {CW0, CW1, CW2, …, CWn}。在该算法中有两个权重，默认权重表示服务器的原始权重，当前权重表示每次访问后重新计算的权重，当前权重的出初始值为默认权重值，当前权重值最大的服务器为 maxWeightServer，所有默认权重之和为 weightSum，服务器列表为 serverList，算法可以描述为： 找出当前权重值最大的服务器 maxWeightServer； 计算 {W0, W1, W2, …, Wn} 之和 weightSum； 将 maxWeightServer.CW = maxWeightServer.CW - weightSum； 重新计算 {S0, S1, S2, …, Sn} 的当前权重 CW，计算公式为 Sn.CW = Sn.CW + Sn.Wn 返回 maxWeightServer 服务发现是怎么实现的参考 我们可以考虑用etcd来做,服务发现要解决的也是分布式系统中最常见的问题之一，即在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发现就是想要了解集群中是否有进程在监听 udp 或 tcp 端口，并且通过名字就可以查找和连接。要解决服务发现的问题，需要有下面三大支柱，缺一不可。 一个强一致性、高可用的服务存储目录。基于 Raft 算法的 etcd 天生就是这样一个强一致性高可用的服务存储目录。 一种提供方的注册服务。提供方可以在 etcd 中注册服务，并且对注册的服务设置key TTL，定时保持服务的心跳以达到监控健康状态的效果, 比如每隔 30s 发送一次心跳设置一下这个key使代表该机器存活的节点继续存在，否则当etcd 没有检测到心跳这个key的ttl到了过期了就会把这个键值对删了 需求方可以即时更新提供方服务状态的机制.需求方通过watch机制监听自己需要用到的提供方信息的改动，提供方相关信息有变动的时候需求方就会收到消息,在接收到信息变动的时候立即从etcd获取相应最新的信息即可, 实现方式通常是这样：不同系统都在 etcd 上对同一个目录进行注册，同时设置 Watcher 观测该目录的变化（如果对子目录的变化也有需要，可以设置递归模式），当某个系统更新了 etcd 的目录，那么设置了 Watcher 的系统就会收到通知，并作出相应处理。 下面我们来看服务发现对应的具体场景。微服务协同工作架构中，服务动态添加。随着 Docker 容器的流行，多种微服务共同协作，构成一个相对功能强大的架构的案例越来越多。透明化的动态添加这些服务的需求也日益强烈。通过服务发现机制，在 etcd 中注册某个服务名字的目录，在该目录下存储可用的服务节点的 IP。在使用服务的过程中，只要从服务目录下查找可用的服务节点去使用即可。 熔断是怎么实现的什么是服务熔断呢？ 服务熔断：当下游的服务因为某种原因突然变得不可用或响应过慢，上游服务为了保证自己整体服务的可用性，不再继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。 需要说明的是熔断其实是一个框架级的处理，那么这套熔断机制的设计，基本上业内用的是断路器模式: 最开始处于closed状态，一旦检测到错误到达一定阈值，便转为open状态； 这时候会有个 reset timeout，到了这个时间了，会转移到half open状态； 尝试放行一部分请求到后端，一旦检测成功便回归到closed状态，即恢复服务； 业内目前流行的熔断器很多，例如阿里出的 Sentinel, 以及最多人使用的 Hystrix 在 Hystrix 中，对应配置如下123456//滑动窗口的大小，默认为20circuitBreaker.requestVolumeThreshold //过多长时间，熔断器再次检测是否开启，默认为5000，即5s钟circuitBreaker.sleepWindowInMilliseconds //错误率，默认50%circuitBreaker.errorThresholdPercentage 每当 20 个请求中，有 50% 失败时，熔断器就会打开，此时再调用此服务，将会直接返回失败，不再调远程服务。直到 5s 钟之后，重新检测该触发条件，判断是否把熔断器关闭，或者继续打开。这些属于框架层级的实现，我们只要实现对应接口就好！ 服务降级 降级的本质: 降级就是为了解决资源不足和访问量增加的矛盾 在有限的资源情况下，为了能抗住大量的请求，就需要对系统做出一些牺牲，有点“弃卒保帅”的意思。放弃一些功能，保证整个系统能平稳运行 降级牺牲的是: 强一致性变成最终一致性 大多数的系统是不需要强一致性的。 强一致性就要求多种资源的占用，减少强一致性就能释放更多资源 这也是我们一般利用消息中间件来削峰填谷，变强一致性为最终一致性，也能达到效果 干掉一些次要功能 停止访问不重要的功能，从而释放出更多的资源 举例来说，比如电商网站，评论功能流量大的时候就能停掉，当然能不直接干掉就别直接，最好能简化流程或者限流最好简化功能流程。把一些功能简化掉 降级的注意点: 对业务进行仔细的梳理和分析 哪些是核心流程必须保证的，哪些是可以牺牲的 什么指标下能进行降级 吞吐量、响应时间、失败次数等达到一个阈值才进行降级处理 如何降级: 降级最简单的就是在业务代码中配置一个开关或者做成配置中心模式，直接在配置中心上更改配置，推送到相应的服务。 限流限流就是通过对并发访问进行限速。限流的实现方式: 计数器: 最简单的实现方式 ，维护一个计数器，来一个请求计数加一，达到阈值时，直接拒绝请求。 一般实践中用 ngnix + lua + redis 这种方式，redis 存计数值 漏斗模式: 流量就像进入漏斗中的水一样，而出去的水和我们系统处理的请求一样，当流量大于漏斗的流出速度，就会出现积水，水多了会溢出, 漏斗很多是用一个队列实现的，当流量过多时，队列会出现积压，队列满了，则开始拒绝请求。 令牌桶: 看图例，令牌通和漏斗模式很像，主要的区别是增加了一个中间人，这个中间人按照一定的速率放入一些token，然后，处理请求时，需要先拿到token才能处理，如果桶里没有token可以获取，则不进行处理。 熔断-降级-限流三者的关系 熔断强调的是服务之间的调用能实现自我恢复的状态； 限流是从系统的流量入口考虑，从进入的流量上进行限制，达到保护系统的作用； 降级，是从系统内部的平级服务或者业务的维度考虑，流量大了，可以干掉一些，保护其他正常使用； 熔断是降级方式的一种；降级又是限流的一种方式；三者都是为了通过一定的方式去保护流量过大时，保护系统的手段。 id生成器如何实现全局递增参考 介绍一下美团在用的工业级 Leaf-snowflake 方案。 41-bit的时间是毫秒级时间, 可以表示(1L&lt;&lt;41)/(1000L*3600*24*365)=69年的时间， 10-bit机器可以分别表示1024台机器。如果我们对IDC划分有需求，还可以将10-bit分5-bit给IDC，分5-bit给工作机器。这样就可以表示32个IDC，每个IDC下可以有32台机器，可以根据自身需求定义。 12个自增序列号可以表示2^12个ID，理论上snowflake方案的QPS约为409.6w/s， 这种分配方式可以保证在任何一个IDC的任何一台机器在任意毫秒内生成的ID都是不同的 优点： 毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。 不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。 可以根据自身业务特性分配bit位，非常灵活。 缺点：强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。 解决时钟回拨问题解决方案: 由于强依赖时钟，对时间的要求比较敏感，在机器工作时 NTP 同步也会造成秒级别的回退，建议可以直接关闭 NTP 同步。 在时钟回拨的时候直接不提供服务直接返回 ERROR_CODE，等时钟追上即可 偏差在5秒之内, 比如就等待2倍的偏差时间(比如比上次的还小3秒, 那我就等6秒)。然后做一层重试, 如果还是小于上次的时间, 就上报报警系统 更或者是发现有时钟回拨之后自动摘除本身节点并报警 代码如下：123456789101112131415161718192021//发生了回拨，此刻时间小于上次发号时间if (timestamp &lt; lastTimestamp) &#123; long offset = lastTimestamp - timestamp; if (offset &lt;= 5) &#123; try &#123; //时间偏差大小小于5ms，则等待两倍时间 wait(offset &lt;&lt; 1);//wait timestamp = timeGen(); if (timestamp &lt; lastTimestamp) &#123; //还是小于，抛异常并上报 throwClockBackwardsEx(timestamp); &#125; &#125; catch (InterruptedException e) &#123; throw e; &#125; &#125; else &#123; //throw throwClockBackwardsEx(timestamp); &#125;&#125; //分配ID 从上线情况来看，在 2017 年闰秒出现那一次出现过部分机器回拨，由于 Leaf-snowflake 的策略保证，成功避免了对业务造成的影响。Leaf 在美团点评公司内部服务包含金融、支付交易、餐饮、外卖、酒店旅游、猫眼电影等众多业务线。目前 Leaf 的性能在 4C8G 的机器上 QPS 能压测到近 5w/s，TP999 1ms，已经能够满足大部分的业务的需求。每天提供亿数量级的调用量，作为公司内部公共的基础技术设施，必须保证高 SLA 和高性能的服务，我们目前还仅仅达到了及格线，还有很多提高的空间。 挖坑 ✓ 熟悉Python / 熟悉C++ / 会用Go ✓ 熟悉Linux / 熟悉Redis / 掌握MySQL / 了解Nginx ✓ 分布式架构设计与开发经验 ✓ 带队管理经验一年多 ✓ 技术支持培训分享经验 ✓ 性能分析与优化经验 ✓ 多款上线项目的运营事务处理经验 ✓ 前后端协同开发经验 猎德之手总结网络延迟/时间轮/事件驱动模型不睡眠条件变量/不卡顿gc负载均衡/中心管理进程无状态多点etcd原理/通用登陆服减少游戏服资源占用/排队服/完善断线重连剥离namecard/ 客户端角度 关系图如下: 周边服务角度 ⚫ 苹果App Store首页多日重磅推荐, 2.5D即时多人战术竞技游戏, 主要负责核心模块的架构设计开发与优化 一般的，在延迟不敏感的情况下，客户端通过连接 gate 来访问 MobileServer。gate 负责代理转发客户端与 game 之间的网络通信数据。由 gate 负责完成对通信数据加密解析、压缩解压操作。 新版本的 gate 基于 TCP 或 KCP 的 mobilerpc 协议对外提供服务。gate 启动后会从 etcd 获取所有集群内所有进程信息，主动连接所有的 game、game manger。 事件驱动模型是咋样的 服务器引擎底层的网络模块多线程的: 底层网络模块一直死循环地在跑通过epoll wait会把已经收到的数据包放到一个无锁队列FQ里 业务线程是python线程: 注册一个15帧供tick游戏逻辑的定时器 会一直死循环的跑 poll 函数 去检查是否有定时器是否到期, 到期了则对应的定时器回调逻辑, 比如上述的15帧的tick逻辑, 比如游戏技能cd结束回调等等(也可以把这种游戏业务本身的那种定时器专门分开放到tick里去做这种游戏业务定时器是否到期的检查, 不过这样的话, 这种游戏业务如技能的定时器就没那么精准了) 去检查是否FQ里有数据来了, 有数据来了则执行对应的回调逻辑一般会执行各种rpc对应的业务逻辑 检查完毕, 就去拿下一个最近的定时器的时间nt, 然后线程就在一个条件变量上睡眠等待nt的时间, 睡眠超时就会立马醒来处理下一个定时器回调 如果期间有rpc网络数据到来会立即notify这个条件变量唤醒py线程来处理rpc 可以看出我们的rpc处理和定时器回调处理都是实时的, 独立于那个15帧的tick定时器, 这样可以减少rpc网络延迟, 定时器精度也比较准确 地图多大承载多少人 地图: 750*350 游戏大厅服承载: 15个进程, 每个进程800人, 则15*800 = 12k, 也就是1万人左右 战斗服承载: 40个进程, 每个进程2场战斗, 每场战斗60人, 则40*2*60 = 5k, 也就是5千人左右 服务器架构的分布式改造 优化 gameMgr还可以主备方案, 那就涉及到主备切换选主的raft了 ⚫ 服务器架构基于etcd的分布式改造, 解决全局单点问题, 重构广播框架, 整体承载提高80% 做了啥就能提高80%? 因为原来在用的引擎版本的gamemanager有单点问题, 所有服务器进程连到gm进程上面算是一个mb游戏服, 现在是所有服务器通过从etcd下注册和监控来管理连接 引入etcd后通过ttl以及watch机制来服务注册与发现 把gm改造成无状态多点, 不再负责entity信息的注册以及各种连接信息的管理, 而只剩下消息转发和广播和开服/关服等无状态功能 与etcd的交互使用的是其V2版本的HTTP API，如上图，为了保证每一步骤的可靠性，都做了异常的重试。关于数据变动更新，结合了增量更新与全局更新的两种方式： 增量更新：etcd的watch基于longpoll的机制，请求到达etcd服务器之后，如果没有任何变动，etcd会挂起该请求，直到有变动数据之后立刻将变动数据返回，本地可以根据收到的变动数据情况对本地缓存做增量的更新即可，待数据更新完再向etcd发起watch请求即可，这样可以减少与etcd的交互次数，降低其负担。 全量更新：正常情况下增量更新就可以满足数据同步的需求了，但是其会有惊群的风险，假如集群中有2000个节点，在很短的时间内有200个节点的数据有更新，如果所有节点都使用增量更新，因为每次watch请求只能获取到一个更改情况，所以整个集群就需要2000*200次请求，短时间内就会对etcd服务造成很大的压力，对节点进程本身也有不小的开销。这种情况下，每个进程就可以采取延迟一定时间之后再一次性更新全量数据即可。 为啥选etcd不选zookeeper? 两个应用实现的目的不同。etcd的目的是一个高可用的 Key/Value 存储系统，主要用于分享配置和服务发现；zookeeper的目的是高有效和可靠的协同工作系统。 接口调用方式不同。etcd是基于HTTP+JSON的API，直接使用curl就可以轻松使用，方便集群中每一个主机访问；zookeeper基于TCP，需要专门的客户端支持。 功能就比较相似了。etcd和zookeeper都是提供了key，value存储服务，集群队列同步服务，观察一个key的数值变化。 部署方式也是差不多：采用集群的方式，可以达到上千节点。只是etcd是go写的，直接编译好二进制文件部署安装即可；zookeeper是java写的，需要依赖于jdk，需要先部署jdk。 实现语言： go 拥有几乎不输于C的效率，特别是go语言本身就是面向多线程，进程通信的语言。在小规模集群中性能非常突出；java，实现代码量要多于go，在小规模集群中性能一般，但是在大规模情况下，使用对多线程的优化后，也和go相差不大。 啥单点问题? MobileServer现有的结构，GameManager进程维护了整个系统的元数据以及一些逻辑层的entity注册等动态数据管理，它的可靠性非常重要，但是偏偏它又是一个单点，随着规模的扩大，风险也越来越大。 广播框架怎么重构的? 放弃原来的有跑马灯广播就实时发给gm再发给gate的做法, 因为一来这样不仅会给gate极大压力, 另外玩家客户端的跑马灯几乎停不下来… 在game准备一个定长队列来保证不爆内存 用两个定时器来控制发送广播, A定时器为20min+随机20min来保证一定会把当前广播队列里的东西发出去, B定时器每隔60秒检查一次是否需要flush广播( 可直接flush广播的时间间隔暂定为30秒, 如果大于此值则检查完毕, 发送广播), 基本满足策划的人少快发, 人多慢发的需求. 跑马灯广播策略 可适当丢弃, 设置一个中等大小的定长队列, game服通过检测与上次的发送时间间隔来决定是否即时发送, 不发则拼成一个大包, 然后每隔一段时间检查是否不繁忙了就随机选一个gm进程发给他, 障眼法: 保证自己的在线好友与工会队员能即时看到即可 gm那边也准备一个定长队列, 以平稳的缓慢的频率发送给各个gate即可 原来承载多少人? 六千多吧, 现在一万多 如何进入战斗的申请战斗流程: 匹配战斗流程: 上报puppet_dict流程: 分配战斗流程图1: 分配战斗流程图2: 断线重连怎么做? 客户端没杀进程: 战斗中/大厅中: KCP / TCP: 客户端重新发起正常的kcp再次加密握手建立新的 kcp_conn, 然后第一个消息就是发送原来的session id以及 重连标记 给服务器, 服务器验证之后就知道客户端是重连了, 然后服务器把 原来的 session 分配给新的 kcp_conn, 然后通知 业务层的 entity 客户端重连了啊(call一下on_entity_reconn方法) KCP可优化之处: kcp的话还可以做得更好, 因为一条kcp链接是用 conn id 标识的而不是用ip/port标识的, 所以心跳超时之后, 可以只是把这条kcp链接的状态设置为disconnected但是不close这条链接, 当重连的时候可以发送原来的conn id而非session id, 这样的话, 原来kcp_conn缓冲区里还没ack的数据其实可以直接重发 客户端杀了进程: 战斗中: 点我 大厅中: 那就重登录咯 战斗中杀进程掉线重连还要排队么不需要 玩家在登上游戏服的时候会给namecard微服务(以下简称nc)记录一个在线状态 玩家主动下线的时候也会让游戏服给nc记录一个下线状态, 如果玩家是杀进程的话 因为客户端跟游戏服有在线心跳, 所以游戏服知道玩家掉线了, 但是这种情况会为玩家保持5分钟的avatar状态, 5分钟过了, 玩家还没有重连则通知nc玩家下线了 如果玩家5分钟内重新打开游戏, 此时先会先去渠道拿到token, 然后连接登陆微服务(loginService, 以下简称ls), ls这边去认证服认证之后拿到aid, 然后先去nc里查询他是否处于在线状态, 因为还在5分钟内, 所以nc会返回玩家还在线, 此时玩家不需要走排队流程, 登陆服直接给aid以及随机一个gate地址给客户端, 然后客户端就去连gate连游戏服了, 可能并不会连到原来的游戏服进程, 此时当走到avatarservice里面的时候，会通知原来的游戏服进程让原来的avatar先下线，然后再在当前连上的游戏服上面创建avatar 为啥不做成一定会连回原来的游戏服呢? 因为玩家与游戏服之间还有个gate, gate是无状态的(只负责转发/压缩解压/加解密), 就算nc里记录了原来连的gate的addr也无法连回原来的gs了 在上述分配战斗流程图中: 当 BattleAllocatorCenter 通知 BattleAllocatorStub 分配战斗 create_remote_battle 成功后 调用BattleAllocatorCenter的create_battle_success 调用BattleAllocatorCenter的notify_avt_battle_create BattleAllocatorCenter的notify_avt_battle_create就会通知大厅服的avt记录last_battle_info包括ip/端口/hostnum啥的, 并且last_battle_info会存盘 当玩家重连的时候玩家把这个last_battle_info信息load出来去相应的战斗服查询是否还有战斗然后重连或者退回大厅即可 登录微服务和排队微服务登录序列图 sequence-diagram: 简化版如下: 请求登录的数据格式(从UniSDK取出的验证json串的base64格式, sauth_json信息)大致如下:123456789101112131415// AuthReq is sauth json structtype AuthReq struct &#123; GameID string `json:"gameid"` LoginChannel string `json:"login_channel"` AppChannel string `json:"app_channel"` Platform string `json:"platform"` SdkUID string `json:"sdkuid"` Udid string `json:"udid"` SessionID string `json:"sessionid"` SdkVersion string `json:"sdk_version"` DeviceID string `json:"deviceid"` Step string `json:"step"` HostID int `json:"hostid"` Raw []byte&#125; 登录回复的数据格式AuthReply大致如下:123456789101112131415161718192021222324252627282930// AuthReply sauth api reply&#123; "code": 200, "subcode": 0, "aid": 127, "username": "aebfgebpoqaabah4@ios.netease.win.163.com", "SN": "1271005435583216289", "message": "eyJ1c2VyIjogeyJ1c2VybmFtZSI6ICJzYWlsb3IuZmVuZ0AxNjMuY29tIiwgImFjY291bnQiOiAic2FpbG9yLmZlbmdAMTYzLmNvbSIsICJyZWFsbmFtZV9zdGF0dXMiOiAxLCAiZXh0X3Rva2VuIjogImQ2NzQ1M2NjYjI5ZjcxYTBkZDhmMjFiOTc3MTcyMTYyIiwgImxvZ2luX3R5cGUiOiAxLCAibG9naW5fdGltZSI6IDE1MzkxMzkyMTJ9fQ==", "events": 0, "status": "ok", "unisdk_login_json": "eyJhaWQiOiIxMjciLCJzZGt1aWQiOiJhZWJmZ2VicG9xYWFiYWg0IiwiYWNjZXNzX3Rva2VuIjoiIiwiZXhwaXJlc19pbiI6IiIsInJlZnJlc2hfdG9rZW4iOiIifQ==", "data": &#123; "app_channel": "app_store" &#125;, "uidcount": 3, "newtm": 1537947598, "req": &#123; // 这个是sauth_json的拷贝 "gameid": "g68", "ip": "127.0.0.1", "login_channel": "netease", "app_channel": "app_store", "platform": "ios", "sdkuid": "aebfgebpoqaabah4", "udid": "DB24AEEB-90CB-4CE7-8331-E4431D154C3C", "sessionid": "1-eyJzIjogImQ2NzQ1M2NjYjI5ZjcxYTBkZDhmMjFiOTc3MTcyMTYyIiwgInQiOiAxfSAg", "sdk_version": "3.3.2", "deviceid": "aeavvs4dtnwhryzq-d", "step": "" &#125;&#125; ⚫ 内部开源的通用登录微服务核心开发者, 被较多项目接入使用, 提供登陆控制、排队、验证等功能 session各种流程搞清楚， 是否可以用别人的 AuthReply_Encrypted 来登陆别人的账号? 根据上面的流程图, 如果真的拿到了别人的AuthReply_Encrypted, 确实是可以登录别人的账号的, 但是玩家客户端和排队微服务是https安全加密连接的, 按道理即使在网络途中抓到包了也是无法从https数据中解密提取出AuthReply_Encrypted的. 所以这一点的安全性由https保证. 即使真的把AuthReply_Encrypted提取出来了, 我们也是可以在游戏大厅服做一下AuthReply.req.ip或者AuthReply.req.deviceid等等的校验, 因为客户端是没有存放解开AuthReply_Encrypted的密钥的, 也就是说客户端本地是无法更改这个AuthReply_Encrypted里的信息的.这个密钥只在游戏大厅服和登陆微服务存放了. 用的啥语言? go语言 登陆控制是啥? 黑白名单, GM名单可提前进入测试游戏 特定渠道的人数控制 激活码 游戏服和客户端怎么拿到登录微服务地址的? 不直接拿, 而是通过 api-gateway 这个网关, 这个api-gateway是sa负责维护的, 基于k8s的service和ingress开发 k8s的service介绍: 因为很多运行的容器存在动态、弹性的变化（容器的重启IP地址会变化），因此便产生了service，其资源为此类POD对象提供一个固定、统一的访问接口及负载均衡能力，并借助DNS系统的服务发现功能，解决客户端发现容器难得问题 k8s的ingress介绍: 作为HTTP(S)负载均衡器 参考这里 外网和内网用户访问的api-gateway地址不同 排队部分, 怎么判断afk? 谁来配置流速和限额? 游戏服和排队微服务直连么?游戏服是怎么拿到排队微服务的IP的? 有一个叫active的redis哈希表, 会存上次玩家的 query_pos_info 的时间戳ts, 每隔一段时间就去这个哈希表hscan拿100个出来看看 now_ts -ts是不是已经大于afk阈值了 游戏服和排队微服务不直连, 是游戏服通过不断的检测redis中的当前在线人数来配置流速和限额的(比如配置的最大在线为8k人, 当前在线2k人, 那发送给排队服的限额就是6k), 游戏服集群这边的avatarService通过api-gateway这个网关的后缀加上/queue来访问排队微服务 排队怎么实现的? 通过redis的zset来存储入队时间和用户id, 然后给玩家一个ticket, 然后按照配置好的出队速度以及名额来从队列中抽取玩家发放游戏服许可证 玩家需要定时刷新ticket, 不然会被放入afk队列 timer ⚫ 开发定时器库替换网易服务器引擎自带的定时器库, 并暴露给Python调用, 性能提升40% 用python的原生官方Python API（应用程序编程接口）来做, 因为从火焰图看到boost.python和pybind11本身也有消耗 当时拿到的引擎版本的定时器库是基于最小堆的 网络优化 kcp的消息模式和流模式回顾一下 KCP 协议 1234567891011120 4 5 6 8 (BYTE)+---------------+---+---+-------+| conv |cmd|frg| wnd |+---------------+---+---+-------+ 8| ts | sn |+---------------+---------------+ 16| una | len |+---------------+---------------+ 24| || DATA (optional) || |+-------------------------------+ frg : 分片，用户数据可能会被分成多个 KCP 包，发送出去 sn : 序列号 len : 数据长度 (DATA 的长度) data : 用户数据 消息模式特点: 在消息模式下，每个 KCP 包最多包含一个上层应用的消息。 消息模式减少了上层应用从流中拆解出消息的麻烦，但是它对网络的利用率较低。Payload(有效载荷) 少，KCP 头占比过大。 假定一个场景，在 IM 中，A 向 B 发送了一个消息，这个消息可能是文本消息，也可能是图片消息。那么应用程序发送的数据是有逻辑的 消息 的概念的。 kCP 协议提供了一种能力把不同的 消息 (应用程序) 划分在不同的 KCP 包中。 KCP 定义 MSS 的默认大小为 1400 bytes， MSS (maximum segment size) 表示最大段大小，它本身是 TCP 中的概念，表示包含 TCP header，整个数据包的最大大小。在 KCP 协议中，概念类似，表示包含 KCP header 在内，整个 KCP 包的最大大小。 超过 MSS 的数据将会被拆分到成多个 KCP 包。根据是否拆包将会分成 2 种情况。 不拆包 3 条消息 Msg1 , Msg2 , Msg3 分别包含在 sn 为 90、91、92 的 KCP 包中 拆包 假定这个消息是个图片消息，比较大，大小为 3252 bytes Msg 被拆成了 3 部分，包含在 3 个 KCP 包中。 注意 , frg 的序号是从大到小的，一直到 0 为止。这样接收端收到 KCP 包时，只有拿到 frg 为 0 的包，才会进行组装并交付给上层应用程序。由于 frg 在 header 中占 1 个字节，也就是最大能支持 （1400 – 24） * 256 / 1024 = 344kB 的消息 流模式 在流模式下，KCP 试图让每个 KCP 包尽可能装满。一个 KCP 包中可能包含多个 消息 。在上图中， Msg1 、 Msg2 、 Msg3 的一部分被包含在 sn 为 234 的 KCP 包中。 上层应用需要自己来判断每个消息的边界。 ⚫ 网络优化, 平均降低延迟50%, 增强其抗网络抖动能力, 使其在70ms的RTT延迟30%丢包率的环境下依旧可以流畅运行且正常操作 怎么降低延迟的? 如何抗抖动? 主要是冗余策略降低丢包率带来的影响 FEC, 详见 FEC介绍 冗余策略 在原有rudp基础上加入冗余包, 并且根据一段时间内检测的srtt与丢包率来动态冗余(100ms以下本身表现较好不用冗余了), head部分指的是除了len与rdc_len之外的所有包头内容，len指的是body的长度，rdc_len指的是rdc_body的长度。其中rdc_body是由一个个rdc_data组成的，每个rdc_data又包含了rdc_sn，data_len与data三个部分。这样设计下，可以动态的调节携带冗余的数量。 如何设计成动态冗余的呢? 改造包头加入rdc_len和rdc_body, rdc_len指的是rdc_body的长度。 其中 rdc_body 是由一个个rdc_data组成的, 而每个rdc_data又包含了三个部分: rdc_sn data_len data 可总结如下: rdc_body = rdc_data_1 rdc_sn_1 data_len_1 data_1 rdc_data_2 rdc_sn_2 data_len_2 data_2 … rdc_data_n rdc_sn_n data_len_n data_n 为什么要设置冗余次数？因为当一个包被当做冗余携带N次都丢失的话，证明游戏已经到了一个比较卡的程度, 可以通过限制冗余次数来减少无谓的流量消耗, 且KCP本身有快速重传的机制， 精简包头(32位改16位), 并合批数据且压缩(方便冗余更多) 降低25%的包头消耗(24字节-&gt;18) ack机制优化 UNA: 包头中的UNA表示此编号前所有包已收到 CMD为ack类型的cmd时: 普通ack机制 IKCP_CMD_ACK: 每个ack包只表示收到了某个编号的包, 会遍历acklist然后其中的每个ack都对应发一个ack包. sn为收到了的包的序号,(注意, 这一点和tcp是不同的, tcp包头有专门的ack_sn, 当ack标志位为1时ack_sn就生效.) dupack机制 IKCP_CMD_DUPACK: 当acklist数组里需要发送的ack小于等于3时, 用包头中的len/rdc_len/sn来表示收到了的包的序号 IKCP_CMD_DUPACKS(可以理解为冗余ack机制): 当acklist数组里需要发送的ack大于3时, body里会包含合并acklist的所有ack以及之前发过的ack, 直到包大小到达mss. 什么时候清理acklist呢? 每次flush都会把acklist的所有ack发完, 然后就会把kcp-&gt;ackcount置为0, 然后下回对方发push数据包过来又从头开始覆盖acklist往里面加入ack, 循环利用, 这就达到了清理acklist的效果. 不遵守公平退让规则, KCP正常模式同TCP一样使用公平退让法则，即发送窗口大小由：发送缓存大小、接收端剩余接收缓存大小、丢包退让(拥塞避免)及慢启动这四要素决定。但传送及时性要求很高的小数据时，可选择通过配置跳过后两步，仅用前两项来控制发送频率。以牺牲部分公平性及带宽利用率之代价，换取了开着BT都能流畅传输的效果。 解决服务器间歇性卡顿问题 ⚫ 解决服务器间歇性卡顿问题, 各种场景下的卡顿频次平均降低90% 参考 https://www.cnblogs.com/xybaby/p/7491656.html#_label_9 循环引用代码检查报警工具制作(通过在tick插桩代码gc.set_debug和他的debug_flag参数), 类似代码如下: 123456789101112131415161718192021222324252627282930# -*- coding: utf-8 -*-import gc, timeclass OBJ(object): def __init__(self): self.attr_a = None self.attr_b = Nonedef show_cycle_reference(): a, b = OBJ(), OBJ() a.attr_b = 1 b.attr_a = adef show_cycle_reference2(): a, b = OBJ(), OBJ() a.attr_b = b b.attr_a = aif __name__ == '__main__': gc.disable() # 这里是否disable事实上无所谓 gc.set_debug(gc.DEBUG_COLLECTABLE | gc.DEBUG_OBJECTS) for _ in xrange(1): show_cycle_reference() print "first gc.collect() result:" gc.collect() time.sleep(1) print "-------------------" show_cycle_reference2() print "second gc.collect() result:" gc.collect() 输出如下： 1234567first gc.collect() result:-------------------second gc.collect() result:gc: collectable &lt;OBJ 03231A10&gt;gc: collectable &lt;OBJ 032319F0&gt;gc: collectable &lt;dict 03234DB0&gt;gc: collectable &lt;dict 03227E40&gt; 注意：只有当对象是unreachable且collectable的时候，在collect的时候才会被输出，也就是说，如果是reachable，比如被global作用域的变量引用，那么也是不会输出的。通过上面的输出，我们已经知道OBJ类的实例存在循环引用， 关闭python的gc 手动垃圾回收, 游戏结束的时候回收, 如果本进程还有其他战斗则让下一场战斗分配到其他进程上 调高垃圾回收阈值, 例如一个游戏可能在某个时刻产生大量的子弹对象(假如是2000个). 而此时Python的垃圾回收的threshold0为1000. 则一次垃圾回收会被触发, 但这2000个子弹对象并不需要被回收. 如果此时 Python的垃圾回收的threshold0为10000, 则不会触发垃圾回收. 若干秒后, 这些子弹命中目标被删除, 内存被引用计数机制 自动释放, 一次(可能很耗时的)垃圾回收被完全的避免了.调高阈值的方法能在一定程度上避免内存溢出的问题(但不能完全避免), 同时可能减少可观的垃圾回收开销. 根据具体项目 的不同, 甚至是程序输入的不同, 合适的阈值也不同. 因此需要反复测试找到一个合适的阈值, 这也算调高阈值这种手段 的一个缺点. 代码规范尽量自己解引用(=None即可) 代码规范尽量用弱引用 FlyNet服务器引擎 ⚫ 支持TCP/UDP/可靠UDP的多线程网络库 网络库用的什么网络模型? reactor, epoll多线程, 水平触发, 这种方案的特点是one loop per thread，有一个main Reactor负责accept(2)连接，然后把连接挂在某个sub Reactor中（muduo采用round-robin的方式来选择sub Reactor），这样该连接的所有操作都在那个sub Reactor所处的线程中完成。多个连接可能被分派到多个线程中，以充分利用CPU。 非阻塞的connect/accept有啥用 阻塞和非阻塞的send和sendto和recv和recvfrom区别? reuseaddr和reuseport timewait和closewait太多咋办 udp怎么和epoll配合? 参考 https://cloud.tencent.com/developer/article/1004555 服务器bind一个listen_fd然后放到epoll中监听可读事件 epoll_wait返回时，如果epoll_wait返回的事件fd是listen_fd，调用recvfrom接收client第一个UDP包并根据recvfrom返回的client地址, 创建一个新的socket(new_fd)与之对应，设置new_fd为REUSEADDR和REUSEPORT、同时bind本地地址local_addr，然后connect上recvfrom返回的client地址, 将新创建的new_fd加入到epoll中并监听其可读等事件 client要使用固定的ip和端口和server端通信，也就是client需要bind本地local address。如果client没有bind本地local address，那么在发送UDP数据包的时候，可能是不同的Port了，这样如果server 端的new_fd connect的是client的Port_CA端口，那么当Client的Port_CB端口的UDP数据包来到server时，内核不会投递到new_fd，相反是投递到listen_fd。 ⚫ 提供RPC、二进制序列化协议传输、压缩协议包、加解密数据等功能 rpc的原理大概是先把各个函数名扫描出来, 然后通过msgpack打包发送, 通过python的反射找到相关函数执行 加密算法是 RC4(对称加密)，在通信开始会有确定rc4加密种子的过程，这个过程有点类似于https握手, 客户端存了一份服务器的非对称加密(rsa)的公钥用来协商加密种子, 协商好了之后, 开始用这个加密种子来走rc4加密通信。 压缩解压通过 zlib 完成， ⚫ 支持敏捷开发, 暴露接口到脚本层, 涉及到Python热更新、日志、定时器、数据持久化等方面 python热更: python原生reload函数的问题? python本身提供了reload函数来进行模块的热更，但是只会对reload之后创建的对象生效，旧对象所运行的依然是旧代码 热更流程: 1. 准备新的代码文件，直接替换进程集群的原有代码文件（外网环境一般通过 luna 完成） 2. 执行 AdminClient.py 让 GameManager 通知集群内所有 game 进程载入新代码，执行热更新流程`python -m %MOBILENAME%.tools.AdminClient --configfile %CONFIG% --runscript %SCRIPT% --script_args AvatarCenter` 3. 所有的 game 进程收到热更新命令之后，执行 reload.py 热更的具体实现: 1. 协议在 PEP320 中被提出，有两个主要的组成概念：finder 和 loader 。finder 的任务是确定能否根据已知的策略找到该名称的模块。同时实现了 finder 和 loader 接口的对象叫做 importer 2. 我们根据此协议实现一个 包含 finder 和 loader 的 importer 模块, 我们在这个模块中主要是在finder中实现对func与property的替换, 走的是替换func_code的路子 3. 往 sys.meta_path 添加这个 finder 注册 meta_hook(import hook的一种, 你可以在这里重载对 sys.path、frozen module 甚至内置 module 的处理) 4. 从sys.modules中把想要热更的module先pop出去 5. 执行__import__(wanna_reload_module_name) 来导入相应想reload的模块 替换func_code的路子: 如果是property类的attr, 则直接替换 如果是func/method类型的, 则替换func_code 如果有装饰器则注意递归替换func里的closure里的cell里的cell_content(因为用了装饰器的话, function object里的func_closure里还有 function object) 增量热更/全量热更: 增量热更通过记录改动了的文件来做热更, 但是容易产生from…import…的那些没有更到 定义回调接口: after_reload/on_reload等 日志: 游戏主线程将日志数据保存在据缓存队列中，由专门的日志线程负责执行写入硬盘，保持主线程不阻塞玩法逻辑的执行。在游戏进程发生crash的时候，保存在内存中的数据可能来不及写入硬盘而丢失。 默认的 Linux 环境下日志会写往标准输出，由 SA 负责重定向到特定的日志文件 双缓冲技术，基本思路是准备两块缓冲：A与B，前端负责往buffer A中填数据（日志消息），后端线程等待在条件变量上负责将buffer B中的数据写入文件。当buffer A写满之后，交换A与B，然后notify这个条件变量唤醒后端将buffer A中的数据写入文件，而前端负责往buffer B中填入新的日志文件。如此往复。 用两个buffer的好处是在新建日志消息的时候不必等待磁盘文件操作，也避免每条消息都触发（唤醒）了后端日志线程。换言之，前端不是将一条条消息分别传送给后端，而是将多个日志消息拼成一个大的buffer传送给后端，相当于批处理，减少了线程唤醒的频率，降低了开销。另外，为了及时将消息写入文件，即使前端的buffer A未写满，日志库也会每三秒(条件变量的超时时间为3秒)执行一次上述交换写入操作。 如果前端拼命写消息日志, 超过了后端的处理输出能力, 则直接丢弃 数据持久化: game、game manager 之类需要操作数据库的进程并不直接连接数据库，而是 db manager 提供数据库读写的服务，供其他进程调用。线上数据库出现机器故障、换主时，game 进程的代码无需做错误处理，db mangaer 会负责自动重试。 准备好一个线上事故的处理事例 准备好一个难忘的优化建议从游戏卡顿说起, 后端卡顿优化: 高性能时间轮定时器也算是 py垃圾回收卡顿优化 rudp优化, dupack优化, 动态冗余, 选择性重传, 不丢包退让, 精简包头并加入rdcLen 前端优化: 状态缓冲器 平滑插值 前端预表现, 加入前摇后摇机制, 技能先砍, 然后延迟补偿, 由服务器控制飘血 预先计算技能move路线, 计算静态碰撞, 计算落点与曲线, 动态碰撞通过特效掩盖 个人开源-realtime-server服务器框架 ⚫ 目前在GitHub已有315个star ⚫ 为著名开源项目kcp快速可靠传输协议贡献了通用的单头文件的会话实现, 以及移动弱网的针对性改造, 达到了以20%流量换取代价换取35%的延迟降低效果 ⚫ 为著名开源项目muduo网络库贡献了添加了UDP扩展支持 muduo的难点详解 muduo为什么采用epoll水平触发? 与poll兼容 LT模式不会发生漏掉事件的BUG，但POLLOUT事件不能一开始就关注，否则会出现busy loop，而应该在write无法完全写入内核缓冲区的时候才关注，将未写入内核缓冲区的数据添加到应用层output buffer，直到应用层output buffer写完，停止关注POLLOUT事件。 读写的时候不必等候EAGAIN，可以节省系统调用次数，降低延迟。（注：如果用ET模式，读的时候读到EAGAIN,写的时候直到output buffer写完或者EAGAIN）所以可见LT模式（可以尽可能多读减少系统调用）效率不一定比ET要低（多了一次系统调用，检测EAGAIN） muduo的buffer怎么做的, 看muduo书吧 123456/// +-------------------+------------------+------------------+/// | prependable bytes | readable bytes | writable bytes |/// | | (CONTENT) | |/// +-------------------+------------------+------------------+/// | | | |/// 0 &lt;= readerIndex &lt;= writerIndex &lt;= size 在非阻塞网络编程中，如何设计并使用缓冲区？一方面希望减少系统调用，一次读取的数据越多越划算；另一方面希望减少内存的占用。这两方面似乎是矛盾的，假设C10K ，每个连接一建立就分配50KB 的内存的话，那么将占用1GB 内存，但是大多数的连接并不需要这么多内存。muduo 巧妙的使用了readv() 结合栈上空间巧妙的解决了这个问题。在栈上准备一个64KB的extrabuf , 然后利用readv() 来读取数据，iovec有两块，第一块是指向muduo Buffer （为每个连接准备1KB的buf）中的writeable 字节，另一块是指向extrabuf。这样如果读入的数据不多，直接读到内置的buf；如果长度超过内置buf 的大小，就会读到栈上的extrabuf 中，然后程序再把extrabuf 里的数据append() 到 buf 中。 为啥extrabuf是64KB? 因为平时一般都阻塞在poll上, 来了数据马上就处理的话也就几k而已, 即使是千兆网卡100MB/s, 500微秒也就是500/1000000秒也就是0.5毫秒, 100MB/s * (500/1000000) = 50000B, 也就是说0.5毫秒就是50000字节的数据, 64k已经足够容纳千兆网在0.5毫秒全速收到的数据了. 一般来说, 一次readv就能读完这次过来的数据了, 如果用完了writeable和extrabuf还是不够, 那就再readv一次嘛 muduo每个连接都有一个buffer么?初始大小多大?缩扩容的时机是? 是的, 每个连接都有一个send的buffer和read的buffer, 都是初始大小1024字节(即1KB), 一般不缩容, 只扩容1234567891011121314151617181920void makeSpace(size_t len)&#123; if (writableBytes() + prependableBytes() &lt; len + kCheapPrepend) &#123; // FIXME: move readable data buffer_.resize(writerIndex_+len); &#125; else &#123; // move readable data to the front, make space inside buffer assert(kCheapPrepend &lt; readerIndex_); size_t readable = readableBytes(); std::copy(begin()+readerIndex_, begin()+writerIndex_, begin()+kCheapPrepend); readerIndex_ = kCheapPrepend; writerIndex_ = readerIndex_ + readable; assert(readable == readableBytes()); &#125;&#125; 有啥可以问他的技术: 技术栈 偏向有招聘有哪些经验的? 视频?音频?电商开发?(问完之后可以回去好好了解相关领域) 团队成员数量与年龄组成 人事: 职级体系/晋升通道 公司架构体系, 有哪些事业部 警告 尽量把熟悉的东西说多一点， 背下来， 说全一点， 填满整个面试时间， 他就没有那么多问题要问， 问了越多， 言多必失 不要说自己之前的工作内容跟他们目前的工作内容不相关， 不要找借口啥的， 反而会加深他岗位不匹配的印象 不要问面得怎么样， 他不会告诉你的 面筋比狗 大数相加 全排列 mysql语句 orderby/limit 严格递增的分布式id生成器怎么做 美团这篇文章的”Leaf-segment数据库方案” 异地多活 io线程和业务线程要分开吗? 是的 合并字符串中的连续空格为一个如&quot;^he^^^^ll^o^^&quot; 得到 &quot;^he^ll^o^&quot; 虾toc_one python实现 继承 字典 linux 内存布局(文初堆栈)Linux虚拟地址空间如何分布 动态库放在哪儿 通过进程找他正在读写的文件 可能是 /proc? proc里有个fd 通过文件找进程 lsof什么参数 Linux常用运维命令(netstat和lsof)笔记整理(二) 内核态和用户态区别 快排原址排序的话要用几个index来做? 参考: 排序算法三之谈一谈快排优化和二分查找 两个, 一个index指向最后一个比pivot小的元素, 初始化为-1, 一个指向将要遍历的元素 二叉树 找最近公共父节点: 参考: https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-tree/solution/236-er-cha-shu-de-zui-jin-gong-gong-zu-xian-jian-j/ 两个节点 p,q 分为两种情况：1) p 和 q 在相同子树中2) p 和 q 在不同子树中从根节点遍历，递归向左右子树查询节点信息递归终止条件：如果当前节点为空或等于 p 或 q，则返回当前节点递归遍历左右子树，因为是递归，使用函数后可认为左右子树已经算出结果，这句话要记住，道出了递归的精髓,如果左右子树查到节点都不为空，则表明 p 和 q 分别在左右子树中，因此，当前节点即为最近公共祖先；如果左右子树其中一个不为空，则返回非空节点。12345678910class Solution &#123;public: TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) &#123; if (!root || root == p || root == q) return root; TreeNode *left = lowestCommonAncestor(root-&gt;left, p, q); TreeNode *right = lowestCommonAncestor(root-&gt;right, p, q); if (left &amp;&amp; right) return root; return left ? left : right; &#125;&#125;; 判断是否为一个二叉搜索树 记住一点, 其中序遍历是一个有序数组 判断一棵树是不是BST树方法：使用中序遍历 1) 对树进行中序遍历，将结果保存在temp数组中。 2) 检测temp数组中元素是否为升序排列。如果是，则这棵树为BST. redis集群 怎么扩缩容 客户端怎么找到某个键所在的节点 持久化有哪些 rewrite_aof怎么替换原来的aof文件的?AOF重写的实现 send和sendto是阻塞的么?阻塞到底意味着什么?阻塞和非阻塞的send和recv和sendto和recvfrom send返回值意味着啥?意味着对方真的就收到了多少数据么? infra_one lsm-tree pending_fin muduo的buffer怎么做的, 看muduo书吧 也可参考这里 跳表增加数据的时候, 索引怎么变化? 跳表增加数据时索引怎么变化 zset为什么不用红黑树, 用跳表? 答: 为什么zset用跳表不用红黑树 cas的aba问题: 给数据加版本号 乐观锁怎么实现: 加版本号 cas cap的c是哪种一致性? 线性一致性是啥? c是强一致性 线性一致性是怎么做到的? raft协议怎么做到? 答: etcd线性一致性读 同构问题, leetcode-isomorphic-strings infra_two 十个并发请求写日志，传入的两个参数（参数1时间戳， 参数2内容）， 然后按照传入的参数1的时间戳排序写日志（提示：hbase就有这个特性，全局有序的存储？ 不用严格有序， 阶段有序 pending_fini rpc框架原理,msgpack是啥,出异常怎么办？被调用方出了问题,调用方怎么办？ http://www.voycn.com/article/fenbushifuwurpcfenbushixiaoxiduiliemqmianshitijingxuan 分布式事务: 分布式事务 登录session的实现, 登录微服务/排队微服务各种流程搞清楚， aid是干嘛的 登录微服务和排队微服务 git rebase原理/git reset是啥意思？原理 git rebase: rebase git reset: reset 分布式全局递增id: id生成器如何实现全局递增 做登录的时候OAuth2么？OAuth是啥？ 参考: https://www.ruanyifeng.com/blog/2019/04/oauth-grant-types.html 参考: http://www.ruanyifeng.com/blog/2019/04/github-oauth.html OAuth 2.0 规定了四种获得令牌的流程。你可以选择最适合自己的那一种，向第三方应用颁发令牌。下面就是这四种授权方式。 授权码（authorization-code） 隐藏式（implicit） 密码式（password）： 客户端凭证（client credentials）注意，不管哪一种授权方式，第三方应用申请令牌之前，都必须先到系统备案，说明自己的身份，然后会拿到两个身份识别码：客户端 ID（client ID）和客户端密钥（client secret）。这是为了防止令牌被滥用，没有备案过的第三方应用，是不会拿到令牌的。 举个OAuth授权码方式的例子,授权码（authorization code）方式，指的是第三方应用先申请一个授权码，然后再用该码获取令牌。这种方式是最常用的流程，安全性也最高，它适用于那些有后端的 Web 应用。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。 流程如下: 1. A 网站让用户跳转到 GitHub。 2. GitHub 要求用户登录，然后询问”A 网站要求获得 xx 权限，你是否同意？” 3. 用户同意，GitHub 就会重定向回 A 网站，同时发回一个授权码。 4. A 网站使用授权码，向 GitHub 请求令牌。 5. GitHub 返回令牌. 6. A 网站使用令牌，向 GitHub 请求用户数据。 算法与数据结构 推荐参考本博客总结的 algo_newbie 推荐参考本博客总结的 算法企业真题实战练习 A星算法 pending_fin dijkstra算法 pending_fin 动态规划与贪心有什么区别: 贪心着眼现实当下，动规谨记历史进程。 动态规划希望复用子问题的解，最好被反复依赖。其本质还是穷举，所以当前并不知道哪个子问题的解会构成最终最优解。但知道这个子问题可能会被反复计算，所以把结果缓存起来。整个过程是树状的搜索过程。 贪心希望每次都能排除一堆子问题。它不需要复用子问题的解，当前最优解从子问题最优解即可得出。整个过程是线性的推导过程。 如何判断一个图是否有环 桶排序/线段树/统计树/排序树 海量问题: 可参考 https://juejin.im/entry/6844903519640616967 10个1G数据, 内存200MB, 如何去重 pending_fin 设计模式 单例模式 适配器模式: 将一个类的接口转换成客户希望的另外一个接口。适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。主要解决的问题：主要解决在软件系统中，常常要将一些”现存的对象”放到新的环境中，而新环境要求的接口是现对象不能满足的。 工厂模式 观察者模式 享元模式: 池的思想 各个语言之间的BENCHMARK对比结论: luajit比lua快10倍 …比cpython快60倍, 和c慢一点也差不太多 https://github.com/DNS/benchmark-language 语言 耗时 LUA 5.3.4 6.87s total LUAC 5.3.4 6.84s total LuaJIT 2.0.5 0.67s total C (MSVC 18, VS 2013) 0.56s total C (GCC 7.2.0) 0.67s total C (CLANG LLVM 6.0.0) 0.56s total C (CYGWIN GCC 10.2.0) 0.68s total C (CYGWIN CLANG 8.0.1) 0.59s total C (MINGW GCC 10.2.0) 0.67s total C (MINGW CLANG 8.0.1) 0.58s total C (Embarcadero C++ 6.60 for Win32) 1.01s total Java JRE 1.8.0_20 0.65s total Perl 5.26.1 26.89s total Javascript (Node.js 4.4.6) 2.49s total Python 3.7.1 40.17s total C# .NET CLR (CSC 12) 0.67s total C# Mono 5.18.0 1.06s total C# Mono 5.18.0 (Interpreter) 26.38s total Ruby 2.5.1 12.97s total R 3.5.1 15.56s total Python mro问题 怎么实现一个协程库? mock是啥: https://zhuanlan.zhihu.com/p/30380243 有GIL那py的多线程还有用吗?在python2.x里，GIL的释放逻辑是当前线程遇见IO操作或者ticks计数达到100（ticks可以看作是python自身的一个计数器，专门做用于GIL，每次释放后归零，这个计数可以通过 sys.setcheckinterval 来调整），进行释放。 而每次释放GIL锁，线程进行锁竞争、切换线程，会消耗资源。并且由于GIL锁存在，python里一个进程永远只能同时执行一个线程(拿到GIL的线程才能执行)，这就是为什么在多核CPU上，python的多线程效率并不高。 那么是不是python的多线程就完全没用了呢？ 在这里我们进行分类讨论： CPU密集型代码(各种循环处理、计数等等)，在这种情况下，ticks计数很快就会达到阈值，然后触发GIL的释放与再竞争（多个线程来回切换当然是需要消耗资源的），所以python下的多线程对CPU密集型代码并不友好。(而在python3.x中，GIL不使用ticks计数，改为使用计时器（执行时间达到阈值后，当前线程释放GIL），这样对CPU密集型程序更加友好，但依然没有解决GIL导致的同一时间只能执行一个线程的问题，所以效率依然不尽如人意。) IO密集型代码(文件处理、网络爬虫等)，多线程能够有效提升效率(单线程下有IO操作会进行IO等待，造成不必要的时间浪费，而开启多线程能在线程A等待时，自动切换到线程B，可以不浪费CPU的资源，从而能提升程序执行效率)。所以python的多线程对IO密集型代码比较友好。 在python程序中调用cpp的库创建的线程是否受制于GIL?首先要理解什么是GIL.Python 的多线程是真的多线程，只不过在任意时刻，它们中只有一个线程能够取得 GIL 从而被允许执行 Python 代码。其它线程要么等着，要么干别的和 Python 无关的事情（比如等待系统 I/O，或者算点什么东西）。 那如果是通过CPP扩展创建出来的线程，可以摆脱这个限制么？很简单，不访问 Python 的数据和方法，就和 GIL 没任何关系。如果需要访问 Python，还是需要先取得 GIL. GIL 是为了保护 Python 数据不被并发访问破坏，所以当你不访问 Python 的数据的时候自然就可以释放（或者不取得）GIL。反过来，如果需要访问 Python 的数据，就一定要取得 GIL 再访问。PyObject 等不是线程安全的。多线程访问任何非线程安全的数据都需要先取得对应的锁。Python 所有的 PyObject 什么的都共享一个锁，它就叫 GIL。 使用objgraph解决内存泄漏问题对于 python 这种支持垃圾回收的语言来说，怎么还会有内存泄露？ 概括来说，有以下三种原因： 所用到的用 C 语言开发的底层模块中出现了内存泄露。代码中用到了全局的 list、 dict 或其它容器，不停的往这些容器中插入对象，而忘记了在使用完之后进行删除回收代码中有“引用循环”，并且被循环引用的对象定义了del方法，就会发生内存泄露。 为什么循环引用的对象定义了del方法后collect就不起作用了呢？ gc模块最常使用的方法就是gc.collect()方法，使用collect方法对循环引用的对象进行垃圾回收。如果我们在类中重载了del方法。del方法定义了在用del语句删除对象时除了释放内存空间以外的操作。一般而言，在使用了del语句的时候解释器首先会看要删除对象的引用计数，如果为0，那么就释放内存并执行del方法。在这里，首先del语句出现时本身引用计数就不为0（因为有循环引用的存在），所以解释器不释放内存；再者，执行collect方法时应该会清除循环引用所产生的无效引用计数从而达到del的目的，对于这两个循环引用对象而言，python无法判断调用它们的del方法时会不会要用到对方那个对象，比如在进行b.del()时可能会用到b._a也就是a，如果在那之前a已经被释放，那么就彻底GG了。为了避免这种情况，collect方法默认不对重载了del方法的循环引用对象进行回收，而它们俩的状态也会从unreachable转变为uncollectable。由于是uncollectable的，自然就不会被collect处理，所以就进入了garbage列表。 内存泄露的诊断思路无论是哪种方式的内存泄露，最终表现的形式都是某些 python 对象在不停的增长；因此，首先是要找到这些异常的对象。 诊断步骤:用到的工具： gc 模块和 objgraph 模块 gc模块 是Python的垃圾收集器模块，gc使用标记清除算法回收垃圾 objgraph 是一个用于诊断内存问题的工具 objgraph的实现调用了gc的这几个函数：gc.get_objects(), gc.get_referents(), gc.get_referers()，然后构造出对象之间的引用关系。objgraph的代码和文档都写得比较好，建议一读。 下面先介绍几个十分实用的API1def count(typename) 返回该类型对象的数目，其实就是通过gc.get_objects()拿到所用的对象，然后统计指定类型的数目。1def by_type(typename) 返回该类型的对象列表。线上项目，可以用这个函数很方便找到一个单例对象1def show_most_common_types(limits = 10) 打印实例最多的前N（limits）个对象，这个函数非常有用。在《Python内存优化》一文中也提到，该函数能发现可以用slots进行内存优化的对象1def show_growth() 统计自上次调用以来增加得最多的对象，这个函数非常有利于发现潜在的内存泄露。函数内部调用了gc.collect()，因此即使有循环引用也不会对判断造成影响。 步骤:1、 在服务程序的循环逻辑中，选择出一个诊断点2、 在诊断点，插入如下诊断语句 例如:12345678import gcimport objgraph### 强制进行垃圾回收 gc.collect() ### 打印出对象数目最多的 50 个类型信息 objgraph.show_most_common_types(limit=50) 如何解决python进程cpu的100%问题监控系统报警某服务进程出现 CPU 100% 情况，该业务进程不响应任何请求，无日志输出，进程 IO 和内存占用都正常。 但经过一番排查，原来是代码 BUG 导致了死循环。 具体查证过程如下： top 查看 CPU 和内存占用12345678# top -bn1p 6325Tasks: 608 total, 9 running, 596 sleeping, 0 stopped, 3 zombie%Cpu(s): 20.5 us, 6.3 sy, 0.0 ni, 72.3 id, 0.5 wa, 0.0 hi, 0.5 si, 0.0 stKiB Mem: 65983744 total, 65372892 used, 610852 free, 574684 buffersKiB Swap: 4194300 total, 0 used, 4194300 free. 42109148 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 6325 cc 20 0 4840544 628972 69656 R 100.0 1.0 927:41.47 python2.7 显示进程 CPU 100%，内存占用正常。 strace 查看系统调用1# strace -p 6325 结果一直卡住，无任何输出，说明进程没有进行任何系统调用。 ltrace 查看库函数调用123456789101112131415# ltrace -cp 6325^C% time seconds usecs/call calls function------ ----------- ----------- --------- -------------------- 71.84 10.207067 81 124827 memset 23.87 3.392006 81 41608 strchr 2.15 0.304948 82 3708 sem_wait 2.14 0.303884 81 3708 sem_post------ ----------- ----------- --------- --------------------100.00 14.207905 173851 total# ltrace -p 6325[pid 6325] memset(0x7f44e98309a8, &apos;\0&apos;, 8) = 0x7f44e98309a8[pid 6325] strchr(&quot;O|O:enumerate&quot;, &apos;:&apos;) = &quot;:enumerate&quot;[pid 6325] memset(0x7f44e98309a8, &apos;\0&apos;, 8) = 0x7f44e98309a8[pid 6325] strchr(&quot;O|O:enumerate&quot;, &apos;:&apos;) = &quot;:enumerate&quot;... 结果显示进程一直在调用memset和strchr，极有可能是遇到死循环了，而且死循环里面重复执行enumerate函数。 gcore生成coredump文件为了避免gdb attach进程造成的其他影响（比如可能出现进程异常退出，死锁突然恢复，影响线上服务等），最好将进程生成一个 coredump 文件，然后再慢慢分析。 123# gcore 6325# ls -lsh core.63252.7G -rw-r--r-- 1 root root 2.7G 4月 14 00:56 core.6325 生成完 coredump 文件，如果出问题进程无法从线上摘除，则可以直接停掉该进程了。 gdb 分析 coredump 文件1234567# gdb python core.6325GNU gdb (Debian 7.7.1+dfsg-5) 7.7.1...Reading symbols from python...Reading symbols from /usr/lib/debug/usr/bin/python2.7...done.done....Core was generated by `/xxx/service_xx/venv/bin/python2.7&apos;. 可以继续分析： 12345678(gdb) info threads Id Target Id Frame 25 Thread 0x7f8361666700 (LWP 6325) _PyCode_CheckLineNumber (bounds=&lt;optimized out&gt;, lasti=&lt;optimized out&gt;, co=&lt;optimized out&gt;) at ../Objects/codeobject.c:565 24 Thread 0x7f82f056a700 (LWP 7439) pthread_cond_timedwait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_timedwait.S:238 ...(省略)... 3 Thread 0x7f83161d4700 (LWP 6411) sem_wait () at ../nptl/sysdeps/unix/sysv/linux/x86_64/sem_wait.S:85 2 Thread 0x7f83159d3700 (LWP 6410) sem_wait () at ../nptl/sysdeps/unix/sysv/linux/x86_64/sem_wait.S:85* 1 Thread 0x7f83151d2700 (LWP 6405) sem_wait () at ../nptl/sysdeps/unix/sysv/linux/x86_64/sem_wait.S:85 使用info threads查看当前进程的线程列表，发现大部分都在wait信号，只有 25 号线程在做其他事情，切换到 25 号线程，分析调用栈： 12345678(gdb) thread 25[Switching to thread 25 (Thread 0x7f8361666700 (LWP 6325))]#0 _PyCode_CheckLineNumber (bounds=&lt;optimized out&gt;, lasti=&lt;optimized out&gt;, co=&lt;optimized out&gt;) at ../Objects/codeobject.c:565565 ../Objects/codeobject.c: 没有那个文件或目录.(gdb) py-btPython Exception &lt;class &apos;gdb.MemoryError&apos;&gt; Cannot access memory at address 0x58: Error occurred in Python command: Cannot access memory at address 0x58 使用py-bt报内存访问错误，只能用原始bt来分析了（添加full参数可以看更详细的内容）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344(gdb) bt full#0 _PyCode_CheckLineNumber (bounds=&lt;optimized out&gt;, ...) at ../Objects/codeobject.c:565 size = 46 p = 0x7f82acb60f3c &quot;\006\001\006\001\r\001\006\001...&quot;#1 maybe_call_line_trace (instr_prev=&lt;optimized out&gt;, ...) at ../Python/ceval.c:3743 line = 79#2 PyEval_EvalFrameEx () at ../Python/ceval.c:1050 opcode = 6#3 0x00000000004c7a59 in PyEval_EvalCodeEx () at ../Python/ceval.c:3265 f = &lt;unknown at remote 0x849d0c8&gt; retval = &lt;code at remote 0x7f835a1b5db0&gt; fastlocals = 0x2e tstate = 0xa2c0a0 u = &lt;unknown at remote 0xf2&gt;#4 0x00000000004cad3b in fast_function (nk=&lt;optimized out&gt;, ...) at ../Python/ceval.c:4129 co = 0x563c10 &lt;trace_trampoline&gt; globals = &lt;unknown at remote 0xf2&gt; argdefs = &lt;unknown at remote 0xf2&gt;#5 call_function (oparg=&lt;optimized out&gt;, pp_stack=&lt;optimized out&gt;) at ../Python/ceval.c:4054 func = &lt;function at remote 0x7f835a1dbb90&gt; w = &lt;function at remote 0x7f835a1dbb90&gt; nk = 682340552 n = 4 pfunc = 0x3276ea8#6 PyEval_EvalFrameEx () at ../Python/ceval.c:2679 sp = 0x3276ec8 opcode = 2#7 0x00000000004c996a in fast_function (nk=&lt;optimized out&gt;, ...) at ../Python/ceval.c:4119 f = Frame 0x3276cd0, for file ./service/recall/newuser.py, line 49, in get_gametype_anchor_by_sn (...(truncated) tstate = 0xa2c0a0 stack = 0xf2 co = 0x563c10 &lt;trace_trampoline&gt; globals = &lt;unknown at remote 0xf2&gt; argdefs = &lt;unknown at remote 0xf2&gt;#8 call_function (oparg=&lt;optimized out&gt;, pp_stack=&lt;optimized out&gt;) at ../Python/ceval.c:4054 func = &lt;function at remote 0x7f835a1dbaa0&gt; w = &lt;function at remote 0x7f835a1dbaa0&gt; nk = 682340552 n = 7 pfunc = 0x5b971d8...(省略)...#43 0x00007f83243691b0 in ?? ()No symbol table info available.#44 0x0000000000000000 in ?? ()No symbol table info available. 分析Frame # 7发现当前线程正在执行./service/recall/newuser.py, line 49, in get_gametype_anchor_by_sn方法。 找到对应的源代码： 12345def get_predict_gametype_anchor(self, gt_scores, limit): ... while pool_can_use and curr_sample_cnt &lt; sample_cnt: for idx, ent in enumerate(gt_pool): ... 果然，这里有一个while循环嵌套了enumerate调用。通过仔细分析代码，发现在某种情况下确实会出现死循环情况，至此问题解决。 总结 遇到线上问题时，优先使用 gcore PID 来保存现场 再 使用 strace、ltrace 和 gdb 分析 如果没有什么线索，可以尝试 pyrasite-shell 或 lptrace gdb 调试 Python 进程的时候，运行进程的 Python 版本和 python-dbg 一定要匹配 asyncio协程原理有一个任务调度器event loop，我们可以把需要执行的coroutine打包成task加入到event loop的调度列表里面（以Handle形式）。 在event loop的每个帧里面，它会检查需要执行那些task，然后运行这些task，可能拿到最终结果，也可能执行一半继续await别的任务，任务之间互相wait，通过回调来把任务串联起来 任务可能会依赖别的IO消息，在每一帧，event loop都会用selector(这个select就是类似某种多路复用机制，比如select，epoll和iocp)处理相应的消息，执行相应的callback函数。 我们当前的介绍里，只有一个event loop，这个event loop跑在主线程里面。当然，event loop还可以开线程池处理别的任务，或者，多个线程里执行多个event loop，他们之间还有交互，我们这里不在介绍。 单个event loop跑在单个线程有个好处，只要自己不主动await，就会一直占有主线程，换句话说，同步函数一定没有数据冲突（data racing）。对比多线程方案，如果需要处理数据冲突，就需要加锁了，这在很多情况下会降低程序的性能。所以协程这种设计思路，非常适合有多个用户、但是每个用户之间没有共享数据的场景。如果需要实现并行，多开几个进程就行了 asyncio协程栈帧python 中的上下文，被封装成了一个叫做 PyFrameObject 的结构，又称之为栈帧，看一下他的源码。12345678910111213141516171819202122232425262728293031typedef struct _frame &#123; PyObject_VAR_HEAD struct _frame *f_back; /* previous frame, or NULL 上一个栈帧*/ PyCodeObject *f_code; /* code segment 代码段*/ PyObject *f_builtins; /* builtin symbol table (PyDictObject) 内建变量表*/ PyObject *f_globals; /* global symbol table (PyDictObject) 全局变量表*/ PyObject *f_locals; /* local symbol table (any mapping) 局部变量表*/ PyObject **f_valuestack; /* points after the last local 栈底*/ /* Next free slot in f_valuestack. Frame creation sets to f_valuestack. Frame evaluation usually NULLs it, but a frame that yields sets it to the current stack top. */ PyObject **f_stacktop; /* 栈顶 */ PyObject *f_trace; /* Trace function */ char f_trace_lines; /* Emit per-line trace events? */ char f_trace_opcodes; /* Emit per-opcode trace events? */ /* Borrowed reference to a generator, or NULL 专为生成器设计的指针*/ PyObject *f_gen; int f_lasti; /* Last instruction if called 运行的上一个字节码位置*/ /* Call PyFrame_GetLineNumber() instead of reading this field directly. As of 2.3 f_lineno is only valid when tracing is active (i.e. when f_trace is set). At other times we use PyCode_Addr2Line to calculate the line from the current bytecode index. */ int f_lineno; /* Current line number 运行字节码对应的python源代码的行数*/ int f_iblock; /* index in f_blockstack */ char f_executing; /* whether the frame is still executing */ PyTryBlock f_blockstack[CO_MAXBLOCKS]; /* for try and loop blocks */ PyObject *f_localsplus[1]; /* locals+stack, dynamically sized */&#125; PyFrameObject; import流程 当Python的解释器遇到import语句或者其他上述导入语句时,它会先去查看sys.modules中是否已经有同名模块被导入了, 如果有就直接取来用;没有就去查阅sys.path里面所有已经储存的目录. sys.path这个列表初始化的时候,通常包含一些来自外部的库(external libraries)或者是来自操作系统的一些库,当然也会有一些类似于dist-package的标准库在里面.这些目录通常是被按照顺序或者是直接去搜索想要的–如果说他们当中的一个包含有期望的package或者是module,这个package或者是module将会在整个过程结束的时候被直接提取出来保存在sys.modules中(sys.modules是一个模块名:模块对象的字典结构). 当然，这个 sys.path 是可以修改的（正如上文提到的一种解决办法）。值得注意的是，如果当前目录包含有和标准库同名的模块，会直接使用当前目录的模块而不是标准模块。 当在这些个地址中实在是找不着时,它就会抛出一个ModuleNotFoundError错误. 当我们要导入一个模块（比如 foo ）时，解释器首先会根据命名查找内置模块，如果没有找到，它就会去查找 sys.path 列表中的目录，看目录中是否有 foo.py 。sys.path 的初始值来自于： 运行脚本所在的目录（如果打开的是交互式解释器则是当前目录） PYTHONPATH 环境变量（类似于 PATH 变量，也是一组目录名组成） Python 安装时的默认设置 为啥字符串join比加号连接快字符串是不可变对象，当用操作符+连接字符串的时候，每执行一次+都会申请一块新的内存，然后复制上一个+操作的结果和本次操作的右操作符到这块内存空间，因此用+连接字符串的时候会涉及好几次内存申请和复制。而join在连接字符串的时候，会先计算需要多大的内存存放结果，然后一次性申请所需内存并将字符串复制过去，这是为什么join的性能优于+的原因。所以在连接字符串数组的时候，我们应考虑优先使用join。 is和==的区别官方文档中说 is 表示的是对象标示符（object identity），而 == 表示的是相等（equality）。is 的作用是用来检查对象的标示符是否一致，也就是比较两个对象在内存中的地址是否一样，而 == 是用来检查两个对象是否相等。 我们在检查 a is b 的时候，其实相当于检查 id(a) == id(b)。而检查 a == b 的时候，实际是调用了对象 a 的 eq() 方法，a == b 相当于 a.eq(b)。 一般情况下，如果 a is b 返回True的话，即 a 和 b 指向同一块内存地址的话，a == b 也返回True，即 a 和 b 的值也相等。 元类参考: https://www.liaoxuefeng.com/wiki/1016959663602400/1017592449371072 python元类的使用场景, 比如orm框架, ORM全称“Object Relational Mapping”，即对象-关系映射，就是把关系数据库的一行映射为一个对象，也就是一个类对应一个表，这样，写代码更简单，不用直接操作SQL语句。 type()函数既可以返回一个对象的类型，又可以创建出新的类型，比如，我们可以通过type()函数创建出Hello类，而无需通过class Hello(object)...的定义： 1234567891011\&gt;&gt;&gt; def fn(self, name=&apos;world&apos;): # 先定义函数... print(&apos;Hello, %s.&apos; % name)...&gt;&gt;&gt; Hello = type(&apos;Hello&apos;, (object,), dict(hello=fn)) # 创建Hello class&gt;&gt;&gt; h = Hello()&gt;&gt;&gt; h.hello()Hello, world.&gt;&gt;&gt; print(type(Hello))&lt;class &apos;type&apos;&gt;&gt;&gt;&gt; print(type(h))&lt;class &apos;__main__.Hello&apos;&gt; 要创建一个 class 对象，type()函数依次传入 3 个参数： class 的名称； 继承的父类集合，注意 Python 支持多重继承，如果只有一个父类，别忘了 tuple 的单元素写法； class 的方法名称与函数绑定，这里我们把函数fn绑定到方法名hello上。 通过type()函数创建的类和直接写 class 是完全一样的，因为 Python 解释器遇到 class 定义时，仅仅是扫描一下 class 定义的语法，然后调用type()函数创建出 class。 正常情况下，我们都用class Xxx...来定义类，但是，type()函数也允许我们动态创建出类来，也就是说，动态语言本身支持运行期动态创建类，这和静态语言有非常大的不同，要在静态语言运行期创建类，必须构造源代码字符串再调用编译器，或者借助一些工具生成字节码实现，本质上都是动态编译，会非常复杂。 metaclass除了使用type()动态创建类以外，要控制类的创建行为，还可以使用 metaclass。metaclass，直译为元类，简单的解释就是：当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。但是如果我们想创建出类呢？那就必须根据 metaclass 创建出类，所以：先定义 metaclass，然后创建类。连接起来就是：先定义 metaclass，就可以创建类，最后创建实例。所以，metaclass 允许你创建类或者修改类。换句话说，你可以把类看成是 metaclass 创建出来的 “实例”。我们先看一个简单的例子，这个 metaclass 可以给我们自定义的 MyList 增加一个add方法：定义ListMetaclass，按照默认习惯，metaclass 的类名总是以 Metaclass 结尾，以便清楚地表示这是一个 metaclass：1234class ListMetaclass(type): def __new__(cls, name, bases, attrs): attrs\['add'\] = lambda self, value: self.append(value) return type.__new__(cls, name, bases, attrs) 有了 ListMetaclass，我们在定义类的时候还要指示使用 ListMetaclass 来定制类，传入关键字参数metaclass：12class MyList(list, metaclass=ListMetaclass): pass 当我们传入关键字参数metaclass时，魔术就生效了，它指示 Python 解释器在创建MyList时，要通过ListMetaclass.__new__()来创建，在此，我们可以修改类的定义，比如，加上新的方法，然后，返回修改后的定义。 __new__()方法接收到的参数依次是： 当前准备创建的类的对象； 类的名字； 类继承的父类集合； 类的方法集合。 测试一下MyList是否可以调用add()方法：1234\&gt;&gt;&gt; L = MyList()&gt;&gt;&gt; L.add(1)&gt;&gt; L\[1\] 而普通的list没有add()方法：12345\&gt;&gt;&gt; L2 = list()&gt;&gt;&gt; L2.add(1)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &apos;list&apos; object has no attribute &apos;add&apos; 装饰器12345def log(func): def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper 观察上面的log，因为它是一个decorator，所以接受一个函数作为参数，并返回一个函数。我们要借助Python的@语法，把decorator置于函数的定义处：123@logdef now(): print('2015-3-25') 调用now()函数，不仅会运行now()函数本身，还会在运行now()函数前打印一行日志： 123&gt;&gt;&gt; now()call now():2015-3-25 把@log放到now()函数的定义处，相当于执行了语句：now = log(now)由于log()是一个decorator，返回一个函数，所以，原来的now()函数仍然存在，只是现在同名的now变量指向了新的函数，于是调用now()将执行新函数，即在log()函数中返回的wrapper()函数。 wrapper()函数的参数定义是(args, *kw)，因此，wrapper()函数可以接受任意参数的调用。在wrapper()函数内，首先打印日志，再紧接着调用原始函数。 如果decorator本身需要传入参数，那就需要编写一个返回decorator的高阶函数，写出来会更复杂。比如，要自定义log的文本：1234567def log(text): def decorator(func): def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator 这个3层嵌套的decorator用法如下：123@log('execute')def now(): print('2015-3-25') 执行结果如下：123&gt;&gt;&gt; now()execute now():2015-3-25 和两层嵌套的decorator相比，3层嵌套的效果是这样的：now = log(&#39;execute&#39;)(now)我们来剖析上面的语句，首先执行log(‘execute’)，返回的是decorator函数，再调用返回的函数，参数是now函数，返回值最终是wrapper函数。 python命令行参数 -u参数的使用：python命令加上-u（unbuffered）参数后会强制其标准输出也同标准错误一样不通过缓存直接打印到屏幕。 -c参数，支持执行单行命令/脚本。如: python -c &quot;import os;print(&#39;hello&#39;),print(&#39;world&#39;)&quot; python -m test_folder/test.py与python test_folder/test有什么不同桌面的test_folder文件夹下有个test.pytest.py12import sysprint(sys.path) 运行看看:1234567hulinhong@GIH-D-14531 MINGW64 ~/Desktop$ python test_folder/test.py[&apos;C:\\Users\\hulinhong\\Desktop\\test_folder&apos;, &apos;C:\\Program Files\\Python37\\python37.zip&apos;, &apos;C:\\Program Files\\Python37\\DLLs&apos;, &apos;C:\\Program Files\\Python37\\lib&apos;, &apos;C:\\Program Files\\Python37&apos;, &apos;C:\\Program Files\\Python37\\lib\\site-packages&apos;, &apos;C:\\Program Files\\Python37\\lib\\site-packages\\redis_py_cluster-2.1.0-py3.7.egg&apos;]hulinhong@GIH-D-14531 MINGW64 ~/Desktop$ python -m test_folder.test[&apos;C:\\Users\\hulinhong\\Desktop&apos;, &apos;C:\\Program Files\\Python37\\python37.zip&apos;, &apos;C:\\Program Files\\Python37\\DLLs&apos;, &apos;C:\\Program Files\\Python37\\lib&apos;, &apos;C:\\Program Files\\Python37&apos;, &apos;C:\\Program Files\\Python37\\lib\\site-packages&apos;, &apos;C:\\Program Files\\Python37\\lib\\site-packages\\redis_py_cluster-2.1.0-py3.7.egg&apos;] 细心的同学会发现，区别就是在第一个路径: python直接启动是把test.py文件所在的目录放到了sys.path属性中。 模块启动是把你输入命令的目录（也就是当前路径），放到了sys.path属性中 所以就会有下面的情况: 目录结构如下123456package/ __init__.py mod1.pypackage2/ __init__.py run.py run.py 内容如下123import sysfrom package import mod1print(sys.path) 如何才能启动run.py文件？ 直接启动（失败） 12345➜ test_import_project git:(master) ✗ python package2/run.pyTraceback (most recent call last): File &quot;package2/run.py&quot;, line 2, in &lt;module&gt; from package import mod1ImportError: No module named package 以模块方式启动（成功） 1234➜ test_import_project git:(master) ✗ python -m package2.run[&apos;C:\\Users\\hulinhong\\Desktop&apos;,&apos;/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python27.zip&apos;,...] 当需要启动的py文件引用了一个模块。你需要注意：在启动的时候需要考虑sys.path中有没有你import的模块的路径！这个时候，到底是使用直接启动，还是以模块的启动？目的就是把import的那个模块的路径放到sys.path中。你是不是明白了呢？ 官方文档参考： http://www.pythondoc.com/pythontutorial3/modules.html 导入一个叫 mod1 的模块时，解释器先在当前目录中搜索名为 mod1.py 的文件。如果没有找到的话，接着会到 sys.path 变量中给出的目录列表中查找。 sys.path 变量的初始值来自如下： 输入脚本的目录（当前目录）。 环境变量 PYTHONPATH 表示的目录列表中搜索(这和 shell 变量 PATH 具有一样的语法，即一系列目录名的列表)。 Python 默认安装路径中搜索。 实际上，解释器由 sys.path 变量指定的路径目录搜索模块，该变量初始化时默认包含了输入脚本（或者当前目录）， PYTHONPATH 和安装目录。这样就允许 Python程序了解如何修改或替换模块搜索目录。 __new__ 与 __del__ 与 __init__先来看一个单例模式的实现12345678910111213141516171819class Demo: __isinstance = False def __new__(cls, *args, **kwargs): if not cls.__isinstance: # 如果被实例化了 cls.__isinstance = object.__new__(cls) # 否则实例化 return cls.__isinstance # 返回实例化的对象 def __init__(self, name): self.name = name print('my name is %s'%(name)) def __del__(self): print('886, %s'%(self.name))d1 = Demo('Alice')d2 = Demo('Anew')print(d1)print(d2) 打印:12345my name is Alicemy name is Anew&lt;__main__.Demo object at 0x000001446604D3C8&gt;&lt;__main__.Demo object at 0x000001446604D3C8&gt;886, Anew __new__ 是负责对当前类进行实例化，并将实例返回，并传给__init__方法，__init__方法中的self就是指代__new__传过来的对象，所以再次强调，__init__是实例化后调用的第一个方法。 __del__在对象销毁时被调用，往往用于清除数据或还原环境等操作，比如在类中的其他普通方法中实现了插入数据库的语句，当对象被销毁时我们需要将数据还原，那么这时可以在__del__方法中实现还原数据库数据的功能。__del__被成为析构方法，同样和C++中的析构方法类似。 python垃圾回收总体来说，在Python中，主要通过引用计数进行垃圾回收；通过 “标记-清除” 解决容器对象可能产生的循环引用问题；通过 “分代回收” 以空间换时间的方法提高垃圾回收效率。 引用计数 标记清除(Mark and Sweep) 分代回收 标记清除咋弄的参考: https://zhuanlan.zhihu.com/p/83251959 Python 采用了 “标记-清除”(Mark and Sweep) 算法，解决容器对象可能产生的循环引用问题。(注意，只有容器对象才会产生循环引用的情况，比如列表、字典、用户自定义类的对象、元组等。而像数字，字符串这类简单类型不会出现循环引用。作为一种优化策略，对于只包含简单类型的元组也不在标记清除算法的考虑之列) 跟其名称一样，该算法在进行垃圾回收时分成了两步，分别是： A）标记阶段，遍历所有的对象，如果是可达的（reachable），也就是还有对象引用它，那么就标记该对象为可达； B）清除阶段，再次遍历对象，如果发现某个对象没有标记为可达，则就将其回收。 如下图所示，在标记清除算法中，为了追踪容器对象，需要每个容器对象维护两个额外的指针，用来将容器对象组成一个双端链表，指针分别指向前后两个容器对象，方便插入和删除操作。python 解释器 (Cpython) 维护了两个这样的双端链表，一个链表存放着需要被扫描的容器对象，另一个链表存放着临时不可达对象。在图中，这两个链表分别被命名为”Object to Scan”和”Unreachable”。图中例子是这么一个情况：link1,link2,link3 组成了一个引用环，同时 link1 还被一个变量 A(其实这里称为名称 A 更好)引用。link4 自引用，也构成了一个引用环。从图中我们还可以看到，每一个节点除了有一个记录当前引用计数的变量 ref_count 还有一个 gc_ref 变量，这个 gc_ref 是 ref_count 的一个副本，所以初始值为 ref_count 的大小。 gc 启动的时候，会逐个遍历”Object to Scan” 链表中的容器对象，并且将当前对象所引用的所有对象的 gc_ref 减一。(扫描到 link1 的时候，由于 link1 引用了 link2, 所以会将 link2 的 gc_ref 减一，接着扫描 link2, 由于 link2 引用了 link3, 所以会将 link3 的 gc_ref 减一…..) 像这样将”Objects to Scan” 链表中的所有对象考察一遍之后，两个链表中的对象的 ref_count 和 gc_ref 的情况如下图所示。这一步操作就相当于解除了循环引用对引用计数的影响。 接着，gc 会再次扫描所有的容器对象，如果对象的 gc_ref 值为 0，那么这个对象就被标记为 GC_TENTATIVELY_UNREACHABLE，并且被移至”Unreachable” 链表中。下图中的 link3 和 link4 就是这样一种情况。 如果对象的 gc_ref 不为 0，那么这个对象就会被标记为 GC_REACHABLE。同时当 gc 发现有一个节点是可达的，那么他会递归式的将从该节点出发可以到达的所有节点标记为 GC_REACHABLE, 这就是下图中 link2 和 link3 所碰到的情形。 除了将所有可达节点标记为 GC_REACHABLE 之外，如果该节点当前在”Unreachable” 链表中的话，还需要将其移回到”Object to Scan” 链表中，下图就是 link3 移回之后的情形。 第二次遍历的所有对象都遍历完成之后，存在于”Unreachable” 链表中的对象就是真正需要被释放的对象。如上图所示，此时 link4 存在于 Unreachable 链表中，gc 随即释放之。 上面描述的垃圾回收的阶段，会暂停整个应用程序，等待标记清除结束后才会恢复应用程序的运行。 为啥标记清除回收无法回收重写了__del__方法的类对象 Circular references which are garbage are detected when the option cycle detector is enabled (it’s on by default), but can only be cleaned up if there are no Python-level __del__() methods involved. 官方文档中表明启用周期检测器时会检测到垃圾的循环引用（默认情况下它是打开的)，但只有在没有涉及 Python __del__() 方法的情况下才能清除。Python 不知道破坏彼此保持循环引用的对象的安全顺序，因此它则不会为这些方法调用析构函数。简而言之，如果定义了 __del__ 函数，那么在循环引用中Python解释器无法判断析构对象的顺序，因此就不做处理。 分代回收在循环引用对象的回收中，整个应用程序会被暂停，为了减少应用程序暂停的时间，Python 通过“分代回收”(Generational Collection)以空间换时间的方法提高垃圾回收效率。 分代回收是基于这样的一个统计事实，对于程序，存在一定比例的内存块的生存周期比较短；而剩下的内存块，生存周期会比较长，甚至会从程序开始一直持续到程序结束。生存期较短对象的比例通常在 80%～90% 之间，这种思想简单点说就是：对象存在时间越长，越可能不是垃圾，应该越少去收集。这样在执行标记-清除算法时可以有效减小遍历的对象数，从而提高垃圾回收的速度。 python gc给对象定义了三种世代(0,1,2),每一个新生对象在generation zero中，如果它在一轮gc扫描中活了下来，那么它将被移至generation one,在那里他将较少的被扫描，如果它又活过了一轮gc,它又将被移至generation two，在那里它被扫描的次数将会更少。 gc的扫描在什么时候会被触发呢?答案是当某一世代中被分配的对象与被释放的对象之差达到某一阈值的时候，就会触发gc对某一世代的扫描。值得注意的是当某一世代的扫描被触发的时候，比该世代年轻的世代也会被扫描。也就是说如果世代2的gc扫描被触发了，那么世代0,世代1也将被扫描，如果世代1的gc扫描被触发，世代0也会被扫描。 该阈值可以通过下面两个函数查看和调整: 12gc.get_threshold() # (threshold0, threshold1, threshold2).gc.set_threshold(threshold0[, threshold1[, threshold2]]) 下面对set_threshold()中的三个参数threshold0, threshold1, threshold2进行介绍。gc会记录自从上次收集以来新分配的对象数量与释放的对象数量，当两者之差超过threshold0的值时，gc的扫描就会启动，初始的时候只有世代0被检查。如果自从世代1最近一次被检查以来，世代0被检查超过threshold1次，那么对世代1的检查将被触发。相同的，如果自从世代2最近一次被检查以来，世代1被检查超过threshold2次，那么对世代2的检查将被触发。get_threshold()是获取三者的值，默认值为(700,10,10). CPP参考: 看之前一个哥们总结的c++要点 https://interview.huihut.com/ new 和 delete 为什么要配对用: 12345class A&#123;//...&#125;;A *pa = new A();A *pas = new A[NUM](); delete []pas; //详细流程: delete[] pas 用来释放pas指向的内存！！还逐一调用数组中每个对象的destructor！！ delete []pa; //会发生什么, 答案是调用未知次数的A的析构函数. 因为delete[]会去通过pa+offset找一个基于pa的偏移量找一个内存里的数据, 他假定这个内存里放了要调用析构的次数n这个数据, 而这个内存里到底是多少是未知的. delete pas; //哪些指针会变成野指针, 答案是pas和A[0]中的指针会变成野指针. 因为只有这两个指针指向的内存被释放了, 也就是说, 仅释放了pas指针指向的这个数组的全部内存空间, 以及只调用了a[0]对象的析构函数 cqq vec set map list vector和string的内存分配与使用注意点 stl关联容器的特性 map的[]和insert的区别? insert 含义是：如果key存在，则插入失败，如果key不存在，就创建这个key－value。实例: map.insert((key, value)) 利用下标操作的含义是：如果这个key存在，就更新value；如果key不存在，就创建这个key－value对 实例：map[key] = value vector的resize和reserve的区别? 总结: resize既分配了空间，也创建了对象，可以通过下标访问。当new_size大于原size, 则resize既修改capacity大小，也修改size大小。否则只修改size大小. reserve只分配了空间, 也就是说它只修改capacity大小，不修改size大小, 若 new_cap 小于等于当前的 capacity(), 它啥也不干. resize: 重设容器大小以容纳 count 个元素。 若当前大小大于 count ，则减小容器为其首 count 个元素。 若当前大小小于 count: 则后附额外的默认插入的元素 则后附额外的 value 的副本 reserve: 增加 vector 的容量到大于或等于 new_cap 的值。若 new_cap 大于当前的 capacity() ，则分配新存储，否则该方法不做任何事。reserve() 不更改 vector 的 size 。若 new_cap 大于 capacity() ，则所有迭代器，包含尾后迭代器和所有到元素的引用都被非法化。否则，没有迭代器或引用被非法化。 字节对齐 对象模型之内存对齐基础 定位new 12345678910111213141516171819202122#include &lt;iostream&gt;using namespace std;int main() &#123; char buffer[512]; //chunk of memory内存池 int *p2, *p3; //定位new: p2 = new (buffer) int[10]; p2[0] = 99; p2[1] = 88; cout &lt;&lt; "buffer = " &lt;&lt;(void *)buffer &lt;&lt; endl; //内存池地址 cout &lt;&lt; "p2 = " &lt;&lt; p2 &lt;&lt; endl; //定位new指向的地址 cout &lt;&lt; "p2[0] = " &lt;&lt; p2[0] &lt;&lt; endl; p3 = new (buffer) int[2]; p3[0] = 1; p3[1] = 2; cout &lt;&lt; "p3 = " &lt;&lt; p3 &lt;&lt; endl; cout &lt;&lt; "p2[0] = " &lt;&lt; p2[0] &lt;&lt; endl; cout &lt;&lt; "p2[1] = " &lt;&lt; p2[1] &lt;&lt; endl; cout &lt;&lt; "p2[2] = " &lt;&lt; p2[2] &lt;&lt; endl; cout &lt;&lt; "p2[3] = " &lt;&lt; p2[3] &lt;&lt; endl; return 0;&#125; 结果发现p3和p2还有buffer都是使用同样的内存地址，符合指定地址的内存块，而且p3在指定位置覆盖了p2的前两处的值。 c++一个空类会生成什么 (答: 默认构造/析构(非虚)/赋值运算符/默认拷贝/取地址/const取地址) 虚函数（virtual）可以是内联函数（inline）吗？ 虚函数可以是内联函数，内联是可以修饰虚函数的，但是当虚函数表现多态性的时候不能内联。 内联是在编译器建议编译器内联，而虚函数的多态性在运行期，编译器无法知道运行期调用哪个代码，因此虚函数表现为多态性时（运行期）不可以内联。 inline virtual 唯一可以内联的时候是：编译器知道所调用的对象是哪个类（如 Base::who()），这只有在编译器具有实际对象而不是对象的指针或引用时才会发生。虚函数内联使用:12345678910// 此处的虚函数 who()，是通过类（Base）的具体对象（b）来调用的，// 编译期间就能确定了，所以它可以是内联的，// 但最终是否内联取决于编译器。 Base b;b.who();// 此处的虚函数是通过指针调用的，呈现多态性，// 需要在运行时期间才能确定，所以不能为内联。 Base *ptr = new Derived();ptr-&gt;who(); 虚函数指针、虚函数表 虚函数指针：在含有虚函数类的对象中，指向虚函数表，在运行时确定。 虚函数表：在程序内存的只读数据段（.rodata section，见：CPP目标文件内存布局），存放虚函数指针，如果派生类实现了基类的某个虚函数，则在虚表中覆盖原本基类的那个虚函数指针，在编译时根据类的声明创建。 virtual修饰符: 如果一个类是局部变量则该类数据存储在栈区，如果一个类是通过new/malloc动态申请的，则该类数据存储在堆区。 如果该类是virutal继承而来的子类，则该类的虚函数表指针和该类其他成员一起存储。虚函数表指针指向只读数据段中的类虚函数表，虚函数表中存放着一个个函数指针，函数指针指向代码段中的具体函数。 内存泄漏的工具 vargrid..? 还有啥工具 了解ASAN查找内存越界问题 cpp找找冰川, 大梦龙图的面试题，网上常用题 gdb怎么切换线程 C++ 的动态多态怎么实现的？ C++ 的构造函数可以是虚函数吗？ 无锁队列原理是否一定比有锁快?(不一定, 如果临界区小因为有上下文切换则mutex慢, 再来看lockfree的spin，一般都遵循一个固定的格式：先把一个不变的值X存到某个局部变量A里，然后做一些计算，计算/生成一个新的对象，然后做一个CAS操作，判断A和X还是不是相等的，如果是，那么这次CAS就算成功了，否则再来一遍。如果上面这个loop里面“计算/生成一个新的对象”非常耗时并且contention很严重，那么lockfree性能有时会比mutex差。另外lockfree不断地spin引起的CPU同步cacheline的开销也比mutex版本的大。关于ABA问题) gcc 命令的常用选项选项解释-ansi只支持 ANSI 标准的 C 语法。这一选项将禁止 GNU C 的某些特色， 例如 asm 或 typeof 关键词。-c只编译并生成目标文件。-DMACRO以字符串 “1” 定义 MACRO 宏。-DMACRO=DEFN以字符串 “DEFN” 定义 MACRO 宏。-E只运行 C 预编译器。-g生成调试信息。GNU 调试器可利用该信息。-IDIRECTORY指定额外的头文件搜索路径 DIRECTORY。-LDIRECTORY指定额外的函数库搜索路径 DIRECTORY。-lLIBRARY连接时搜索指定的函数库 LIBRARY。-m486针对 486 进行代码优化。-o FILE生成指定的输出文件。用在生成可执行文件时。-O0不进行优化处理。-O 或 -O1优化生成代码。-O2进一步优化。-O3比 -O2 更进一步优化，包括 inline 函数。-shared生成共享目标文件。通常用在建立共享库时。-static禁止使用共享连接。-UMACRO取消对 MACRO 宏的定义。-w不生成任何警告信息。-Wall生成所有警告信息。 使用gcore或strace和pstack查询线上CPU的100%问题12345678# top -bn1p 6325Tasks: 608 total, 9 running, 596 sleeping, 0 stopped, 3 zombie%Cpu(s): 20.5 us, 6.3 sy, 0.0 ni, 72.3 id, 0.5 wa, 0.0 hi, 0.5 si, 0.0 stKiB Mem: 65983744 total, 65372892 used, 610852 free, 574684 buffersKiB Swap: 4194300 total, 0 used, 4194300 free. 42109148 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 6325 cc 20 0 4840544 628972 69656 R 100.0 1.0 927:41.47 python2.7 先用strace查看系统调用1strace -p 6325 结果一直卡住，无任何输出，说明进程没有进行任何系统调用。 用pstack查看当前调用堆栈对一个服务 (room_status_server) 进行了一些优化，并顺便修改了部分配置文件，重启后用top命令观察，发现该程序cpu几乎占到了100%。 发现这个问题后，想到前两天还上线了该服务，立马去线上看了看，还好线上是正常的。那么问题肯定是刚才的修改导致的！把线上的版本拿过来运行，还是cpu几乎占到了100%，那很大可能是配置文件哪里改错了（后面验证表明我的猜测是对的）。 想到这是一个好的学习的机会，我想还是从运行的程序来看看到底出了什么事。 思路： 程序占用 100% 的 cpu，程序即进程，也就是说进程占了 100% 的 cpu（一个核） 一个进程有多个线程，究竟是哪一个线程占了 100% 的 cpu？ 这个线程在干什么？ 查看程序的进程号命令：top -c。 输入大写P，top 的输出会按使用 cpu 多少排序。 PID就是进程号，我程序的进程号是4918。 查看耗 CPU 的线程号命令：top -Hp 进程号。 同样输入大写P，top 的输出会按使用 cpu 多少排序。 输入top -Hp 4918，展示内容如图：可以看出 PID 是4927的线程占到了 100% 的 cpu，我的业务日志是打印线程号的，打开日志，哦~~ 原来是这个原因（先卖个关子不说）。 查看耗 CPU 的任务上面找到了耗 CPU 的线程，那这个线程在做什么呢？看线程在干什么，可以看线程的堆栈，命令是pstack 进程号，会输出所有线程的堆栈信息。 输入pstack 4918，并搜索线程4927的堆栈，展示内容如图： 从堆栈信息看，程序在执行 boost 创建 socket 监听等任务，为什么一直执行这个呢？因为，我的端口号重复使用了。 其实从堆栈信息定位问题还是有些抽象的，但是大概可以看出线程在做什么，至少给排查问题指明了方向。 使用gcore来处理 大致流程总结: 为了避免gdb attach对线上进程造成影戏, 先使用gcore生成coredump文件, 然后再使用gdb分析coredump文件, 然后在gdb内info thread查看线程状态 然后thread 线程号切换线程, 然后bt查调用堆栈可以查到了 用ASAN处理内存泄漏与越界等问题用-fsanitize=address选项编译和链接你的程序;用-fno-omit-frame-pointer编译，以在错误消息中添加更好的堆栈跟踪。增加-O1以获得更好的性能。下面以简单的测试代码leaktest.c为例12345678910// leaktest.cchar leaktest() &#123; char *x = (char*)malloc(10 * sizeof(char*)); return x[5];&#125;int main() &#123; leaktest(); return 0;&#125; 在终端中输入以下命令编译leaktest.c1gcc -fsanitize=address -fno-omit-frame-pointer -O1 -g leaktest.c -o leaktest 运行leaktest，会打印下面的错误信息： 第一部分（ERROR），指出错误类型detected memory leaks；第二部分给出详细的错误信息：Direct leak of 80 byte(s) in 1 object(s)，以及发生错误的对象名/源文件位置/行数；第三部分是以上信息的一个总结（SUMMARY）。 左值右值通用引用 左值持久 变量是左值, 因为变量是持久的 直至离开作用域时才被销毁。 凡是取地址（&amp;）操作可以成功的都是左值，其余都是右值 右值短暂, 右值要么是字面常量，要么是在表达式求值过程中创建的临时对象 所引用的对象将要被销毁 该对象没有其他用户 使用右值引用的代码可以自由地接管所引用的对象的资源 std::move 可以显式地将一个左值转换为对应的右值引用类型 12345678910111213141516171819202122232425// 形参是个右值引用void change(int&amp;&amp; right_value) &#123; right_value = 8;&#125; int main() &#123; int a = 5; // a是个左值 int &amp;ref_a_left = a; // ref_a_left是个左值引用 int &amp;&amp;ref_a_right = std::move(a); // ref_a_right是个右值引用 change(a); // 编译不过，a是左值，change参数要求右值 change(ref_a_left); // 编译不过，左值引用ref_a_left本身也是个左值 change(ref_a_right); // 编译不过，右值引用ref_a_right本身也是个左值 change(std::move(a)); // 编译通过 change(std::move(ref_a_right)); // 编译通过 change(std::move(ref_a_left)); // 编译通过 change(5); // 当然可以直接接右值，编译通过 cout &lt;&lt; &amp;a &lt;&lt; ' '; cout &lt;&lt; &amp;ref_a_left &lt;&lt; ' '; cout &lt;&lt; &amp;ref_a_right; // 打印这三个左值的地址，都是一样的&#125; std::move应用场景按上文分析，std::move只是类型转换工具，不会对性能有好处；右值引用在作为函数形参时更具灵活性，看上去还是挺鸡肋的。他们有什么实际应用场景吗？ 在实际场景中，右值引用和std::move被广泛用于在STL和自定义类中实现移动语义，避免拷贝，从而提升程序性能。该类的拷贝构造函数、赋值运算符重载函数已经通过使用左值引用传参来避免一次多余拷贝了，但是内部实现要深拷贝，无法避免。 这时，有人提出一个想法：是不是可以提供一个移动构造函数，把被拷贝者的数据移动过来，被拷贝者后边就不要了，这样就可以避免深拷贝了，如：12345678910class Array &#123;public: // 移动构造函数，可以浅拷贝 Array(const Array&amp; temp_array, bool move) &#123; data_ = temp_array.data_; size_ = temp_array.size_; // 为防止temp_array析构时delete data，提前置空其data_ temp_array.data_ = nullptr; &#125;&#125;; 这么做有2个问题： 不优雅，表示移动语义还需要一个额外的参数(或者其他方式)。 无法实现！temp_array是个const左值引用，无法被修改，所以temparray.data = nullptr;这行会编译不过。当然函数参数可以改成非const：Array(Array&amp; temp_array, bool move){…}，这样也有问题，由于左值引用不能接右值，Array a = Array(Array(), true);这种调用方式就没法用了。 可以发现左值引用真是用的很不爽，右值引用的出现解决了这个问题，在STL的很多容器中，都实现了以右值引用为参数的移动构造函数和移动赋值重载函数，或者其他函数，最常见的如std::vector的push_back和emplace_back。参数为左值引用意味着拷贝，为右值引用意味着移动。12345678910111213141516class Array &#123;public: // 优雅 Array(Array&amp;&amp; temp_array) &#123; data_ = temp_array.data_; size_ = temp_array.size_; // 为防止temp_array析构时delete data，提前置空其data_ temp_array.data_ = nullptr; &#125;&#125;int main()&#123; Array a; // 左值a，用std::move转化为右值 Array b(std::move(a));&#125; 通用引用注意：只有当发生自动类型推断时（如函数模板的类型自动推导，或auto关键字），&amp;&amp;才是一个universal references。 当右值引用和模板结合的时候，就复杂了。T&amp;&amp;并不一定表示右值引用，它可能是个左值引用又可能是个右值引用。例如：1234567template&lt;typename T&gt;void f( T&amp;&amp; param)&#123; &#125;f(10); //10是右值int x = 10; //f(x); //x是左值 如果上面的函数模板表示的是右值引用的话，肯定是不能传递左值的，但是事实却是可以。这里的&amp;&amp;是一个未定义的引用类型，称为universal references，它必须被初始化，它是左值引用还是右值引用却决于它的初始化，如果它被一个左值初始化，它就是一个左值引用；如果被一个右值初始化，它就是一个右值引用。 std::forward和std::move一样，它的兄弟std::forward也充满了迷惑性，虽然名字含义是转发，但他并不会做转发，同样也是做类型转换. 与move相比，forward更强大，move只能转出来右值，forward都可以。 std::forward(u)有两个参数：T与 u。 a. 当T为左值引用类型时，u将被转换为T类型的左值； b. 否则u将被转换为T类型右值。举个例子，有main，A，B三个函数，调用关系为：main-&gt;A-&gt;B，建议先看懂2.3节对左右值引用本身是左值还是右值的讨论再看这里： 12345678910111213141516171819202122232425void change2(int&amp;&amp; ref_r) &#123; ref_r = 1;&#125; void change3(int&amp; ref_l) &#123; ref_l = 1;&#125; // change的入参是右值引用// 有名字的右值引用是 左值，因此ref_r是左值void change(int&amp;&amp; ref_r) &#123; change2(ref_r); // 错误，change2的入参是右值引用，需要接右值，ref_r是左值，编译失败 change2(std::move(ref_r)); // ok，std::move把左值转为右值，编译通过 change2(std::forward&lt;int &amp;&amp;&gt;(ref_r)); // ok，std::forward的T是右值引用类型(int &amp;&amp;)，符合条件b，因此u(ref_r)会被转换为右值，编译通过 change3(ref_r); // ok，change3的入参是左值引用，需要接左值，ref_r是左值，编译通过 change3(std::forward&lt;int &amp;&gt;(ref_r)); // ok，std::forward的T是左值引用类型(int &amp;)，符合条件a，因此u(ref_r)会被转换为左值，编译通过 // 可见，forward可以把值转换为左值或者右值&#125; int main() &#123; int a = 5; change(std::move(a));&#125; 完美转发123456789101112131415161718192021#include &lt;iostream&gt;using std::cout;using std::endl;template&lt;typename T&gt;void func(T&amp; param) &#123; cout &lt;&lt; "传入的是左值" &lt;&lt; endl;&#125;template&lt;typename T&gt;void func(T&amp;&amp; param) &#123; cout &lt;&lt; "传入的是右值" &lt;&lt; endl;&#125;template&lt;typename T&gt;void warp(T&amp;&amp; param) &#123; func(param);&#125;int main() &#123; int num = 2019; warp(num); warp(2019); return 0;&#125; 输出:12传入的是左值传入的是左值 是不是和我们预期的不一样，下面我们来分析一下原因： warp()函数本身的形参是一个万能引用，即可以接受左值又可以接受右值； 第一个warp()函数调用实参是左值，所以，warp()函数中调用func()中传入的参数也应该是左值； 第二个warp()函数调用实参是右值，根据上面所说的引用折叠规则，warp()函数接收的参数类型是右值引用，那么为什么却调用了调用func()的左值版本了呢？这是因为在warp()函数内部，右值引用类型变为了左值，因为参数有了名称，我们能通过变量名取得变量地址了。 那么问题来了，怎么保持函数调用过程中，变量类型的不变呢？这就是我们所谓的“完美转发”技术，在C++11中通过std::forward()函数来实现。我们修改我们的warp()函数如下：1234template&lt;typename T&gt;void warp(T&amp;&amp; param) &#123; func(std::forward&lt;T&gt;(param));&#125; 则可以输出预期的结果。 关于enable_shared_from_this如果一个T类型的对象t，是被std::shared_ptr管理的，且类型T继承自std::enable_shared_from_this，那么T就有个shared_from_this的成员函数，这个函数返回一个新的std::shared_ptr的对象，也指向对象t。 那么这个特性的应用场景是什么呢？一个主要的场景是保证异步回调函数中操作的对象仍然有效。比如有这样一个类：123456class Foo &#123;public: void Bar(std::function&lt;void(Foo*)&gt; p_fnCallback) &#123; // async call p_fnCallback with this &#125;&#125; Foo::Bar接受一个函数对象，这个对象需要一个Foo*指针，其实要的就是Foo::Bar的this指针，但是这个回调是异步的，也就是说可能在调用这个回调函数时，this指向的Foo对象已经提前析构了。 首先肯定不能那样写shared_ptr&lt; A &gt; ( this )，这会调用shared_ptr智能指针的构造函数，对this指针指向的对象，又建立了一份引用计数对象，已经对这个A对象建立的引用计数对象，又成了两个引用计数对象，对同一个资源都记录了引用计数，为1，最终两次析构对象释放内存，错误！ 这时候，std::enable_shared_from_this就派上用场了。修改后如下：12345678class Foo &#123;public:- void Bar(std::function&lt;void(Foo*)&gt; p_fnCallback) &#123;+ void Bar(std::function&lt;void(std::shared_ptr&lt;Foo&gt;)&gt; p_fnCallback) &#123;+ std::shared_ptr&lt;Foo&gt; _foo = shared_from_this(); // async call p_fnCallback with this &#125;&#125; 这样就可以保证异步回调时，Foo对象仍然有效。 注意到cppreference中说道，必须要是std::shared_ptr管理的对象，调用shared_from_this才是有效的，为什么呢？这个就需要看看std::enable_shared_from_this的实现原理了： 一个类继承enable_shared_from_this会怎么样？看看enable_shared_from_this基类的成员变量有什么，如下：123456789101112131415161718192021222324template&lt;class T&gt; class enable_shared_from_this&#123;public: shared_ptr&lt;T&gt; shared_from_this() &#123; shared_ptr&lt;T&gt; p( weak_this_ ); // 从原来的弱智能指针里变成强智能指针, 则是在原来的强智能指针上面引用计数+1 BOOST_ASSERT( p.get() == this ); return p; &#125;public: // actually private, but avoids compiler template friendship issues // Note: invoked automatically by shared_ptr; do not call template&lt;class X, class Y&gt; void _internal_accept_owner( shared_ptr&lt;X&gt; const * ppx, Y * py ) const &#123; if( weak_this_.expired() ) &#123; weak_this_ = shared_ptr&lt;T&gt;( *ppx, py ); &#125; &#125;private: mutable weak_ptr&lt;T&gt; weak_this_;&#125;; 也就是说，如果一个类继承了enable_shared_from_this，那么它产生的对象就会从基类enable_shared_from_this继承一个成员变量_Wptr，当定义第一个智能指针对象的时候shared_ptr&lt; A &gt; ptr1(new A())，调用shared_ptr的普通构造函数，就会初始化A对象的成员变量_Wptr，作为观察A对象资源的一个弱智能指针观察者（在shared_ptr的构造函数中实现，如下代码片段, 有兴趣可以自己调试跟踪源码实现）。1234567891011121314151617181920212223242526template&lt;class T&gt; class shared_ptr&#123; template&lt;class Y&gt; explicit shared_ptr( Y * p ): px( p ), pn() // Y must be complete &#123; boost::detail::sp_pointer_construct( this, p, pn ); &#125;&#125;;template&lt; class T, class Y &gt; inline void sp_pointer_construct( boost::shared_ptr&lt; T &gt; * ppx, Y * p, boost::detail::shared_count &amp; pn )&#123; boost::detail::shared_count( p ).swap( pn ); boost::detail::sp_enable_shared_from_this( ppx, p, p );&#125;template&lt; class X, class Y, class T &gt; inline void sp_enable_shared_from_this( boost::shared_ptr&lt;X&gt; const * ppx, Y const * py, boost::enable_shared_from_this&lt; T &gt; const * pe )&#123; if( pe != 0 ) &#123; pe-&gt;_internal_accept_owner( ppx, const_cast&lt; Y* &gt;( py ) ); &#125;&#125;inline void sp_enable_shared_from_this( ... )&#123;&#125; 综上所说，所有过程都没有再使用shared_ptr的普通构造函数，没有在产生额外的引用计数对象，不会存在把一个内存资源，进行多次计数的过程；关键的是, weak_ptr到shared_ptr的提升, 通过判断资源的引用计数是否还在，判定对象的存活状态， 对象存活，提升成功； 对象析构，提升失败！ 编译过程 预处理(Preprocessing): 做一些类似于将所有的#define删除，并且展开所有的宏定义的操作, 然后生成hello.i 编译(Compilation): 编译过程就是把预处理完的文件进行一系列的词法分析，语法分析，语义分析及优化后生成相应的汇编代码。得到hello.a 汇编(Assembly): 汇编器是将汇编代码转变成机器可以执行的命令，每一个汇编语句几乎都对应一条机器指令。汇编相对于编译过程比较简单，根据汇编指令和机器指令的对照表一一翻译即可。得到hello.o 链接(Linking): 通过调用链接器ld来链接程序运行需要的一大堆目标文件，以及所依赖的其它库文件，最后生成可执行文件 静态链接: 指在编译阶段直接把静态库加入到可执行文件中去，这样可执行文件会比较大 动态链接: 指链接阶段仅仅只加入一些描述信息，而程序执行时再从系统中把相应动态库加载到内存中去。 目标文件编译器编译源代码后生成的文件叫做目标文件。目标文件从结构上讲，它是已经编译后的可执行文件格式，只是还没有经过链接的过程，其中可能有些符号或有些地址还没有被调整。 可执行文件（Windows 的 .exe 和 Linux 的 ELF）、动态链接库（Windows 的 .dll 和 Linux 的 .so）、静态链接库（Windows 的 .lib 和 Linux 的 .a）都是按照可执行文件格式存储（Windows 按照 PE-COFF，Linux 按照 ELF） 目标文件格式: Windows 的 PE（Portable Executable），或称为 PE-COFF，.obj 格式 Linux 的 ELF（Executable Linkable Format），.o 格式 Intel/Microsoft 的 OMF（Object Module Format） Unix 的 a.out 格式 MS-DOS 的 .COM 格式 PE 和 ELF 都是 COFF（Common File Format）的变种 CPP目标文件内存布局段功能File Header文件头，描述整个文件的文件属性（包括文件是否可执行、是静态链接或动态连接及入口地址、目标硬件、目标操作系统等）.text section代码段，执行语句编译成的机器代码.data section数据段，已初始化的全局变量和局部静态变量.bss sectionBSS 段（Block Started by Symbol），未初始化的全局变量和局部静态变量（因为默认值为 0，所以只是在此预留位置，不占空间）.rodata section只读数据段，存放只读数据，一般是程序里面的只读变量（如 const 修饰的变量）和字符串常量.comment section注释信息段，存放编译器版本信息.note.GNU-stack section堆栈提示段 Go pending_fin defer: 推迟执行, 一般用于做一些收尾工作 sync库 sync.Map: Go 语言原生 map 并不是线程安全的，对它进行并发读写操作的时候，需要加锁。而sync.Map是线程安全的，读取，插入，删除也都保持着常数级的时间复杂度。具体实现可参考: https://juejin.im/post/6844903895227957262 https://www.cnblogs.com/qcrao-2018/p/12833787.html sync.Pool: 临时对象池, 提供put/get方法, 临时对象池 sync.Pool 非常适用于在并发编程中用作临时对象缓存，实现对象的重复使用，优化 GC，提升系统性能，但是由于不能设置对象池大小，而且放进对象池的临时对象每次 GC 运行时会被清除，所以只能用作简单的临时对象池，不能用作持久化的长连接池，比如数据库连接池、Redis 连接池。 sync.Mutex: 互斥锁 sync.RWMutex: 读写锁 sync.Once: 类似pthread_once sync.Atomic: 原子数 select: pending_fin context: pending_fin 比如有一个网络请求Request，每个Request都需要开启一个goroutine做一些事情，这些goroutine又可能会开启其他的goroutine。这样的话， 我们就可以通过Context，来跟踪这些goroutine，并且通过Context来控制他们的目的，这就是Go语言为我们提供的Context，中文可以称之为“上下文”。另外一个实际例子是，在Go服务器程序中，每个请求都会有一个goroutine去处理。然而，处理程序往往还需要创建额外的goroutine去访问后端资源，比如数据库、RPC服务等。由于这些goroutine都是在处理同一个请求，所以它们往往需要访问一些共享的资源，比如用户身份信息、认证token、请求截止时间等。而且如果请求超时或者被取消后，所有的goroutine都应该马上退出并且释放相关的资源。这种情况也需要用Context来为我们取消掉所有goroutine goroutine协程调度参考: https://studygolang.com/articles/26795 https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-goroutine/#65-%E8%B0%83%E5%BA%A6%E5%99%A8 Golang 则引入了 G/P/M 模型来实现调度，那么 Golang 的运行时（runtime）如何实现对 goroutine 的调度从而合理分配 CPU 资源呢？ M:N模型，内核空间开启M个内核线程，一个内核空间线程对应N个用户空间线程。效率非常高，但是管理复杂。 本质上goroutine就是协程，但是完全运行在用户态，借鉴了M:N模型.相比其他语言，golang采用了MPG模型管理协程，更加高效，但是管理非常复杂。 M：内核级线程 G：代表一个goroutine P：Processor，处理器，用来管理和执行goroutine的。 G-M-P三者的关系与特点： P的个数取决于设置的GOMAXPROCS，go新版本默认使用最大内核数，比如你有8核处理器，那么P的数量就是8 M的数量和P不一定匹配，可以设置很多M，M和P绑定后才可运行，多余的M处于休眠状态。 P包含一个LRQ（Local Run Queue）本地运行队列，这里面保存着P需要执行的协程G的队列 除了每个P自身保存的G的队列外，调度器还拥有一个全局的G队列GRQ（Global Run Queue），这个队列存储的是所有未分配的协程G。 设计策略: 复用线程：避免频繁的创建、销毁线程，而是对线程的复用。 work stealing机制: 当本线程无可运行的G时，尝试从其他线程绑定的P偷取G，而不是销毁线程。 hand off机制: 当本线程因为G进行系统调用阻塞时，线程释放绑定的P，把P转移给其他空闲的线程执行。 利用并行：GOMAXPROCS设置P的数量，最多有GOMAXPROCS个线程分布在多个CPU上同时运行。GOMAXPROCS也限制了并发的程度，比如GOMAXPROCS = 核数/2，则最多利用了一半的CPU核进行并行。 抢占：在coroutine中要等待一个协程主动让出CPU才执行下一个协程，在Go中，一个goroutine最多占用CPU 10ms，防止其他goroutine被饿死，这就是goroutine不同于coroutine的一个地方。 全局G队列：在新的调度器中依然有全局G队列，但功能已经被弱化了，当M执行work stealing从其他P偷不到G时，它可以从全局G队列获取G。 网络安全最后就是网络安全，主要考察也是 WEB 安全，包括 XSS CSRF SQL注入 … XSS参考: https://tech.meituan.com/2018/09/27/fe-security.html 什么是 XSS? Cross-Site Scripting（跨站脚本攻击）简称 XSS，是一种代码注入攻击。攻击者通过在目标网站上注入恶意脚本，使之在用户的浏览器上运行。利用这些恶意脚本，攻击者可获取用户的敏感信息如 Cookie、SessionID 等，进而危害数据安全。 为了和 CSS 区分，这里把攻击的第一个字母改成了 X，于是叫做 XSS。XSS 的本质是：恶意代码未经过滤，与网站正常的代码混在一起；浏览器无法分辨哪些脚本是可信的，导致恶意脚本被执行。而由于直接在用户的终端执行，恶意代码能够直接获取用户的信息，或者利用这些信息冒充用户向网站发起攻击者定义的请求。在部分情况下，由于输入的限制，注入的恶意脚本比较短。但可以通过引入外部的脚本，并由浏览器执行，来完成比较复杂的攻击策略。 XSS是如何注入的这里有一个问题：用户是通过哪种方法“注入”恶意脚本的呢？不仅仅是业务上的“用户的 UGC 内容”可以进行注入，包括 URL 上的参数等都可以是攻击的来源。在处理输入时，以下内容都不可信： 来自用户的 UGC 信息 来自第三方的链接 URL 参数 POST 参数 Referer （可能来自不可信的来源） Cookie （可能来自其他子域注入） 举个简单例子:123function escape(s) &#123; return '&lt;script&gt;console.log("'+s+'");&lt;/script&gt;';&#125; 如果输入的s 为 &quot;);alert(1);// , 则将 return &lt;script&gt;console.log(&quot;&quot;);alert(1);//&quot;);&lt;/script&gt; , 这就会弹出警告窗口alert(1) 这就是恶意脚本注入 再举个真实的例子, 新浪微博名人堂反射型 XSS 漏洞: 攻击者发现 http://weibo.com/pub/star/g/xyyyd 这个 URL 的内容未经过滤直接输出到 HTML 中。于是攻击者构建出一个 URL，然后诱导用户去点击：http://weibo.com/pub/star/g/xyyyd&quot;&gt;&lt;script src=//xxxx.cn/image/t.js&gt;&lt;/script&gt;用户点击这个 URL 时，服务端取出请求 URL，拼接到 HTML 响应中：&lt;li&gt;&lt;a href=&quot;http://weibo.com/pub/star/g/xyyyd&quot;&gt;&lt;script src=//xxxx.cn/image/t.js&gt;&lt;/script&gt;&quot;&gt;按分类检索&lt;/a&gt;&lt;/li&gt;浏览器接收到响应后就会加载执行恶意脚本 //xxxx.cn/image/t.js，在恶意脚本中利用用户的登录状态进行关注、发微博、发私信等操作，发出的微博和私信可再带上攻击 URL，诱导更多人点击，不断放大攻击范围。这种窃用受害者身份发布恶意内容，层层放大攻击范围的方式，被称为 “XSS 蠕虫”。 如何防范XSS虽然很难通过技术手段完全避免 XSS，但我们可以总结以下原则减少漏洞的产生： 利用模板引擎 开启模板引擎自带的 HTML 转义功能。 避免拼接 HTML 前端采用拼接 HTML 的方法比较危险，如果框架允许，使用 createElement、setAttribute 之类的方法实现。或者采用比较成熟的渲染框架，如 Vue/React 等。 时刻保持警惕 在插入位置为 DOM 属性、链接等位置时，要打起精神，严加防范。 增加攻击难度，降低攻击后果 通过 CSP、输入长度配置、接口安全措施等方法，增加攻击的难度，降低攻击的后果。 主动检测和发现 可使用 XSS 攻击字符串和自动扫描工具寻找潜在的 XSS 漏洞。 举个例子: 某天，公司需要一个搜索页面，根据 URL 参数决定关键词的内容。小明很快把页面写好并且上线。代码如下：12345&lt;input type="text" value="&lt;%= getParameter("keyword") %&gt;"&gt;&lt;button&gt;搜索&lt;/button&gt;&lt;div&gt; 您搜索的关键词是：&lt;%= getParameter("keyword") %&gt;&lt;/div&gt; 然而，在上线后不久，小明就接到了安全组发来的一个神秘链接：http://xxx/search?keyword=&quot;&gt;&lt;script&gt;alert(&#39;XSS&#39;);&lt;/script&gt;小明带着一种不祥的预感点开了这个链接 [请勿模仿，确认安全的链接才能点开]。果然，页面中弹出了写着”XSS” 的对话框。 可恶，中招了！小明眉头一皱，发现了其中的奥秘： 当浏览器请求 http://xxx/search?keyword=&quot;&gt;&lt;script&gt;alert(&#39;XSS&#39;);&lt;/script&gt; 时，服务端会解析出请求参数 keyword，得到 &quot;&gt;&lt;script&gt;alert(&#39;XSS&#39;);&lt;/script&gt;，拼接到 HTML 中返回给浏览器。形成了如下的 HTML：12345&lt;input type="text" value=""&gt;&lt;script&gt;alert('XSS');&lt;/script&gt;"&gt;&lt;button&gt;搜索&lt;/button&gt;&lt;div&gt; 您搜索的关键词是："&gt;&lt;script&gt;alert('XSS');&lt;/script&gt;&lt;/div&gt; 浏览器无法分辨出 &lt;script&gt;alert(&#39;XSS&#39;);&lt;/script&gt; 是恶意代码，因而将其执行。这里不仅仅 div 的内容被注入了，而且 input 的 value 属性也被注入， alert 会弹出两次。面对这种情况，我们应该如何进行防范呢？其实，这只是浏览器把用户的输入当成了脚本进行了执行。那么只要告诉浏览器这段内容是文本就可以了。聪明的小明很快找到解决方法，把这个漏洞修复： 12345&lt;input type="text" value="&lt;%= escapeHTML(getParameter("keyword")) %&gt;"&gt;&lt;button&gt;搜索&lt;/button&gt;&lt;div&gt; 您搜索的关键词是：&lt;%= escapeHTML(getParameter("keyword")) %&gt;&lt;/div&gt; escapeHTML() 按照如下规则进行转义： | 字符 | 转义后的字符 | |-|-| |&amp;|&amp;amp;| |&lt;|&amp;lt;| |&gt;|&amp;gt;| |&quot;|&amp;quot;| |&#39;|&amp;#x27;| |/|&amp;#x2F;| 经过了转义函数的处理后，最终浏览器接收到的响应为： 12345&lt;input type="text" value="&amp;quot;&amp;gt;&amp;lt;script&amp;gt;alert(&amp;#x27;XSS&amp;#x27;);&amp;lt;&amp;#x2F;script&amp;gt;"&gt;&lt;button&gt;搜索&lt;/button&gt;&lt;div&gt; 您搜索的关键词是：&amp;quot;&amp;gt;&amp;lt;script&amp;gt;alert(&amp;#x27;XSS&amp;#x27;);&amp;lt;&amp;#x2F;script&amp;gt;&lt;/div&gt; 恶意代码都被转义，不再被浏览器执行，而且搜索词能够完美的在页面显示出来。 CSRF参考: https://tech.meituan.com/2018/10/11/fe-security-csrf.html CSRF（Cross-site request forgery）跨站请求伪造：攻击者诱导受害者进入第三方网站，在第三方网站中，向被攻击网站发送跨站请求。利用受害者在被攻击网站已经获取的注册凭证，绕过后台的用户验证，达到冒充用户对被攻击的网站执行某项操作的目的。 一个典型的CSRF攻击有着如下的流程： 1) 受害者登录a.com，并保留了登录凭证（Cookie）。2) 攻击者引诱受害者访问了b.com。3) b.com 向 a.com 发送了一个请求：a.com/act=xx。浏览器会默认携带a.com的Cookie。4) a.com接收到请求后，对请求进行验证，并确认是受害者的凭证，误以为是受害者自己发送的请求。5) a.com以受害者的名义执行了act=xx。6) 攻击完成，攻击者在受害者不知情的情况下，冒充受害者，让a.com执行了自己定义的操作。 CSRF 的特点 攻击一般发起在第三方网站，而不是被攻击的网站。被攻击的网站无法防止攻击发生。 攻击利用受害者在被攻击网站的登录凭证，冒充受害者提交操作；而不是直接窃取数据。 整个过程攻击者并不能获取到受害者的登录凭证，仅仅是 “冒用”。 跨站请求可以用各种方式：图片 URL、超链接、CORS、Form 提交等等。部分请求方式可以直接嵌入在第三方论坛、文章中，难以进行追踪。 CSRF 通常是跨域的，因为外域通常更容易被攻击者掌控。但是如果本域下有容易被利用的功能，比如可以发图和链接的论坛和评论区，攻击可以直接在本域下进行，而且这种攻击更加危险。 防护策略CSRF 通常从第三方网站发起，被攻击的网站无法防止攻击发生，只能通过增强自己网站针对 CSRF 的防护能力来提升安全性。 上文中讲了 CSRF 的两个特点： CSRF（通常）发生在第三方域名。 CSRF 攻击者不能获取到 Cookie 等信息，只是使用。 针对这两点，我们可以专门制定防护策略，如下： 阻止不明外域的访问 同源检测, 检查Referer字段 : HTTP头中有一个Referer字段，这个字段用以标明请求来源于哪个地址。在处理敏感数据请求时，通常来说，Referer字段应和请求的地址位于同一域名下。以上文银行操作为例，Referer字段地址通常应该是转账按钮所在的网页地址，应该也位于www.examplebank.com之下。而如果是CSRF攻击传来的请求，Referer字段会是包含恶意网址的地址，不会位于www.examplebank.com之下，这时候服务器就能识别出恶意的访问。 这种办法简单易行，工作量低，仅需要在关键访问处增加一步校验。但这种办法也有其局限性，因其完全依赖浏览器发送正确的Referer字段。虽然http协议对此字段的内容有明确的规定，但并无法保证来访的浏览器的具体实现，亦无法保证浏览器没有安全漏洞影响到此字段。并且也存在攻击者攻击某些浏览器，篡改其Referer字段的可能。CSRF大多数情况下来自第三方域名，但并不能排除本域发起。如果攻击者有权限在本域发布评论（含链接、图片等，统称UGC），那么它可以直接在本域发起攻击，这种情况下同源策略无法达到防护的作用。 Samesite Cookie : Chrome 51开始，浏览器的Cookie新增加了一个SameSite属性，用来防止CSRF攻击和用户追踪。Samesite有三个可选值，分别为Strict、Lax、None。 Strict：最严格模式，完全禁止第三方Cookie，跨站点访问时，任何情况下都不会发送Cookie。换言之，只有当前网页的 URL与请求目标一致，才会带上Cookie。此方式虽然安全，但是存在严重的易用性问题，用户从第三方页面访问一个已登录的系统时，由于未携带Cookie，总是需要重新登录。 Lax：Chrome默认模式，对于从第三方站点以link标签，a标签，GET形式的Form提交这三种方式访问目标系统时，会带上目标系统的Cookie，对于其他方式，如 POST形式的Form提交、AJAX形式的GET、img的src访问目标系统时，不到Cookie。 None：原始方式，任何情况都提交目标系统的Cookie。由于Samesite是Google提出来的，其他浏览器目前并未普及，存在兼容性问题，目前不推荐使用。 提交时要求附加本域才能获取的信息 CSRF Token : 而CSRF攻击之所以能够成功，是因为服务器误把攻击者发送的请求当成了用户自己的请求。那么我们可以要求所有的用户请求都携带一个CSRF攻击者无法获取到的Token。服务器通过校验请求是否携带正确的Token，来把正常的请求和攻击的请求区分开，也可以防范CSRF的攻击。 1. 将CSRF Token输出到页面中(这个token不存在session中, 用jwt来做这个token即可) 2. 页面提交的请求携带这个Token 3. 服务器验证Token是否正确 双重 Cookie 验证 : 1. 在用户访问网站页面时，向请求域名注入一个Cookie，内容为随机字符串（例如csrfcookie=v8g9e4ksfhw）。 2. 在前端向后端发起请求时，取出Cookie，并添加到URL的参数中（接上例POST https://www.a.com/comment?csrfcookie=v8g9e4ksfhw）。 3. 后端接口验证Cookie中的字段与URL参数中的字段是否一致，不一致则拒绝。 编码知识Base64 的原理？编码后比编码前是大了还是小了。结论: 大了. 因为Base64 编码本质上是一种将二进制数据转成文本数据的方案。对于非二进制数据，是先将其转换成二进制形式，然后每连续 6 比特（2 的 6 次方 = 64）计算其十进制值，根据该值在上面的索引表中找到对应的字符，最终得到一个文本字符串。也就是说, 每 3 个原始字符编码成 4 个字符，如果原始字符串长度不能被 3 整除，那怎么办？使用 0 值来补充原始字符串。 base64的原理Base64 编码之所以称为 Base64，是因为其使用 64 个字符来对任意数据进行编码，同理有 Base32、Base16 编码。标准 Base64 编码使用的 64 个字符为： 这 64 个字符是各种字符编码（比如 ASCII 编码）所使用字符的子集，基本，并且可打印。唯一有点特殊的是最后两个字符，因对最后两个字符的选择不同，Base64 编码又有很多变种，比如 Base64 URL 编码。 Base64 编码本质上是一种将二进制数据转成文本数据的方案。对于非二进制数据，是先将其转换成二进制形式，然后每连续 6 比特（2 的 6 次方 = 64）计算其十进制值，根据该值在上面的索引表中找到对应的字符，最终得到一个文本字符串。 假设我们要对 Hello! 进行 Base64 编码，按照 ASCII 表，其转换过程如下图所示： 可知 Hello! 的 Base64 编码结果为 SGVsbG8h ，原始字符串长度为 6 个字符，编码后长度为 8 个字符，每 3 个原始字符经 Base64 编码成 4 个字符，编码前后长度比 4/3，这个长度比很重要 - 比原始字符串长度短，则需要使用更大的编码字符集，这并不我们想要的；长度比越大，则需要传输越多的字符，传输时间越长。Base64 应用广泛的原因是在字符集大小与长度比之间取得一个较好的平衡，适用于各种场景。 是不是觉得 Base64 编码原理很简单？ 但这里需要注意一个点：Base64 编码是每 3 个原始字符编码成 4 个字符，如果原始字符串长度不能被 3 整除，那怎么办？使用 0 值来补充原始字符串。 以 Hello!! 为例，其转换过程为： 注：图表中蓝色背景的二进制 0 值是额外补充的。 Hello!! Base64 编码的结果为 SGVsbG8hIQAA 。最后 2 个零值只是为了 Base64 编码而补充的，在原始字符中并没有对应的字符，那么 Base64 编码结果中的最后两个字符 AA 实际不带有效信息，所以需要特殊处理，以免解码错误。 标准 Base64 编码通常用 = 字符来替换最后的 A，即编码结果为 SGVsbG8hIQ==。因为 = 字符并不在 Base64 编码索引表中，其意义在于结束符号，在 Base64 解码时遇到 = 时即可知道一个 Base64 编码字符串结束。 如果 Base64 编码字符串不会相互拼接再传输，那么最后的 = 也可以省略，解码时如果发现 Base64 编码字符串长度不能被 4 整除，则先补充 = 字符，再解码即可。 解码是对编码的逆向操作，但注意一点：对于最后的两个 = 字符，转换成两个 A 字符，再转成对应的两个 6 比特二进制 0 值，接着转成原始字符之前，需要将最后的两个 6 比特二进制 0 值丢弃，因为它们实际上不携带有效信息。 utf8编码和unicode字符集总结: unicode是个字符集, 只是一个符号对应表, 它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储 utf8是unicode符号具体的编码方式, 规定了该怎么存储 说到utf8，就不得不说一下unicode了。 Unicode是一个很大的集合，每一个unicode对应一个符号，不管是中文的汉字，英文字符，日文，韩文等等。现在的规模可以容纳100多万个符号。每个符号的编码都不一样，比如，U+0639表示阿拉伯字母 Ain，U+0041表示英语的大写字母A，U+4E25表示汉字“严”。具体的符号对应表，可以查询unicode.org，或者专门的汉字对应表。 需要注意的是，Unicode只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。 比如，汉字“严”的unicode是十六进制数4E25，转换成二进制数足足有15位（100111000100101），也就是说这个符号的表示至少需要2个字节。表示其他更大的符号，可能需要3个字节或者4个字节，甚至更多。 这里就有两个严重的问题，第一个问题是：如何才能区别unicode和ascii？计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？第二个问题是：我们已经知道，英文字母只用一个字节表示就够了，如果unicode统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。 它们造成的结果是： 1）出现了unicode的多种存储方式，也就是说有许多种不同的二进制格式，可以用来表示unicode。 2）unicode在很长一段时间内无法推广，直到互联网的出现。 UTF-8互联网的普及，强烈要求出现一种统一的编码方式。UTF-8就是在互联网上使用最广的一种unicode的实现方式。其他实现方式还包括UTF-16和UTF-32，不过在互联网上基本不用。重复一遍，这里的关系是，UTF-8是Unicode的实现方式之一。 UTF-8最大的一个特点，就是它是一种变长的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。 UTF-8的编码规则很简单，只有二条： 1）对于单字节的符号，字节的第一位（字节的最高位）设为0，后面7位为这个符号的unicode码。因此对于英语字母，UTF-8编码和ASCII码是相同的。 2）对于n字节的符号（n&gt;1），第一个字节的前n位都设为1，第n+1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的unicode码。 下表总结了编码规则，字母x表示可用编码的位。 Unicode符号范围 UTF-8编码方式(十六进制) | （二进制）12345—————+———————————————————————0000 0000-0000 007F | 0xxxxxxx0000 0080-0000 07FF | 110xxxxx 10xxxxxx0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 下面，还是以汉字“严”为例，演示如何实现UTF-8编码：已知“严”的unicode是4E25（100111000100101），根据上表，可以发现4E25处在第三行的范围内（0000 0800-0000 FFFF），因此“严”的UTF-8编码需要三个字节，即格式是“1110xxxx 10xxxxxx 10xxxxxx”。然后，从“严”的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。这样就得到了，“严”的UTF-8编码是“11100100 10111000 10100101”，转换成十六进制就是E4B8A5。 QUICQUIC的通讯过程在初次没有建立过连接时使用1-RTT的握手机制，同时保证连接的建立和达到安全的保障。以下是QUIC的1-RTT的握手过程： Server端会持有0-RTT公私钥对，并且生成SCFG（服务端的配置信息对象），把公钥放入SCFG中； 客户端初次请求时，需要向服务端获取0-RTT公钥，这个需要消耗一个RTT，这也QUIC的1-RTT的所在； 客户端在收到0-RTT公钥以后会缓存起来，同时生成自己的临时公私钥对，经过前面的一个RTT后客户端把自己的临时私钥与服务端发过来的0-RTT的公钥根据DH算法生成一个加密密钥K1，同时使用K1加密数据同时附送自己的临时公钥一起发送服务端，此时已有用户数据发送； 在服务端收到用户使用K1加密的用户数据和客户端发来的临时公钥以后，会做如下几件事： 使用0-RTT私钥与客户端发来的临时公钥通过DH算法生成K1解密用户数据并递交到应用； 生成服务端临时公私钥对，使用临时公私钥对的私钥，与客户端发来的客户端临时公钥，生成K2加密服务端要传输的数据 把服务端的临时公钥和使用K2加密的应用数据发送到客户端 客户端收到服务端发送的服务端临时公钥和使用K2加密的应用数据后会再次使用DH算法把服务端的临时公钥和客户端原来的临时私钥重新生成K2解密数据，并且从此以后使用K2进行数据层的加解密 备注：这里服务端为什么要重新再生成临时公私钥对再使用DH算法来生成加密密钥K2呢？ 其核心考虑到的是安全性，如果没有服务端的临时公私钥和K2，那么在通讯过程中使用的K1是不安全的，因为服务端的SCFG中的0-RTT公私钥是对所有客户端，并且长期保持直到过期，而且这个过期时间一般会比较长。一旦服务端的0-RTT私钥泄露则所有客户端的通讯都无法确保前向安全性了。攻击者只需要把包抓下来，获取到0-RTT私钥即可破解所有通讯数据。 QUIC握手流程图QUIC的0-RTT握手效率极大提升0-RTT是QUIC一个很关键的属性，能够在连接的第一个数据报文就可以携带用户数据。但是我们也可以看到如果客户端和服务端从来没有通讯过，那么是不存在0-RTT的，需要一个完成的RTT之后才能承载用户数据。 这个是QUIC的1-RTT过程，那么他的0-RTT又是怎么做的呢？其实很明显，客户端把0-RTT的握手公钥和相关信息保存起来，后续再建连接的时候就可以直接使用之前保存的数据了，只要这个数据没有过期，服务端都会承认的。因此可以避免掉公钥发送的这一个RTT，直接生成K1加密用户数据传输。 QUIC如何防止中间人攻击从前面的分析，我们可以看到SCFG的重要性非常关键，在0-RTT的场景完全依靠这个数据来获得0-RTT握手公钥，而且需要在客户端和服务端传输流转。那么它的安全可信就非常重要，QUIC是如何保障呢，如何防止中间人攻击呢，是否会带来其他安全风险？ 在QUIC中给这个数据增加了一个签名机制，签名是通过公有证书的私钥来签的，在客户端需要对证书进行认证，这样可以确保无法实现中间人攻击, 类似HTTPS 同时也设置了过期时间来保障安全性。SCFG的过期时间也可以很大程度上缓解SCFG被恶意收集。 QUIC的重放攻击问题 对于安全性要求比较高的业务操作，例如具备有POST或者PUT操作时为了确保安全性通常会把0-RTT关闭，包括像facebook或者cloudflare也都是在一些关键操作上禁用0-RTT功能，只有幂等操作（如GET、HEAD等）才使用0-RTT。 AOI接下来打算做一个国战类的MMO手游，国战类手游首相要解决的就是多人同屏AOI问题。稍微看了一下主流的解决方案，下面就把MMORPG手游AOI解决方案简单记录下， 希望能帮到大家。 目前最常见的是有两种解决方案，九宫格和十字链表。 九宫格主要思路是讲场景地图分成多个格子，每个格子记录其周围的格子信息。 九宫格AOI算法核心是把整个地图划分成大小相等的正方形格子，每个格子用一个数组存储在格子里的玩家，玩家的视野即上图中标了数字的九个格子（如果视野大小为2个格子，再往外扩一圈即可，依此类推）。 1.进入角色进入场景，根据其坐标，将其置于一个格子之中；然后向角色所在格子及其周围格子中的所有玩家发送add消息。 2.移动设玩家移动之前的九宫格集合为old_set，移动之后的集合为new_set，则向(old_set - new_set)集合中玩家发送leave消息；向(new_set - old_set)集合中玩家发送add消息；向（old_set &amp; old_set）集合中玩家发送move消息； 3.离开角色离开场景，向角色所在格子及其周围格子中的所有玩家发送leave消息。 优缺点: cpu消耗小, 宫格的优点是效率高，拿到坐标后即可跳转到对应的格子，视野范围内需要遍历的格子也不多，配合经典的格子地图（tile map）再合适不过，都不需要把像素坐标转格子坐标。 其缺点是占用内存有点大，因为必须为所有格子预留一个数组，即使是一个数组指针，长宽为1024的一个地图也要1024 1024 8 = 8M内存，这还不算真正要存数据的结构，仅仅是必须预留的。 内存开销大,内存消耗不仅和实体数有关,还和场景大小成正比 格子越大, 包含的多余的实体就越多, 还得通过半径计算来剔除这些多余的实体, 但是格子越小, 精度虽然越高, 多余的实体越少, 同等半径下要计算的格子就越多, cpu消耗高 十字链表首相，根据场景中所有角色的x坐标排序，将其放入一个链表x_list中；然后，根据场景中所有角色的y坐标排序，将其放入一个链表y_list中。这样，所有的角色同时位于两个链表中。 1.进入玩家进入时，根据x、y坐标排序，分别插入到x_list,y_list中。同时，根据可视距离，得到x_list中可视的角色集合x_set，y_set中可视的角色集合y_list,那么(x_set &amp; y_set)就是真正可视的角色集合，向其发送add消息 2.移动根据角色之前的位置可以得到old_set；移动之后，需要根据新的x、y坐标，重新找到角色在x_list，y_list中的位置，然后得到新的可见角色集合为new_set，则向(old_set - new_set)集合中玩家发送leave消息；向(new_set - old_set)集合中玩家发送add消息；向（old_set &amp; old_set）集合中玩家发送move消息； 3.离开向当前真正可视的角色发送levea消息，然后从x_list和y_list中删除即可。 优缺点: 每次移动需要重新更新角色在链表中的位置，然后再计算新的可见集合, 而九宫格不需要 浪费CPU misc etcd怎么选主的? etcd的leader选举过程 阿里巴巴面试官手册 消息队列原理 秒杀 分布式事务 日志记录 LRU缓存 分布式锁 HR面试 java的hashmap 是怎样实现的？ 秒杀系统的实现? cgi是啥? pending_fin 研究一下卡夫卡的热点面试题 pending_fin dns用的什么协议 DNS占用53号端口，同时使用TCP和UDP协议。那么DNS在什么情况下使用这两种协议？DNS在区域传输的时候使用TCP协议，其他时候使用UDP协议。 DNS区域传输的时候使用TCP协议: 辅域名服务器会定时（一般3小时）向主域名服务器进行查询以便了解数据是否有变动。如有变动，会执行一次区域传送，进行数据同步。区域传送使用TCP而不是UDP，因为数据同步传送的数据量比一个请求应答的数据量要多得多。 TCP是一种可靠连接，保证了数据的准确性。 域名解析时使用UDP协议：客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。不用经过三次握手，这样DNS服务器负载更低，响应更快。理论上说，客户端也可以指定向DNS服务器查询时用TCP，但事实上，很多DNS服务器进行配置的时候，仅支持UDP查询包。 为什么计算机用补码表示负数? 参考 什么是补码？ 也就是我们常说的取反加1它是一种数值的转换方法，要分二步完成：第一步，每一个二进制位都取相反值，0变成1，1变成0。比如，00001000的相反值就是11110111。第二步，将上一步得到的值加1。11110111就变成11111000。所以，00001000的2的补码就是11111000。也就是说，-8在计算机（8位机）中就是用11111000表示。 补码的好处? 在正常的加法规则下，可以利用2的补码得到正数与负数相加的正确结果。换言之，计算机只要部署加法电路和补码电路，就可以完成所有整数的加法。 如果不用补码, 在这种情况下，正常的加法规则不适用于正数与负数的加法，因此必须制定两套运算规则，一套用于正数加正数，还有一套用于正数加负数。从电路上说，就是必须为加法运算做两种电路。 nginx nginx架构图: 多进程的工作模式 master 进程主要工作: 读 nginx.conf 配置、 创建、绑定、关闭 Socket 创建、管理、关闭 worker 进程 其他管理工作 worker 进程主要工作: 处理网络事件 一个连接请求过来，每个进程都有可能处理这个连接，怎么做到的呢？首先，每个worker进程都是从master进程fork过来，在master进程里面，先建立好需要listen的socket（listenfd）之后，然后再fork出多个worker进程。所有worker进程的listenfd会在新连接到来时变得可读，为保证只有一个进程处理该连接，所有worker进程在注册listenfd读事件前抢accept_mutex，抢到互斥锁的那个进程注册listenfd读事件，在读事件里调用accept接受该连接。当一个worker进程在accept这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完整的请求就是这样的了。我们可以看到，一个请求，完全由worker进程来处理，而且只在一个worker进程中处理。 Nginx 多进程模型是如何实现高并发的？ 每进来一个request，会有一个worker进程去处理。但不是全程的处理，处理到什么程度呢？处理到可能发生阻塞的地方，比如向上游（后端）服务器转发request，并等待请求返回。那么，这个处理的worker不会这么傻等着，他会在发送完请求后，注册一个事件：“如果upstream返回了，告诉我一声，我再接着干”。于是他就休息去了。 此时，如果再有request 进来，他就可以很快再按这种方式处理。而一旦上游服务器返回了，就会触发这个事件，worker才会来接手，这个request才会接着往下走。由于web server的工作性质决定了每个request的大部份生命都是在网络传输中，实际上花费在server机器上的时间片不多。这是几个进程就解决高并发的秘密所在。webserver刚好属于网络io密集型应用，不算是计算密集型。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>noodle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在材质传值给GBuffer]]></title>
    <url>%2F2018%2F08%2F05%2Fzen_%E5%9C%A8%E6%9D%90%E8%B4%A8%E9%87%8C%E9%9D%A2%E7%A9%BF%E5%80%BC%E7%BB%99GBuffer%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[. . . 在材质里面传值给GBuffer在材质里面设置一个值如果需要在材质里面设置一个值，然后把这个值传给GBuffer，然后可以通过设置这个值来动态修改效果，就像材质实例一般。首先在材质里面传值，首先可以自定义一个 shadermodel，然后把这个 shadermodel 的customdata1 的pin 打开。当然也可以不通过 customdata1 来传值。也可以通过在材质细节里面，找到Num Customized UVs 改成1，然后通过这里 pin 来传值。当然也可以通过 Base Color, Metallic 这些，这一部其实只是为了传值进去而已。并无所谓是在哪里传值。而选择customdata1 的意义只是因为这个节点一般不会被使用到，传值到这里去并不会影响到其它。只要确定传进去的值不会被其它地方用到。怎么传其实都无所谓的。 把值设置给GBuffer打开引擎文件中的 Shaders/Private/ShadingModelsMaterial.ush，这个文件就是引擎由ShderModel 设置GBuffer 的地方。进到这个文件后，找到自定义的shader model，或者直接修改引擎自带的shader model。然后写下如下代码：12float TmpValue = saturate( GetMaterialCustomData1(MaterialParameters) );GBuffer.CustomData.y = TmpValue; 这样之后，GBuffer 中的 CustomData.y 就和材质里面你设置的值绑定了。然后你就可以在需要传值的地方传 GBuffer.CustomData.y 就可以了。 注意点这个做法首先要确保绑定的GBuffer 的值没有和其它效果关联。不然的话，修改值的同时也会影响到其它效果。]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
        <tag>Zen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[地图加载完毕后的回调]]></title>
    <url>%2F2018%2F07%2F20%2Fzen_map_load_finished_callback%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[在地图加载完后的回调首先自定义一个 UObject，UObject 可以跨地图存在。 . . . Object 如果切换地图的时候如果没有对它有引用的话，会自动释放掉，如果不让他释放的话，可以在GameInstance 里面用一个指针指向这个Obj。或者是Obj 生成的时候，在构造函数加上 1this-&gt;AddToRoot() 地图加载完成后的回调： 123// ExcuteFun 函数是回调会执行的函数FCoreUObjectDelegates::PostLoadMap.AddUObject(this, &amp;UMyObject::ExcuteFun);FCoreUObjectDelegates::PostLoadMapWithWorld.AddUObject(this, &amp;UMyObject::ExcuteFun); 上面的是老版本的用法，如 void UMyObject::ExcuteFun()){}下面的是新版本的用法，需要在绑定函数的参数添加上 UWorld* 的参数，如1void UMyObject::ExcuteFun(UWorld* world))&#123;&#125; 添加了AddToRoot函数，再使用完成后就必须要记得 1this-&gt;RemoveFromRoot()]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
        <tag>Zen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不受灯光颜色影响]]></title>
    <url>%2F2018%2F07%2F20%2Fzen_not_affected_by_the_color_of_the_light%2F</url>

    <encrypted>1</encrypted>

    <content type="text"><![CDATA[. . . 让物体不受光照的颜色影响 让物体不受光照颜色影响，总的来说，我的实现方式就是找到光照颜色的地方，然后对颜色去饱和度。去饱和度的代码大概如下： 123// 计算亮度，这个各个颜色分量可以自己感觉调整，我用的是虚幻默认的数值,alpha 可以调整去多少饱和度，用 0 ~ 1 表示，1表示完全去除float lum = Color.r * 0.3f + Color.g * 0.59f + Color.b * 0.11f;Color = lerp(Color, float4(lum, lum, lum, 0), alpha); 自定义一个shader model网上已经有比较完整的流程了，可以参考： 自定义shadingmodel 虚幻4渲染编程(材质编辑器篇)【第二卷：自定义光照模型】 最好结合着看，第一个的有点老，第二个的流程顺序有的不对。 首先修改普通光源如果已经按照上面的方法自定义shader model 的话，就可以在 DeferredLightingCommon.ush 这个文件，在shader model 的文章中的他们自定义的那个float3 AttenuationColor 变量，把他的式子删了，然后把他改成 123// 把灯光的颜色去饱和度float Lum = LightColor.r * 0.3f + LightColor.g * 0.59f + LightColor.b * 0.11f;AttenuationColor = lerp(LightColor, float4(Lum, Lum, Lum, 0), 1.0f) * (NoL * SurfaceAttenuation) ; 简单的说就是去饱和度。然后在ShadingModels.ush 文件里面的SurfaceShading 函数中，自定义的函数放在 12345678910111213141516171819202122232425float3 SurfaceShading( FGBufferData GBuffer, float3 LobeRoughness, float3 LobeEnergy, float3 L, float3 V, half3 N, uint2 Random )&#123; switch( GBuffer.ShadingModelID ) &#123; case SHADINGMODELID_UNLIT: case SHADINGMODELID_DEFAULT_LIT: case SHADINGMODELID_SUBSURFACE: case SHADINGMODELID_PREINTEGRATED_SKIN: case SHADINGMODELID_SUBSURFACE_PROFILE: case SHADINGMODELID_TWOSIDED_FOLIAGE: // 就是把自己添加的模块加进来,上面自定义模块教程里面的自己写的函数就可以不要了 case SHADINGMODELID_MyTestModel: return StandardShading( GBuffer.DiffuseColor, GBuffer.SpecularColor, LobeRoughness, LobeEnergy, L, V, N ); case SHADINGMODELID_CLEAR_COAT: return ClearCoatShading( GBuffer, LobeRoughness, LobeEnergy, L, V, N ); case SHADINGMODELID_CLOTH: return ClothShading( GBuffer, LobeRoughness, LobeEnergy, L, V, N ); case SHADINGMODELID_EYE: return EyeShading( GBuffer, LobeRoughness, LobeEnergy, L, V, N ); default: return 0; &#125;&#125; 这样的话，除了天光以外的光源颜色就不会对用了这个shader model 的材质造成影响。 处理天光天光分两步，一个是动态一个是非动态。动态天光可以在 SkyLighting.usf 文件的 1Lighting += DiffuseIrradiance * GBuffer.DiffuseColor * (GBuffer.GBufferAO * ScreenSpaceData.AmbientOcclusion); 为动态天光去饱和度这行代码下面为动态天光去饱和度 1234567Lighting += DiffuseIrradiance * GBuffer.DiffuseColor * (GBuffer.GBufferAO * ScreenSpaceData.AmbientOcclusion);if (ShadingModelId == SHADINGMODELID_MyTestModel)&#123; float Lum = Lighting.r * 0.3f + Lighting.g * 0.59f + Lighting.b * 0.11f; Lighting = lerp(Lighting, float4(Lum, Lum, Lum, 0), 1.0f) ;&#125; 为非动态天光去饱和度非动态天光就在 BasePassPixelShader.usf 文件的 GetPrecomputedIndirectLightingAndSkyLight(MaterialParameters, Interpolants, BasePassInterpolants, DiffuseDir, VolumetricLightmapBrickTextureUVs, DiffuseIndirectLighting, SubsurfaceIndirectLighting, IndirectIrradiance);这行代码下面添加判断让颜色去饱和度 123456if (GBuffer.ShadingModelID == SHADINGMODELID_MyTestModel)&#123; half DiffLum = DiffuseIndirectLighting.r * 0.3f + DiffuseIndirectLighting.g * 0.59f + DiffuseIndirectLighting.b * 0.11f; DiffuseIndirectLighting = lerp(DiffuseIndirectLighting, half4(DiffLum, DiffLum, DiffLum, 0), 0.8f);&#125;LightAccumulator_Add(LightAccumulator, Color, 0, 1.0f, false); 这样就可以让这个材质不会受到光照颜色的影响，但是会有明暗对比。 这样做之后，只要在材质里面使用这个自定义 shader model 的物体，就不会受到灯光颜色的影响，但是会有明暗对比。再次说明一次，lerp函数的第3个参数就是一个alpha 值，可以通过调整这个值来改变受光颜色的比例。我设置成 1， 所以不会受到灯光颜色的影响 出现的问题目前遇到的问题有，在受到的天光，纵使天光的颜色是白色的，也会和原来的颜色不会完全一样。总的来说，就是相对来说比原来偏白了一点。补上漫反射颜色后就和原来一样，但是这样就会受到漫反射颜色，导致了物体会受光的颜色，所以暂时没找到解决方法。]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
        <tag>Zen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sol2实现Cpp和Lua绑定]]></title>
    <url>%2F2018%2F06%2F20%2Flua_cpp_sol2%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Sol2简介Sol是一个用于C++绑定Lua脚本的库，仅由头文件组成，方便集成，并提供了大量易用的API接口，可以便利地将Lua脚本与C++代码绑定起来，而不必去关心如何使用那些晦涩的Lua C API。正如其作者所言，Sol的目的就是提供极其简洁的API，并能高效到与C语言媲美，极大地来方便人们使用。 编译条件Sol支持Lua的绝大多数版本，包括 5.1、5.2、5.3和LuaJit等，但由于代码中用到了许多C++11/14特性，因此编译时需要编译器支持C++14标准甚至C++17标准, 本人测试g++4.8.2无法编译过Sol2.20+的版本, 用g++6.2方能编过. 安装方法去 https://github.com/ThePhD/sol2 下载一个sol.hpp , 然后放到 /usr/local/include 里即可 为什么强大只需要包含一个sol.hpp头文件即可,需要任何其他的东西, 没有什么静态库/动态库之类的东西, 也不需要像tolua++一样那么麻烦每个类都要写pkg文件. 只需要稍微学习一下Sol2的导出API即可. . . . 基础使用从Sol的Github仓库clone下代码后，我们发现其目录下很多test开头的cpp/hpp文件，这些文件里面有着大量的Sol的使用示例以及各种特性的展示，而在example目录下的cpp文件都仅仅是一些最基础的使用示例。为了方便测试和体验Sol，你也可以自己建立一些自己的test.cpp文件，首先你要在源文件中include引用sol.hpp头文件，这样才能使用Sol提供的接口。而在使用gcc编译的时候，需要指定关联头文件的路径，可以使用类似于如下命令： g++ test.cpp -Isolpath/single/sol -llua -std=c++1z 其中solpath是你Sol2的具体路径，在Sol2的项目目录下，有一个single/sol/sol.hpp头文件，这个头文件集成了所有的相关代码到一起，所以编译时 -I 后仅指定这一个路径就可以了，同时要保证你的gcc编译器支持C++14或17标准。 一个简单例子例子目录结构如下 : ├─test_sol2.cpp ├─assert.hpp ├─test_sol2.lua ├─sol.hpp 编译命令 : g++ *.cpp -llua -std=c++1z test_sol2.cpp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#define SOL_CHECK_ARGUMENTS 1#include "sol.hpp"#include &lt;iostream&gt;#include "assert.hpp"int main()&#123; std::cout &lt;&lt; "=== namespacing ===" &lt;&lt; std::endl; struct my_class &#123; int b = 24; int f() const &#123; return 24; &#125; void g() &#123; ++b; &#125; &#125;; sol::state lua; lua.open_libraries(); // "bark" namespacing in Lua // namespacing is just putting things in a table sol::table bark = lua.create_named_table("bark"); bark.new_usertype&lt;my_class&gt;("my_class", "f", &amp;my_class::f, "g", &amp;my_class::g); // the usual // can add functions, as well (just like the global table) bark.set_function("print_my_class", [](my_class &amp;self) &#123; std::cout &lt;&lt; "my_class &#123; b: " &lt;&lt; self.b &lt;&lt; " &#125;" &lt;&lt; std::endl; &#125;); // // this works // lua.script("obj = bark.my_class.new()"); // lua.script("obj:g()"); // // calling this function also works // lua.script("bark.print_my_class(obj)"); // load and execute from file lua.script_file("test_sol2.lua"); my_class &amp;obj = lua["obj"]; c_assert(obj.b == 25); std::cout &lt;&lt; (obj.b == 25 ? "assert success" : "assert fail") &lt;&lt; std::endl; return 0;&#125; assert.hpp123456789101112131415161718192021222324252627282930313233343536373839404142#ifndef EXAMPLES_ASSERT_HPP#define EXAMPLES_ASSERT_HPP#ifdef SOL2_CIstruct pre_main &#123; pre_main() &#123; #ifdef _MSC_VER _set_abort_behavior(0, _WRITE_ABORT_MSG); #endif &#125;&#125; pm;#endif // Prevent lockup when doing Continuous Integration#ifndef NDEBUG#include &lt;exception&gt;#include &lt;iostream&gt;#include &lt;cstdlib&gt;# define m_assert(condition, message) \ do &#123; \ if (! (condition)) &#123; \ std::cerr &lt;&lt; "Assertion `" #condition "` failed in " &lt;&lt; __FILE__ \ &lt;&lt; " line " &lt;&lt; __LINE__ &lt;&lt; ": " &lt;&lt; message &lt;&lt; std::endl; \ std::terminate(); \ &#125; \ &#125; while (false)# define c_assert(condition) \ do &#123; \ if (! (condition)) &#123; \ std::cerr &lt;&lt; "Assertion `" #condition "` failed in " &lt;&lt; __FILE__ \ &lt;&lt; " line " &lt;&lt; __LINE__ &lt;&lt; std::endl; \ std::terminate(); \ &#125; \ &#125; while (false)#else# define m_assert(condition, message) do &#123; if (false) &#123; (void)(condition); \ (void)sizeof(message); &#125; &#125; while (false)# define c_assert(condition) do &#123; if (false) &#123; (void)(condition); &#125; &#125; while (false)#endif#endif // EXAMPLES_ASSERT_HPP test_sol2.lua123obj = bark.my_class.new()obj:g()bark.print_my_class(obj) 打印结果=== namespacing === my_class { b: 25 } assert success]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11的Raw String Literals]]></title>
    <url>%2F2018%2F06%2F17%2Fcpp11_raw_string%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[如何绕过 g++ 4.8.1 那个不能在宏里面使用 R”(…)” 的 bug？ 看到形如：R”” 这样的写法，相信学过 Python 的童鞋会感到似曾相识。Python 支持所谓的 “raw string”。Python 文档这样介绍 raw string： Both string and bytes literals may optionally be prefixed with a letter ‘r’ or ‘R’; such strings are called raw strings and treat backslashes as literal characters. As a result, in string literals, ‘\U’ and ‘\u’ escapes in raw strings are not treated specially. Given that Python 2.x’s raw unicode literals behave differently than Python 3.x’s the ‘ur’ syntax is not supported.从这段文字中我们可以看出，raw string 最大的特点就是：它不会对反斜杠’\’进行特殊的转义处理。那么，它的这一特性有什么好处呢？不用正则，不知 raw string 大法好！我们知道，正则表达式里，有很多元字符，当没有 raw string 时，我们需要在书写正则表达式的时候使用’\‘来表示元字符里的’\’，这样将导致正则表达式变得冗长，而且可读性也会降低。 . . . C++ 11 中的 raw string，简化了我们在使用 regex 库时正则表达式的书写。下面是我找到的一些资料： C++11 raw strings literals tutorialWikipedia: C++ 11 # New String Literals 示例代码123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;#include &lt;string&gt;int main()&#123; // 一个普通的字符串，'\n'被当作是转义字符，表示一个换行符。 std::string normal_str = "First line.\nSecond line.\nEnd of message.\n"; // 一个raw string，'\'不会被转义处理。因此，"\n"表示两个字符：字符反斜杠 和 字母n。 // 注意其语法格式，稍后会介绍C++ 11中为什么会采用这种语法格式来表达一个raw string。 std::string raw_str = R"(First line.\nSecond line.\nEnd of message.\n)"; std::cout &lt;&lt; normal_str &lt;&lt; std::endl; std::cout &lt;&lt; raw_str &lt;&lt; std::endl; std::cout &lt;&lt; R"foo(Hello, world!)foo" &lt;&lt; std::endl; // raw string可以跨越多行，其中的空白和换行符都属于字符串的一部分。 std::cout &lt;&lt;R"( Hello, world! )" &lt;&lt; std::endl; // 下面两行代码意图说明C++ 11采用一对圆括号以及自定义分割字符串来表示raw string的原因。 // 1. // 如果没有一对圆括号及空的分割字符串做定界处理，R"""将会出现语法错误。Python中，r"""也不会是一个合法的 // raw string literal。 std::cout &lt;&lt; R"(")" &lt;&lt; std::endl; // 输出一个双引号：" // 2. // 自定义分割字符串为：delimiter。分割字符串的长度以及其中包含的字符集，都有明文规定。维基百科： // The string delimiter can be any string up to 16 characters in length, including the empty string. // This string cannot contain spaces, control characters, '(', ')', or the '\' character. // // 如果不使用自定义分割字符串，这里：R"()")"编译器无法识别raw string在何处结束。自定义分割字符串的用途 // 维基百科中也有介绍： // The use of this delimiter string allows the user to have ")" characters within raw string literals. std::cout &lt;&lt; R"delimiter()")delimiter" &lt;&lt; std::endl; // 输出：)" return 0;&#125; 打印结果First line. Second line. End of message. First line.\nSecond line.\nEnd of message.\n Hello, world! Hello, world! &quot; )&quot; 分析上面这段代码及其中注释大致讲解了 C++ 11 中的 raw string 的特点。但是为什么我们要在字符串中使用一对小括号呢？我找到了如下资料： What is the rationale for parenthesis in C++11’s raw string literals R“(…)”? C++11 FAQ 中文版：原生字符串标识 所以，小伙伴们以后在 C++ 11 中书写正则表达式的时候，记得用 raw string literals 啊。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个轻量级的游戏服务器引擎]]></title>
    <url>%2F2018%2F05%2F02%2Fa_real_time_game_server_and_a_ue4_demo_for_it%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[一个轻量级的游戏服务器引擎 要点 业务层基于ECS框架来做开发, 继承实体基类与组件基类即可 基于etcd的 服务注册 / TTL / 服务发现 / 负载均衡 / 上报负载 / Watch机制 一体化 基于msgpack的RPC框架, 支持 ip地址直接call以及配合ECS的remote虚拟实体/组件直接call 基于asyncio异步IO的协程业务层支持, 可实现类似 result = await rpc_call() 的直接调RPC拿返回的效果 实现了协程池, 封装成了简洁的装饰器便于业务层调用 支持TCP与RUDP 基于sanic开发的异步HTTP微服务框架供方便开发各类公共服务 基于jwt的auth模块 基于redis的数据落地模块 基于umongo的ODM模块 热更新reload模块 全量式, 安全保障 增量式, 速度更快, 方便平时开发 支持异步的TimedRotating日志模块 根据日期时间自动滚动切换日志文件 支持协程对象的callback 根据日志level改变颜色, 方便查询 报trace可打印堆栈与locals 对于 warning 以上的日志级别直接对Pycharm提供文件跳转支持 支持1:N模型的定时器模块, 避免覆盖同一个key的易错点 可以重复使用一个key, 并不会冲掉之前key的timer, 但是当调用cancel_timer的时候, 会一次性全部cancel掉所有 制作了增强型json解析器, 支持注释/自动去除逗号/变量宏 基于MongoDB的数据落地模块 client端的模拟与自动化测试配套 大厅服务器的前置网关gate服务器, 负责压缩/解压, 加密/解密数据以及鉴权 架构图本架构图根据 PlantUML 自动生成]]></content>
      <categories>
        <category>GitHub</category>
      </categories>
      <tags>
        <tag>UE4</tag>
        <tag>GitHub</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跨平台开发之CMake笔记]]></title>
    <url>%2F2018%2F04%2F21%2Fcmake_tutorial%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[因为近来需要把一些 Linux 项目转到 Windows 上来开发, 所以有了一些跨平台开发的笔记, 此篇讲CMake, 供以后查阅. CMake介绍你或许听过好几种 Make 工具，例如 GNU Make ，QT 的 qmake ，微软的 MS nmake，BSD Make（pmake），Makepp，等等。这些 Make 工具遵循着不同的规范和标准，所执行的 Makefile 格式也千差万别。这样就带来了一个严峻的问题：如果软件想跨平台，必须要保证能够在不同平台编译。而如果使用上面的 Make 工具，就得为每一种标准写一次 Makefile ，这将是一件让人抓狂的工作。 就是针对上面问题所设计的工具：它首先允许开发者编写一种平台无关的 CMakeList.txt 文件来定制整个编译流程，然后再根据目标用户的平台进一步生成所需的本地化 Makefile 和工程文件，如 Unix 的 Makefile 或 Windows 的 Visual Studio 工程。从而做到“Write once, run everywhere”。 显然， . . . CMake 是一个比上述几种 make 更高级的编译配置工具。一些使用 CMake 作为项目架构系统的知名开源项目有 VTK、ITK、KDE、OpenCV、OSG 等。 CMake一些有用的网站 CMake官网 CMake入门实战 进阶 将build和项目源文件分离的方法假设项目A的根目录下有一个 CMakeLists, 在项目的根目录新建一个叫 build 的文件夹, 然后进入 build 文件夹内, 执行命令 cmake .. 即可. CMakeLists实例讲解比如有一个目录结构如下的项目 : ├─RealTimeServer ├─RealTimeServer │ ├─CMakeFiles.txt │ ├─a.h │ ├─a.cpp │ ├─b.h │ ├─b.cpp │ ├─TestFolder │ │ ├─a.h │ │ ├─a.cpp │ │ ├─b.h │ │ ├─b.cpp │ └─... ├─Tool 这是一个比较通用的CMakeLists.txt : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120# CMake 最低版本号要求cmake_minimum_required (VERSION 2.8)# support C++11add_definitions(-std=c++11)# 项目信息set (PROJ_NAME RealTimeServer)set (BIN_NAME rts)project ($&#123;PROJ_NAME&#125;)# 设置执行文件输出目录# SET(EXECUTABLE_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;/bin)# 设置库输出路径# SET(LIBRARY_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;/lib)# 查找当前目录下的所有源文件# 并将名称保存到 DIR_SRCS 变量# aux_source_directory(./$&#123;PROJ_NAME&#125; DIR_SRCS)# 查找当前目录以及子目录下的所有头文件# 并将名称保存到 CURRENT_HEADERS 变量# file(GLOB_RECURSE CURRENT_HEADERS *.h *.hpp)# 此命令可以用来收集源文件 CURRENT_HEADERS 作为变量保存收集的结果。 # 后面为文件过滤器，其中 PROJ_NAME 为起始搜索的文件夹，即在 RealTimeServer 目录下，# 开始收集，而且会遍历子目录# file(# GLOB_RECURSE CURRENT_HEADERS # LIST_DIRECTORIES false# "$&#123;PROJ_NAME&#125;/*.h*"# )# 生成一个名为Include的VS筛选器# source_group("Include" FILES $&#123;CURRENT_HEADERS&#125;) IF(WIN32) # Check if we are on Windows file(GLOB_RECURSE project_headers *.h) file(GLOB_RECURSE project_cpps *.c*) set(all_files $&#123;project_headers&#125; $&#123;project_cpps&#125;) macro(create_filters source_files) if(MSVC) # 获取当前目录 set(current_dir $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;) foreach(src_file $&#123;$&#123;source_files&#125;&#125;) # 求出相对路径 string(REPLACE $&#123;current_dir&#125;/ "" rel_path_name $&#123;src_file&#125;) # 删除相对路径中的文件名部分 string(REGEX REPLACE "(.*)/.*" \\1 rel_path $&#123;rel_path_name&#125;) # 比较是否是当前路径下的文件 string(COMPARE EQUAL $&#123;rel_path_name&#125; $&#123;rel_path&#125; is_same_path) # 替换成Windows平台的路径分隔符 string(REPLACE "/" "\\" rel_path $&#123;rel_path&#125;) if(is_same_path) set(rel_path "\\") endif(is_same_path) # CMake 命令 source_group($&#123;rel_path&#125; FILES $&#123;src_file&#125;) endforeach(src_file) endif(MSVC) endmacro(create_filters) create_filters(all_files) # add_executable(app $&#123;all_files&#125;) # 指定生成目标 add_executable($&#123;PROJ_NAME&#125; $&#123;all_files&#125;) if(MSVC) # Check if we are using the Visual Studio compiler # set_target_properties($&#123;PROJ_NAME&#125; PROPERTIES LINK_FLAGS "/SUBSYSTEM:WINDOWS") # works for all build modes set_target_properties($&#123;PROJ_NAME&#125; PROPERTIES LINK_FLAGS "/SUBSYSTEM:CONSOLE") # works for all build modes target_link_libraries($&#123;PROJ_NAME&#125; wsock32 ws2_32) set_target_properties($&#123;PROJ_NAME&#125; PROPERTIES COMPILE_FLAGS /wd"4819" ) elseif(CMAKE_COMPILER_IS_GNUCXX) # SET(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -mwindows") # Not tested else() message(SEND_ERROR "You are using an unsupported Windows compiler! (Not MSVC or GCC)") endif()elseif(UNIX) file(GLOB_RECURSE all_files "*.*") include_directories($&#123;PROJECT_SOURCE_DIR&#125;) # add_subdirectory(muduo/base) # add_subdirectory(muduo/net) # 指定生成目标 add_executable($&#123;BIN_NAME&#125; $&#123;all_files&#125;) # 添加链接库 target_link_libraries($&#123;BIN_NAME&#125; pthread rt) # Inhibit all warning messages. if(CMAKE_COMPILER_IS_GNUCC OR CMAKE_COMPILER_IS_GNUCXX) # set(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -Wall -Wno-long-long -pedantic") set(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -w") endif() # For gdb set(CMAKE_BUILD_TYPE "Debug") set(CMAKE_CXX_FLAGS_DEBUG "$ENV&#123;CXXFLAGS&#125; -O0 -Wall -g -ggdb") set(CMAKE_CXX_FLAGS_RELEASE "$ENV&#123;CXXFLAGS&#125; -O3 -Wall")else() message(SEND_ERROR "You are on an unsupported platform! (Not Win32 or Unix)")ENDIF() 对于像上面这样一个CMake的CMakeLists来说, 需要着重解释的有以下几点 : add_definitions(-std=c++11) 这句是为了解决 linux 默认不支持 c++11 的问题 set_target_properties(${PROJ_NAME} PROPERTIES LINK_FLAGS &quot;/SUBSYSTEM:WINDOWS&quot;) # works for all build modes 这句是为了解决WinMain的问题, 否则在vs平台会报main非法引用的问题. 这句会影响到vs的 “项目属性-链接器-系统-子系统” target_link_libraries(${PROJ_NAME} wsock32 ws2_32) 这句是为了在vs下链接ws2_32库, windows需要链接这个库才能用socket. 这句会影响到vs的 “项目属性-链接器-输入-附加依赖项” file(GLOB_RECURSE CURRENT_HEADERS *.h *.hpp) source_group(&quot;Include&quot; FILES ${CURRENT_HEADERS}) 这句是为了解决在vs下不显示头文件的问题 set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -w&quot;)-w的意思是关闭编译时的警告，也就是编译后不显示任何warning，因为有时在编译之后编译器会显示一些例如数据转换之类的警告，这些警告是我们平时可以忽略的。-Wall选项意思是编译后显示所有警告。-W选项类似-Wall，会显示警告，但是只显示编译器认为会出现错误的警告。在编译一些项目的时候可以-W和-Wall选项一起使用。这里可以查看gcc的各种警告级别.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>CrossPlatform</tag>
        <tag>Compile</tag>
        <tag>Make</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fec intro]]></title>
    <url>%2F2018%2F04%2F06%2Ffec_intro%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[名词解释FEC：Forward Error Correction，前向纠错 FEC 是一种通过在网络传输中增加数据包的冗余信息，使得接收端能够在网络发生丢包后利用这些冗余信息直接恢复出丢失的数据包的一种方法。 FEC 的基础理论：异或异或的规则 两个值不相等则为 1，相等则为 0； 12340 ^ 0 = 01 ^ 1 = 00 ^ 1 = 11 ^ 0 = 1 注：按位异或 ^，则是把两个数转换为二进制，按位进行异或运算。 异或的特性 1234恒等律：X ^ 0 = X归零律：X ^ X = 0交换律：A ^ B = B ^ A结合律：A ^ (B ^ C) = (A ^ B) ^ C 注：可以通过数学方法推导证明，我们这里只需要记住这些规则即可，后面有大量的应用。 XOR 的应用案例有了这些 XOR 的基础理论，我们看看它是怎么应用到实际中的 “校验” 和 “纠错” 的。 奇偶校验（Parity Check） 判断一个二进制数中 1 的数量是奇数还是偶数（应用了异或的 恒等律 和 归零律）： 123// 例如：求 10100001 中 1 的数量是奇数还是偶数// 结果为 1 就是奇数个 1，结果为 0 就是偶数个 11 ^ 0 ^ 1 ^ 0 ^ 0 ^ 0 ^ 0 ^ 1 = 1 这条性质可用于奇偶校验（Parity Check），每个字节的数据都计算一个校验位，数据和校验位一起发送出去，这样接收方可以根据校验位粗略地判断接收到的数据是否有误。 磁盘阵列 - RAID5 使用 3 块磁盘（A、B、C）组成 RAID5 阵列来存储用户的数据，把每份数据切分为 A、B 两部分，然后把 A xor B 的结果作为 C ，分别写入 A、B、C 三块磁盘。最终，任意一块磁盘出错，都是可以通过另外两块磁盘的数据进行恢复的。 实现原理：应用了异或的 恒等律 和 结合律 123c = a ^ ba = a ^ (b ^ b) = (a ^ b) ^ b = c ^ bb = (a ^ a) ^ b = a ^ c 基于 XOR 的 FEC 假设网络通信有 N 个 packet 需要发送，那么，可以类似上述 RAID5 的策略，每 2 个 packet 生成一个 FEC packet，这样，连续的 3 个 packet 的任意一个 packet 丢失，都能通过另外 2 个恢复出来的。 但考虑到每 2 个 packet 就产生 1 个 fec packet，冗余度可能有点高（比较浪费带宽），我们能否每 3 个或者每 N 个 packet 再产生一个 fec packet 呢？当然可以，我们以每 3 个 packet（A、B、C） 产生 1 个 fec packet（D）为例来推导一下： 1234d = a ^ b ^ ca = a ^ (b ^ b) ^ (c ^ c) = (b ^ c) ^ (a ^ b ^ c) = b ^ c ^ db = (a ^ a) ^ b ^ (c ^ c) = (a ^ c) ^ (a ^ b ^ c) = a ^ c ^ dc = (a ^ a) ^ (b ^ b) ^ c = (a ^ b) ^ (a ^ b ^ c) = a ^ b ^ d 由上述公式推导即可知道，这 4 个 packet，任意丢失 1 个 packet，均可以由其他 3 个 packet 恢复出来。 对象存储 - EC 纠删码 一些互联网云计算公司提供的对象存储服务，都会宣称自己具有极高的数据可靠性，使用了如三副本技术、EC 纠删码技术等等，后者大致方案如图所示： 图中采用的是 8+4 的纠删码策略（即：原始数据切割为 8 份，计算出 4 份冗余信息），将这 12 份分别存储在 不同机柜的 12 台不同节点上，即使同一时刻出现多台节点（至多 4 台）损坏或不可访问，只要有不少于 8 个节点可用，数据即可恢复。 不知道大家看出来点什么没有？相比于上面基于 N 个 packet 产生 1 个 FEC packet 的方案，这种 K + M 的纠删码策略具有更好的扛丢失能力，总结下来就是： 通过 K 个有效数据，产生 M 个 FEC 冗余包，这 K + M 个数据，任意丢失 M 个数据，都能把 K 个有效数据恢复出来。 其实这种方案，最早也是应用于网络传输领域的，只不过被借用到存储领域来提高磁盘的利用率。要实现这种 K + M 的 FEC 策略，使用简单的 XOR 异或来推导比较难，需要借助矩阵相关的计算，实现方案有很多种，下面简单介绍下最著名和常用的 Reed-solomon codes。 Reed-Solomon Codes里德 - 所罗门码（Reed-solomon codes，简称 RS codes），利用该原理实现的 FEC 策略，通常也叫做 RS-FEC。网上关于它的介绍特别多，本文就不详细展开了，仅简单以示意图的形式给出大致的原理： RS codes 编码过程 大致原理如下：假设有效数据有 K 个，期望生成 M 个 FEC 数据 把 K 个有效数据组成一个单位向量 D 生成一个变换矩阵 B：由一个 K 阶的单位矩阵 和一个 K * M 的范德蒙特 矩阵（Vandemode）组成 两个矩阵相乘得到的矩阵 G，即包含了 M 个冗余的 FEC 数据 RS codes 解码过程 假设数据 D1，D4，C2 丢失了，则取对应行的范德蒙矩阵的逆 * 没有丢失的数据矩阵，则可以恢复出原始的数据矩阵。 大致原理如下：假设数据 D1，D4，C2 丢失了 对矩阵 B 和 D，分别取没有丢失的行构成 B‘ 和 G’ 根据如下公式，即可计算恢复出有效数据向量 D 1B&apos; x D = G&apos; -&gt;&gt;&gt; D = B&apos; 的逆 x G&apos; 参考文章 感受异或的神奇 纠删码 Erasure Code https://zhuanlan.zhihu.com/p/104579290]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>FEC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git设置代理]]></title>
    <url>%2F2018%2F03%2F11%2Fgit_shadow_socks_proxy%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[介绍目前，网络上可选择的Git远程仓库比较多，其中用的较多的可能就是github和bitbucket（当然，你也可以使用自己搭建的远程仓库）。github和bitbucket的主要区别在于：bitbucket创建私人库是免费的。如果你不介意自己的代码公开，那你就可以使用github。如果，你有些私人的代码的话，又需要版本控制，这时候bitbucket就满足需要了。 但是，在国内的主要问题是：网络不稳定。这样，就会经常发生git不能push的情况。所以，这时候如果你有个代理服务器，就可以通过设置使git通过代理访问远程仓库，达到家和公司代码同步的目的。 Git允许使用三种协议来连接远程仓库：ssh、http、git。所以，如果你要设置代理， . . . 必须首先明确本地git使用何种协议连接远程仓库，然后根据不同协议设置代理。 本文前提socks5代理服务器，默认端口1080 设置SSH协议的代理如果你的远程仓库拥有如下的格式： git@github.com:archerie/learngit.git 那么，你使用的是SSH协议连接的远程仓库。因为git依赖ssh去连接，所以， 我们需要配置ssh的socks5代理实现git的代理。在ssh的配置文件~/.ssh/config（没有则新建）使用ProxyCommand配置：1234567891011121314151617181920212223242526272829# Linux 环境Host bitbucket.orgHostname bitbucket.orgUser gitPort 22ProxyCommand nc -x 127.0.0.1:1080 %h %portability# Linux 环境Host github github.comHostname github.comUser gitPort 22ProxyCommand nc -x 127.0.0.1:1080 %h %portability# windows 环境Host bitbucket.orgUser gitPort 22Hostname bitbucket.orgProxyCommand connect -S 127.0.0.1:1080 %h %portability# windows 环境Host github github.comHostname github.comPort 22User git# 这里的 -a none 是 NO-AUTH 模式，参见 https://bitbucket.org/gotoh/connect/wiki/Home 中的 More detail 一节# 之前写成ProxyCommand connect -S 127.0.0.1:1080 %h %p遇到过git bash不能clone项目的问题ProxyCommand connect -S 127.0.0.1:1080 -a none %h %p 设置http/https协议代理如果你的远程仓库链接拥有如下格式：12http://github.com/archerie/learngit.githttps://github.com/archerie/learngit.git 说明你使用的是http/https协议，所以可以使用git配套的CMSSW支持的代理协议：SOCKS4、SOCKS5和HTTPS/HTTPS。可通过配置http.proxy配置, 有两种方式分别为： 直接编辑.gitconfig文件的方式可以直接编辑 git 的设置文件，该文件通常位于用户目录下，名为 .gitconfig，如果看不到，需要显示隐藏文件。在 .gitconfig 文件的末尾加上1234[https] proxy = https://127.0.0.1:1080[http] proxy = http://127.0.0.1:1080 敲命令的方式当然，你也可以敲命令, 命令如下：12git config --global http.proxy http://127.0.0.1:1080git config --global https.proxy https://127.0.0.1:1080 经过测试，不需要设置sock5。取消的命令如下：12git config --global --unset http.proxygit config --global --unset https.proxy 还可以设置只本次用一下代理 :12# 本次设置git clone https://github.com/example/example.git --config &quot;http.proxy=127.0.0.1:1080&quot; 设置Git协议的代理Git协议是Git提供的一个守护进程，它监听专门的端口（9418），然后提供类似于ssh协议一样的服务，只是它不需要验证。所以，然后用户通过网络都可以使用git协议连接提供git连接的仓库。如果远程仓库的链接是如下形式： git://github.com/archerie/learngit.git 那么，该仓库使用git协议连接。所以，需要使用CMSSW提供的简单脚本去通过socks5代理访问：git-proxy。配置如下： 12git config --global core.gitproxy &quot;git-proxy&quot;git config --global socks.proxy &quot;localhost:1080&quot; 还想了解更多，使用git-proxy –help。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS备忘]]></title>
    <url>%2F2018%2F02%2F01%2Fcentos_notes%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[查询系统相关信息$ cat /etc/centos-release CentOS release 6.7 (Final) [root@host ~]# uname -i i386 安装gcc4.8centos 6 的gcc版本太低才4.4.7, 要安装高版本的4.8才完全支持c++11 . . . 卸载gcc先卸载当前gcc sudo yum remove --skip-broken gcc 安装步骤安装 devtoolset-2 工具链 wget http://people.centos.org/tru/devtools-2/devtools-2.repo -O /etc/yum.repos.d/devtools-2.repo yum install devtoolset-2-gcc devtoolset-2-binutils devtoolset-2-gcc-c++ sudo yum install devtoolset-2 启用 devtoolset-2 bash 环境 scl enable devtoolset-2 bash 检查gcc版本看是否安装成功 gcc --version 为了配合cmake, 需要创建相关的软链, cmake才找得到相关编译器 ln -s $(which gcc) /usr/bin/cc ln -s $(which g++) /usr/bin/c++ 以后每次想启用 devtoolset-2 bash 环境来使用gcc都需要命令 : scl enable devtoolset-2 bash 当然你也可以把下面这条语句加入到你的 .bashrc 里来让 devtoolset-2 bash 环境一直保持开启 : source /opt/rh/devtoolset-2/enable]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3分钟理解一致性hash]]></title>
    <url>%2F2018%2F01%2F27%2F3%E5%88%86%E9%92%9F%E7%90%86%E8%A7%A3%E4%B8%80%E8%87%B4%E6%80%A7hash%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[在了解一致性哈希算法之前，最好先了解一下缓存中的一个应用场景，了解了这个应用场景之后，再来理解一致性哈希算法，就容易多了，也更能体现出一致性哈希算法的优点，那么，我们先来描述一下这个经典的分布式缓存的应用场景。 场景描述假设，我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为0号、1号、2号，现在，有3万张图片需要缓存，我们希望这些图片被均匀的缓存到这3台服务器上，以便它们能够分摊缓存的压力。也就是说，我们希望每台服务器能够缓存1万张左右的图片，那么，我们应该怎样做呢？如果我们没有任何规律的将3万张图片平均的缓存在3台服务器上，可以满足我们的要求吗？可以！但是如果这样做，当我们需要访问某个缓存项时，则需要遍历3台缓存服务器，从3万个缓存项中找到我们需要访问的缓存，遍历的过程效率太低，时间太长，当我们找到需要访问的缓存项时，时长可能是不能被接收的，也就失去了缓存的意义，缓存的目的就是提高速度，改善用户体验，减轻后端服务器压力，如果每次访问一个缓存项都需要遍历所有缓存服务器的所有缓存项，想想就觉得很累，那么，我们该怎么办呢？原始的做法是对缓存项的键进行哈希，将hash后的结果对缓存服务器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上，这样说可能不太容易理解，我们举例说明，仍然以刚才描述的场景为例，假设我们使用图片名称作为访问图片的key，假设图片名称是不重复的，那么，我们可以使用如下公式，计算出图片应该存放在哪台服务器上。 hash（图片名称）% N . . . 因为图片的名称是不重复的，所以，当我们对同一个图片名称做相同的哈希计算时，得出的结果应该是不变的，如果我们有3台服务器，使用哈希后的结果对3求余，那么余数一定是0、1或者2，没错，正好与我们之前的服务器编号相同，如果求余的结果为0， 我们就把当前图片名称对应的图片缓存在0号服务器上，如果余数为1，就把当前图片名对应的图片缓存在1号服务器上，如果余数为2，同理，那么，当我们访问任意一个图片的时候，只要再次对图片名称进行上述运算，即可得出对应的图片应该存放在哪一台缓存服务器上，我们只要在这一台服务器上查找图片即可，如果图片在对应的服务器上不存在，则证明对应的图片没有被缓存，也不用再去遍历其他缓存服务器了，通过这样的方法，即可将3万张图片随机的分布到3台缓存服务器上了，而且下次访问某张图片时，直接能够判断出该图片应该存在于哪台缓存服务器上，这样就能满足我们的需求了，我们暂时称上述算法为HASH算法或者取模算法，取模算法的过程可以用下图表示。 但是，使用上述HASH算法进行缓存时，会出现一些缺陷，试想一下，如果3台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？没错，很简单，多增加两台缓存服务器不就行了，假设，我们增加了一台缓存服务器，那么缓存服务器的数量就由3台变成了4台，此时，如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在的服务器编号必定与原来3台服务器时所在的服务器编号不同，因为除数由3变为了4，被除数不变的情况下，余数肯定不同，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变，换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据，同理，假设3台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从3台变为2台，如果想要访问一张图片，这张图片的缓存位置必定会发生改变，以前缓存的图片也会失去缓存的作用与意义，由于大量缓存在同一时间失效，造成了缓存的雪崩，此时前端缓存已经无法起到承担部分压力的作用，后端服务器将会承受巨大的压力，整个系统很有可能被压垮，所以，我们应该想办法不让这种情况发生，但是由于上述HASH算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，一致性哈希算法诞生了。 我们来回顾一下使用上述算法会出现的问题。 问题1：当缓存服务器数量发生变化时，会引起缓存的雪崩，可能会引起整体系统压力过大而崩溃（大量缓存同一时间失效）。 问题2：当缓存服务器数量发生变化时，几乎所有缓存的位置都会发生改变，怎样才能尽量减少受影响的缓存呢？ 其实，上面两个问题是一个问题，那么，一致性哈希算法能够解决上述问题吗？ 我们现在就来了解一下一致性哈希算法。 一致性哈希算法的基本概念其实，一致性哈希算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性哈希算法是对2^32取模，什么意思呢？我们慢慢聊。 首先，我们把二的三十二次方想象成一个圆，就像钟表一样，钟表的圆可以理解成由60个点组成的圆，而此处我们把这个圆想象成由2^32个点组成的圆，示意图如下： 圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^32-1,也就是说0点左侧的第一个点代表2^32-1 我们把这个由2的32次方个点组成的圆环称为hash环。 那么，一致性哈希算法与上图中的圆环有什么关系呢？我们继续聊，仍然以之前描述的场景为例，假设我们有3台缓存服务器，服务器A、服务器B、服务器C，那么，在生产环境中，这三台服务器肯定有自己的IP地址，我们使用它们各自的IP地址进行哈希计算，使用哈希后的结果对2^32取模，可以使用如下公式示意。 hash（服务器A的IP地址） % 2^32 通过上述公式算出的结果一定是一个0到2^32-1之间的一个整数，我们就用算出的这个整数，代表服务器A，既然这个整数肯定处于0到2^32-1之间，那么，上图中的hash环上必定有一个点与这个整数对应，而我们刚才已经说明，使用这个整数代表服务器A，那么，服务器A就可以映射到这个环上，用下图示意 同理，服务器B与服务器C也可以通过相同的方法映射到上图中的hash环中 hash（服务器B的IP地址） % 2^32 hash（服务器C的IP地址） % 2^32 通过上述方法，可以将服务器B与服务器C映射到上图中的hash环上，示意图如下 假设3台服务器映射到hash环上以后如上图所示（当然，这是理想的情况，我们慢慢聊）。 好了，到目前为止，我们已经把缓存服务器与hash环联系在了一起，我们通过上述方法，把缓存服务器映射到了hash环上，那么使用同样的方法，我们也可以将需要缓存的对象映射到hash环上。 假设，我们需要使用缓存服务器缓存图片，而且我们仍然使用图片的名称作为找到图片的key，那么我们使用如下公式可以将图片映射到上图中的hash环上。 hash（图片名称） % 2^32 映射后的示意图如下，下图中的橘黄色圆形表示图片 好了，现在服务器与图片都被映射到了hash环上，那么上图中的这个图片到底应该被缓存到哪一台服务器上呢？上图中的图片将会被缓存到服务器A上，为什么呢？因为从图片的位置开始，沿顺时针方向遇到的第一个服务器就是A服务器，所以，上图中的图片将会被缓存到服务器A上，如下图所示。 没错，一致性哈希算法就是通过这种方法，判断一个对象应该被缓存到哪台服务器上的，将缓存服务器与被缓存对象都映射到hash环上以后，从被缓存对象的位置出发，沿顺时针方向遇到的第一个服务器，就是当前对象将要缓存于的服务器，由于被缓存对象与服务器hash后的值是固定的，所以，在服务器不变的情况下，一张图片必定会被缓存到固定的服务器上，那么，当下次想要访问这张图片时，只要再次使用相同的算法进行计算，即可算出这个图片被缓存在哪个服务器上，直接去对应的服务器查找对应的图片即可。 刚才的示例只使用了一张图片进行演示，假设有四张图片需要缓存，示意图如下 1号、2号图片将会被缓存到服务器A上，3号图片将会被缓存到服务器B上，4号图片将会被缓存到服务器C上。 一致性哈希算法的优点经过上述描述，我想兄弟你应该已经明白了一致性哈希算法的原理了，但是话说回来，一致性哈希算法能够解决之前出现的问题吗，我们说过，如果简单的对服务器数量进行取模，那么当服务器数量发生变化时，会产生缓存的雪崩，从而很有可能导致系统崩溃，那么使用一致性哈希算法，能够避免这个问题吗？我们来模拟一遍，即可得到答案。 假设，服务器B出现了故障，我们现在需要将服务器B移除，那么，我们将上图中的服务器B从hash环上移除即可，移除服务器B以后示意图如下。 在服务器B未移除时，图片3应该被缓存到服务器B中，可是当服务器B移除以后，按照之前描述的一致性哈希算法的规则，图片3应该被缓存到服务器C中，因为从图片3的位置出发，沿顺时针方向遇到的第一个缓存服务器节点就是服务器C，也就是说，如果服务器B出现故障被移除时，图片3的缓存位置会发生改变 但是，图片4仍然会被缓存到服务器C中，图片1与图片2仍然会被缓存到服务器A中，这与服务器B移除之前并没有任何区别，这就是一致性哈希算法的优点，如果使用之前的hash算法，服务器数量发生改变时，所有服务器的所有缓存在同一时间失效了，而使用一致性哈希算法时，服务器的数量如果发生改变，并不是所有缓存都会失效，而是只有部分缓存会失效，前端的缓存仍然能分担整个系统的压力，而不至于所有压力都在同一时间集中到后端服务器上。 这就是一致性哈希算法所体现出的优点。 hash环的偏斜在介绍一致性哈希的概念时，我们理想化的将3台服务器均匀的映射到了hash环上，如下图所示 但是，理想很丰满，现实很骨感，我们想象的与实际情况往往不一样。 在实际的映射中，服务器可能会被映射成如下模样。 聪明如你一定想到了，如果服务器被映射成上图中的模样，那么被缓存的对象很有可能大部分集中缓存在某一台服务器上，如下图所示。 上图中，1号、2号、3号、4号、6号图片均被缓存在了服务器A上，只有5号图片被缓存在了服务器B上，服务器C上甚至没有缓存任何图片，如果出现上图中的情况，A、B、C三台服务器并没有被合理的平均的充分利用，缓存分布的极度不均匀，而且，如果此时服务器A出现故障，那么失效缓存的数量也将达到最大值，在极端情况下，仍然有可能引起系统的崩溃，上图中的情况则被称之为hash环的偏斜，那么，我们应该怎样防止hash环的偏斜呢？一致性hash算法中使用”虚拟节点”解决了这个问题，我们继续聊。 虚拟节点话接上文，由于我们只有3台服务器，当我们把服务器映射到hash环上的时候，很有可能出现hash环偏斜的情况，当hash环偏斜以后，缓存往往会极度不均衡的分布在各服务器上，聪明如你一定已经想到了，如果想要均衡的将缓存分布到3台服务器上，最好能让这3台服务器尽量多的、均匀的出现在hash环上，但是，真实的服务器资源只有3台，我们怎样凭空的让它们多起来呢，没错，就是凭空的让服务器节点多起来，既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来，这些由实际节点虚拟复制而来的节点被称为”虚拟节点”。加入虚拟节点以后的hash环如下。 “虚拟节点”是”实际节点”（实际的物理服务器）在hash环上的复制品,一个实际节点可以对应多个虚拟节点。 从上图可以看出，A、B、C三台服务器分别虚拟出了一个虚拟节点，当然，如果你需要，也可以虚拟出更多的虚拟节点。引入虚拟节点的概念后，缓存的分布就均衡多了，上图中，1号、3号图片被缓存在服务器A中，5号、4号图片被缓存在服务器B中，6号、2号图片被缓存在服务器C中，如果你还不放心，可以虚拟出更多的虚拟节点，以便减小hash环偏斜所带来的影响，虚拟节点越多，hash环上的节点就越多，缓存被均匀分布的概率就越大。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个优化过的NexT主题]]></title>
    <url>%2F2018%2F01%2F18%2Foptimized_hexo_next_theme%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[A optimized hexo-theme-next一个优化过的NexT主题. 我修改了很多NexT的代码来对原版 NexT 做了优化, 如下 : 改了NexT的很多地方来优化移动端的表现 header的布局 移动端和PC端的侧边栏更加统一 移动端的文章目录列表现在可以滑动了 添加了文章加密的支持 升级到了fancybox3并完成适配, 3更流畅且拥有更多效果 重做了本地搜索引擎 现在移动端不会经常无故弹不出键盘了 也不会列出加密文章的内容了 更优雅的过渡动画 添加了headroom支持, 现在有一个可以会自动隐藏的header了, 往下滚一下鼠标则隐藏, 往上则出现 Live PreviewMy Blog Usage 0. 点击这里下载nvm-setup.zip, 解压并安装nvm之后, nvm install 10.13.0 然后再 nvm use 10.13.0则安装了node.js的10.13.0的版本, 尝试node -v和nvm ls看看版本, 这个node版本很关键, 不要乱改 1. Delete my source folder 删除我项目中的 source 文件夹 2. Create a new source folder 新建一个 source 文件夹 2.5. npm install -g hexo-cli然后再npm install --force 3. Unzip my_node_modules.tar.gz to the current directory: tar -zxvf my_node_modules.tar.gz 解压 my_node_modules.tar.gz 到当前目录(确保这一步在安装了hexo之后): tar -zxvf my_node_modules.tar.gz 4. Learn Hexo’s base usage for Writing/Generating/Deployment 学习Hexo的基本操作来写作/生成/部署 5. Learn NexT 学习 NexT 6. Modify /_config.yml &amp; /themes/next/_config.yml 修改 /_config.yml 和 /themes/next/_config.yml 7. hexo clean删除缓存 8. hexo generate生成静态文件 9. hexo server启动一个本地hexo服务器]]></content>
      <categories>
        <category>Misc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[两个例子对比理解Actor模型]]></title>
    <url>%2F2018%2F01%2F17%2Ftwo_example_for_understanding_actor_model%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Actor模型介绍Actor模式是一种并发模型，与另一种模型共享内存完全相反，Actor模型share nothing。所有的线程(或进程)通过消息传递的方式进行合作，这些线程(或进程)称为Actor。共享内存更适合单机多核的并发编程，而且共享带来的问题很多，编程也困难。 随着多核时代和分布式系统的到来，共享模型已经不太适合并发编程，因此几十年前就已经出现的Actor模型又重新受到了人们的重视。MapReduce就是一种典型的Actor模式，而在语言级对Actor支持的编程语言Erlang又重新火了起来，Scala也提供了Actor，但是并不是在语言层面支持，Java也有第三方的Actor包，Go语言channel机制也是一种类Actor模型。 Actor的基础就是消息传递, Actor由状态(state)、行为(Behavior)和邮箱(mailBox)三部分组成 : 状态(state)：Actor中的状态指的是Actor对象的变量信息，状态由Actor自己管理，避免了并发环境下的锁和内存原子性等问题 行为(Behavior)：行为指定的是Actor中计算逻辑，通过Actor接收到消息来改变Actor的状态 邮箱(mailBox)：邮箱是Actor和Actor之间的通信桥梁，邮箱内部通过FIFO消息队列来存储发送方Actor消息，接受方Actor从邮箱队列中获取消息 . . . 使用Actor模型的好处 事件模型驱动–Actor之间的通信是异步的，即使Actor在发送消息后也无需阻塞或者等待就能够处理其他事情 强隔离性–Actor中的方法不能由外部直接调用，所有的一切都通过消息传递进行的，从而避免了Actor之间的数据共享，想要 观察到另一个Actor的状态变化只能通过消息传递进行询问 位置透明–无论Actor地址是在本地还是在远程机上对于代码来说都是一样的 轻量性–Actor是非常轻量的计算单机，单个Actor仅占400多字节，只需少量内存就能达到高并发 单线程编程 单核单机时代一般都是单线程编程，如果把程序比作一个工厂，那么只有一个工人，这个工人负责所有的事情，所有的原料，工具产品等都放到一个地方，因为只有一个人，因此使用一套工具就行，取原料也不用排队等。 多线程编程-共享内存 到了多核时代，有多个工人，这些工人共同使用一个仓库和车间，干什么都要排队。比如我要从一块钢料切出一块来用，我得等别人先用完。有个扳手，另一个人在用，我得等他用完。两个人都要用一个切割机从一块钢材切一块钢铁下来用，但是一个人拿到了钢材，一个人拿到了切割机，他们互相都不退让，结果谁都干不了活。假如现在有一个任务，找100000以内的素数的个数，最多使用是个线程，如果用共享内存的方法，可以用下面的代码实现。可以看到，这些线程共享了currentNum和totalPrimeCount，对它们做操作时必须上锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class PrimeCount implements Runnable &#123; private int currentNum = 2; //从2开始找 private int totalPrimeCount = 0; //当前已经找到的 //取一个数，不能重复，最大到100000 private int incrCurrentNum() &#123; synchronized (this) &#123; //如果不用锁，必然会出错。 if(currentNum &gt; 100000) &#123; return -1; &#125; else &#123; int result = currentNum; currentNum++; return result; &#125; &#125; &#125; //把某个线程找到的素数个数加上 private void accPrimeCount(int count) &#123; synchronized (this) &#123; totalPrimeCount += count; &#125; &#125; @Override //一直取数并判断是否为素数，取不到了就把找到的个数累加 public void run() &#123; int primeCount = 0; int num; while((num=incrCurrentNum()) != -1) &#123; if(isPrime(num)) &#123; primeCount++; &#125; &#125; accPrimeCount(primeCount); &#125; private boolean isPrime(int num) &#123; for(int i = 2; i &lt; num; i++) &#123; if(num % i == 0) &#123; return false; &#125; &#125; return true; &#125; @SuppressWarnings("static-access") public static void main(String[] args)&#123; PrimeCount pc = new PrimeCount(); for(int i = 0; i &lt; 10; i++) &#123; new Thread(pc).start(); &#125; try &#123; Thread.currentThread().sleep(5000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System.out.println(pc.getTotalPrimeCount()); &#125; public int getTotalPrimeCount() &#123; return totalPrimeCount; &#125; &#125; 多线程/分布式编程-Actor模型 到了分布式系统时代，工厂已经用流水线了，每个人都有明确分工，这就是Actor模式。每个线程都是一个Actor，这些Actor不共享任何内存，所有的数据都是通过消息传递的方式进行的。如果用Actor模型实现统计素数个数，那么我们需要1个actor做原料的分发，就是提供要处理的整数，然后10个actor加工，每次从分发actor那里拿一个整数进行加工，最终把加工出来的半成品发给组装actor，组装actor把10个加工actor的结果汇总输出。用scala实现，下面是工程的结构：这是它们传递的消息，有一些指令，剩下的都是Int数据:一个Actor的代码结构一般是下面这种结构，不停的接受消息并处理，没有消息就等待：组装者代码：分发者代码：加工者代码：主线程代码：工程代码可以在附件中下载。这个代码实现的效果与前面用Java实现的是一样的，但是各个线程没有共享内存，也没有锁，这样开发起来容易，而且更适合分布式编程，因为分布式编程本身就不适合共享内存。Scala的Actor不能原生的支持分布式，但是Erlang可以，使用Erlang的Actor，分布式编程就和本地编程基本一样。但是Erlang的语法难懂，而且没有变量，几乎所有需要使用循环的地方都得用递归。 参考 : 十分钟理解Actor模式 Actor模型原理]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Actor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python处理Excel和MySQL数据]]></title>
    <url>%2F2018%2F01%2F07%2Fpython_excel_mysql%2F</url>

    <encrypted>1</encrypted>

    <content type="text"><![CDATA[. . . mysql数据的处理和excel数据的读比如mysql数据的处理和excel数据的读(写在最下方), 如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202# encoding=utf-8import pandas as pdimport pymysqlimport mathimport tracebackclass DB: def __init__(self, host='localhost', port=3306, db='', user='root', passwd='root', charset='utf8'): # 建立连接 self.conn = pymysql.connect(host=host, port=port, db=db, user=user, passwd=passwd, charset=charset) # 创建游标，操作设置为字典类型 self.cur = self.conn.cursor() # self.cur = self.conn.cursor(cursor=pymysql.cursors.DictCursor) def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): # 关闭游标 self.cur.close() # 关闭数据库连接 self.conn.close() def get_label_id(self, label_name): # SQL 查询语句 sql = "SELECT * FROM label WHERE name = '%s'" % label_name try: # 执行SQL语句 self.cur.execute(sql) # 获取所有记录列表 results = self.cur.fetchall() for row in results: _id = row[0] # lname = row[1] # age = row[2] # sex = row[3] # income = row[4] # 打印结果 # print("fname=%s,lname=%s,age=%s,sex=%s,income=%s" % \ # (fname, lname, age, sex, income)) return _id except: print("Error: unable to fetch data, label_name=%s" % label_name) return None def get_label_dimension_id(self, label_dimension_name): # SQL 查询语句 sql = "SELECT * FROM label_dimension WHERE name = '%s'" % label_dimension_name try: # 执行SQL语句 self.cur.execute(sql) # 获取所有记录列表 results = self.cur.fetchall() for row in results: _id = row[0] return _id except: print("Error: unable to fetch data, label_dimension_name=%s" % label_dimension_name) return None def iter_all_material_id_by_game_id(self, game_id=1827): # SQL 查询语句 sql = "SELECT * FROM material WHERE game_id = %d" % game_id try: # 执行SQL语句 self.cur.execute(sql) # 获取所有记录列表 results = self.cur.fetchall() for row in results: _id = row[0] yield _id except: print("Error: unable to fetch data, game_id=%s" % game_id) return None def clear_label_by_material_id(self, material_id): # SQL 查询语句 sql = "DELETE FROM label_material_rel WHERE material_id = %d" % material_id try: # 执行SQL语句 print(sql) self.cur.execute(sql) # 向数据库提交 # self.conn.commit() except: # 发生错误时回滚 # self.conn.rollback() print("Error: clear_label_by_material_id material_id=%d" % material_id) def get_label_name_2_id_map(self): # SQL 查询语句 sql = "SELECT * FROM label" ret_map = &#123;&#125; try: # 执行SQL语句 self.cur.execute(sql) # 获取所有记录列表 results = self.cur.fetchall() for row in results: _id = row[0] _name = row[1] ret_map[_name] = _id except: # 发生错误时回滚 # self.conn.rollback() print("Error: get_label_id_2_name_map") return ret_map def get_label_dimension_name_2_id_map(self, game_id=1827): # SQL 查询语句 sql = "SELECT * FROM label_dimension where game_id = %d" % game_id ret_map = &#123;&#125; try: # 执行SQL语句 self.cur.execute(sql) # 获取所有记录列表 results = self.cur.fetchall() for row in results: _id = row[0] _name = row[1] ret_map[_name] = _id except: # 发生错误时回滚 # self.conn.rollback() print("Error: get_label_id_2_name_map") return ret_map def update_label_material_rel(self, label_id, material_id): # SQL 插入语句 sql = """INSERT INTO label_material_rel (label_id, material_id) VALUES (%d, %d)""" % (label_id, material_id) try: print(sql) # 执行sql语句 self.cur.execute(sql) # 提交到数据库执行 # self.conn.commit() except: # 如果发生错误则回滚 # self.conn.rollback() print("Error: update_label_material_rel (label_id=%d, material_id=%d)" % (label_id, material_id))if __name__ == '__main__': with DB(host='42.186.102.210', port=9110, user='fba_user', db='fba', passwd='fba.edt') as db: # with DB(host='42.186.102.211', port=9110, user='fba_user', db='fba', passwd='fba.edt.dvp') as db: df = pd.read_excel('pending_proc.xlsx' # , sheet_name='汇总' ) col_list = df.columns.to_list() exclude_col_name_list = ['视频编码', '提交时间', '负责人'] for e in exclude_col_name_list: col_list.remove(e) # label_dimension_id_list = [] # for col in col_list: # _label_dimension_id = db.get_label_dimension_id(col) # label_dimension_id_list.append(_label_dimension_id) pending_import_data = [] # _material_id_list = set() for i in df.index.values: # 获取行号的索引，并对其进行遍历： # 根据i来获取每一行指定的数据 并利用to_dict转成字典 # break _row_data = df.ix[i, col_list].to_dict() # _material_id_list.add(_row_data['id']) pending_import_data.append(_row_data) # if i &gt; 10: # break # print("最终获取到的数据test_data是：&#123;0&#125;".format(test_data)) # print("最终获取到的数据_material_id_list是：&#123;0&#125;".format(_material_id_list)) # try: for m_id in db.iter_all_material_id_by_game_id(): db.clear_label_by_material_id(m_id) _label_name_2_id_map = db.get_label_name_2_id_map() for _row_data in pending_import_data: if math.isnan(_row_data['id']): continue _material_id = int(_row_data['id']) for col_name, label_name_list_str in _row_data.items(): if col_name == 'id': continue _label_list = str(label_name_list_str).strip().split('，') for _label in _label_list: _label_id = _label_name_2_id_map.get(_label, None) if _label_id is None: continue db.update_label_material_rel(_label_id, _material_id) # 提交数据库并执行 db.conn.commit() except Exception as e: traceback.print_exc() print(e) db.conn.rollback() excel数据的写123456789101112131415161718import pandas as pd# import numpy as nppending_write_data = \[[0.01197952 0.49298068 0.76897227 0.76737911] [0.82765185 0.70592612 0.07624629 0.41206483] [0.96358469 0.14713271 0.34609823 0.45701923] [0.87425111 0.18451819 0.55125788 0.50195541]]frame = pd.DataFrame( pending_write_data, # 待写入的每一行的行数据 # index=['exp1','exp2','exp3','exp4'], # 索引名字, 注释了索引就是默认的`0, 1, 2, ..` columns=['jan2015','Fab2015','Mar2015','Apr2005']) # 列名字print(frame)frame.to_excel("data2.xlsx", index=False, # False则为不需要索引了)# print(np.random.random((4,4))) requirements.txt的自动生成使用pigar, pip install pigar requirements.txt1234567891011# Automatically generated by https://github.com/damnever/pigar.# E:\working\doc\sdc\import_label_material.py: 4PyMySQL == 0.10.1# E:\working\doc\sdc\test.py: 2numpy == 1.19.1# E:\working\doc\sdc\import_label_material.py: 3# E:\working\doc\sdc\test.py: 1pandas == 0.25.1]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VirtualBox安装Ubuntu教程]]></title>
    <url>%2F2018%2F01%2F01%2Fvbox_install_ubuntu_tutorial%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[最近因某些原因重装了Win10, 虚拟机也需要重装, 记录一下过程, 供以后查阅, 以免走更多弯路 一些常用命令 scp : 比如把自己windows上的某个文件test.py上传到某台linux上, 则在windows上用git bash到达这个文件所在的目录然后输入命令scp test.py hlh@192.168.80.8:/home/test_dir 下载则把命令顺序反过来: make &amp;&amp; make install 之前用configure的时候记得用--prefix指定安装目录, 这样就只会安装到这个目录不会分散到各处, 卸载的时候就很方便, 比如编译安装python3.8的时候就会有类似如这样的命令: 123$ ./configure --prefix=/usr/local/python3 --enable-optimizations$ make$ make install 需准备的工具和材料 虚拟机软件 Ubuntu : 我用的是16.04版本的ubuntu的server版本(不是desktop桌面版) 建议给虚拟机分配20G硬盘空间 建议给虚拟机分配4G内存 . . . 设置root密码与sudo免密码权限设置root密码: sudo passwd root打开sudo免密码权限, 则sudo vi /etc/sudoers, 然后编辑%sudo ALL=(ALL) NOPASSWD:ALL 设置ssh的公钥登陆在自己的home目录下 mkdir .ssh chmod 700 .ssh cd .ssh vi authorized_keys然后编辑加入自己的公钥 `chmod 644 authorized_keys SSH 证书登陆配置: sudo vi /etc/ssh/sshd_config 取消注释 : #AuthorizedKeysFile .ssh/authorized_keys 修改yes-&gt;no : PasswordAuthentication no sudo service ssh restart 网络设置 直接桥接网卡即可, 然后给虚拟服务器设置静态IP, 操作如下: 先ifconfig查看自己当前的ip, 使用vim编辑/etc/network/interfaces, 然后将自己的当前ip填入, 比如是192.168.1.14, 则改为 1234567891011121314151617# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).source /etc/network/interfaces.d/*# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto enp0s3# iface enp0s3 inet dhcpiface enp0s3 inet staticaddress 192.168.1.14netmask 255.255.255.0gateway 192.168.1.1dns-nameservers 114.114.114.114 8.8.8.8 重启网络(service networking restart)或者系统 ping baidu.com 看看是否通了 设置全局代理, 比如宿主机的vray的http代理为http://127.0.0.1:10809, 而宿主机的ip为192.168.82.177, 则在vbox里的ubuntu的~/.bashrc 最后加上 123# export ALL_PROXY=&quot;http://192.168.82.177:10809&quot;alias setproxy=&quot;export ALL_PROXY=http://192.168.82.177:10809&quot;alias unsetproxy=&quot;unset ALL_PROXY&quot; 然后source ~/.bashrc即可 说明： auto 后为 ifconfig 查出来的虚拟机网卡 iface enp0s3 inet 后的 dhcp 改为 static address 虚拟机 ip 设置为当前自动分配的 ip 即可，配置好后面重启就一直保持这个 ip netmask 子网掩码与宿主机一致 gateway 默认网关与宿主机一致 dns-nameserver DNS 服务器 更换源见原网站 Ubuntu 的软件源配置文件是 /etc/apt/sources.list。将系统自带的该文件做个备份，将该文件替换为下面内容，即可使用 TUNA 的软件源镜像。替换之后记得 sudo apt-get update # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse # 预发布软件源，不建议启用 # deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse bash增强可参考 Bash定制 必装软件 sudo apt-get install openssh-server sudo apt-get install g++ sudo apt-get install cmake sudo apt-get install gdb 装完gdb之后添加一个pstack脚本方便查看运行时的程序堆栈(用法 : pstack pid) : sudo vi /usr/bin/pstacksudo chmod +x /usr/bin/pstack pstack脚本的内容如下 : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/shif test $# -ne 1; then echo &quot;Usage: `basename $0 .sh` &lt;process-id&gt;&quot; 1&gt;&amp;2 exit 1fiif test ! -r /proc/$1; then echo &quot;Process $1 not found.&quot; 1&gt;&amp;2 exit 1fi# GDB doesn&apos;t allow &quot;thread apply all bt&quot; when the process isn&apos;t# threaded; need to peek at the process to determine if that or the# simpler &quot;bt&quot; should be used.backtrace=&quot;bt&quot;if test -d /proc/$1/task ; then # Newer kernel; has a task/ directory. if test `/bin/ls /proc/$1/task | /usr/bin/wc -l` -gt 1 2&gt;/dev/null ; then backtrace=&quot;thread apply all bt&quot; fielif test -f /proc/$1/maps ; then # Older kernel; go by it loading libpthread. if /bin/grep -e libpthread /proc/$1/maps &gt; /dev/null 2&gt;&amp;1 ; then backtrace=&quot;thread apply all bt&quot; fifiGDB=$&#123;GDB:-/usr/bin/gdb&#125;if $GDB -nx --quiet --batch --readnever &gt; /dev/null 2&gt;&amp;1; then readnever=--readneverelse readnever=fi# Run GDB, strip out unwanted noise.$GDB --quiet $readnever -nx /proc/$1/exe $1 &lt;&lt;EOF 2&gt;&amp;1 |set width 0set height 0set pagination no$backtraceEOF/bin/sed -n \ -e &apos;s/^\((gdb) \)*//&apos; \ -e &apos;/^#/p&apos; \ -e &apos;/^Thread/p&apos; 若是ubuntu桌面版的话可以安装一下增强功能 桌面版ubuntu安装增强功能 注 : 如果在侧边找到如下图加载的虚拟光驱，就需要先右击，点击弹出，然后才可正常安装增强功能点击安装增强功能点击“运行”输入登录系统的密码，点击授权，就开始自动安装了如图，为安装界面，安装完成后按下回车键，就按照成功了。安装好后关闭ubuntu再次启动ubuntu的时候，虚拟机就可以在无缝模式和自动显示尺寸下运行了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>VBox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10原版重装]]></title>
    <url>%2F2018%2F01%2F01%2Foriginal_win10_reinstall%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[最近因某些原因重装了Win10, 记录一下过程, 供以后查阅, 以免走更多弯路, 搞得新装的Win10都一堆广告… 目的 可以格式化系统盘 原版Win10, 正版激活(某宝买一个非一次性的可重装的激活码, 用这种激活码激活过的机器, 重装后会自动激活的) 装完没有广告和预装的垃圾软件 : 不要用大白菜和老毛桃之类的, 否则装完就会有一堆垃圾软件和改过的主页, 真是无利不起早. . . . 需提前准备的工具 UltraISO(软碟通) : UltraISO(软碟通)下载网站, 下载一个试用版即可 U盘 : 6G以上, 4G可能不够 下载原版Win10的ISO镜像文件到微软官网, 点击”立即下载工具”, 用此工具一步一步的把win10的ISO镜像文件下载下来. 主要注意下面两步要按照下图中的选对 : 用UltraISO把Win10的ISO镜像文件写入U盘 2.1、打开工具“UltraISO”（工具自行搜索名称找到下载），选择“文件~打开”2.2、在路径中选择下载到的Win10 ISO镜像文件2.3、打开后点击“启动~写入硬盘映像”2.4、在弹出的窗口中选择需写入的U盘，U盘需提前插入电脑USB口中，同时不要插入多个U盘先。其它设置保持默认可以，然后点“写入”2.5、弹出的窗口点击“是”（注：过程会清空U盘数据，请注意提前备份U盘数据）2.6、写入中，请耐心等待完成即可2.7、刻录成功后就可以关闭工具2.8、同时检查U盘也会发现里面带了Win10镜像的相关文件了，接下来可以拿它去装逼（机）了 安装Win10 2.9、接着用这个制作好的U盘安装原版Win10，把U盘插入想安装的电脑，按笔记本的品牌（台式机按主板厂商）选择启动热键，热键对应如下表：2.10、制作好的U盘需先插进电脑2.11、接着是一系列的过程截图，有需要注意的地方已注明注意：没有产品密钥请跳过输入密钥，待安装完系统再想办法激活注意：全新不保留原系统的内容可格式化一下系统盘再安装请耐心等待即可，待上一步完成后会自动重启]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UML类图与类的关系详解]]></title>
    <url>%2F2017%2F11%2F22%2FUML%E7%B1%BB%E5%9B%BE%E4%B8%8E%E7%B1%BB%E7%9A%84%E5%85%B3%E7%B3%BB%E8%AF%A6%E8%A7%A3%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文出处 在画类图的时候，理清类和类之间的关系是重点。类的关系有泛化(Generalization)、实现（Realization）、依赖(Dependency)和关联(Association)。其中关联又分为一般关联关系和聚合关系(Aggregation)，合成关系(Composition)。下面我们结合实例理解这些关系。 基本概念类图（Class Diagram）: 类图是面向对象系统建模中最常用和最重要的图，是定义其它图的基础。类图主要是用来显示系统中的类、接口以及它们之间的静态结构和关系的一种静态模型。类图的3个基本组件： 类名 属性 方法。 . . . 泛化泛化(generalization)：表示is-a的关系，是对象之间耦合度最大的一种关系，子类继承父类的所有细节。直接使用语言中的继承表达。在类图中使用带三角箭头的实线表示，箭头从子类指向父类。 实现实现（Realization）:在类图中就是接口和实现的关系。这个没什么好讲的。在类图中使用带三角箭头的虚线表示，箭头从实现类指向接口。 依赖依赖(Dependency)：对象之间最弱的一种关联方式，是临时性的关联。代码中一般指由局部变量、函数参数、返回值建立的对于其他对象的调用关系。一个类调用被依赖类中的某些方法而得以完成这个类的一些职责。在类图使用带箭头的虚线表示，箭头从使用类指向被依赖的类。 关联关联(Association) : 对象之间一种引用关系，比如客户类与订单类之间的关系。这种关系通常使用类的属性表达。关联又分为一般关联、聚合关联与组合关联。后两种在后面分析。在类图使用带箭头的实线表示，箭头从使用类指向被关联的类。可以是单向和双向。 聚合聚合(Aggregation) : 表示has-a的关系，是一种不稳定的包含关系。较强于一般关联,有整体与局部的关系,并且没有了整体,局部也可单独存在。如公司和员工的关系，公司包含员工，但如果公司倒闭，员工依然可以换公司。在类图使用空心的菱形表示，菱形从局部指向整体。 组合组合(Composition) : 表示contains-a的关系，是一种强烈的包含关系。组合类负责被组合类的生命周期。是一种更强的聚合关系。部分不能脱离整体存在。如公司和部门的关系，没有了公司，部门也不能存在了；调查问卷中问题和选项的关系；订单和订单选项的关系。在类图使用实心的菱形表示，菱形从局部指向整体。 多重性多重性(Multiplicity) : 通常在关联、聚合、组合中使用。就是代表有多少个关联对象存在。使用数字..星号（数字）表示。如下图，一个割接通知可以关联0个到N个故障单。 聚合和组合的区别 这两个比较难理解，重点说一下。聚合和组合的区别在于：聚合关系是“has-a”关系，组合关系是“contains-a”关系；聚合关系表示整体与部分的关系比较弱，而组合比较强；聚合关系中代表部分事物的对象与代表聚合事物的对象的生存期无关，一旦删除了聚合对象不一定就删除了代表部分事物的对象。组合中一旦删除了组合对象，同时也就删除了代表部分事物的对象。 实例分析 联通客户响应OSS。系统有故障单、业务开通、资源核查、割接、业务重保、网络品质性能等功能模块。现在我们抽出部分需求做为例子讲解。大家可以参照着类图，好好理解。 通知分为一般通知、割接通知、重保通知。这个是继承关系。 NoticeService和实现类NoticeServiceImpl是实现关系。 NoticeServiceImpl通过save方法的参数引用Notice,是依赖关系。同时调用了BaseDao完成功能，也是依赖关系。 割接通知和故障单之间通过中间类(通知电路)关联，是一般关联。 重保通知和预案库间是聚合关系。因为预案库可以事先录入，和重保通知没有必然联系，可以独立存在。在系统中是手工从列表中选择。删除重保通知，不影响预案。 割接通知和需求单之间是聚合关系。同理，需求单可以独立于割接通知存在。也就是说删除割接通知，不影响需求单。 通知和回复是组合关系。因为回复不能独立于通知存在。也就是说删除通知，该条通知对应的回复也要级联删除。 经过以上的分析，相信大家对类的关系已经有比较好的理解了。大家有什么其它想法或好的见解，欢迎拍砖。PS：还是那句话：以上类图用Enterprise Architect 7.5所画，在此推荐一下EA,非常不错。可以替代Visio和Rose了。Visio功能不够强大，Rose太重。唯有EA比较合适。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>UML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[next的文章使用自定义的css]]></title>
    <url>%2F2017%2F11%2F14%2Fnext%E7%9A%84%E6%96%87%E7%AB%A0%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84css%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[今天，创建自己的about页面的时候，像使用自定义的css样式，就像这是不是NexT可以使用自定义的CSS样式，片尝试了一下，还是可以的，因为markdown支持html标签，使用自定义的CSS样式还是不错的。下面总结一下具体的使用过程：添加样式支持 为了不吧原先的像是文件搞得太乱，这里，添加子集的样式文件。 首先，在样式文件的source文件夹下找到css文件夹，打开main.styl文件，在最后添加：123//My Layer//————————————————–@import &quot;_my/mycss&quot;;新建自定义样式找到样式文件夹css 新建_my文件夹，在其中新建mycss.styl文件，之后就可以按照stylus的格式自定义样式了。例子例如：我想在文章中添加个自定义样式的按钮，怎么做呢？？？打开新建的mycss.styl文件，在其中添加样式：123456789101112131415161718192021.myButton &#123; background-color:#0f94bd; -moz-border-radius:15px; -webkit-border-radius:15px; border-radius:15px; display:inline-block; cursor:pointer; color:#ffffff; font-family:Arial; font-size:17px; padding:11px 27px; text-decoration:none; text-shadow:0px 1px 0px #2f6627;&#125;.myButton:hover &#123; background-color:#5cbf2a;&#125;.myButton:active &#123; position:relative; top:1px;&#125;(ps:这里直接使用的css的格式写的，因为css的代码在网上很好找到，而stylus预处理器的就不那么容易找到了，stylus一样支持css格式，所以在这里直接使用了css文件，没有写成stylus语法。其实让我写我也不会，哈哈！) 引用：在想要引用的时候添加1&lt;a href=&quot;#&quot; class=&quot;myButton&quot;&gt;MyButton&lt;/a&gt;]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修复next搜索框弹不出手机键盘bug和更简洁的搜索框]]></title>
    <url>%2F2017%2F11%2F07%2F%E4%BF%AE%E5%A4%8Dnext%E6%90%9C%E7%B4%A2%E6%A1%86%E5%BC%B9%E4%B8%8D%E5%87%BA%E6%89%8B%E6%9C%BA%E9%94%AE%E7%9B%98bug%E5%92%8C%E6%9B%B4%E7%AE%80%E6%B4%81%E7%9A%84%E6%90%9C%E7%B4%A2%E6%A1%86%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[动机 next自带的搜索框有手机键盘不能自动弹出来的bug, 需要点击两次才可以弹出来 渴望拥有更简洁的搜索框 展示图如我博客右上角所示 GitHub我的博客源码, 欢迎Fork+Star. 如果你也喜欢这样的搜索框, 参考这个commit便可改成跟我一样.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Next</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何同时把项目放在coding和github之ssh的config篇]]></title>
    <url>%2F2017%2F10%2F20%2F%E5%A6%82%E4%BD%95%E5%90%8C%E6%97%B6%E6%8A%8A%E9%A1%B9%E7%9B%AE%E6%94%BE%E5%9C%A8coding%E5%92%8Cgithub%E4%B9%8Bssh%E7%9A%84config%E7%AF%87%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[无法使用 22 端口的 SSH 服务怎么办？遇到了以下这两种错误怎么办? connect to host git.coding.net port 22: Connection timed out connect to host github.com port 22: Connection timed out SSH 的默认端口是 22，有时您或您的公司的防火墙会完全屏蔽掉这个端口。如果此时您不方便通过 HTTPS 方式进行 Git 操作，您可以使用 Coding.net和GitHub 提供的 443 端口的 SSH 服务. ssh的config配置在home目录下的.ssh文件夹里新建一个config文件, 添加如下代码即可 12345678910111213Host github.comUser &quot;xxxxx@email.com&quot;Hostname ssh.github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 443Host git.coding.netUser &quot;xxxxx@email.com&quot;Hostname git-ssh.coding.netPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 443 测试命令您需要确保 SSH 已配置成功，然后执行以下命令测试： ssh -T git@git.coding.net ssh -T git@github.com]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何展开next的所有标题]]></title>
    <url>%2F2017%2F10%2F19%2F%E5%A6%82%E4%BD%95%E5%B1%95%E5%BC%80next%E7%9A%84%E6%89%80%E6%9C%89%E6%A0%87%E9%A2%98%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[根目录下的\themes\next\source\css_custom\custom.styl 的最上方加入一行加入一行 .post-toc .nav .nav-child { display: block; }即可]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode配置与技巧]]></title>
    <url>%2F2017%2F10%2F16%2Fvscode%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%8A%80%E5%B7%A7%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[序听说网易云音乐可以一玩就是一个下午, 但有些编辑器怕是一玩就是一辈子… 转vscode原因因为工作关系, sublime对于gb2312编码的问题无法良好解决.即使装了convert2utf还是无法解决搜索中文字符串的问题, 因为搜索的时候sublime是没有转码的所以无法搜到想要的中文.所以转战vscode, 谨以这篇笔记来记录vscode心得. . . . 编码问题摘自 比如解决我上述的gb2312乱码的问题的步骤如下 : 在你项目的根目录新建一个名为 .vscode的文件夹(其实这个文件夹就是专门针对这个项目的配置文件了) 在文件夹里新建一个名为 settings.json 的文件 上述文件中加入一对花括号 花括号中添加下面内容 12345678&quot;files.encoding&quot;: &quot;shiftjis&quot;,&quot;files.encoding&quot;: &quot;eucjp&quot;,&quot;files.encoding&quot;: &quot;big5hkscs&quot;,&quot;files.encoding&quot;: &quot;Big5&quot;,&quot;files.encoding&quot;: &quot;GB18030&quot;,&quot;files.encoding&quot;: &quot;GBK&quot;,&quot;files.encoding&quot;: &quot;utf8&quot;,&quot;files.encoding&quot;: &quot;GB2312&quot;, 这样支持这样几种编码方式，最下面的就是默认的， 这个默认支持GB2312,还需要其他编码可以自己按需要添加 设置搜索文件排除文件夹例如在此工作区设置内设置不搜索Map文件里的文件和扩展名为xml的文件, 1234&quot;search.exclude&quot;: &#123; &quot;**/Map&quot;: true, &quot;*.xml&quot;: true,&#125;, vscode常用快捷键以及改键 ctrl+p 搜文件 ctrl+shift+o 搜当前文件的符号 f12 转到定义 f12+ctrl 转到声明 alt+-&gt; 导航前进 alt+&lt;- 导航后退 ctrl+shift+\ 跳到对应的括号 ctrl+shift+p 打开命令输入框 ctrl+shift+a 切换块注释 ctrl+/ 切换行注释 多行编辑 : alt+shift+拖动鼠标左键或者alt+左键多处选择 不同机器间同步vscode设置的插件(必装) Shan khan的Settings Sync插件, 可以同步你的快键键/用户设置/icon/snippet/主题等等, 这样你另一台电脑上就可以同步你在公司所用的设置了. 另一个机器也获得上传settings权限的方法 这个是插件官方说明页面没有写的, 官方页面只教了你如何在另一台机器上下载之前机器的settings 到另外一个机器也装上这个插件, 然后ctrl+shift+p调出控制命令板然后输出sync选中Sync : Advanced Options, 然后选中Edit Extension local settings, 就会打开一个配置文件, 然后填入你的token即可 Vim插件1:amVim作者为auiWorks的名为amVim的插件, 装上即可使用, 无须配置, 跟vscode默认的多行编辑也不冲突 切记在键盘快捷方式中把amVim的ctrl+c快捷键改成其他键或者直接删除, 否则vscode内的东西无法复制到vscode外 MetaGo(一个类似EasyMotion的插件)目前类似EasyMotion的插件只有这个兼容amVim, 改键改成ctrl+shift之后非常好用. Vim插件2:vim(不推荐)作者为vscodevim的名为vim的插件, 不推荐使用这个插件, 下面说一下他的不足之处 vscode的欢迎页就有推荐vim的插件, 几乎所有键盘映射都移植过来了,还加了一些特性,比如多行编辑. 如果要用此插件建议改键 alt+shift+f 从在文件夹中查找改为当前文件中查找, 因为ctrl+f跟vim的冲突 alt+shift+q 改为格式化选中代码快捷键 ctrl+d改为ctrl+q, 因为ctrl+d跟vim的冲突 难用之处不过这个插件自带的多行编辑极为难用.而且他的多行编辑还处于beta阶段, 所以我们还是使用vscode自带的多行编辑, 解决方案首先因为vscode多行编辑的ctrl+d快捷键被插件占用了, 所以我们要把vscode原来的ctrl+d改为ctrl+q 对于多个不止一个单字符多处编辑的情况 : 123456当处于normal mode的时候, 按下v键, 选中想要多处编辑的字符串, 然后按ctrl+q,此时vscode编辑器左下角会提示你目前处于visual mode multi cursor状态,我们得按下esc退到normal mode multi cursor状态,然后按下i进入insert mode multi mode状态, 此时就可以多处编辑了,按两次esc就可以退回到normal mode了 对于多个单字符多处编辑的情况 : 因为vim的v模式选中一个字符, vscode的ctrl+q选中了多个单字符还是无法同时编辑, 所以我们用vim的宏, 例子如下 : 1234567891011121314151617比如：aaa,bbb,ccc,dddeee,fff,ggg,hhhiii,jjj,kkk,lll怎样把b, f, j后面的逗号改成引号？ 最快速方便的方法是使用宏命令模式下按qa进入录制状态，按照以下顺序操作就可以了，“#”字符之后为注释，宏将保存在寄存器a中0 #定位到行首2f, #定位到第二个,字符r&apos; #将光标下的字符替换为&apos;j #进入下一行q #退出宏录制状态针对剩余的行调用宏就可以了，比如在命令行模式下键入“100@a”，就是重复执行100次或者键入&quot;@@&quot;一个一个的执行, &quot;@@&quot;的意思是执行最近录入的一个宏 对于某一纵列多行编辑的情况 : 12345vim进了多行编辑模式：&lt;ESC&gt;之后按CTRL+V进入visual block模式（列编辑）。光标移到某行行首，上下键选择行，按I（i的大写字母），输入##，然后按&lt;ESC&gt;键，这样就在多行行首添加##了。也可以在多行的固定位置添加固定字符。切记一定要按了I之后再按键盘上的home或者end键光标才能百分之百到行首或者行尾如果要删除这些##，进入visual block模式，选中这些##，按d即可。 还可以使用vscode自带的多行编辑快捷键, alt+shift+拖动鼠标左键或者alt+左键多处选择 Lua插件lua插件三件套(推荐) keyring 的 Lua插件来完成提示 xxxg0001的lua-for-vscode来完成跳转 trixnz的vscode-lua来完成代码linting和列出属性以及方法 luaide国人的插件(不推荐)参考LuaIde文档导航页 到这个页面下载他的免费版本安装即可. luaide免费版本的已知问题 对于包含lua文件很多的文件夹来说:他要扫描非常久, 而且每次都要扫描, 解决方案就是把要用的小文件夹单独开一个vscode窗口来工作 有时候不解析, 无法跳转或者无法列出当前文件的方法和属性:随便编辑一下这个lua文件, 再ctrl+z, 这个文件就被解析了 Python插件直接安装vscode商城中推荐的作者为Don Jayamanne的python插件 把terminal换成GitBash如果你的PATH里有 git bash 的话, 按照直接 Shift-Ctrl-p 然后 输入 Select Default Shell即可选择.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>VSCode</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Base64编码原理与应用]]></title>
    <url>%2F2017%2F10%2F13%2FBase64%E7%BC%96%E7%A0%81%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[简单解释作者：郭无心链接：https://www.zhihu.com/question/36306744/answer/71626823来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 我们知道在计算机中任何数据都是按ascii码存储的，而ascii码的128～255之间的值是不可见字符。而在网络上交换数据时，比如说从A地传到B地，往往要经过多个路由设备，由于不同的设备对字符的处理方式有一些不同，这样那些不可见字符就有可能被处理错误，这是不利于传输的。 所以就先把数据先做一个Base64编码，统统变成可见字符，这样出错的可能性就大降低了。 对证书来说，特别是根证书，一般都是作Base64编码的，因为它要在网上被许多人下载。 电子邮件的附件一般也作Base64编码的，因为一个附件数据往往是有不可见字符的。 那么Base64到底是怎样编码的呢？简单来说，任何一个数据无非可以看作一个比特流，如01000100010011101100111010111100011001010……那么我们取6个比特为一组，计算它的ascii值，得到一个字符，这个字符肯定是可见字符，好，把它对应的字符写出来，再取6个比特，计算…，如此下去，直到最后，就完成了编码。 1.标准base64只有64个字符（英文大小写、数字和+、/）以及用作后缀等号；2.base64是把3个字节变成4个可打印字符，所以base64编码后的字符串一定能被4整除（不算用作后缀的等号）；3.等号一定用作后缀，且数目一定是0个、1个或2个。 这是因为如果原文长度不能被3整除，base64要在后面添加\0凑齐3n位。 为了正确还原，添加了几个\0就加上几个等号。 显然添加等号的数目只能是0、1或2；4.严格来说base64不能算是一种加密，只能说是编码转换。 使用base64的初衷。 是为了方便把含有不可见字符串的信息用可见字符串表示出来，以便复制粘贴 . . . 详细解释转自Base64编码原理与应用 Base64编码原理 Base64编码之所以称为Base64，是因为其使用64个字符来对任意数据进行编码，同理有Base32、Base16编码。标准Base64编码使用的64个字符为：这64个字符是各种字符编码（比如ASCII编码）所使用字符的子集，基本，并且可打印。唯一有点特殊的是最后两个字符，因对最后两个字符的选择不同，Base64编码又有很多变种，比如Base64 URL编码。Base64编码本质上是一种将二进制数据转成文本数据的方案。对于非二进制数据，是先将其转换成二进制形式，然后每连续6比特（2的6次方=64）计算其十进制值，根据该值在上面的索引表中找到对应的字符，最终得到一个文本字符串。假设我们要对 Hello! 进行Base64编码，按照ASCII表，其转换过程如下图所示：可知 Hello! 的Base64编码结果为 SGVsbG8h ，原始字符串长度为6个字符，编码后长度为8个字符，每3个原始字符经Base64编码成4个字符，编码前后长度比4/3，这个长度比很重要 - 比原始字符串长度短，则需要使用更大的编码字符集，这并不我们想要的；长度比越大，则需要传输越多的字符，传输时间越长。Base64应用广泛的原因是在字符集大小与长度比之间取得一个较好的平衡，适用于各种场景。是不是觉得Base64编码原理很简单？但这里需要注意一个点：Base64编码是每3个原始字符编码成4个字符，如果原始字符串长度不能被3整除，那怎么办？使用0值来补充原始字符串。以 Hello!! 为例，其转换过程为：注：图表中蓝色背景的二进制0值是额外补充的。Hello!! Base64编码的结果为 SGVsbG8hIQAA 。最后2个零值只是为了Base64编码而补充的，在原始字符中并没有对应的字符，那么Base64编码结果中的最后两个字符 AA 实际不带有效信息，所以需要特殊处理，以免解码错误。标准Base64编码通常用 = 字符来替换最后的 A，即编码结果为 SGVsbG8hIQ==。因为 = 字符并不在Base64编码索引表中，其意义在于结束符号，在Base64解码时遇到 = 时即可知道一个Base64编码字符串结束。如果Base64编码字符串不会相互拼接再传输，那么最后的 = 也可以省略，解码时如果发现Base64编码字符串长度不能被4整除，则先补充 = 字符，再解码即可。解码是对编码的逆向操作，但注意一点：对于最后的两个 = 字符，转换成两个 A 字符，再转成对应的两个6比特二进制0值，接着转成原始字符之前，需要将最后的两个6比特二进制0值丢弃，因为它们实际上不携带有效信息。为了理解Base64编码解码过程，个人实现了一个非常简陋的Base64编码解码程序，见：youngsterxyf/xiaBase64。由于Base64应用广泛，所以很多编程语言的标准库都内置Base64编码解码包，如：PHP：base64_encode、base64_decode Python：base64包Go：encoding/base64…Base64编码应用本文开始提到的青云应用例子只是Base64编码的应用场景之一。由于Base64编码在字符集大小与编码后数据长度之间做了较好的平衡，以及Base64编码变种形式的多样，使得Base64编码的应用场景非常广泛。下面举2个常用常见的例子。HTML内嵌Base64编码图片前端在实现页面时，对于一些简单图片，通常会选择将图片内容直接内嵌在页面中，避免不必要的外部资源加载，增大页面加载时间，但是图片数据是二进制数据，该怎么嵌入呢？绝大多数现代浏览器都支持一种名为 Data URLs 的特性，允许使用Base64对图片或其他文件的二进制数据进行编码，将其作为文本字符串嵌入网页中。以百度搜索首页为例，其中语音搜索的图标是个背景图片，其内容以 Data URLs 形式直接写在css中，这个css内容又直接嵌在HTML页面中，如下图所示：Data URLs 格式为：url(data:文件类型;编码方式,编码后的文件内容)。当然，也可以直接基于image标签嵌入图片，如下所示：&lt;img alt=&quot;Embedded Image&quot; src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIA…&quot; /&gt;但请注意：如果图片较大，图片的色彩层次比较丰富，则不适合使用这种方式，因为其Base64编码后的字符串非常大，会明显增大HTML页面，影响加载速度。MIME（多用途互联网邮件扩展）我们的电子邮件系统，一般是使用SMTP（简单邮件传输协议）将邮件从客户端发往服务器端，邮件客户端使用POP3（邮局协议，第3版本）或IMAP（交互邮件访问协议）从服务器端获取邮件。SMTP协议一开始是基于纯ASCII文本的，对于二进制文件（比如邮件附件中的图像、声音等）的处理并不好，所以后来新增MIME标准来编码二进制文件，使其能够通过SMTP协议传输。举例来说，我给自己发封邮件，正文为空，带一个名为hello.txt的附件，内容为 您好！世界！。导出邮件源码，其关键部分如下图所示：MIME-Version: 1.0：表示当前使用MIME标准1.0版本。Content-Type: text/plain; name=”hello.txt”：表示附件文件名为 hello.txt ，格式为纯文本。Content-Transfer-Encoding: base64：表示附件文件内容使用base64编码后传输。5oKo5aW977yM5LiW55WM77yB：则是文件内容 您好，世界！ Base64编码后的结果。不过，MIME使用的不是标准Base64编码。切忌误用可能会有人在不理解Base64编码的情况下，将其误用于数据加密或数据校验。Base64是一种数据编码方式，目的是让数据符合传输协议的要求。标准Base64编码解码无需额外信息即完全可逆，即使你自己自定义字符集设计一种类Base64的编码方式用于数据加密，在多数场景下也较容易破解。对于数据加密应该使用专门的目前还没有有效方式快速破解的加密算法。比如：对称加密算法AES-128-CBC，对称加密需要密钥，只要密钥没有泄露，通常难以破解；也可以使用非对称加密算法，如 RSA，利用极大整数因数分解的计算量极大这一特点，使得使用公钥加密的数据，只有使用私钥才能快速解密。对于数据校验，也应该使用专门的消息认证码生成算法，如 HMAC - 一种使用单向散列函数构造消息认证码的方法，其过程是不可逆的、唯一确定的，并且使用密钥来生成认证码，其目的是防止数据在传输过程中被篡改或伪造。将原始数据与认证码一起传输，数据接收端将原始数据使用相同密钥和相同算法再次生成认证码，与原有认证码进行比对，校验数据的合法性。那么针对各大网站被脱库的问题，请问应该怎么存储用户的登录密码？答案是：在注册时，根据用户设置的登录密码，生成其消息认证码，然后存储用户名和消息认证码，不存储原始密码。每次用户登录时，根据登录密码，生成消息认证码，与数据库中存储的消息认证码进行比对，以确认是否为有效用户，这样即使网站被脱库，用户的原始密码也不会泄露，不会为用户使用的其他网站带来账号风险。当然，使用的消息认证码算法其哈希碰撞的概率应该极低才行，目前一般在HMAC算法中使用SHA256。对于这种方式需要注意一点：防止用户使用弱密码，否则也可能会被暴力破解。现在的网站一般要求用户密码6个字符以上，并且同时有数字和大小写字母，甚至要求有特殊字符。另外，也可以使用加入随机salt的哈希算法来存储校验用户密码。这里暂不细述。总结Base64兼顾字符集大小和编码后数据长度，并且可以灵活替换字符集的最后两个字符，以应对多样的需求，使其适用场景非常广泛。当然，很多场景下有多种编码方式可选择，并非Base64编码不可，视需求，权衡利弊而定。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Base64</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏五之演示Demo]]></title>
    <url>%2F2017%2F10%2F10%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%BA%94%E4%B9%8B%E6%BC%94%E7%A4%BADemo%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[在浏览器中玩 移动蓝球 ：受 Player1 控制, 用左右箭头键 移动红球 ：受 Player2 控制, 用A和D键 . . . canvas { border: dotted 1px; padding: 0; background: lightgray;}This is a sample implementation of a client-server architecture demonstrating the main concepts explained in my Fast-Paced Multiplayer(原文出处) series of articles. It won’t make much sense unless you’ve read the articles first.The code is pure JavaScript and it’s fully contained in this page. It’s less than 500 lines of code, including a lot of comments, showing that once you really understand the concepts, implementing them is relatively straightforward.Although it’s not production-quality code, you may use this code in your own applications. Credit is appreciated although not required.Player 1 view - move with LEFT and RIGHT arrow keys Lag = ms · Prediction · Reconciliation · InterpolationWaiting for connection…Server view · Update times per secondPlayer 2 view - move with A and D keys Lag = ms · Prediction · Reconciliation · InterpolationWaiting for connection… Guided Tour Move the blue ball. There’s considerable delay between pressing the arrow keys and the blue ball actually moving. Without client-side prediction, the client only renders the new position of the ball only after a round-trip to the server. Because of the 250ms lag, this takes a while.Set the player 1 Lag to 0ms, and try again. Now the client and the server move in sync because there’s no delay between them, but the movement isn’t smooth, because the server only updates its internal state 3 times per second. If you increase the update rate of the server to 60, we get smooth movement.But this is not a very realistic scenario. Set the player 1 lag back to 250ms, and the server update rate back to 3. This is closer to the awful conditions where a real game still needs to work.Client-side prediction and server reconciliation to the rescue! Enable both of them for Player 1 and move the blue ball. Now the movement is very smooth, and there’s no perceptible delay between pressing the arrow keys and moving the ball.This still works if you make the conditions even worse - try setting the player 1 lag to 500ms and the server update rate to 1.Now things look fantastic for player 1’s own entity, the blue ball. However, player 2’s view of this same entity looks terrible. Because the low update rate of the server, player 2 only gets a new position for player 1’s entity once per second, so the movement is very jumpy.Enabling client-side prediction and server reconciliation for player 2 do nothing to smooth the movement of the blue ball, because these techniques only affect how a player renders its own entity. It does make a difference if you move the red ball, but now we have the same jumpiness in player 1’s view.To solve this, we use entity interpolation. Enable entity interpolation for player 2 and move the blue ball. Now it moves smoothly, but is always rendered “in the past” compared to player 1 and to the server.You may notice the speed of the interpolated entities may vary. This is an artifact of the interpolation, caused by setting the server update rate too low in relationship with the speeds. This effect should disappear almost entirely if you set the server update rate to 10, which is still pretty low.SummaryClient-Side Prediction and Server Reconciliation are very powerful techniques to make multiplayer games feel responsive even under extremely bad network conditions. Therefore, they are a fundamental part of almost any client/server multiplayer network architecture. // ============================================================================= // An Entity in the world. // ============================================================================= var Entity = function() { this.x = 0; this.speed = 2; // units/s this.position_buffer = []; } // Apply user's input to this entity. Entity.prototype.applyInput = function(input) { this.x += input.press_time*this.speed; } // ============================================================================= // A message queue with simulated network lag. // ============================================================================= var LagNetwork = function() { this.messages = []; } // "Send" a message. Store each message with the timestamp when it should be // received, to simulate lag. LagNetwork.prototype.send = function(lag_ms, message) { this.messages.push({recv_ts: +new Date() + lag_ms, payload: message}); } // Returns a "received" message, or undefined if there are no messages available // yet. LagNetwork.prototype.receive = function() { var now = +new Date(); for (var i = 0; i < this.messages.length; i++) { var message = this.messages[i]; if (message.recv_ts]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[白话TCP快速重传]]></title>
    <url>%2F2017%2F09%2F23%2F%E7%99%BD%E8%AF%9Dtcp%E5%BF%AB%E9%80%9F%E9%87%8D%E4%BC%A0%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[快速重传算法概绍在收到一个失序的报文段时， TCP立即需要产生一个ACK(一个重复的ACK)。这个重复的ACK不应该被迟延。该重复的ACK的曰的在于让对方知道收到一个失序的报文段，并告诉对方自己希望收到的序号。 由于我们不知道一个重复的ACK足由一个丢失的报文段引起的，还是由于仅仅出现了几个报文段的重新排序，因此我们等待少量重复的ACK到来。假如这只是一些报文段的重新排序，则在重新排序的报文段被处理并产生一个新的ACK之前，只可能产生1～2个重复的ACK。如果一连串收到3个或3个以上的重复ACK，就非常可能足一个报文段丢失了。于足我们就重传丢失的数据报文段，而无需等待超时定时器溢出。这就是快速重传算法。接下来执行的不是慢启动算法而是拥塞避免算法。这就是快速恢复算法。 在这种情况下没有执行慢启动的原因是由于收到重复的ACK不仅仅告诉我们一个分组丢失了。由于接收方只有在收到另一个报文段耐才会产生重复的ACK，而该报文段已经离开了网络并进入了接收方的缓存。也就是说，在收发两端之问仍然有流动的数据，而我们不想执行慢启动来突然减少数据流。 . . . 为什么是3个重复ACK之后就要重传?两次duplicated ACK肯定是乱序造成的！丢包肯定会造成三次duplicated ACK!假定通信双方如下，A发送4个TCP Segment 给B，编号如下，N-1成功到达，因为A收到B的ACK(N)，其它按照到达顺序，分别收到ACK(N)的数目： A ———&gt; BA方发送顺序N-1，N，N+1，N+2B方到达顺序N-1，N，N+1，N+2 A收到1个ACK (N)N-1，N，N+2，N+1 A收到1个ACK (N)N-1，N+1，N，N+2 A收到2个ACK (N)N-1，N+1，N+2，N A收到3个ACK (N)N-1，N+2，N，N+1 A收到2个ACK (N)N-1，N+2，N+1，N A收到3个ACK (N)如果N丢了，没有到达BN-1，N+1，N+2 A收到3个ACK (N)N-1，N+2，N+1 A收到3个ACK (N)TCP segment 乱序 有2/5 = 40% 的概率会造成A收到三次 duplicated ACK(N);而如果N丢了，则会100%概率A会收到三次duplicated ACK(N);基于以上的统计，当A接收到三次 duplicated ACK(N)启动 Fast Retransmit 算法是合理的，即立马retransmit N，可以起到Fast Recovery的功效，快速修复一个丢包对TCP管道的恶劣影响。而如果A接收到二次 duplicated ACK(N)，则一定说明是乱序造成的，即然是乱序，说明 数据都到达了B，B的TCP负责重新排序而已，没有必要A再来启动Fast Retransmit算法。补充阅读——————————————–TCP segment 乱序的由来TCP segment 封装在IP包里，如果IP包乱序，则相应TCP也会乱序，乱序的原因一般如下：1）ECMP 负载均衡多路径的负载均衡，基于per-packet load balance，比如 packet 1，3，5…走路径1，packet 2，4，6…走路径2，很难保证packet 1 在 packet 2 之前到达目的地。Per-session load balance 会基于TCP五元组来负载均衡，同一个TCP会话会走同一条路径，克服多路径造成的乱序。2）路由器内部流量调度有些路由器采用多个流量处理单元，比如packet 1，3，5…由处理单元1来处理，packet 2，4，6…由处理单元2来处理，也很难保证packet 1 在 packet 2 之前到达目的地。TCP接收到乱序的segment，会放在自己的接收缓冲区，等所有乱序的segment 都顺利到达，TCP重新排序，并将数据提交给 application。乱序的segment 会占用接收缓冲区，直接造成B advertised window size 变小，造成对方A发送window 一直在变小，影响A发送效率。即使A不快速重传，最后也会由retransmit timer timeout 超时重传，但这个时候A的发送window 非常小，发送速率也从天上掉到了地下。———-///——–在没有fast retransmit / recovery 算法之前，重传依靠发送方的retransmit timeout，就是在timeout内如果没有接收到对方的ACK，默认包丢了，发送方就重传，包的丢失原因 1）包checksum 出错 2）网络拥塞 3）网络断，包括路由重收敛，但是发送方无法判断是哪一种情况，于是采用最笨的办法，就是将自己的发送速率减半，即CWND 减为1/2，这样的方法对2是有效的，可以缓解网络拥塞，3则无所谓，反正网络断了，无论发快发慢都会被丢；但对于1来说，丢包是因为偶尔的出错引起，一丢包就对半减速不合理。于是有了fast retransmit 算法，基于在反向还可以接收到ACK，可以认为网络并没有断，否则也接收不到ACK，如果在timeout 时间内没有接收到&gt; 2 的duplicated ACK，则概率大事件为乱序，乱序无需重传，接收方会进行排序工作；而如果接收到三个或三个以上的duplicated ACK，则大概率是丢包，可以逻辑推理，发送方可以接收ACK，则网络是通的，可能是1、2造成的，先不降速，重传一次，如果接收到正确的ACK，则一切OK，流速依然（包出错被丢）。而如果依然接收到duplicated ACK，则认为是网络拥塞造成的，此时降速则比较合理。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于hexo的next个性化配置小技巧]]></title>
    <url>%2F2017%2F09%2F21%2F%E5%9F%BA%E4%BA%8Ehexo%E7%9A%84next%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文出处 1. 在右上角或者左上角实现fork me on github实现效果图具体实现方法点击这里挑选自己喜欢的样式，并复制代码。 例如，我是复制如下代码：然后粘贴刚才复制的代码到themes/next/layout/_layout.swig文件中(放在&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;的下面)，并把href改为你的github地址2.添加RSS实现效果图具体实现方法切换到你的blog（我是取名blog，具体的看你们的取名是什么）的路径，例如我是在/Users/chenzekun/Code/Hexo/blog这个路径上，也就是在你的根目录下然后安装 Hexo 插件：(这个插件会放在node_modules这个文件夹里)1$ npm install &minus;&minus;save hexo-generator-feed接下来打开画红线的文件，如下图：在里面的末尾添加：(请注意在冒号后面要加一个空格，不然会发生错误！)123# Extensions## Plugins: http://hexo.io/plugins/plugins: hexo-generate-feed然后打开next主题文件夹里面的_config.yml,在里面配置为如下样子：(就是在rss:的后面加上/atom.xml,注意在冒号后面要加一个空格)1234# Set rss to false to disable feed link.# Leave rss as empty to use site‘s feed link.# Set rss to specific value if you have burned your feed already.rss: /atom.xml配置完之后运行：1$ hexo g重新生成一次，你会在./public 文件夹中看到 atom.xml 文件。然后启动服务器查看是否有效，之后再部署到 Github 中。3. 添加动态背景实现效果图具体实现方法这个我之前有一篇文章有讲过了，详情点击我的博客4. 实现点击出现桃心效果实现效果图具体实现方法在网址输入如下1http://7u2ss1.com1.z0.glb.clouddn.com/love.js然后将里面的代码copy一下，新建love.js文件并且将代码复制进去，然后保存。将love.js文件放到路径/themes/next/source/js/src里面，然后打开\themes\next\layout_layout.swig文件,在末尾（在前面引用会出现找不到的bug）添加以下代码：12&lt;!– 页面点击小红心 –&gt;&lt;script type=“text/javascript” src=“/js/src/love.js”&gt;&lt;/script&gt;5. 修改文章内链接文本样式实现效果图具体实现方法修改文件 themes\next\source\css_common\components\post\post.styl，在末尾添加如下css样式，：1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125;&#125;其中选择.post-body 是为了不影响标题，选择 p 是为了不影响首页“阅读全文”的显示样式,颜色可以自己定义。6. 修改文章底部的那个带#号的标签实现效果图具体实现方法修改模板/themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将 # 换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;7. 在每篇文章末尾统一添加“本文结束”标记实现效果图具体实现方法在路径 \themes\next\layout_macro 中新建 passage-end-tag.swig 文件,并添加以下内容：12345&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style=“text-align:center;color: #ccc;font-size:14px;”&gt;————-本文结束&lt;i class=“fa fa-paw”&gt;&lt;/i&gt;感谢您的阅读————-&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt;接着打开\themes\next\layout_macro\post.swig文件，在post-body 之后， post-footer 之前添加如下画红色部分代码（post-footer之前两个DIV）：代码如下：12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include ‘passage-end-tag.swig’ %&#125; &#123;% endif %&#125;&lt;/div&gt;然后打开主题配置文件（_config.yml),在末尾添加：123# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true完成以上设置之后，在每篇文章之后都会添加如上效果图的样子。8. 修改作者头像并旋转实现效果图具体实现方法打开\themes\next\source\css_common\components\sidebar\sidebar-author.styl，在里面添加如下代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; / 头像圆形 / border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; / 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]/ / 鼠标经过头像旋转360度 / -webkit-transition: -webkit-transform 1.0s ease-out; -moz-transition: -moz-transform 1.0s ease-out; transition: transform 1.0s ease-out;&#125;img:hover &#123; / 鼠标经过停止头像旋转 -webkit-animation-play-state:paused; animation-play-state:paused;/ / 鼠标经过头像旋转360度 / -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);&#125;/ Z 轴旋转动画 /@-webkit-keyframes play &#123; 0% &#123; -webkit-transform: rotateZ(0deg); &#125; 100% &#123; -webkit-transform: rotateZ(-360deg); &#125;&#125;@-moz-keyframes play &#123; 0% &#123; -moz-transform: rotateZ(0deg); &#125; 100% &#123; -moz-transform: rotateZ(-360deg); &#125;&#125;@keyframes play &#123; 0% &#123; transform: rotateZ(0deg); &#125; 100% &#123; transform: rotateZ(-360deg); &#125;&#125;9. 博文压缩在站点的根目录下执行以下命令：12$ npm install gulp -g$ npm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp &minus;&minus;save在如下图所示，新建 gulpfile.js ，并填入以下内容：123456789101112131415161718192021222324252627282930313233var gulp = require(‘gulp’);var minifycss = require(‘gulp-minify-css’);var uglify = require(‘gulp-uglify’);var htmlmin = require(‘gulp-htmlmin’);var htmlclean = require(‘gulp-htmlclean’);// 压缩 public 目录 cssgulp.task(‘minify-css’, function() &#123; return gulp.src(‘./public//*.css’) .pipe(minifycss()) .pipe(gulp.dest(‘./public’));&#125;);// 压缩 public 目录 htmlgulp.task(‘minify-html’, function() &#123; return gulp.src(‘./public//.html’) .pipe(htmlclean()) .pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest(‘./public’))&#125;);// 压缩 public/js 目录 jsgulp.task(‘minify-js’, function() &#123; return gulp.src(‘./public/**/.js’) .pipe(uglify()) .pipe(gulp.dest(‘./public’));&#125;);// 执行 gulp 命令时执行的任务gulp.task(‘default’, [ ‘minify-html’,‘minify-css’,‘minify-js’]);生成博文是执行 hexo g &amp;&amp; gulp 就会根据 gulpfile.js 中的配置，对 public 目录中的静态资源文件进行压缩。 10. 修改代码块自定义样式实现效果图具体实现方法打开\themes\next\source\css_custom\custom.styl,向里面加入：(颜色可以自己定义)123456789101112131415// Custom styles.code &#123; color: #ff7600; background: #fbf7f8; margin: 2px;&#125;// 大代码块的自定义样式.highlight, pre &#123; margin: 5px 0; padding: 5px; border-radius: 3px;&#125;.highlight, code, pre &#123; border: 1px solid #d6d6d6;&#125;11. 侧边栏社交小图标设置实现效果图具体实现方法打开主题配置文件（_config.yml），搜索social_icons:,在图标库找自己喜欢的小图标，并将名字复制在如下位置，保存即可12. 主页文章添加阴影效果实现效果图具体实现方法打开\themes\next\source\css_custom\custom.styl,向里面加入：12345678// 主页文章添加阴影效果 .post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5); &#125;13. 在网站底部加上访问量实现效果图具体实现方法打开\themes\next\layout_partials\footer.swig文件,在copyright前加上画红线这句话：代码如下：1&lt;script async src=“https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js“&gt;&lt;/script&gt;然后再合适的位置添加显示统计的代码，如图：代码如下：12345&lt;div class=“powered-by”&gt;&lt;i class=“fa fa-user-md”&gt;&lt;/i&gt;&lt;span id=“busuanzi_container_site_uv”&gt; 本站访客数:&lt;span id=“busuanzi_value_site_uv”&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;在这里有两中不同计算方式的统计代码：pv的方式，单个用户连续点击n篇文章，记录n次访问量123&lt;span id=“busuanzi_container_site_pv”&gt; 本站总访问量&lt;span id=“busuanzi_value_site_pv”&gt;&lt;/span&gt;次&lt;/span&gt;uv的方式，单个用户连续点击n篇文章，只记录1次访客数123&lt;span id=“busuanzi_container_site_uv”&gt; 本站总访问量&lt;span id=“busuanzi_value_site_uv”&gt;&lt;/span&gt;次&lt;/span&gt;添加之后再执行hexo d -g，然后再刷新页面就能看到效果14. 添加热度实现效果图具体实现方法next主题集成leanCloud，打开/themes/next/layout/_macro/post.swig,在画红线的区域添加℃：然后打开，/themes/next/languages/zh-Hans.yml,将画红框的改为热度就可以了15. 网站底部字数统计实现效果图具体方法实现切换到根目录下，然后运行如下代码1$ npm install hexo-wordcount &minus;&minus;save然后在/themes/next/layout/_partials/footer.swig文件尾部加上：1234&lt;div class=“theme-info”&gt; &lt;div class=“powered-by”&gt;&lt;/div&gt; &lt;span class=“post-count”&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;&lt;/div&gt;16. 添加 README.md 文件每个项目下一般都有一个 README.md 文件，但是使用 hexo 部署到仓库后，项目下是没有 README.md 文件的。在 Hexo 目录下的 source 根目录下添加一个 README.md 文件，修改站点配置文件 _config.yml，将 skip_render 参数的值设置为1skip_render: README.md保存退出即可。再次使用 hexo d 命令部署博客的时候就不会在渲染 README.md 这个文件了。17. 设置网站的图标Favicon实现效果图具体方法实现在EasyIcon中找一张（3232）的ico图标,或者去别的网站下载或者制作，并将图标名称改为favicon.ico，然后把图标放在/themes/next/source/images里，并且修改主题配置文件：12# Put your favicon.ico into hexo-site/source/ directory.favicon: /favicon.ico18. 实现统计功能实现效果图具体实现方法在根目录下安装 hexo-wordcount,运行：1$ npm install hexo-wordcount &minus;&minus;save然后在主题的配置文件中，配置如下：123456# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcountpost_wordcount: item_text: true wordcount: true min2read: true19. 添加顶部加载条实现效果图具体实现方法打开/themes/next/layout/_partials/head.swig文件，添加红框上的代码代码如下：12&lt;script src=“//cdn.bootcss.com/pace/1.0.2/pace.min.js”&gt;&lt;/script&gt;&lt;link href=“//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css” rel=“stylesheet”&gt;但是，默认的是粉色的，要改变颜色可以在/themes/next/layout/_partials/head.swig文件中添加如下代码（接在刚才link的后面）12345678910111213&lt;style&gt; .pace .pace-progress &#123; background: #1E92FB; /进度条颜色/ height: 3px; &#125; .pace .pace-progress-inner &#123; box-shadow: 0 0 10px #1E92FB, 0 0 5px #1E92FB; /阴影颜色/ &#125; .pace .pace-activity &#123; border-top-color: #1E92FB; /上边框颜色/ border-left-color: #1E92FB; /左边框颜色/ &#125;&lt;/style&gt;目前，博主的增加顶部加载条的pull request 已被Merge😀===&gt;详情现在升级最新版的next主题，升级后只需修改主题配置文件(_config.yml)将pace: false改为pace: true就行了，你还可以换不同样式的加载条，如下图：20. 在文章底部增加版权信息实现效果图在目录 next/layout/_macro/下添加 my-copyright.swig：1234567891011121314151617181920212223242526272829303132&#123;% if page.copyright %&#125;&lt;div class=“my_post_copyright”&gt; &lt;script src=“//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js”&gt;&lt;/script&gt; &lt;!– JS库 sweetalert 可修改路径 –&gt; &lt;script type=“text/javascript” src=“http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js“&gt;&lt;/script&gt; &lt;script src=“http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js“&gt;&lt;/script&gt; &lt;link rel=“stylesheet” type=“text/css” href=“http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css“&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href=“&#123;&#123; url_for(page.path) &#125;&#125;“&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href=“/“ title=“访问 &#123;&#123; theme.author &#125;&#125; 的个人博客”&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format(“YYYY年MM月DD日 - HH:MM”) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;最后更新:&lt;/span&gt;&#123;&#123; page.updated.format(“YYYY年MM月DD日 - HH:MM”) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href=“&#123;&#123; url_for(page.path) &#125;&#125;“ title=“&#123;&#123; page.title &#125;&#125;“&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class=“copy-path” title=“点击复制文章链接”&gt;&lt;i class=“fa fa-clipboard” data-clipboard-text=“&#123;&#123; page.permalink &#125;&#125;“ aria-label=“复制成功！”&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class=“fa fa-creative-commons”&gt;&lt;/i&gt; &lt;a rel=“license” href=“https://creativecommons.org/licenses/by-nc-nd/4.0/“ target=“_blank” title=“Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)”&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard(‘.fa-clipboard’); clipboard.on(‘success’, $(function()&#123; $(“.fa-clipboard”).click(function()&#123; swal(&#123; title: “”, text: ‘复制成功’, html: false, timer: 500, showConfirmButton: false &#125;); &#125;); &#125;)); &lt;/script&gt;&#123;% endif %&#125;在目录next/source/css/_common/components/post/下添加my-post-copyright.styl：123456789101112131415161718192021222324252627282930313233343536373839404142434445.my_post_copyright &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.my_post_copyright p&#123;margin:0;&#125;.my_post_copyright span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.my_post_copyright .raw &#123; margin-left: 1em; width: 5em;&#125;.my_post_copyright a &#123; color: #808080; border-bottom:0;&#125;.my_post_copyright a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.my_post_copyright:hover .fa-clipboard &#123; color: #000;&#125;.my_post_copyright .post-url:hover &#123; font-weight: normal;&#125;.my_post_copyright .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.my_post_copyright .copy-path:hover &#123; color: #808080; cursor: pointer;&#125;修改next/layout/_macro/post.swig，在代码12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include ‘wechat-subscriber.swig’ %&#125; &#123;% endif %&#125;&lt;/div&gt;之前添加增加如下代码：12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include ‘my-copyright.swig’ %&#125; &#123;% endif %&#125;&lt;/div&gt;如下：修改next/source/css/_common/components/post/post.styl文件，在最后一行增加代码：1@import “my-post-copyright”保存重新生成即可。如果要在该博文下面增加版权信息的显示，需要在 Markdown 中增加copyright: true的设置，类似：小技巧：如果你觉得每次都要输入copyright: true很麻烦的话,那么在/scaffolds/post.md文件中添加：这样每次hexo new &quot;你的内容&quot;之后，生成的md文件会自动把copyright:加到里面去(注意：如果解析出来之后，你的原始链接有问题：如：http://yoursite.com/前端小项目：使用canvas绘画哆啦A梦.html,那么在根目录下_config.yml中写成类似这样：）就行了。21. 添加网易云跟帖(跟帖关闭，已失效，改为来必力)实现效果图具体方法实现有两种实现方法：①更新next主题，因为最新版本的主题已经支持这种评论。直接在主题配置文件_config.yml 文件中添加如下配置:1gentie_productKey: #your-gentie-product-key②如果你不想更新的话，那么按下面步骤进行：首先，还是在主题配置文件_config.yml 文件中添加如下配置:1gentie_productKey: #your-gentie-product-key你的productKey就是下面画红线部分然后在在layout/_scripts/third-party/comments/ 目录中添加 gentie.swig，文件内容如下：1234567891011121314&#123;% if not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname and not theme.disqus_shortname and not theme.hypercomments_id %&#125; &#123;% if theme.gentie_productKey %&#125; &#123;% set gentie_productKey = theme.gentie_productKey %&#125; &lt;script&gt; var cloudTieConfig = &#123; url: document.location.href, sourceId: “”, productKey: “&#123;&#123;gentie_productKey&#125;&#125;“, target: “cloud-tie-wrapper” &#125;; &lt;/script&gt; &lt;script src=“https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js“&gt;&lt;/script&gt; &#123;% endif %&#125;&#123;% endif %&#125;然后在layout/_scripts/third-party/comments.swig文件中追加：1&#123;% include ‘./comments/gentie.swig’ %&#125;最后，在 layout/_partials/comments.swig 文件中条件最后追加网易云跟帖插件引用的判断逻辑：123&#123;% elseif theme.gentie_productKey %&#125; &lt;div id=“cloud-tie-wrapper” class=“cloud-tie-wrapper”&gt; &lt;/div&gt;具体位置如下：可能你hexo s时可能看不到，直接hexo d就可以看到了近日，我朋友发来消息，说网易云跟帖要关了，我网上查了一下，果然如此😭都是泪,上次用了多说，结果多说关了，接着是网易云跟帖😷，这次直接用国外的来必力，应该不会这么容易关吧😏方法其实还是跟上面差不多的首先在 _config.yml 文件中添加如下配置：(注意！如果主题是最新版的，直接写你的liver_uid就行了)123# Support for LiveRe comments system.# You can get your uid from https://livere.com/insight/myCode (General web site)livere_uid: your uid其中，livere_uid就是画红线的部分然后在 layout/_scripts/third-party/comments/ 目录中添加 livere.swig，文件内容如下：1234567891011121314&#123;% if not (theme.duoshuo and theme.duoshuo.shortname) and not theme.duoshuo_shortname and not theme.disqus_shortname and not theme.hypercomments_id and not theme.gentie_productKey %&#125; &#123;% if theme.livere_uid %&#125; &lt;script type=“text/javascript”&gt; (function(d, s) &#123; var j, e = d.getElementsByTagName(s)[0]; if (typeof LivereTower === ‘function’) &#123; return; &#125; j = d.createElement(s); j.src = ‘https://cdn-city.livere.com/js/embed.dist.js‘; j.async = true; e.parentNode.insertBefore(j, e); &#125;)(document, ‘script’); &lt;/script&gt; &#123;% endif %&#125;&#123;% endif %&#125;然后在 layout/_scripts/third-party/comments.swig文件中追加：1&#123;% include ‘./comments/livere.swig’ %&#125;最后，在 layout/_partials/comments.swig 文件中条件最后追加 LiveRe 插件是否引用的判断逻辑：123&#123;% elseif theme.livere_uid %&#125; &lt;div id=“lv-container” data-id=“city” data-uid=“&#123;&#123; theme.livere_uid &#125;&#125;“&gt;&lt;/div&gt;&#123;% endif %&#125;完22. 隐藏网页底部powered By Hexo / 强力驱动打开themes/next/layout/_partials/footer.swig,使用””隐藏之间的代码即可，或者直接删除。位置如图：23. 修改网页底部的桃心还是打开themes/next/layout/_partials/footer.swig，找到：，然后还是在图标库中找到你自己喜欢的图标，然后修改画红线的部分就可以了。24. 文章加密访问实现效果图具体实现方法打开themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig文件,在以下位置插入这样一段代码：代码如下：12345678910&lt;script&gt; (function()&#123; if(‘&#123;&#123; page.password &#125;&#125;’)&#123; if (prompt(‘请输入文章密码’) !== ‘&#123;&#123; page.password &#125;&#125;’)&#123; alert(‘密码错误！’); history.back(); &#125; &#125; &#125;)();&lt;/script&gt;然后在文章上写成类似这样：25. 添加jiathis分享在主题配置文件中，jiathis为true，就行了，如下图默认是这样子的：如果你想自定义话，打开themes/next/layout/_partials/share/jiathis.swig修改画红线部分就可以了26. 博文置顶修改 hero-generator-index 插件，把文件：node_modules/hexo-generator-index/lib/generator.js 内的代码替换为：12345678910111213141516171819202122232425262728‘use strict’;var pagination = require(‘hexo-pagination’);module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 &#125; else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); var paginationDir = config.pagination_dir || ‘page’; return pagination(‘’, posts, &#123; perPage: config.index_generator.per_page, layout: [‘index’, ‘archive’], format: paginationDir + ‘/%d/‘, data: &#123; __index: true &#125; &#125;);&#125;;在文章中添加 top 值，数值越大文章越靠前，如12345678—title: 解决Charles乱码问题date: 2017-05-22 22:45:48tags: 技巧categories: 技巧copyright: truetop: 100—27. 修改字体大小打开\themes\next\source\css\ _variables\base.styl文件，将$font-size-base改成16px，如下所示：1$font-size-base =16px28. 修改打赏字体不闪动修改文件next/source/css/_common/components/post/post-reward.styl，然后注释其中的函数wechat:hover和alipay:hover，如下：123456789101112/ 注释文字闪动函数 #wechat:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125; #alipay:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125;/29. 侧边栏推荐阅读今天有位网友问推荐阅读是怎么弄，其实挺简单的，打开主题配置文件修改成这样就行了(links里面写你想要的链接):1234567891011# Blogrollslinks_title: 推荐阅读#links_layout: blocklinks_layout: inlinelinks: 优设: http://www.uisdc.com/ 张鑫旭: http://www.zhangxinxu.com/ Web前端导航: http://www.alloyteam.com/nav/ 前端书籍资料: http://www.36zhen.com/t?id=3448 百度前端技术学院: http://ife.baidu.com/ google前端开发基础: http://wf.uisdc.com/cn/30. 自定义鼠标样式打开themes/next/source/css/_custom/custom.styl,在里面写下如下代码1234567// 鼠标样式 &#123; cursor: url(“http://om8u46rmb.bkt.clouddn.com/sword2.ico“),auto!important &#125; :active &#123; cursor: url(“http://om8u46rmb.bkt.clouddn.com/sword1.ico“),auto!important &#125;其中 url 里面必须是 ico 图片，ico 图片可以上传到网上（我是使用七牛云图床），然后获取外链，复制到 url 里就行了31.为博客加上萌萌的宠物实现效果图具体实现方法在终端切换到你的博客的路径里，然后输入如下代码：1npm install --save hexo-helper-live2d然后打开Hexo/blog/themes/next/layout的_layout.swig,将下面代码放到&lt;/body&gt;之前：1&#123;&#123; live2d() &#125;&#125;然后在在 hexo 的 _config.yml中添加参数：123live2d: model: wanko bottom: -30然后hexo clean ，hexo g ，hexo d 就可以看到了。下面是一些model，可以换不同的宠物model 模型名称 默认值: z16Gantzert_FelixanderEpsilon2.1harumikuni-jniconitonipsilonnietzscheshizukutsumikiwankoz16hibikikoharuharutoUnitychantororohijikiwidth 宽度 默认值: 150height 高度 默认值： 300className &lt;canvas&gt;元素的类名 默认值： live2did &lt;canvas&gt; 元素的id 默认值： live2dcanvasbottom &lt;canvas&gt; 元素的底部偏移 默认值： -20 如果嫌模型位置不正确 可以调整这个参数用这个有缺点，如果是在手机上看的话，感觉不是很好，宠物一直挡着文字😂😂，还有就是加载有点慢注意！如果你在 hexo d 的时候出现我下面这个问题你可以这样，首先删除hexo 下面的.deploy_git文件夹，然后运行1git config –global core.autocrlf false重新 hexo clean,hexo g,hexo d就行了致谢感谢大神们的文章，真的学到了许多，有些忘了记录下来，在这里由衷的感谢。虽然比较折腾，但是确实满满的成就感，Road endless its long and far, I will seek up and down！欢迎访问我的博客参考的文章：http://blog.csdn.net/MasterAnt_D/article/details/56839222http://zidingyi4qh.com/2017/04/27/NexT%E5%BA%95%E9%83%A8logo%E6%B7%BB%E5%8A%A0%E8%AE%BF%E9%97%AE%E9%87%8F/https://fuyis.me/2017/01/25/Hexo-theme-next-and-optimized-configuration/http://www.vitah.net/posts/20f300cc/http://thief.one/2017/03/03/Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Next</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kbe服务端笔记(二)]]></title>
    <url>%2F2017%2F07%2F29%2Fkbe_note_two%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[FixedMessages：FixedMessages存储所有固定消息（有显示制定id的消息，当然，这并不表示非固定消息就没有id，也是有的，只是不是显示制定的）。 它的构造地方如下（lib/network/message_handler.cpp）：123456789101112MessageHandlers::MessageHandlers():msgHandlers_(),msgID_(1),exposedMessages_()&#123; g_fm = Network::FixedMessages::getSingletonPtr(); if(g_fm == NULL) g_fm = newNetwork::FixedMessages; Network::FixedMessages::getSingleton().loadConfig(&quot;server/messages_fixed.xml&quot;); messageHandlers().push_back(this);&#125; 意即MessageHandlers构造的时候，如果它还没构造，那就构造。它的初始化（配置）是由loadConfig接口来完成的，代码见上。 . . . loginapp Loginapp组件主要用来处理账户登录/注册的业务 消息与handler映射的建立：两次包含xxx_interface.h，实现声明和定义：每个app组件的接口定义都在xxxapp_interface.cpp中开始，代码如下：123456789#include&quot;loginapp_interface.h&quot;#defineDEFINE_IN_INTERFACE#defineLOGINAPP#include&quot;loginapp_interface.h&quot;namespaceKBEngine&#123;namespaceLoginappInterface&#123;&#125;&#125; 所有的戏法都是通过包含loginapp_interface.h前后定义了DEFINE_IN_INTERFACE和LOGINAPP来完成的。第一次的包含就是各种变量，类的声明（当然也有一些类是声明类时使用类inline函数定义完成了，比如MESSAGE_ARGS0/1/2……）。我们看看loginapp_interface.h中的代码： 消息与handlers的存储首先是这一句：NETWORK_INTERFACE_DECLARE_BEGIN(LoginappInterface)此句展开的话声明和定义了Network::MessageHandlers messageHandlers（记住它们都在LoginappInterface命名空间内），展开宏之后的代码看起来像这样（是的，你的眼睛是好的，没有}闭合）：声明：12345namespaceLoginappInterface &#123;extern Network::MessageHandlers messageHandlers;定义：namespaceLoginappInterface &#123; Network::MessageHandlers messageHandlers; 消息与handle建立映射然后是这一句：LOGINAPP_MESSAGE_DECLARE_ARGS0(importClientMessages, NETWORK_FIXED_MESSAGE)此句展开的话分明声明和定义了一个importClientMessagesLoginappMessagehandler0的类，这个类继承自Network::MessageHandler，这里就是实现了handle的虚函数接口；声明和定义了importClientMessagesLoginappMessagehandler0的一个名为importClientMessages的全局变量；声明和定义了importClientMessagesArgs0的类，这个类继承自Network::MessageArgs。我们一个个地分析一下：首先展开下面的宏：1LOGINAPP_MESSAGE_DECLARE_ARGS0(importClientMessages, NETWORK_FIXED_MESSAGE) 之后是这样：12345678910111213141516#defineLOGINAPP_MESSAGE_DECLARE_ARGS0(NAME, MSG_LENGTH) \LOGINAPP_MESSAGE_HANDLER_ARGS0(NAME) \NETWORK_MESSAGE_DECLARE_ARGS0(Loginapp, NAME, \ NAME#LoginappMessagehandler0, MSG_LENGTH)展开LOGINAPP_MESSAGE_HANDLER_ARGS0(NAME)之后分别得到importClientMessagesLoginappMessagehandler0的声明和定义：声明：classimportClientMessagesLoginappMessagehandler0 : public Network::MessageHandler&#123;public:virtualvoidhandle(Network::Channel* pChannel, KBEngine::MemoryStream&amp;s);&#125;;定义：voidimportClientMessagesLoginappMessagehandler0::handle(Network::Channel* pChannel, KBEngine::MemoryStream&amp;s)&#123; KBEngine::Loginapp::getSingleton().importClientMessages(pChannel);&#125; （handle/handler，傻傻分不清楚。。。这里的handle是xxxApp中真正用来处理这个消息的接口，而这里的handler提供一个中间层的作用，集中处理一些通用的工作，可以将耦合减少一点）上面完成了相当于是importClientMessages消息的handler的声明和定义，下面则将这个类实例化之后添加到messageHandlers：12345#defineNETWORK_MESSAGE_DECLARE_ARGS0(DOMAIN, NAME, MSGHANDLER, \ MSG_LENGTH) \ NETWORK_MESSAGE_HANDLER(DOMAIN, NAME, MSGHANDLER, MSG_LENGTH, 0)\ MESSAGE_ARGS0(NAME) \ 展开NETWORK_MESSAGE_HANDLER(DOMAIN, NAME, MSGHANDLER, MSG_LENGTH, 0)之后得到importClientMessages的handler类（importClientMessagesLoginappMessagehandler0）的名为importClientMessages的全局变量（不过欣慰的是他们都在各自的XXXInterface命名空间内）。声明：externconstimportClientMessagesLoginappMessagehandler0&amp;importClientMessages; 定义：12importClientMessagesLoginappMessagehandler0* pimportClientMessages = static_cast&lt;importClientMessagesLoginappMessagehandler0*&gt;(messageHandlers.add(&quot;Loginapp::importClientMessages&quot;,new importClientMessagesArgs0, NETWORK_FIXED_MESSAGE, newimportClientMessagesLoginappMessagehandler0);constimportClientMessagesLoginappMessagehandler0&amp;importClientMessages = *pimportClientMessages; 下面的MESSAGE_ARGS0(NAME)展开后对importClientMessagesArgs0进行了声明和定义（其他它声明的时候就已经完成了全部的定义），声明的时候就是个空语句：声明兼定义：1234567891011121314151617181920212223classimportClientMessagesArgs0 : public Network::MessageArgs&#123;public:importClientMessagesArgs0() :Network::MessageArgs() &#123;&#125; ~importClientMessagesArgs0() &#123;&#125;staticvoidstaticAddToBundle(Network::Bundle&amp;s) &#123; &#125;staticvoidstaticAddToStream(MemoryStream&amp;s) &#123; &#125;virtual int32 dataSize(void) &#123;return 0; &#125;virtualvoidaddToStream(MemoryStream&amp;s) &#123; &#125;virtualvoidcreateFromStream(MemoryStream&amp;s) &#123; &#125;&#125;; 唯一需要小注意一下的就是importClientMessagesArgs0的声明（兼定义）是和importClientMessagesLoginappMessagehandler0的实例的声明和定义是错开的，因为后者实例化添加到messageHandlers的时候需要new一个importClientMessagesArgs0的实例。 流程的伪代码稍微整理一下之后，使用LOGINAPP_MESSAGE_HANDLER_ARGSn建立一个消息到handler的映射的代码很像是这样： 声明：（第一次包含loginapp_interface.h产生的代码） 12345678910111213141516171819202122232425262728293031classimportClientMessagesLoginappMessagehandler0 : public Network::MessageHandler&#123;public:virtualvoidhandle(Network::Channel* pChannel, KBEngine::MemoryStream&amp;s);&#125;;externconstimportClientMessagesLoginappMessagehandler0&amp;importClientMessages;classimportClientMessagesArgs0 : public Network::MessageArgs&#123;public:importClientMessagesArgs0() :Network::MessageArgs() &#123;&#125; ~importClientMessagesArgs0() &#123;&#125;staticvoidstaticAddToBundle(Network::Bundle&amp;s) &#123; &#125;staticvoidstaticAddToStream(MemoryStream&amp;s) &#123; &#125;virtual int32 dataSize(void) &#123;return 0; &#125;virtualvoidaddToStream(MemoryStream&amp;s) &#123; &#125;virtualvoidcreateFromStream(MemoryStream&amp;s) &#123; &#125;&#125;; 定义：（定义DEFINE_IN_INTERFACE和LOGINAPP之后第二次包含loginapp_interface.h产生的代码） 1234567891011voidimportClientMessagesLoginappMessagehandler0::handle(Network::Channel* pChannel, KBEngine::MemoryStream&amp;s)&#123; KBEngine::Loginapp::getSingleton().importClientMessages(pChannel);&#125;importClientMessagesLoginappMessagehandler0* pimportClientMessages = static_cast&lt;importClientMessagesLoginappMessagehandler0*&gt;(messageHandlers.add(&quot;Loginapp::importClientMessages&quot;,newimportClientMessagesArgs0, NETWORK_FIXED_MESSAGE,newimportClientMessagesLoginappMessagehandler0);constimportClientMessagesLoginappMessagehandler0&amp;importClientMessages = *pimportClientMessages; 消息id：固定消息与非固定消息要接着v0.0.3的分析继续写，回过头来要看之前写的东西说实话自己都有点难以理解。。。不过出于幸运或者努力，总算是看懂了;-(，读源代码（感觉特别是C++）本来就不是件容易的事，所以读源代码一定要做好长期战斗的准备。上面我们分析到了，其实一个消息，就是由这样一个宏来和它的handle建立链接的：LOGINAPP_MESSAGE_DECLARE_ARGS0(importClientMessages, NETWORK_FIXED_MESSAGE)通过上面的分析，我们得知，实际上建立消息和handle映射，起到核心作用的接口是messageHandlers.add(xxx, xxxx)，所以我们跟进去看看（lib/network/message_handler.cpp）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869MessageHandler* MessageHandlers::add(std::stringihName, MessageArgs* args, int32msgLen, MessageHandler* msgHandler)&#123; if(msgID_ == 1) &#123; //printf(&quot;\n------------------------------------------------------------------\n&quot;); //printf(&quot;KBEMessage_handlers begin:\n&quot;); &#125; //bool isfixedMsg = false; FixedMessages::MSGInfo* msgInfo = FixedMessages::getSingleton().isFixed(ihName.c_str()); if(msgInfo == NULL) &#123; while(true) &#123; if(FixedMessages::getSingleton().isFixed(msgID_)) &#123; msgID_++; //isfixedMsg = true; &#125; else &#123; break; &#125; &#125;; msgHandler-&gt;msgID = msgID_++; &#125; else &#123; msgHandler-&gt;msgID = msgInfo-&gt;msgid; &#125; msgHandler-&gt;name = ihName; msgHandler-&gt;pArgs = args; msgHandler-&gt;msgLen = msgLen; msgHandler-&gt;exposed = false; msgHandler-&gt;pMessageHandlers = this; msgHandler-&gt;onInstall(); msgHandlers_[msgHandler-&gt;msgID] = msgHandler; if(msgLen == NETWORK_VARIABLE_MESSAGE) &#123; //printf(&quot;\tMessageHandlers::add(%d): name=%s, msgID=%d, size=Variable.\n&quot;, // (int32)msgHandlers_.size(), ihName.c_str(), msgHandler-&gt;msgID); &#125; else &#123; if(msgLen == 0) &#123; msgHandler-&gt;msgLen = args-&gt;dataSize(); if(msgHandler-&gt;type() == NETWORK_MESSAGE_TYPE_ENTITY) &#123; msgHandler-&gt;msgLen += sizeof(ENTITY_ID); &#125; &#125; //printf(&quot;\tMessageHandlers::add(%d): name=%s, msgID=%d, size=Fixed(%d).\n&quot;, // (int32)msgHandlers_.size(), ihName.c_str(), msgHandler-&gt;msgID, msgHandler-&gt;msgLen); &#125; //if(isfixedMsg) // printf(&quot;\t\t!!!message is fixed.!!!\n&quot;); returnmsgHandler;&#125; 大意可以理解为，首先看看消息名称是不是一个固定消息，我们跟进去看看（lib/network/fixed_messages.cpp）：123456789101112131415161718192021222324252627FixedMessages::MSGInfo* FixedMessages::isFixed(constchar* msgName)&#123; MSGINFO_MAP::iteratoriter = _infomap.find(msgName); if(iter != _infomap.end()) &#123; MSGInfo* infos = &amp;iter-&gt;second; returninfos; &#125; returnNULL;&#125;//-------------------------------------------------------------------------------------boolFixedMessages::isFixed(MessageIDmsgid)&#123; MSGINFO_MAP::iteratoriter = _infomap.begin(); while (iter != _infomap.end()) &#123; FixedMessages::MSGInfo&amp;infos = iter-&gt;second; if(infos.msgid == msgid) returntrue; ++iter; &#125; returnfalse;&#125; 固定消息通过通读FixedMessages（fixed_message.h/.cpp）可以看到这个_infomap是在loadConfig中建立的，这个_infomap就是所谓的固定消息（fixed message）与其id的映射表。loadConfig就是检视server/messages_fixed.xml，将其中的消息名称与其id关联建立这个映射表。我们继续接着看MessageHandlers::add接口。 非固定消息对于isFixed为假的消息（非固定消息），则为其生成一个id（随着调用add的次序依次递增），这个id是在MessageHandlers类中唯一的，而每个组件的MessageHandlers又是处于自己的命名空间内，所以当出现某个组件的非固定消息时，则会为其生成单一组件内唯一的id（但这个id并不是所有组件内唯一的）。于是可能出现这种情况，Loginapp::xxxx与Dbmgr::yyyy都是非固定消息，但他们却有着同样的消息id，此时若有其他组件发送其中任一消息给其他组件，接受消息的组件将无法识别到底是Loginapp::xxxx或者是Dbmgr::yyyy。当然，只要我们将非固定消息发送给所属的组件，则不会有问题（上例中任何组件将Loginapp::xxxx发送给loginapp都是不会出乱子的）。 dbmgr dbmgr组件主要负责数据库相关的事务，比如：账户登录/注册事务；账户充值]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>KBE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNP（UNIX网络编程）13章到31章笔记整理（结合TLPI和APUE两书的笔记整理）(二)]]></title>
    <url>%2F2017%2F07%2F29%2Funp_chapter_thirdteen_to_thirty_one_note_second_part%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[16章 16.3节 ： 非阻塞connect 有三个用途： 我们想在connect的时候处理其他事情 可以同时建立多个连接 可以通过select设置一个更短一点的超时时间 实现步骤： a. 用fcntl把套接字设置为非阻塞 b. 处理客户端和服务器都在同一主机上的情况 c. 使用select设置超时，并处理超时情况 d. 处理当连接建立的时候，描述符变为可写；以及当连接建立遇到错误的时候， 描述符变为可写并可读的情况 . . . 16.6节 ： 非阻塞accept， 用于解决下面问题：用select检测socket状态，如果有连接就调用accept，这样如果在select检测到有连接请求，但在调用accept之前，这个请求断开了，然后调用accept的时候就会阻塞在哪里，除非这时有另外一个连接请求，如果没有，则一直被阻塞在accept调用上, 无法处理任何其他已就绪的描述符。 解决方案：使用select在一个监听套接字准备好要被accept时总是把套接字设置为非阻塞 26章和30章这两章介绍了线程和并发/并行的服务器设计范式. 关于线程可参考 : 阅读开源服务器源码基础 关于服务器设计范式可参考 : 目前Linux比较通用的基于epoll的服务器设计范式大体如epoll扼要总结]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNP（UNIX网络编程）13章到31章笔记整理（结合TLPI和APUE两书的笔记整理）(一)]]></title>
    <url>%2F2017%2F07%2F28%2Funp_chapter_thirdteen_to_thirty_one_note_first_part%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[因为UNP第三部分（第三版13-31章）的内容结合APUE（UNIX环境高级编程）和TLPI（The Linux Programming Interface）来看才能比较清晰，所以笔记整理会穿插很多这两本书的内容 引申知识，作业控制以及相关命令 . . . 注: 但是如上方到后台执行的进程，其父进程还是当前终端shell的进程，而一旦父进程退出，则会发送hangup信号给所有子进程，子进程收到hangup以后也会退出。 13.4节本节主要讲守护进程的相关知识 创建守护进程的步骤自定义一个daemon_init函数，涉及到知识点为“如何创建一个daemon（守护进程）”，实现步骤如下(两fork一set, u工文dev)： (1)首先要做的是调用umask将文件模式创建屏蔽字设置为一个已知值(通常是0)。由继承得来的文件模式创建屏蔽字可能会被设置为拒绝某些权限。如果守护进程要创建文件，那么它可能要设置特定的权限。例如，若守护进程要创建组可读、组可写的文件，继承的文件模式创建屏蔽字可能会屏蔽上述两种权限中的一种，而使其无法发挥作用。另一方面，如果守护进程调用的库函数创建了文件，那么将文件模式创建屏蔽字设置为一个限制性更强的值（如007）可能会更明智，因为库函数可能不允许调用者通过一个显式的函数参数来设置权限。 (2)调用fork，然后使父进程exit。这样做实现了下面几点。第一，如果该守护进程是作为一条简单的shell命令启动的，那么父进程终止会让shell认为这条命令已经执行完毕。第二，虽然子进程继承了父进程的进程组ID，但获得了一个新的进程ID，这就保证了子进程不是一个进程组的组长进程。这是下面将要进行的setsid调用的先决条件。 (3)调用setsid创建一个新会话。使调用进程：(a)成为新会话的首进程，(b)成为一个新进程组的组长进程．(c)没有控制终端。也可概括为 : 开启一个新会话并释放它与控制终端之间的所有关联关系 (4)再次fork并杀掉首进程.这样就确保了子进程不是一个会话首进程， 根据linux中获取终端的规则（只有会话首进程才能请求一个控制终端）， 这样进程永远不会重新请求一个控制终端 (5)将当前工作目录更改为根目录。从父进程处继承过来的当前工作目录可能在一个挂载的文件系统中。因为守护进程通常在系统再引导之前是一直存在的，所以如果守护进程的当前工作目录在一个挂载文件系统中，那么该文件系统就不能被卸载。或者，某些守护进程还可能会把、与前工作目录更改到某个指定位置，并在此位置进行它们的全部工作。例如，如果守护进程的当前工作目录是/usr/home目录，那么管理员在卸载/usr分区时会报错的。为了避免这个问题，可以调用chdir()函数将工作目录设置为根目录/。 (6)关闭不再需要的文件描述符。这使守护进程不再持有从其父进程继承来的任何文件描述符（父进程可能是shell进程，或某个其他进程）。可以使用open_max函数（见2.17节）或getrlimit函数（见7.11节）来判定最高文件描述符值，并关闭直到该值的所有描述符。 (7) (这一步不是必要的)某些守护进程打开/dev/null使其具有文件描述符0、l和2．这样，任何一个试图读标准输入、写标准输出或标准错误的库例程都不会产生任何效果。因为守护进程并不与终端设备相关联，所以其输出无处显示，也无处从交互式用户那里接收输入。即使守护进程是从交互式会话启动的，但是守护进程是在后台运行的，所以登录会话的终止并不影响守护进程。如果其他用户在同一终端设备上登录，我们不希望在该终端上见到守护进程的输出，用户也不期望他们在终端上的输入被守护进程读取。( 在关闭了文件描述符0、1和2之后，daemon通常会打开/dev/null并使用dup2()（或类似的函数）使所有这些描述符指向这个设备。之所以要这样做是因为下面两个原因 : 它确保了当daemon调用了在这些描述符上执行I/O的库函数时不会出乎意料地 失败。 它防止了daemon后面使用描述符1或2打开…个文件的情况，因为库函数会将这些 描述符当做标准输出和标准错误来写入数据（进而破坏了原有的数据）。 ) 进程组与会话1234567 会 话 / | \ / | \ / | \ 前台进程组 后台进程组1 后台进程组2 ... / | \ / | \ / | \进程1 进程2 ... 进程3 进程4 ... ... 进程组 进程组就是一系列相互关联的进程集合，系统中的每一个进程也必须从属于某一个进程组；每个进程组中都会有一个唯一的 ID(process group id)，简称 PGID；PGID 一般等同于进程组的创建进程的 Process ID，而这个进进程一般也会被称为进程组先导 (process group leader)，同一进程组中除了进程组先导外的其他进程都是其子进程； 进程组的存在，方便了系统对多个相关进程执行某些统一的操作，例如，我们可以一次性发送一个信号量给同一进程组中的所有进程。 又例如:12345$ ps -o pid,pgid,ppid,comm | cat PID PGID PPID COMMAND 10179 10179 10177 bash 10263 10263 10179 ps 10264 10263 10179 cat 下边通过简单的示例来理解进程组 bash：进程和进程组ID都是 10179，父进程其实是 sshd(10177)ps：进程和进程组ID都是 10263，父进程是 bash(10179)，因为是在 Shell 上执行的命令cat：进程组 ID 与 ps 的进程组 ID 相同，父进程同样是 bash(10179) 会话 会话（session）是一个若干进程组的集合，同样的，系统中每一个进程组也都必须从属于某一个会话；一个会话只拥有最多一个控制终端（也可以没有），该终端为会话中所有进程组中的进程所共用。一个会话中前台进程组只会有一个，只有其中的进程才可以和控制终端进行交互；除了前台进程组外的进程组，都是后台进程组；和进程组先导类似，会话中也有会话先导 (session leader) 的概念，用来表示建立起到控制终端连接的进程。在拥有控制终端的会话中，session leader 也被称为控制进程(controlling process)，一般来说控制进程也就是登入系统的 shell 进程(login shell)； 执行睡眠后台进程 sleep 50 &amp; 之后，通过 ps 命令查看该进程及 shell 信息如上图： PPID 指父进程 id； PID 指进程 id； PGID 指进程组 id SID 指会话 id； TTY 指会话的控制终端设备； COMMAND 指进程所执行的命令 TPGID 指前台进程组的 PGID。 dup介绍函数原型 : int dup( int oldfd ) dup() 调用复制一个打开的文件描述符 oldfd , 并返回一个新描述符, 二者都指向同一打开的文件句柄. 系统会保证新描述符一定是编号值最低的未用文件描述符. 假设发起如下调用 : newfd = dup(1); 再假定在正常情况下, shell已经代表程序打开了文件描述符0, 1和2, 且没有其他描述符在用, dup()调用会创建文件描述符1的副本, 返回的文件描述符编号值为3. 如果希望返回的文件描述符为2, 可以使用如下技术 : 12close(2);newfd = dup(1); 只有当描述符 0 已经打开时, 这段代码方可工作. dup2介绍如果想进一步简化上述代码, 同时总是能获得所期望的文件描述符, 可以调用dup2(). 函数原型 : int dup2( int oldfd, int newfd ) dup2函数跟dup函数相似，但dup2函数允许调用者规定一个有效描述符和目标描述符的id 。dup2函数成功返回时，目标描述符（dup2函数的第二个参数）将变成源描述符（dup2函数的 第一个参数）的复制品，换句话说，两个文件描述符现在都指向同一个文件，并且是函数第一 个参数指向的文件。 下面我们用一段代码加以说明：1234int oldfd; oldfd = open("app_log", (O_RDWR | O_CREATE), 0644 ); dup2( oldfd, 1 ); close( oldfd ); 本例中，我们打开了一个新文件，称为“app_log”，并收到一个文件描述符，该描述符 叫做fd1。我们调用dup2函数，参数为oldfd和1，这会导致用我们新打开的文件描述符替换掉 由1代表的文件描述符（即stdout，因为标准输出文件的id为1）。任何写到stdout的东西，现 在都将改为写入名为“app_log”的文件中。 umask介绍当我们登录系统之后创建一个文件总是有一个默认权限的，那么这个权限是怎么来的呢？这就是umask干的事情。umask设置了用户创建文件的默认 权限，它与chmod的效果刚好相反，umask设置的是权限“补码”，而chmod设置的是文件权限码。 如何计算umaskumask 命令允许你设定文件创建时的缺省模式，对应每一类用户(文件属主、同组用户、其他用户)存在一个相应的umask值中的数字。对于文件来说，这一数字的最 大值分别是6。系统不允许你在创建一个文本文件时就赋予它执行权限，必须在创建后用chmod命令增加这一权限。目录则允许设置执行权限，这样针对目录来 说，umask中各个数字最大可以到7。 例如，对于umask值0 0 2，相应的文件和目录缺省创建权限是什么呢？ 第一步，我们首先写下目录具有全部权限的模式，即777 (所有用户都具有读、写和执行权限)。第二步，在下面一行按照umask值写下相应的位，在本例中是0 0 2。第三步，在接下来的一行中记下上面两行中没有匹配的位。这就是目录的缺省创建权限。稍加练习就能够记住这种方法。第四步，对于文件来说，在创建时不能具有执行权限，只要拿掉相应的执行权限比特即可。这就是上面的例子， 其中umask值为0 0 2：1) 文件的最大权限 rwx rwx rwx (777)2) umask值为0 0 2 — — -w-3) 目录权限 rwx rwx r-x (775) 这就是目录创建缺省权限4) 文件权限 rw- rw- r– (664) 这就是文件创建缺省权限 下面是另外一个例子，假设这次u m a s k值为0 2 2：1) 文件的最大权限 rwx rwx rwx (777)2 ) u m a s k值为0 2 2 — -w- -w-3) 目录权限 rwx r-x r-x (755) 这就是目录创建缺省权限4) 文件权限 rw- r– r– (644) 这就是文件创建缺省权限 创建守护进程的例子程序下面是becomeDaemon()函数的实现，becomeDaeomon()函数接收一个位掩码参数flags，它允许调用者有选择地执行其中的步骤，具体可参考注释。 become_daemon.h1234567891011121314151617#ifndef BECOME_DAEMON_H /* Prevent double inclusion */#define BECOME_DAEMON_H/* Bit-mask values for 'flags' argument of becomeDaemon() */#define BD_NO_CHDIR 01 /* Don't chdir("/") */#define BD_NO_CLOSE_FILES 02 /* Don't close all open files */#define BD_NO_REOPEN_STD_FDS 04 /* Don't reopen stdin, stdout, and stderr to /dev/null */#define BD_NO_UMASK0 010 /* Don't do a umask(0) */#define BD_MAX_CLOSE 8192 /* Maximum file descriptors to close if sysconf(_SC_OPEN_MAX) is indeterminate */int becomeDaemon(int flags);#endif become_daemon.c12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include "become_daemon.h"#include "tlpi_hdr.h"int /* Returns 0 on success, -1 on error */becomeDaemon(int flags)&#123; int maxfd, fd; switch (fork()) &#123; /* Become background process */ case -1: return -1; case 0: break; /* Child falls through... */ default: _exit(EXIT_SUCCESS); /* while parent terminates */ &#125; if (setsid() == -1) /* Become leader of new session */ return -1; switch (fork()) &#123; /* Ensure we are not session leader */ case -1: return -1; case 0: break; default: _exit(EXIT_SUCCESS); &#125; if (!(flags &amp; BD_NO_UMASK0)) umask(0); /* Clear file mode creation mask */ if (!(flags &amp; BD_NO_CHDIR)) chdir("/"); /* Change to root directory */ if (!(flags &amp; BD_NO_CLOSE_FILES)) &#123; /* Close all open files */ maxfd = sysconf(_SC_OPEN_MAX); if (maxfd == -1) /* Limit is indeterminate... */ maxfd = BD_MAX_CLOSE; /* so take a guess */ for (fd = 0; fd &lt; maxfd; fd++) close(fd); &#125; if (!(flags &amp; BD_NO_REOPEN_STD_FDS)) &#123; // /* Standard file descriptors. */ // #define STDIN_FILENO 0 /* Standard input. */ // #define STDOUT_FILENO 1 /* Standard output. */ // #define STDERR_FILENO 2 /* Standard error output. */ close(STDIN_FILENO); /* Reopen standard fd's to /dev/null */ fd = open("/dev/null", O_RDWR); // open 返回的文件描述符一定是最小的未被使用的描述符。 if (fd != STDIN_FILENO) /* 'fd' should be 0 */ return -1; if (dup2(STDIN_FILENO, STDOUT_FILENO) != STDOUT_FILENO) return -1; if (dup2(STDIN_FILENO, STDERR_FILENO) != STDERR_FILENO) return -1; &#125; return 0;&#125; nohup和setsid用法如果我们要在退出shell的时候继续运行进程，则需要 setsid将父进程设为init进程 使用nohup忽略SIGHUP信号 注: (SIGHUP 信号在 用户终端连接 (正常或非正常) 结束 时发出, 通常是在终端的控制进程结束时, 通知同一 session 内的各个作业, 这时它们与控制终端不再关联. 系统对 SIGHUP 信号的默认处理是终止收到该信号的进程。所以若程序中没有捕捉该信号，当收到该信号时，进程就会退出。)， nohup详解nohup介绍用途: 不挂断地运行命令。 语法：nohup Command [Arg …] [ &amp; ] 无论是否将 nohup 命令的输出重定向到终端，输出都将附加到当前目录的 nohup.out 文件中。 如果当前目录的 nohup.out 文件不可写，输出重定向到 $HOME/nohup.out 文件中。 如果没有文件能创建或打开以用于追加，那么 Command 参数指定的命令不可调用。 退出状态：该命令返回下列出口值： 126 可以查找但不能调用 Command 参数指定的命令。 127 nohup 命令发生错误或不能查找由 Command 参数指定的命令。否则 nohup 命令的退出状态是 Command 参数指定命令的退出状态。 nohup和&amp;的关系使用nohup 运行程序: 输出重定向，默认重定向到当前目录下 nohup.out 文件 使用 Ctrl + C 发送 SIGINT 信号，程序关闭 关闭 Shell Session 发送 SIGHUP 信号，程序免疫使用 &amp; 运行程序： 程序转入后台运行 结果会输出到终端-使用 Ctrl + C 发送 SIGINT 信号，程序免疫 关闭 Shell session 发送 SIGHUP 信号，程序关闭 nohup和&amp;使用实例一般两个一起组合使用不会受 Ctrl C 和 Shell 关闭的影响： 123456789101112# 最简单的后台运行nohup command &amp;# 输出默认重定向到当前目录下 nohup.out 文件nohup python main.py &amp; # 自定义输出文件(标准输出和错误输出合并到 main.log)nohup python main.py &gt;&gt; main.log 2&gt;&amp;1 &amp; # 与上一个例子相同作用的简写方法nohup python main.py &amp;&gt; main.log &amp;# 不记录输出信息nohup python main.py &amp;&gt; /dev/null &amp;# 不记录输出信息并将程序的进程号写入 pidfile.txt 文件中，方便后续杀死进程nohup python main.py &amp;&gt; /dev/null &amp; echo $! &gt; pidfile.txt 由于使用 nohup 时，会自动将输出写入 nohup.out 文件中，如果文件很大的话，nohup.out 就会不停的增大，我们可以利用 Linux 下一个特殊的文件 /dev/null 来解决这个问题，这个文件就相当于一个黑洞，任何输出到这个文件的东西都将消失 只保留输出错误信息 nohup command &gt;/dev/null 2&gt;log &amp; 所有信息都不要 nohup command &gt;/dev/null 2&gt;&amp;1 &amp; 这里解释一下后面的 2&gt;&amp;1 。参考重定向知识 其他相关命令12345678910# 结束当前任务ctrl+c# 将一个正在前台执行的命令放到后台，并且处于暂停状态ctrl+z# 查看任务，返回任务编号 和 进程号jobs -l# 将一个在后台暂停的命令，变成在后台继续执行。如果后台中有多个命令，可以用 bg %jobnumber 将选中的命令调出。bg %jobnumber# 将后台中的命令调至前台继续运行。如果后台中有多个命令，可以用 fg %jobnumber（是命令编号，不是进程号）将选中的命令调出fg %jobnumber 实战1234567891011121314151617181920212223242526272829303132333435363738394041b@b-VirtualBox:~/my_temp_test$ nohup ./o_multi_thread_process &amp;[1] 3487b@b-VirtualBox:~/my_temp_test$ nohup: ignoring input and appending output to ‘nohup.out’^Cb@b-VirtualBox:~/my_temp_test$ jobs[1]+ Running nohup ./o_multi_thread_process &amp;b@b-VirtualBox:~/my_temp_test$ ps -ef | grep multib 3487 3004 0 20:05 pts/3 00:00:00 ./o_multi_thread_processb 3488 3487 0 20:05 pts/3 00:00:00 ./o_multi_thread_processb 3491 3004 0 20:05 pts/3 00:00:00 grep --color=auto multib@b-VirtualBox:~/my_temp_test$ bg %1bash: bg: job 1 already in backgroundb@b-VirtualBox:~/my_temp_test$ fg %1nohup ./o_multi_thread_process^Z[1]+ Stopped nohup ./o_multi_thread_processb@b-VirtualBox:~/my_temp_test$ bg %1[1]+ nohup ./o_multi_thread_process &amp;b@b-VirtualBox:~/my_temp_test$ jobs -l[1]+ 3487 Running nohup ./o_multi_thread_process &amp;b@b-VirtualBox:~/my_temp_test$ fg %1nohup ./o_multi_thread_process^Cb@b-VirtualBox:~/my_temp_test$ jobsb@b-VirtualBox:~/my_temp_test$ ps -ef | grep multib 3499 3004 0 20:11 pts/3 00:00:00 grep --color=auto multib@b-VirtualBox:~/my_temp_test$ setsid ./o_multi_thread_process &amp;[1] 3502b@b-VirtualBox:~/my_temp_test$ ProcessA: 3503 step1ProcessA: 3503 thread 139947724490496 step2ProcessA: 3503 thread 139947724490496 step3ProcessB: 3504 step1ProcessB: 3504 step2ProcessB: 3504 step3^C[1]+ Done setsid ./o_multi_thread_processb@b-VirtualBox:~/my_temp_test$ ps -ef | grep multib 3503 1256 0 20:12 ? 00:00:00 ./o_multi_thread_processb 3504 3503 0 20:12 ? 00:00:00 ./o_multi_thread_processb 3507 3004 0 20:12 pts/3 00:00:00 grep --color=auto multib@b-VirtualBox:~/my_temp_test$ jobs disown用法那么对于已经在后台运行的进程，如果我们要在退出shell的时候继续运行进程, 该怎么办呢？可以使用disown命令 1234567891011121314b@b-VirtualBox:~/my_temp_test$ ./o_multi_thread_process &amp;[1] 3523b@b-VirtualBox:~/my_temp_test$ ProcessA: 3523 step1ProcessA: 3523 thread 140501901821696 step2ProcessA: 3523 thread 140501901821696 step3ProcessB: 3524 step1ProcessB: 3524 step2ProcessB: 3524 step3^Cb@b-VirtualBox:~/my_temp_test$ jobs -l[1]+ 3523 Running ./o_multi_thread_process &amp;b@b-VirtualBox:~/my_temp_test$ disown -h %1b@b-VirtualBox:~/my_temp_test$ jobs[1]+ Running ./o_multi_thread_process &amp; 重定向知识在 shell 脚本中，默认情况下，总是有三个文件处于打开状态，标准输入 (键盘输入)、标准输出（输出到屏幕）、标准错误（也是输出到屏幕），它们分别对应的文件描述符是 0，1，2 。 的意思是把标准输出重定向，与 1&gt; 相同 2&gt;&amp;1 的意思是把 标准错误输出 重定向到 标准输出. &amp;&gt;file 的意思是把标准输出 和 标准错误输出 都重定向到文件 file 中 /dev/null 是一个文件，这个文件比较特殊，所有传给它的东西它都丢弃掉 举例说明: 当前目录只有一个文件 a.txt.1234[root@redhat box]# ls a.txt [root@redhat box]# ls a.txt b.txt ls: b.txt: No such file or directory 由于没有 b.txt 这个文件, 于是返回错误值, 这就是所谓的 2 输出a.txt 而这个就是所谓的 1 输出 再接着看: [root@redhat box]# ls a.txt b.txt 1&gt;file.out 2&gt;file.err执行后, 没有任何返回值. 原因是, 返回值都重定向到相应的文件中了, 而不再前端显示1234[root@redhat box]# cat file.out a.txt [root@redhat box]# cat file.err ls: b.txt: No such file or directory 一般来说, “1&gt;” 通常可以省略成 “&gt;”.即可以把如上命令写成: ls a.txt b.txt &gt;file.out 2&gt;file.err有了这些认识才能理解 “1&gt;&amp;2” 和 “2&gt;&amp;1”. 1&gt;&amp;2 正确返回值传递给 2 输出通道 &amp;2 表示 2 输出通道如果此处错写成 1&gt;2, 就表示把 1 输出重定向到文件 2 中.2&gt;&amp;1 错误返回值传递给 1 输出通道, 同样 &amp; 1 表示 1 输出通道.举个例子.1234[root@redhat box]# ls a.txt b.txt 1&gt;file.out 2&gt;&amp;1[root@redhat box]# cat file.out ls: b.txt: No such file or directory a.txt 现在, 正确的输出和错误的输出都定向到了 file.out 这个文件中, 而不显示在前端.补充下, 输出不只 1 和 2, 还有其他的类型, 这两种只是最常用和最基本的. 例如：rm -f $(find / -name core) &amp;&gt; /dev/null， /dev/null 是一个文件，这个文件比较特殊，所有传给它的东西它都丢弃掉。 例如：注意，为了方便理解，必须设置一个环境使得执行 grep da * 命令会有正常输出和错误输出，然后分别使用下面的命令生成三个文件：1234grep da * &gt; greplog1 grep da * &gt; greplog2 1&gt;&amp;2 grep da * &gt; greplog3 2&gt;&amp;1grep da * 2&gt; greplog4 1&gt;&amp;2 查看 greplog1 会发现里面只有正常输出内容 查看 greplog2 会发现里面什么都没有 查看 greplog3 会发现里面既有正常输出内容又有错误输出内容 查看 greplog4 会发现里面既有正常输出内容又有错误输出内容]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>noodle</tag>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crontab笔记整理]]></title>
    <url>%2F2017%2F07%2F09%2Fcrontab%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[crontab命令被用来提交和管理用户的需要周期性执行的任务，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 -e：编辑该用户的计时器设置； -l：列出该用户的计时器设置； -r：删除该用户的计时器设置； -u&lt;用户名称&gt;：指定要设定计时器的用户名称 . . . m h dom mon dow command分 时 日 月 周 命令 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 例子 */1 * * * * sed -i &#39;$a\nani&#39; /home/b/my_temp_test/practice.cpp每隔一分钟就在practice.cpp文件的最后一行插入字符串“nani” 3,15 8-11/2 * 12 0 sed -i &#39;$a\nani&#39; /home/b/my_temp_test/practice.cpp12月的周日的8-11时的时间段每隔两个小时就在第3分钟和第15分钟的时候，在practice.cpp文件的最后一行插入字符串“nani”]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNIX网络编程第三章到第十一章笔记整理(二)]]></title>
    <url>%2F2017%2F07%2F03%2Funp_chapter_three_to_eleven_note_second_part%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[第七章 7.5节 ： 通用套接字选项， 常用的有 SO_KEEPALIVE SO_REVBUF SO_SNDBUF SO_REUSEADDR 7.9节 ： tcp套接字选项， 常用的有 TCP_NODELAY TCP_MAXSEG 7.11节 ：fcntl函数，常用的用法是使用F_SETFL命令设置O_NOBLOCK文件状态标志， 我们可以把一个套接字设置为非阻塞型。 . . . 第八章基本UDP套接字编程 8.11节 ： UDP的connect函数，可以获得性能提升，因为未连接的udp每次sendto发送数据报的时候都要连接然后发送然后断开， 之后第二个数据报又要重复上述步骤，而连接后的udp套接字只需要连接然后发送第一个数据报然后发送第二个、第三个就行了]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNIX网络编程第三章到第十一章笔记整理(一)]]></title>
    <url>%2F2017%2F07%2F02%2Funp_chapter_three_to_eleven_note_first_part%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[因为第二章之后基本都是纯Socket API的内容， 第三章到第十一章的笔记整理合并到一起。 第三章 3.4 ：字节排序函数，涉及到大小端，处理网络字节序和主机字节序的转换 如何判别是大端(Big-Endian)还是小端(Little-Endian): 1234567891011121314151617181920212223242526union TestBigOrLittle&#123; short var_short; char array_char[2];&#125;;int main()&#123; TestBigOrLittle unTestUnion; unTestUnion.var_short = 0x1234; if (sizeof(short) == 2) &#123; if (unTestUnion.array_char[0] == 0x12) printf("BigEndian\n"); else if(unTestUnion.array_char[0] == 0x34) printf("LittleEndian\n"); else printf("unkonw endian\n"); &#125; else &#123; printf("sizeof(short) : %d \n", sizeof(short)); &#125; return 0; &#125; 网际协议使用大端字节序来传送这些多字节整数, 也就是说网络字节序就是大端字节序. 由图中我们可以知道, htons和ntohs是用于端口的字节序转换的, 而htonl和ntohl是用于32位IP地址的, 下图就是一个例子: 3.6 ： 地址转换函数，它们在ASCII字符串(这是人们偏爱使用的格式)与网络字节序的二进制值(这是存放在套接字地址结构中的值)之间转换网际地址 第四章基本TCP套接字编程 listen函数 int listen(int sockfd, int backlog); 当来自客户的SYN到达时，TCP在未完成连接队列中创建个新项，然后响应以三路握手 的第—个分节服务器的SYN响应，其中捎带对客户SYN的ACK（2.6节）。这一项．直保留在 未完成连接队列中，直到三路握手的第二个分节（客户对服务器SYN的ACK）到达或者该项超 时为止。（源白Berkeley的实现为这些末完成连接的项设置的超时值为75s。）如果3路握手正常 完成，该项就从未完成连接队列移到已完成连接队列的队尾。当进程调用accept时（该函数在 下一节讲解），己完成连接队列巾的队头项将返回给给进程，或者如果该队列为空，那么进程将被 投入睡眠，直到TCP在该队列中放入一项才唤醒它。 4.6节: accept函数 accept函数用于从已完成连接队列对头返回下一个已完成连接 int accept(int sockfd, struct sockaddr *cliaddr, socklent_t *addrlen); 4.7节: fork函数 fork函数的内存语义: 共享代码段, 子指向父 : 父子进程共享同一代码段, 子进程的页表项指向父进程相同的物理内存页(即数据段/堆段/栈段的各页) 写时复制(copy-on-write) : 内核会捕获所有父进程或子进程针对这些页面(即数据段/堆段/栈段的各页)的修改企图, 并为将要修改的页面创建拷贝, 将新的页面拷贝分配给遭内核捕获的进程, 从此父/子进程可以分别修改各自的页拷贝, 不再相互影响. 4.9节： close函数， 涉及到描述符引用计数，所以多进程并发服务器才可以共享已连接套接字，因为父进程调用close函数知识把该套接字标记成已关闭并导致该套接字描述符减1。只要引用计数的值仍大于0，就不会引发tcp的四分组连接终止序列 第五章 5.9节： 处理SIGCHLD信号， 涉及到僵死进程（子进程终止时给父进程发送了一个SIGCHLD信号，若父进程未加处理，则子进程进入僵死状态），所以要建立该信号处理函数，并在函数中调用waitpid来处理 5.10节 ： 使用wait或者waitpid来处理已终止的子进程，通常是使用waitpid并指定WNOHANG选项，来告知waitpid在有尚未终止的子进程在运行时不要阻塞。 . . . 第六章 同步I/O操作：导致请求进程阻塞，知道I/O操作完成 异步I/O操作：不导致请求进程阻塞 6.6节 ： shutdown函数，shutdown可以不用管引用计数就激发tcp的正常连接终止序列。当关闭连接的写这一半，对于tcp连接， 这称为半关闭（half-close）]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNIX网络编程第二章笔记修正（结合TLPI和APUE两书的笔记整理）]]></title>
    <url>%2F2017%2F06%2F05%2Funp_chapter_two_note%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[为加深理解, 故本章老笔记内容大幅删减重写.第二章重点如下 : TCP (Transmission Control Protocol)传输控制协议.特性如下 : TCP头为20字节 面向连接 全双工 可靠, 关心确认/超时/重传等, 保证顺序 流量控制 字节流, 没有任何记录边界 UDP (User Datagram Protocol)用户数据报协议.特性如下 : UDP头为8字节 无连接 不可靠, 不保证顺序/是否到达/是否重复 每个数据报都有一个长度 TCP三路握手(three-way handshake) TCP选项 : MSS选项 发送SYN的TCP一端使用本选项通告对端他的最大分节大小(maximum segment size) 窗口规模选项 时间戳选项, 对于高速网络连接是必要的. . . . TCP连接终止 : TCP状态转换图 TCP连接的分组交换 TIME_WAIT状态存在的理由 : 可靠地实现TCP全双工连接的终止 :我们假设客户端不维护一个TIME_WAIT状态的情况 : 如果服务器没有收到客户端最后一个ACK, 服务器就会重传它最终的那个FIN, 此时客户端就会将会响应一个RST, 该分节将使服务器解释为一个错误. 允许老的重复分节在网络中消逝(分节是TCP传递给IP的数据单元) :我们假设客户端不维护一个TIME_WAIT状态的情况 : 我们先以12.106.32.254的1500端口和206.168.112.219的21端口之间有一个tcp连接A.我们关闭这个连接, 过一段时间后再用相同的IP和端口之间建立另一个连接B, 连接B将有可能收到连接A的老的重复分节. 为啥是2MSL:所谓的2MSL是两倍的MSL(Maximum Segment Lifetime), 发送端只需要等待一个MSL就足够了。但这是没考虑接收端也要发回数据的情况，假设现在一个MSL的时候，接收端需要发送一个应答，这时候，我们也必须等待这个应答的消失，这个应答的消失也是需要一个MSL，所以我们需要等待2MSL。 Linux下面一共有65535个端口 其中1–1023是系统保留的， 1024–65535是供用户使用的。 0到1024是众所周知的端口（知名端口，常用于系统服务等，例如http服务的端口号是80)。个人写的应用程序，尽量不要使用0到1024之间的端口号。 套接字对是一个定义该连接的两个端点的四元组: 本地IP地址 本地TCP端口号 外地IP地址 外地TCP端口号 缓冲区大小及限制 IPv4数据报的最大大小为65535字节, 因为其总长度字段占据16位 以太网的MTU是1500字节, IPv4要求的最小链路MTU是68字节, 这允许最大的IPv4首部(包括20字节的固定长度部分和最多40字节的选项部分)拼接最小的片段 在两个主机之间的路径中最小的MTU成为路径MTU 当一个IP数据报将从某个接口送出时, 如果他的大小超过相应链路的MTU, IPv4和IPv6都将执行分片 IPv4首部的”不分片(don’t fragment)”位(即DF位)若被设置, 那么不管是发送这些数据报的主机还是转发他们的路由器, 都不允许对他们分片 IPv4和IPv6都定义了最小重组缓冲区大小(minimum reassembly buffersize), 它是IPv4或IPv6的任何事先都必须保证支持的最小数据报大小. 对于IPv4是576字节, 对于IPv6是1500字节. 例如, 就IPv4而言, 我们不能主观地认为某个给定目的地一定能接受577字节的数据报(因为我们只能保证它一定能接受576字节的数据报). 所以很多使用UDP的IPv4应用（如DNS）都避免产生大于这个大小的数据报 MSS(maximum segment size) : TCP最大分节大小，用于向对端TCP通告对端在每个分节中能发送的最大TCP数据量. 在以太网中使用IPv4的MSS值为1460（以太网的MTU - IPv4首部 - TCP首部 = 1500 - 20 - 20） TCP输出示意图 : UDP输出示意图(因为UDP是不可靠的, 他不必保存应用进程数据的一个副本, 因此无需一个真正的发送缓冲区, 所以为虚线框): 常见因特网应用所使用的协议 ping ： ICMP DNS ： UDP、TCP DHCP : UDP SSH : TCP FTP : TCP HTTP : TCP]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重读UNIX网络编程第一章笔记修正]]></title>
    <url>%2F2017%2F06%2F02%2F%E9%87%8D%E8%AF%BBUNIX%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%AC%94%E8%AE%B0%E4%BF%AE%E6%AD%A3%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[又准备从头看一遍unp, 把一些老笔记放到博客里来就当网盘吧, 顺便修正以及删减一些之前不够精炼的老笔记内容. 第一章重点如下 : OSI (open systems interconnection), 即计算机通信开放系统互联模型 OSI分为七层, 从上到下依次为 应用层 表现层 会话层 传输层 网络层 数据链路层 物理层 对于网际网协议族, OSI顶上三层合并为一层, 称为应用层. 传输层对应着tcp/udp等, 网络层对应着IPv4/IPv6, OSI的数据链路层和物理层是随系统提供的设备驱动程序和网络硬件 . . . 套接字编程接口是从OSI顶上三层(网际协议的应用层)进入传输层的接口. 为何套接字要设计为顶上三层进入传输层的接口??因为OSI顶上三层处理具体网络应用的所有细节却对通信细节了解很少;底下四层对具体网络应用了解不多, 却处理所有的通信细节.]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UE4中如何解决Failed To Launch Editor]]></title>
    <url>%2F2017%2F05%2F02%2Fue4_fix_failed_to_launch_editor%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[当你点击 .uproject 文件却打不开项目, 弹出一个窗口写着 “Failed TO Launch Editor”的时候,大概率是因为你对 UE4Editor.exe 设置为了以管理员身份打开,所以解决方法就是 : 只要对 UE4Editor.exe 右键-属性-兼容性, 去掉”以管理员身份运行此程序”的勾 以及去掉”更改所有用户的设置”中的以管理员身份运行此程序的√]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UE4打印大全]]></title>
    <url>%2F2017%2F05%2F02%2Fue4_print_tricks%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[介绍都是自描述性的且项目无关的, 可以直接放心使用. . . . 使用例子覆盖了以下几种场合 : Example usage: A_LOG(); Example usage: A_LOG_1( “Action!” ); Example usage: A_LOG_2(“Action!”, “Cut!”); Example usage: A_LOG_N(“Action!”, 88.f); Example usage: A_LOG_M(“Action! %f, %d”, 88.f, 88); 示例代码PrintHelper.h12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// Fill out your copyright notice in the Description page of Project Settings.#pragma once#define ACTION_SHOW_DEBUG_SCREEN_MSG false#define ACTION_SHOW_DEBUG_OUTPUT_LOG false#define ACTION_SHOW_DEBUG_OUTPUT_LOG_EXTRA false//Current Class Name + Function Name where this is called!#define STR_CUR_CLASS_FUNC (FString(__FUNCTION__))//Current Class where this is called!#define STR_CUR_CLASS (FString(__FUNCTION__).Left(FString(__FUNCTION__).Find(TEXT(":"))) )//Current Function Name where this is called!#define STR_CUR_FUNC (FString(__FUNCTION__).Right(FString(__FUNCTION__).Len() - FString(__FUNCTION__).Find(TEXT("::")) - 2 ))//Current Line Number in the code where this is called!#define STR_CUR_LINE (FString::FromInt(__LINE__))//Current Class and Line Number where this is called!#define STR_CUR_CLASS_LINE (STR_CUR_CLASS + "(" + STR_CUR_LINE + ")")//Current Class and Line Number where this is called!#define STR_CUR_CLASS_FUNC_LINE (STR_CUR_CLASS_FUNC + "(" + STR_CUR_LINE + ")")//Current Function Signature where this is called!#define STR_CUR_FUNCSIG (FString(__FUNCSIG__))////////////// Screen Message// Gives you the Class name and exact line number where you print a message to yourself!#define A_MSG(TimeToDisplay ) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, *(STR_CUR_CLASS_FUNC_LINE )) )#define A_MSG_1(TimeToDisplay, StringParam1) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, *(STR_CUR_CLASS_FUNC_LINE + " : " + StringParam1)) )#define A_MSG_2(TimeToDisplay, StringParam1, StringParam2) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, *(STR_CUR_CLASS_FUNC_LINE + " : " + StringParam1 + " " + StringParam2)) )//#define A_SCREENMSG_F(StringParam1, NumericalParam2) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, *(STR_CUR_CLASS_FUNC_LINE + " : " + StringParam1 + " " + FString::SanitizeFloat(NumericalParam2))) )#define A_MSG_N(TimeToDisplay, StringParam1, NumericalParam2) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, FString::Printf( TEXT("%s : %s %f"), *STR_CUR_CLASS_FUNC_LINE, *FString(StringParam1), float(NumericalParam2) ) ) )#define A_MSG_M(TimeToDisplay, FormatString, ...) if (ACTION_SHOW_DEBUG_SCREEN_MSG) (GEngine-&gt;AddOnScreenDebugMessage(-1, (float)TimeToDisplay, FColor::Red, FString::Printf( TEXT("%s : %s"), *STR_CUR_CLASS_FUNC_LINE, *FString::Printf(TEXT(FormatString), ##__VA_ARGS__ ) ) ) )///////// UE LOG!// Example usage: A_LOG();#define A_LOG() if (ACTION_SHOW_DEBUG_OUTPUT_LOG) UE_LOG(LogTemp, Warning, TEXT("%s"), *STR_CUR_CLASS_FUNC_LINE )// Example usage: A_LOG_1( "Action!" );#define A_LOG_1(StringParam1) if (ACTION_SHOW_DEBUG_OUTPUT_LOG) UE_LOG(LogTemp, Warning, TEXT("%s : %s"), *STR_CUR_CLASS_FUNC_LINE, *FString(StringParam1))// Example usage: A_LOG_2("Action!", "Cut!");#define A_LOG_2(StringParam1, StringParam2) if (ACTION_SHOW_DEBUG_OUTPUT_LOG) UE_LOG(LogTemp, Warning, TEXT("%s : %s %s"), *STR_CUR_CLASS_FUNC_LINE, *FString(StringParam1), *FString(StringParam2))// Example usage: A_LOG_N("Action!", 88.f);#define A_LOG_N(StringParam1, NumericalParam2) if (ACTION_SHOW_DEBUG_OUTPUT_LOG) UE_LOG(LogTemp, Warning, TEXT("%s : %s %f"), *STR_CUR_CLASS_FUNC_LINE, *FString(StringParam1), float(NumericalParam2) )// #define A_LOG_M(FormatString, ...) if (ACTION_SHOW_DEBUG_OUTPUT_LOG) UE_LOG(LogTemp, Warning, TEXT("%s : %s"), *STR_CUR_CLASS_FUNC_LINE, *FString::Printf(TEXT(FormatString), ##__VA_ARGS__ ) )class PrintHelper&#123;public: static void ScreenMsg( const FString&amp; Msg ); static void ScreenMsg( const FString&amp; Msg, const FString&amp; Msg2 ); static void ScreenMsg( const FString&amp; Msg, const float FloatValue );&#125;; PrintHelper.cpp123456789101112131415161718192021222324// Fill out your copyright notice in the Description page of Project Settings.#include "PrintHelper.h"//ScreenMsgvoid PrintHelper::ScreenMsg( const FString&amp; Msg )&#123; if (!ACTION_SHOW_DEBUG_SCREEN_MSG) return; GEngine-&gt;AddOnScreenDebugMessage( -1, 55.f, FColor::Red, *Msg );&#125;void PrintHelper::ScreenMsg( const FString&amp; Msg, const FString&amp; Msg2 )&#123; if (!ACTION_SHOW_DEBUG_SCREEN_MSG) return; GEngine-&gt;AddOnScreenDebugMessage( -1, 55.f, FColor::Red, FString::Printf( TEXT( "%s %s" ), *Msg, *Msg2 ) );&#125;void PrintHelper::ScreenMsg( const FString&amp; Msg, const float Value )&#123; if (!ACTION_SHOW_DEBUG_SCREEN_MSG) return; GEngine-&gt;AddOnScreenDebugMessage( -1, 55.f, FColor::Red, FString::Printf( TEXT( "%s %f" ), *Msg, Value ) );&#125;]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UE4中如何不继承UObject就能spawn一个actor]]></title>
    <url>%2F2017%2F04%2F22%2Fue4_how_to_spawn_actor_but_not_use_uobj%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[介绍其实不用继承 UObject 也可以生成一个 actor, 关键的点就是拿到 UWorld ,所以只要从一个拥有 UWorld 的虚幻相关实例中传递它的 UWorld 给一个原生 C++ 类也可以. 示例代码. . . RealTimeSrvEntityFactory.h1234567891011121314151617181920212223242526272829303132333435363738394041// Fill out your copyright notice in the Description page of Project Settings.#pragma once#include &lt;memory&gt;#include "RealTimeSrvEntity.h"#include "RealTimeSrvPawn.h"/** * */typedef RealTimeSrvEntityPtr( *GameObjectCreationFunc )( );class RealTimeSrvEntityFactory&#123;public: static void StaticInit(UWorld * inWorld); UWorld* GetWorld() const; RealTimeSrvEntityPtr CreateGameObject( uint32_t inFourCCName ); void SetDefaultPawnClass( TSubclassOf&lt;class ARealTimeSrvPawn&gt; inDefaultCharacterClasses ) &#123; DefaultCharacterClasses = inDefaultCharacterClasses; &#125;public: static std::unique_ptr&lt;RealTimeSrvEntityFactory&gt; sInstance;private: RealTimeSrvEntityFactory(); RealTimeSrvEntityPtr CreateActionPawn();private: TSubclassOf&lt;class ARealTimeSrvPawn&gt; DefaultCharacterClasses; UWorld* mWorld;&#125;; RealTimeSrvEntityFactory.cpp12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// Fill out your copyright notice in the Description page of Project Settings.#include "RealTimeSrvEntityFactory.h"#include "RealTimeSrvWorld.h"std::unique_ptr&lt;RealTimeSrvEntityFactory&gt; RealTimeSrvEntityFactory::sInstance;RealTimeSrvEntityFactory::RealTimeSrvEntityFactory()&#123;&#125;void RealTimeSrvEntityFactory::StaticInit(UWorld * inWorld)&#123; sInstance.reset( new RealTimeSrvEntityFactory() ); check( sInstance ); if (sInstance) &#123; sInstance-&gt;mWorld = inWorld; &#125;&#125;UWorld* RealTimeSrvEntityFactory::GetWorld() const&#123; return mWorld;&#125;RealTimeSrvEntityPtr RealTimeSrvEntityFactory::CreateGameObject( uint32_t inFourCCName )&#123; switch ( inFourCCName ) &#123; case 'CHRT': return CreateActionPawn(); default: break; &#125; return RealTimeSrvEntityPtr();&#125;RealTimeSrvEntityPtr RealTimeSrvEntityFactory::CreateActionPawn()&#123; check( GetWorld() ); UWorld* const world = GetWorld(); if ( world ) &#123; FActorSpawnParameters SpawnParams; SpawnParams.SpawnCollisionHandlingOverride = ESpawnActorCollisionHandlingMethod::AlwaysSpawn; ARealTimeSrvPawn* const newActionPawn = world-&gt;SpawnActor&lt;ARealTimeSrvPawn&gt;( DefaultCharacterClasses, FTransform::Identity, SpawnParams ); if ( newActionPawn ) &#123; RealTimeSrvWorld::sInstance-&gt;AddGameObject( newActionPawn ); &#125; return RealTimeSrvEntityPtr( newActionPawn ); &#125; return RealTimeSrvEntityPtr();&#125;]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议六之客户端与服务器的连接]]></title>
    <url>%2F2017%2F04%2F09%2Fclient_server_connection%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[本篇自我总结这篇文章网上找不到翻译, 我也没时间详细翻译, 大概总结一下吧.本篇主要讲了把本系列之前五篇文章的技术应用到实战中处理客户端与服务器的的连接.请看总结, 不明之处再看文中具体讲解. 简单的连接协议First up we have the client state machine.The client is in one of three states: Disconnected Connecting Connected The goal is to create an abstraction on top of a UDP socket where our server presents a number of virtual slots for clients to connect to. When a client requests a connection, it gets assigned to one of these slots. If a client requests connection, but no slots are available, the server is full and the connection request is denied. On the server, we have the following data structure: 123456789const int MaxClients = 64;class Server&#123; int m_maxClients; int m_numConnectedClients; bool m_clientConnected[MaxClients]; Address m_clientAddress[MaxClients];&#125;; 这个简单协议存在的问题 易被攻击者利用我们的服务器当做DDos放大攻击的工具 攻击者可以很轻松的占满我们的client slots Traffic between the client and server can be read and modified in transit by a third party. 一旦被攻击者知道了客户端或者服务器的地址, 他就可以伪装服务器或客户端来欺骗对方获取利益 没有一个明确的断开连接的方式, 只能等time out 这些问题需要用授权系统和加密系统来解决. 如何改进这个连接协议 we no longer accept client connections immediately on connection request, instead we send back a challenge packet, and only complete connection when a client replies with information that can only be obtained by receiving the challenge packet. 为了防止攻击者利用我们的服务器当做DDos放大攻击的工具, 我们让客户端发的包比服务器发的包要大些. We’ll add some unique random identifiers, or ‘salts’, to make each client connection unique from previous ones coming from the same IP address and port. 一旦彼此连接上之后, 就用 client salt 和 server salt 的异或值来标识彼此. 以上的防御措施让我们的服务器做到了 no longer able to be used as port of DDoS amplification attacks, and with a trivial xor based authentication, 但对于一个经验丰富的会抓包分析的攻击者来说, 还存在以下问题 : This attacker can read and modify packets in flight. This breaks the trivial identification based around salt values… giving an attacker the power to disconnect any client at will. To solve this, we need to get serious with cryptography to encrypt and sign packets so they can’t be read or modified by a third party. 原文原文出处 原文标题 : Client Server Connection (How to create a client/server connection over UDP) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.So far in this article series we&rsquo;ve discussed how games read and write packets, how to unify packet read and write into a single function, how to fragment and re-assemble packets, and how to send large blocks of data over UDP.Now in this article we&rsquo;re going to bring everything together and build a client/server connection on top of UDP.BackgroundDevelopers from a web background often wonder why games go to such effort to build a client/server connection on top of UDP, when for many applications, TCP is good enough. These days even web servers are transitioning to UDP via Google&rsquo;s QUIC. If you still think TCP is good enough for time critical data in 2016, I encourage you to put that in your pipe and smoke it :) The reason is that games send time critical data.Why don&rsquo;t games use TCP for time critical data? The answer is that TCP delivers data reliably and in-order, and to do this on top of IP (which is unreliable, unordered) it holds more recent packets hostage in a queue while older packets are resent over the network.This is known as head of line blocking and it&rsquo;s a huuuuuge problem for games. To understand why, consider a game server broadcasting the state of the world to clients 10 times per-second. Each client advances time forward and wants to display the most recent state it receives from the server.But if the packet containing state for time t = 10.0 is lost, under TCP we must wait for it to be resent before we can access t = 10.1 and 10.2, even though those packets have already arrived and contain the state the client wants to display.Worse still, by the time the resent packet arrives, it&rsquo;s far too late for the client to actually do anything useful with it. The client has already advanced past 10.0 and wants to display something around 10.3 or 10.4!So why resend dropped packets at all? BINGO! What we&rsquo;d really like is an option to tell TCP: &ldquo;Hey, I don&rsquo;t care about old packets being resent, by they time they arrive I can&rsquo;t use them anyway, so just let me skip over them and access the most recent data&rdquo;.Unfortunately, TCP simply does not give us this option :(All data must be delivered reliably and in-order.This creates terrible problems for time critical data where packet loss and latency exist. Situations like, you know, The Internet, where people play FPS games.Large hitches corresponding to multiples of round trip time are added to the stream of data as TCP waits for dropped packets to be resent, which means additional buffering to smooth out these hitches, or long pauses where the game freezes and is non-responsive.Neither option is acceptable for first person shooters, which is why virtually all first person shooters are networked using UDP. UDP doesn&rsquo;t provide any reliability or ordering, so protocols built on top it can access the most recent data without waiting for lost packets to be resent, implementing whatever reliability they need in radically different ways to TCP.But, using UDP comes at a cost:UDP doesn&rsquo;t provide any concept of connection.We have to build that ourselves. This is a lot of work! So strap in, get ready, because we&rsquo;re going to build it all up from scratch using the same basic techniques first person shooters use when creating their protocols over UDP. You can use this client/server protocol for games or non-gaming applications and, provided the data you send is time critical, I promise you, it&rsquo;s well worth the effort. Client/Server Abstraction The goal is to create an abstraction on top of a UDP socket where our server presents a number of virtual slots for clients to connect to:When a client requests a connection, it gets assigned to one of these slots:If a client requests connection, but no slots are available, the server is full and the connection request is denied:Once a client is connected, packets are exchanged in both directions. These packets form the basis for the custom protocol between the client and server which is game specific.In a first person shooter, packets are sent continuously in both directions. Clients send input to the server as quickly as possible, often 30 or 60 times per-second, and the server broadcasts the state of the world to clients 10, 20 or even 60 times per-second.Because of this steady flow of packets in both directions there is no need for keep-alive packets. If at any point packets stop being received from the other side, the connection simply times out. No packets for 5 seconds is a good timeout value in my opinion, but you can be more aggressive if you want.When a client slot times out on the server, it becomes available for other clients to connect. When the client times out, it transitions to an error state.Simple Connection ProtocolLet&rsquo;s get started with the implementation of a simple protocol. It&rsquo;s a bit basic and more than a bit naive, but it&rsquo;s a good starting point and we&rsquo;ll build on it during the rest of this article, and the next few articles in this series.First up we have the client state machine.The client is in one of three states:DisconnectedConnectingConnectedInitially the client starts in disconnected.When a client connects to a server, it transitions to the connecting state and sends connection request packets to the server:The CRC32 and implicit protocol id in the packet header allow the server to trivially reject UDP packets not belonging to this protocol or from a different version of it.Since connection request packets are sent over UDP, they may be lost, received out of order or in duplicate.Because of this we do two things: 1) we keep sending packets for the client state until we get a response from the server or the client times out, and 2) on both client and server we ignore any packets that don&rsquo;t correspond to what we are expecting, since a lot of redundant packets are flying over the network.On the server, we have the following data structure:const int MaxClients = 64;class Server{ int m_maxClients; int m_numConnectedClients; bool m_clientConnected[MaxClients]; Address m_clientAddress[MaxClients];};Which lets the server lookup a free slot for a client to join (if any are free):int Server::FindFreeClientIndex() const{ for ( int i = 0; i &lt; m_maxClients; ++i ) { if ( !m_clientConnected[i] ) return i; } return -1;}Find the client index corresponding to an IP address and port:int Server::FindExistingClientIndex( const Address &amp; address ) const{ for ( int i = 0; i &lt; m_maxClients; ++i ) { if ( m_clientConnected[i] &amp;&amp; m_clientAddress[i] == address ) return i; } return -1;}Check if a client is connected to a given slot:bool Server::IsClientConnected( int clientIndex ) const{ return m_clientConnected[clientIndex];}&hellip; and retrieve a client’s IP address and port by client index:const Address &amp; Server::GetClientAddress( int clientIndex ) const{ return m_clientAddress[clientIndex];}Using these queries we implement the following logic when the server processes a connection request packet:If the server is full, reply with connection denied.If the connection request is from a new client and we have a slot free, assign the client to a free slot and respond with connection accepted.If the sender corresponds to the address of a client that is already connected, also reply with connection accepted. This is necessary because the first response packet may not have gotten through due to packet loss. If we don&rsquo;t resend this response, the client gets stuck in the connecting state until it times out.The connection accepted packet tells the client which client index it was assigned, which the client needs to know which player it is in the game:Once the server sends a connection accepted packet, from its point of view it considers that client connected. As the server ticks forward, it watches connected client slots, and if no packets have been received from a client for 5 seconds, the slot times out and is reset, ready for another client to connect.Back to the client. While the client is in the connecting state the client listens for connection denied and connection accepted packets from the server. Any other packets are ignored.If the client receives connection accepted, it transitions to connected. If it receives connection denied, or after 5 seconds hasn&rsquo;t received any response from the server, it transitions to disconnected.Once the client hits connected it starts sending connection payload packets to the server. If no packets are received from the server in 5 seconds, the client times out and transitions to disconnected.Naive Protocol is NaiveWhile this protocol is easy to implement, we can&rsquo;t use a protocol like this in production. It&rsquo;s way too naive. It simply has too many weaknesses to be taken seriously:Spoofed packet source addresses can be used to redirect connection accepted responses to a target (victim) address. If the connection accepted packet is larger than the connection request packet, attackers can use this protocol as part of a DDoS amplification attack.Spoofed packet source addresses can be used to trivially fill all client slots on a server by sending connection request packets from n different IP addresses, where n is the number of clients allowed per-server. This is a real problem for dedicated servers. Obviously you want to make sure that only real clients are filling slots on servers you are paying for.An attacker can trivially fill all slots on a server by varying the client UDP port number on each client connection. This is because clients are considered unique on an address + port basis. This isn&rsquo;t easy to fix because due to NAT (network address translation), different players behind the same router collapse to the same IP address with only the port being different, so we can&rsquo;t just consider clients to be unique at the IP address level sans port.Traffic between the client and server can be read and modified in transit by a third party. While the CRC32 protects against packet corruption, an attacker would simply recalculate the CRC32 to match the modified packet.If an attacker knows the client and server IP addresses and ports, they can impersonate the client or server. This gives an attacker the power to completely a hijack a client’s connection and perform actions on their behalf.Once a client is connected to a server there is no way for them to disconnect cleanly, they can only time out. This creates a delay before the server realizes a client has disconnected, or before a client realizes the server has shut down. It would be nice if both the client and server could indicate a clean disconnect, so the other side doesn’t need to wait for timeout in the common case.Clean disconnection is usually implemented with a disconnect packet, however because an attacker can impersonate the client and server with spoofed packets, doing so would give the attacker the ability to disconnect a client from the server whenever they like, provided they know the client and server IP addresses and the structure of the disconnect packet.If a client disconnects dirty and attempts to reconnect before their slot times out on the server, the server still thinks that client is connected and replies with connection accepted to handle packet loss. The client processes this response and thinks it&rsquo;s connected to the server, but it&rsquo;s actually in an undefined state.While some of these problems require authentication and encryption before they can be fully solved, we can make some small steps forward to improve the protocol before we get to that. These changes are instructive.Improving The Connection ProtocolThe first thing we want to do is only allow clients to connect if they can prove they are actually at the IP address and port they say they are.To do this, we no longer accept client connections immediately on connection request, instead we send back a challenge packet, and only complete connection when a client replies with information that can only be obtained by receiving the challenge packet.The sequence of operations in a typical connect now looks like this:To implement this we need an additional data structure on the server. Somewhere to store the challenge data for pending connections, so when a challenge response comes in from a client we can check against the corresponding entry in the data structure and make sure it&rsquo;s a valid response to the challenge sent to that address.While the pending connect data structure can be made larger than the maximum number of connected clients, it&rsquo;s still ultimately finite and is therefore subject to attack. We&rsquo;ll cover some defenses against this in the next article. But for the moment, be happy at least that attackers can&rsquo;t progress to the connected state with spoofed packet source addresses.Next, to guard against our protocol being used in a DDoS amplification attack, we&rsquo;ll inflate client to server packets so they&rsquo;re large relative to the response packet sent from the server. This means we add padding to both connection request and challenge response packets and enforce this padding on the server, ignoring any packets without it. Now our protocol effectively has DDoS minification for requests -&gt; responses, making it highly unattractive for anyone thinking of launching this kind of attack.Finally, we&rsquo;ll do one last small thing to improve the robustness and security of the protocol. It&rsquo;s not perfect, we need authentication and encryption for that, but it at least it ups the ante, requiring attackers to actually sniff traffic in order to impersonate the client or server. We&rsquo;ll add some unique random identifiers, or &lsquo;salts&rsquo;, to make each client connection unique from previous ones coming from the same IP address and port.The connection request packet now looks like this:The client salt in the packet is a random 64 bit integer rolled each time the client starts a new connect. Connection requests are now uniquely identified by the IP address and port combined with this client salt value. This distinguishes packets from the current connection from any packets belonging to a previous connection, which makes connection and reconnection to the server much more robust.Now when a connection request arrives and a pending connection entry can&rsquo;t be found in the data structure (according to IP, port and client salt) the server rolls a server salt and stores it with the rest of the data for the pending connection before sending a challange packet back to the client. If a pending connection is found, the salt value stored in the data structure is used for the challenge. This way there is always a consistent pair of client and server salt values corresponding to each client session.The client state machine has been expanded so connecting is replaced with two new states: sending connection request and sending challenge response, but it&rsquo;s the same idea as before. Client states repeatedly send the packet corresponding to that state to the server while listening for the response that moves it forward to the next state, or back to an error state. If no response is received, the client times out and transitions to disconnected.The challenge response sent from the client to the server looks like this:The utility of this being that once the client and server have established connection, we prefix all payload packets with the xor of the client and server salt values and discard any packets with the incorrect salt values. This neatly filters out packets from previous sessions and requires an attacker to sniff packets in order to impersonate a client or server.Now that we have at least a basic level of security, it&rsquo;s not much, but at least it&rsquo;s something, we can implement a disconnect packet:And when the client or server want to disconnect clean, they simply fire 10 of these over the network to the other side, in the hope that some of them get through, and the other side disconnects cleanly instead of waiting for timeout.ConclusionWe now have a much more robust protocol. It&rsquo;s secure against spoofed IP packet headers. It&rsquo;s no longer able to be used as port of DDoS amplification attacks, and with a trivial xor based authentication, we are protected against casual attackers while client reconnects are much more robust.But it&rsquo;s still vulnerable to a sophisticated actors who can sniff packets:This attacker can read and modify packets in flight.This breaks the trivial identification based around salt values&hellip;&hellip; giving an attacker the power to disconnect any client at will.To solve this, we need to get serious with cryptography to encrypt and sign packets so they can&rsquo;t be read or modified by a third party.]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kbe服务端笔记(一)]]></title>
    <url>%2F2017%2F04%2F01%2Fkbe_note_one%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[main看起来似乎所有的组件都有一个这样的宏(KBENGINE_MAIN)来包裹main函数123456intKBENGINE_MAIN(intargc, char* argv[])&#123; ENGINE_COMPONENT_INFO&amp;info = g_kbeSrvConfig.getXXX(); returnkbeMainT&lt;XXX&gt;(argc, argv, YYY, info.externalPorts_min, info.externalPorts_max, info.externalInterface, 0, info.internalInterface);&#125; . . . 这个宏展开是这样子：123456789101112131415kbeMain(intargc, char* argv[]); \intmain(intargc, char* argv[]) \&#123; \ loadConfig(); \ g_componentID = genUUID64(); \ parseMainCommandArgs(argc, argv); \ char dumpname[MAX_BUF] = &#123;0&#125;; \ kbe_snprintf(dumpname, MAX_BUF, &quot;%&quot;PRAppID, g_componentID); \ KBEngine::exception::installCrashHandler(1, dumpname); \ intretcode = -1; \ THREAD_TRY_EXECUTION; \ retcode = kbeMain(argc, argv); \ THREAD_HANDLE_CRASH; \ returnretcode; \&#125; \ 稍微整理一下之后main函数看起来很像是这个样子：1234567891011121314151617181920intkbeMain(intargc, char* argv[]);intmain(intargc, char* argv[])&#123; loadConfig(); g_componentID = genUUID64(); parseMainCommandArgs(argc, argv);chardumpname[MAX_BUF] = &#123;0&#125;; kbe_snprintf(dumpname, MAX_BUF, &quot;%&quot;PRAppID, g_componentID); KBEngine::exception::installCrashHandler(1, dumpname);intretcode = -1; THREAD_TRY_EXECUTION;retcode = kbeMain(argc, argv); THREAD_HANDLE_CRASH;return (retcode);&#125;intkbeMain(intargc, char* argv[])&#123; ENGINE_COMPONENT_INFO&amp;info = g_kbeSrvConfig.getXXX(); return kbeMainT&lt;XXX&gt;(argc, argv, YYY, info.externalPorts_min, info.externalPorts_max, info.externalInterface, 0, info.internalInterface);&#125; 基本可以理解为每个组件的main函数流程都是一样的，只是在特化kbeMainT时所给参数不一样。 ServerConfig：ServerConfig涉及到服务端每个组件的各种配置选项，比如数据库访问。它的构造在组件名.cpp中，比如loginapp就在loginapp.cpp，machine就在machine.cpp中，loginapp的如下（server/loginapp/loginapp.cpp）：12ServerConfigg_serverConfig;KBE_SINGLETON_INIT(Loginapp); 它的初始化（配置）工作主要由loadConfig接口完成，如下（lib/server/kbemain.h）：12345678910inlinevoidloadConfig()&#123; Resmgr::getSingleton().initialize(); // &quot;../../res/server/kbengine_defs.xml&quot; g_kbeSrvConfig.loadConfig(&quot;server/kbengine_defs.xml&quot;); // &quot;../../../assets/res/server/kbengine.xml&quot; g_kbeSrvConfig.loadConfig(&quot;server/kbengine.xml&quot;);&#125; Resmgr：Resmgr负责管理kbe的所有资源管理，比如资源路径，环境变量。Resmgr的构造地方如下（lib/network/fixed_messages.cpp）：1234567FixedMessages::FixedMessages():_infomap(),_loaded(false)&#123; newResmgr(); Resmgr::getSingleton().initialize();&#125; 我们可以理解为FixedMessages构造的时候Resmgr就构造了。 Resmgr的初始化（配置）工作主要由initialize接口完成，代码如上。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>KBE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议五之可靠的有序消息]]></title>
    <url>%2F2017%2F03%2F29%2Freliable_ordered_messages%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[本篇自我总结本篇主要讲了数据包的分包和重组问题, 到底数据包多大才好呢?是不是越大越好呢?包太大了怎么办呢?请看总结, 不明之处再看文中具体讲解. 为什么需要做这个可靠UDP协议网络协议在动作游戏类型（FPS）中的典型特征就是一个持续发送的数据包，在两个方向上以稳定的速度如20或30包每秒发送。这些数据包都包含有不可靠的无序数据例如t时间内的世界状态；所以，当一个数据包丢失，重新发送它并不是特别有用。当重新发送的数据包到达时，时间t已经过去了。 所以这就是我们将要实现可靠性的现状。对于我们90%的数据包，仅丢弃并不再重新发送它会更好。对于10%或更少（误差允许范围内）的情况，我们确实需要可靠性，但这样的数据是非常罕见的，很少被发送而且比不可靠的数据的平均大小要小得多。这个使用案例适用于所有过去十五年来发布的AAA级的FPS游戏。 应答系统是实现可靠UDP的最重要的部分为实现数据包层级的应答，在每个包的前面添加如下的报头： 123456struct Header&#123; uint16_t sequence; uint16_t ack; uint32_t ack_bits;&#125;; 这些报头元素组合起来以创建应答系统： sequence 是一个数字，随每个数据包发送而增长（并且在达到65535后回往复）。 ack 是从另一方接收到的最新的数据包序列号。 ack_bits 是一个位字段，它编码与ack相关的收到的数据包组合：如果位n已经设置，即 ack– n 数据包被接收了。 ack_bits 不仅是一个节省带宽的巧妙的编码，它同样也增加了信息冗余来抵御包的丢失。每个应答码要被发送32次。如果有一个包丢失了，仍然有其他31个包有着相同的应答码。从统计上来说，应答码还是非常有可能送达的。 但突发的传送数据包的丢失还是有可能发生的，所以重要的是要注意： 如果你收到一个数据包n的应答码，那么这个包肯定已经收到了。 如果你没有收到应答码，那么这个包就很有可能 没有被收到。但是…它也许会是，仅是应答码没有送达。这种情况是极其罕见的。 以我的经验，没有必要设计完善的应答机制。在一个极少丢应答码的系统上构建一个可靠性系统并不会增加什么大问题。 发送方如何追踪数据包是否已经被应答为实现这个应答系统，我们在发送方还需要一个数据结构来追踪一个数据包是否已经被应答，这样我们就可以忽略冗余的应答（每个包会通过 ack_bits多次应答)。我们同样在接收方也还需要一个数据结构来追踪那些已经收到的包，这样我们就可以在数据包的报头填写ack_bits的值。 12345678910111213141516171819const int BufferSize = 1024; uint32_t sequence_buffer[BufferSize]; struct PacketData&#123; bool acked;&#125;; PacketData packet_data[BufferSize]; PacketData * GetPacketData( uint16_t sequence )&#123; const int index = sequence % BufferSize; if ( sequence_buffer[index] == sequence ) return &amp;packet_data[index]; else return NULL;&#125; 你在这可以看到的窍门是这个滚动的缓冲区是以序列号来作为索引的： const int index =sequence % BufferSize; 当条目被顺序添加，就像一个被发送的队列，对插入所需要做的就是把这个序列缓冲区的值更新为新的序列号并且在该索引处重写这个数据： 123456PacketData &amp; InsertPacketData( uint16_t sequence )&#123; const int index = sequence % BufferSize; sequence_buffer[index] = sequence; return packet_data[index];&#125; 原文原文出处 原文标题 : Reliable Ordered Messages (How to implement reliable-ordered messages on top of UDP) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.Many people will tell you that implementing your own reliable message system on top of UDP is foolish. After all, why reimplement TCP?But why limit ourselves to how TCP works? But there are so many different ways to implement reliable-messages and most of them work nothing like TCP!So let&rsquo;s get creative and work out how we can implement a reliable message system that&rsquo;s better and more flexible than TCP for real-time games.Different ApproachesA common approach to reliability in games is to have two packet types: reliable-ordered and unreliable. You&rsquo;ll see this approach in many network libraries.The basic idea is that the library resends reliable packets until they are received by the other side. This is the option that usually ends up looking a bit like TCP-lite for the reliable-packets. It&rsquo;s not that bad, but you can do much better.The way I prefer to think of it is that messages are smaller bitpacked elements that know how to serialize themselves. This makes the most sense when the overhead of length prefixing and padding bitpacked messages up to the next byte is undesirable (eg. lots of small messages included in each packet). Sent messages are placed in a queue and each time a packet is sent some of the messages in the send queue are included in the outgoing packet. This way there are no reliable packets that need to be resent. Reliable messages are simply included in outgoing packets until they are received.The easiest way to do this is to include all unacked messages in each packet sent. It goes something like this: each message sent has an id that increments each time a message is sent. Each outgoing packet includes the start message id followed by the data for n messages. The receiver continually sends back the most recent received message id to the sender as an ack and only messages newer than the most recent acked message id are included in packets.This is simple and easy to implement but if a large burst of packet loss occurs while you are sending messages you get a spike in packet size due to unacked messages.You can avoid this by extending the system to have an upper bound on the number of messages included per-packet n. But now if you have a high packet send rate (like 60 packets per-second) you are sending the same message multiple times until you get an ack for that message.If your round trip time is 100ms each message will be sent 6 times redundantly before being acked on average. Maybe you really need this amount of redundancy because your messages are extremely time critical, but in most cases, your bandwidth would be better spent on other things.The approach I prefer combines packet level acks with a prioritization system that picks the n most important messages to include in each packet. This combines time critical delivery and the ability to send only n messages per-packet, while distributing sends across all messages in the send queue.Packet Level AcksTo implement packet level acks, we add the following packet header:struct Header{ uint16_t sequence; uint16_t ack; uint32_t ack_bits;};These header elements combine to create the ack system: sequence is a number that increases with each packet sent, ack is the most recent packet sequence number received, and ack_bits is a bitfield encoding the set of acked packets.If bit n is set in ack_bits, then ack - n is acked. Not only is ack_bits a smart encoding that saves bandwidth, it also adds redundancy to combat packet loss. Each ack is sent 32 times. If one packet is lost, there&rsquo;s 31 other packets with the same ack. Statistically speaking, acks are very likely to get through.But bursts of packet loss do happen, so it&rsquo;s important to note that:If you receive an ack for packet n then that packet was definitely received.If you don&rsquo;t receive an ack, the packet was most likely not received. But, it might have been, and the ack just didn&rsquo;t get through. This is extremely rare.In my experience it&rsquo;s not necessary to send perfect acks. Building a reliability system on top of a system that very rarely drops acks adds no significant problems. But it does create a challenge for testing this system works under all situations because of the edge cases when acks are dropped.So please if you do implement this system yourself, setup a soak test with terrible network conditions to make sure your ack system is working correctly. You&rsquo;ll find such a soak test in the example source code for this article, and the open source network libraries reliable.io and yojimbo which also implement this technique.Sequence BuffersTo implement this ack system we need a data structure on the sender side to track whether a packet has been acked so we can ignore redundant acks (each packet is acked multiple times via ack_bits. We also need a data structure on the receiver side to keep track of which packets have been received so we can fill in the ack_bits value in the packet header.The data structure should have the following properties:Constant time insertion (inserts may be random, for example out of order packets&hellip;)Constant time query if an entry exists given a packet sequence numberConstant time access for the data stored for a given packet sequence numberConstant time removal of entriesYou might be thinking. Oh of course, hash table. But there&rsquo;s a much simpler way:const int BufferSize = 1024;uint32_t sequence_buffer[BufferSize];struct PacketData{ bool acked;};PacketData packet_data[BufferSize];PacketData GetPacketData( uint16_t sequence ){ const int index = sequence % BufferSize; if ( sequence_buffer[index] == sequence ) return &amp;packet_data[index]; else return NULL;}As you can see the trick here is a rolling buffer indexed by sequence number:const int index = sequence % BufferSize;This works because we don&rsquo;t care about being destructive to old entries. As the sequence number increases older entries are naturally overwritten as we insert new ones. The sequence_buffer[index] value is used to test if the entry at that index actually corresponds to the sequence number you&rsquo;re looking for. A sequence buffer value of 0xFFFFFFFF indicates an empty entry and naturally returns NULL for any sequence number query without an extra branch.When entries are added in order like a send queue, all that needs to be done on insert is to update the sequence buffer value to the new sequence number and overwrite the data at that index:PacketData &amp; InsertPacketData( uint16_t sequence ){ const int index = sequence % BufferSize; sequence_buffer[index] = sequence; return packet_data[index];}Unfortunately, on the receive side packets arrive out of order and some are lost. Under ridiculously high packet loss (99%) I&rsquo;ve seen old sequence buffer entries stick around from before the previous sequence number wrap at 65535 and break my ack logic (leading to false acks and broken reliability where the sender thinks the other side has received something they haven&rsquo;t&hellip;).The solution to this problem is to walk between the previous highest insert sequence and the new insert sequence (if it is more recent) and clear those entries in the sequence buffer to 0xFFFFFFFF. Now in the common case, insert is very close to constant time, but worst case is linear where n is the number of sequence entries between the previous highest insert sequence and the current insert sequence.Before we move on I would like to note that you can do much more with this data structure than just acks. For example, you could extend the per-packet data to include time sent:struct PacketData{ bool acked; double send_time;};With this information you can create your own estimate of round trip time by comparing send time to current time when packets are acked and taking an exponentially smoothed moving average. You can even look at packets in the sent packet sequence buffer older than your RTT estimate (you should have received an ack for them by now&hellip;) to create your own packet loss estimate.Ack AlgorithmNow that we have the data structures and packet header, here is the algorithm for implementing packet level acks:On packet send:Insert an entry for for the current send packet sequence number in the sent packet sequence buffer with data indicating that it hasn&rsquo;t been acked yetGenerate ack and ack_bits from the contents of the local received packet sequence buffer and the most recent received packet sequence numberFill the packet header with sequence, ack and ack_bitsSend the packet and increment the send packet sequence numberOn packet receive:Read in sequence from the packet headerIf sequence is more recent than the previous most recent received packet sequence number, update the most recent received packet sequence numberInsert an entry for this packet in the received packet sequence bufferDecode the set of acked packet sequence numbers from ack and ack_bits in the packet header.Iterate across all acked packet sequence numbers and for any packet that is not already acked call OnPacketAcked( uint16_t sequence ) and mark that packet as acked in the sent packet sequence buffer.Importantly this algorithm is done on both sides so if you have a client and a server then each side of the connection runs the same logic, maintaining its own sequence number for sent packets, tracking most recent received packet sequence # from the other side and a sequence buffer of received packets from which it generates sequence, ack and ack_bits to send to the other side.And that&rsquo;s really all there is to it. Now you have a callback when a packet is received by the other side: OnPacketAcked. The main benefit of this ack system is now that you know which packets were received, you can build any reliability system you want on top. It&rsquo;s not limited to just reliable-ordered messages. For example, you could use it to implement delta encoding on a per-object basis.Message ObjectsMessages are small objects (smaller than packet size, so that many will fit in a typical packet) that know how to serialize themselves. In my system they perform serialization using a unified serialize functionunified serialize function.The serialize function is templated so you write it once and it handles read, write and measure.Yes. Measure. One of my favorite tricks is to have a dummy stream class called MeasureStream that doesn&rsquo;t do any actual serialization but just measures the number of bits that would be written if you called the serialize function. This is particularly useful for working out which messages are going to fit into your packet, especially when messages themselves can have arbitrarily complex serialize functions.struct TestMessage : public Message{ uint32_t a,b,c; TestMessage() { a = 0; b = 0; c = 0; } template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_bits( stream, a, 32 ); serialize_bits( stream, b, 32 ); serialize_bits( stream, c, 32 ); return true; } virtual SerializeInternal( WriteStream &amp; stream ) { return Serialize( stream ); } virtual SerializeInternal( ReadStream &amp; stream ) { return Serialize( stream ); } virtual SerializeInternal( MeasureStream &amp; stream ) { return Serialize( stream ); }};The trick here is to bridge the unified templated serialize function (so you only have to write it once) to virtual serialize methods by calling into it from virtual functions per-stream type. I usually wrap this boilerplate with a macro, but it&rsquo;s expanded in the code above so you can see what&rsquo;s going on.Now when you have a base message pointer you can do this and it just works:Message message = CreateSomeMessage();message-&gt;SerializeInternal( stream );An alternative if you know the full set of messages at compile time is to implement a big switch statement on message type casting to the correct message type before calling into the serialize function for each type. I&rsquo;ve done this in the past on console platform implementations of this message system (eg. PS3 SPUs) but for applications today (2016) the overhead of virtual functions is neglible.Messages derive from a base class that provides a common interface such as serialization, querying the type of a message and reference counting. Reference counting is necessary because messages are passed around by pointer and stored not only in the message send queue until acked, but also in outgoing packets which are themselves C++ structs.This is a strategy to avoid copying data by passing both messages and packets around by pointer. Somewhere else (ideally on a separate thread) packets and the messages inside them are serialized to a buffer. Eventually, when no references to a message exist in the message send queue (the message is acked) and no packets including that message remain in the packet send queue, the message is destroyed.We also need a way to create messages. I do this with a message factory class with a virtual function overriden to create a message by type. It&rsquo;s good if the packet factory also knows the total number of message types, so we can serialize a message type over the network with tight bounds and discard malicious packets with message type values outside of the valid range:enum TestMessageTypes{ TEST_MESSAGE_A, TEST_MESSAGE_B, TEST_MESSAGE_C, TEST_MESSAGE_NUM_TYPES};// message definitions omittedclass TestMessageFactory : public MessageFactory{public: Message Create( int type ) { switch ( type ) { case TEST_MESSAGE_A: return new TestMessageA(); case TEST_MESSAGE_B: return new TestMessageB(); case TEST_MESSAGE_C: return new TestMessageC(); } } virtual int GetNumTypes() const { return TEST_MESSAGE_NUM_TYPES; }};Again, this is boilerplate and is usually wrapped by macros, but underneath this is what&rsquo;s going on.Reliable Ordered Message AlgorithmThe algorithm for sending reliable-ordered messages is as follows:On message send:Measure how many bits the message serializes to using the measure streamInsert the message pointer and the # of bits it serializes to into a sequence buffer indexed by message id. Set the time that message has last been sent to -1Increment the send message idOn packet send:Walk across the set of messages in the send message sequence buffer between the oldest unacked message id and the most recent inserted message id from left -&gt; right (increasing message id order).Never send a message id that the receiver can&rsquo;t buffer or you&rsquo;ll break message acks (since that message won&rsquo;t be buffered, but the packet containing it will be acked, the sender thinks the message has been received, and will not resend it). This means you must never send a message id equal to or more recent than the oldest unacked message id plus the size of the message receive buffer.For any message that hasn&rsquo;t been sent in the last 0.1 seconds and fits in the available space we have left in the packet, add it to the list of messages to send. Messages on the left (older messages) naturally have priority due to the iteration order.Include the messages in the outgoing packet and add a reference to each message. Make sure the packet destructor decrements the ref count for each message.Store the number of messages in the packet n and the array of message ids included in the packet in a sequence buffer indexed by the outgoing packet sequence number so they can be used to map packet level acks to the set of messages included in that packet.Add the packet to the packet send queue.On packet receive:Walk across the set of messages included in the packet and insert them in the receive message sequence buffer indexed by their message id.The ack system automatically acks the packet sequence number we just received.On packet ack:Look up the set of messages ids included in the packet by sequence number.Remove those messages from the message send queue if they exist and decrease their ref count.Update the last unacked message id by walking forward from the previous unacked message id in the send message sequence buffer until a valid message entry is found, or you reach the current send message id. Whichever comes first.On message receive:Check the receive message sequence buffer to see if a message exists for the current receive message id.If the message exists, remove it from the receive message sequence buffer, increment the receive message id and return a pointer to the message.Otherwise, no message is available to receive. Return NULL.In short, messages keep getting included in packets until a packet containing that message is acked. We use a data structure on the sender side to map packet sequence numbers to the set of message ids to ack. Messages are removed from the send queue when they are acked. On the receive side, messages arriving out of order are stored in a sequence buffer indexed by message id, which lets us receive them in the order they were sent.The End ResultThis provides the user with an interface that looks something like this on send:TestMessage message = (TestMessage) factory.Create( TEST_MESSAGE );if ( message ){ message-&gt;a = 1; message-&gt;b = 2; message-&gt;c = 3; connection.SendMessage( message );}And on the receive side:while ( true ){ Message message = connection.ReceiveMessage(); if ( !message ) break; if ( message-&gt;GetType() == TEST_MESSAGE ) { TestMessage testMessage = (TestMessage) message; // process test message } factory.Release( message );}Which is flexible enough to implement whatever you like on top of it. 译文 译文出处 译者：翁僖骏（∈星际长途←） 审校：侯鹏（叶落&amp;无痕） 嗨，我是格伦费德勒，欢迎来到创建一个游戏网络协议第五篇文章。 从上一篇文章到现在已经有很长一段时间了，上次我已经率先而且实现了余下的这一系列文章所需的源码并创建了开源库libyojimbo，是本系列文章所要描述的网络协议的一个质量有保证的的和经过单元测试的版本。 如果你想要有一个开源库来为自己在UDP之上实现可靠消息或是为了其他更多，看看libyojimbo。但是，如果你像我这样是想理解它具体是怎么工作的并且可能自己去实现它，阅读下去，因为我们将要从头到脚地去建立一个在UDP之上用来发送可靠有序消息的完整的系统！ 说明 很多人也许会跟你说，要在UDP之上实现你自己的可靠消息系统是愚蠢的。为什么要撰写你特有的简化版本的TCP？这些人深信，任何可靠性的实现不可避免地 最终会成为一个（简化的）TCP的重实现。 但也有很多不同的方法来在UDP之上实现可靠消息，各有不同的优势和劣势。TCP的方法并不是唯一的选择。事实上，我所了解到的大多数可靠有序信息的选择的原理和TCP并不相同。所以让我们为我们的目标发挥创造力并弄懂我们该如何充分利用我们的现状来实现一个比TCP更好 的可靠性系统。 网络协议在动作游戏类型（FPS）中的典型特征就是一个持续发送的数据包，在两个方向上以稳定的速度如20或30包每秒发送。这些数据包都包含有不可靠的无序数据例如t时间内的世界状态；所以，当一个数据包丢失，重新发送它并不是特别有用。当重新发送的数据包到达时，时间t已经过去了。 所以这就是我们将要实现可靠性的现状。对于我们90%的数据包，仅丢弃并不再重新发送它会更好。对于10%或更少（误差允许范围内）的情况，我们确实需要可靠性，但这样的数据是非常罕见的，很少被发送而且比不可靠的数据的平均大小要小得多。这个使用案例适用于所有过去十五年来发布的AAA级的FPS游戏。 不同的方法 可靠性的一个常用的方法是使用两种包类型：可靠有序的和不可靠的。你在众多网络库中都会看到这个方法。它基本的想法是，这个库不断重新发送可靠的数据包直到它的另一方接收到为止。这是一个最终看起来会有一点像TCP方式传输的可靠包的选择。这并没有很糟糕，但你也可以做得更好。 我更愿意去考虑的方法就是消息其实是更小的位包装元素，它知道如何使它们自己序列化。这就显得非常有意义了，因为按位打包的消息中，用于描述下个字节的前缀或者后缀的字节开销在大部分的情况下是不必需的（例如每个包中包含的许多小的消息）。被发送的消息会被放在一个队列并且每次一个包被发送时，发送队列中的一些消息就会被包含在外发的包中。这样一来，就没有可靠的数据包需要被重新发送了。可靠消息也只会包含在数据包里直到它们被接收。 要做到这样最简单的方法就是，把所有未应答的消息包含到每个被发送的包中。它是这样的：每个被发送的消息都有一个随每当一个消息被发送时递增的id。每个输出数据包包含起始消息id ，紧跟着的是n 个消息的数据。接收方不断发回最新收到消息的id给发送方作为一个应答信号，并且消息要当且仅当比最新的应答消息id要更新，才会被包含在数据包中。 这很简单也易于实现，但当你正在发送消息时如果突发一个很大的包丢失情况，你会遇到一个数据包大小的峰值，因为有很多未应答的消息。。正如在数据包分割和重组中讨论的需要按照MTU分割包的方式来发送大的数据包会增加丢包的情况。在高丢包率下你最不想做的就是增大包的规格并引起更多的包的丢失。这是一个潜在的无底洞。 你可以通过扩展系统来给每个包的消息数量n设置一个上限，来避免这种情况。但现在如果你有一个高数据包发送率（如每秒60包）你就要多次发送同样的消息直到你得到该消息的应答信号。如果的往返时间是100ms，每条消息在被应答之前将要平均被多余发送六次。也许你真的需要这些多余的发送数量因为你的消息是对时间极其敏感的，但在大多数情况下，你应该给队列里的其他消息分配合理的带宽。 我比较喜欢的方法是用一个优先次序系统整合每包的应答信号，这个系统检出n条最重要的消息并包含在每个包中。在散布的消息穿过所有在发送队列中的消息发送时，这样就把对时间敏感的递送与每包仅发送n条消息的能力联合起来了。 数据包层级应答 让我们行动起来实现它。 这种可靠性系统的基础是每个包的应答。但为什么应答是在数据包层级而不是在消息层级呢？简要截说原因就是包的数量会远远少于消息的数量。假设每个包中有32或64条消息，显然让一个包含32或64条消息的包来应答会比让每个消息都分别应答要高效得多。 这样同样也增加了灵活性，因为你可以在数据包层级应答上构建其他可靠性系统，不仅仅是为了可靠有序的消息。例如，使用了数据包层级应答，你就知道哪一个时间先决的不可靠状态更新已结束，所以你可以轻易地构建一个系统，在一旦一个数据包所包含的最后一个状态更新已经应答时，停止发送不再改变的对象状态。 为实现数据包层级的应答，在每个包的前面添加如下的报头：?123456struct Header{ uint16_t sequence; uint16_t ack; uint32_t ack_bits;};这些报头元素组合起来以创建应答系统： sequence 是一个数字，随每个数据包发送而增长（并且在达到65535后回往复）。 ack 是从另一方接收到的最新的数据包序列号。 ack_bits 是一个位字段，它编码与ack相关的收到的数据包组合：如果位n已经设置，即 ack– n 数据包被接收了。 ack_bits 不仅是一个节省带宽的巧妙的编码，它同样也增加了信息冗余来抵御包的丢失。每个应答码要被发送32次。如果有一个包丢失了，仍然有其他31个包有着相同的应答码。从统计上来说，应答码还是非常有可能送达的。 但突发的传送数据包的丢失还是有可能发生的，所以重要的是要注意： 如果你收到一个数据包n的应答码，那么这个包肯定已经收到了。 如果你没有收到应答码，那么这个包就很有可能 没有被收到。但是…它也许会是，仅是应答码没有送达。这种情况是极其罕见的。 以我的经验，没有必要设计完善的应答机制。在一个极少丢应答码的系统上构建一个可靠性系统并不会增加什么大问题。但对于在所有情况下来测试这个系统的工作将会成为很大的挑战，因为还要考虑应答码丢失的边界情况。 所以如果你自己实现这个系统的话，请设置一个浸泡测试来覆盖糟糕的网络情况，用来确保你的应答系统是在正确的工作，相关地，你的消息系统的执行实际上是在这些网络情况下可靠地而且有序的交付可靠有序消息。以我之见（并且我已经写了许多这样的系统的变式至少有十次了），这是确保正确行为的一个必要步骤。 你在这篇文章的示例源代码中会找到这样一个浸泡测试，它对patreon支持是有效的，并且也在开源网络库libyojimbo中。 序列缓冲区 为实现这个应答系统，我们在发送方还需要一个数据结构来追踪一个数据包是否已经被应答，这样我们就可以忽略冗余的应答（每个包会通过 ack_bits多次应答）。我们同样在接收方也还需要一个数据结构来追踪那些已经收到的包，这样我们就可以在数据包的报头填写ack_bits的值。 这个数据结构应该具有以下属性： 常量时间内插入（插入可能会是随机的，例如乱序数据包…） 给定的数据包的序列号在常量时间内查询一个条目是否存在 对给定的数据包序列号，在常量时间内访问数据存储 常量时间内删除条目 你可能会想。哦，当然，哈希表。但还有一个更简单的方法：?12345678910111213141516171819const int BufferSize = 1024; uint32_t sequence_buffer[BufferSize]; struct PacketData{ bool acked;}; PacketData packet_data[BufferSize]; PacketData * GetPacketData( uint16_t sequence ){ const int index = sequence % BufferSize; if ( sequence_buffer[index] == sequence ) return &amp;packet_data[index]; else return NULL;}你在这可以看到的窍门是这个滚动的缓冲区是以序列号来作为索引的： const int index =sequence % BufferSize; 这是可行的，因为我们并不在意旧条目破坏。随着序列号的递增，旧的条目也自然而然地随着我们插入了新条目而被重写。sequence_buffer[index]的值是用来测试该索引的条目是否实际上与你所搜寻的序列号相符。一个缓冲序列的值是0xFFFFFFFF 就表示一个空的条目并自然地对任何序列号查询返回NULL，没有任何其他（代码）分支。 当条目被顺序添加，就像一个被发送的队列，对插入所需要做的就是把这个序列缓冲区的值更新为新的序列号并且在该索引处重写这个数据：?123456PacketData &amp; InsertPacketData( uint16_t sequence ){ const int index = sequence % BufferSize; sequence_buffer[index] = sequence; return packet_data[index];}但在接收端数据包以乱序到达并且有一部分丢失。在高得离谱的丢包率下（99%），我就会看到旧的序列缓冲区条目还存在，但是新条目的序列号已经超过了65535并且循环到达了旧条目之前，并且打破了我的应答逻辑（导致错误应答并打破了可靠性，这时发送方会真的认为对方已经接收到了一些东西但其实并不是…） 解决这个问题的办法是遍历上一个最高的插入序列与最新收到的插入序列之间的条目（如果它是更加新的话）并在缓冲区清除这些条目即都置为0xFFFFFFFF。现在，在一般情况下，插入操作是非常接近 时间常量的，但最糟的情况是，在先前最高的序列号和当前插入的序列号之间线性遍历的次数n等于缓冲区的长度。 在我们继续之前，我想指出，你可以用这个数据结构做更多事情而不仅是对于应答码。例如，你可以加入发送时间，来扩展每个包的数据：?12345struct PacketData{ bool acked; double send_time;};有了这些信息你可以对往返时间通过做指数级的平滑取平均数做修正，最终得到合理的预期往返时间。你甚至可以看到在发送数据包的序列缓存区的数据包会比你RTT预计的（你现在应该已经收到了它们的应答码…）要旧，通过这个往返时间对还没有应答的包做判断，来决定创建你的数据包丢失预计。 应答算法 现在我们来把注意力集中在数据包层级应答的实际算法上。该算法如下：在数据包发送端： 在数据包发送缓冲区插入一个为当前发送的数据包序列号的条目，并且带着表示它还没有被应答的字段 从本地接收到的数据包序列缓存和最新接收到的数据包序列号中生成 ack 和ack_bits 填写数据包报头的sequence, ack 和 ack_bits 值 发送数据包并递增发送数据包的序列号 在数据包接收端： 从数据包报头读取 sequence 如果 sequence 比之前的最新收到的数据包序列号要新，就更新最新的接收到的数据包序列号 在接收数据包序列缓冲区中为这个数据包插入一个条目 从数据包报头中的ack和ack_bits解码应答的数据包序列号组合 迭代应答的数据包序列号以及任何还没有被应答的数据包调用 OnPacketAcked( uint16_t sequence ) 在数据包发送缓冲区把这个数据包设置为‘已应答的’。 重要的一点是这个算法是在两端都可以执行的，所以如果你有一个客户端和一个服务端，然后每一方的连接运行着同样的逻辑，维护自己的序列号发送的数据包，跟踪最新从另一方收到的数据包序列#还有从一个序列缓冲区里接收到的数据包中生成sequence, ack 和ack_bits 来发送到另一方。 并且这真的就是和它有关的全部了。现在当一个数据包被另一方接收到时，你有一个回调：OnPacketAcked 。这个可靠性系统的关键就在于你得知道哪个数据包被接收，你可以在你想的媒介之上创建任何 可靠性系统。它不仅限于可靠有序的消息。例如，你可以用它确认哪个不可靠的状态更新已经完成了，用以实现基于每个物体的增量编码。 消息对象 消息是小型的对象（比数据包大小要小，所以很多消息装配在一个典型的数据包中）并且知道如何将它们自己序列化。在我的系统里，它们使用一个统一的序列化函数来执行序列化。 这个序列化的函数是模板化的，所以你只要写它一次它就会处理读、写以及测量 。 是的。测量。我喜欢的一个技巧就是有一个虚拟流类叫做MeasureStream，如果你调用了序列化函数，它不参与任何的序列化，而只是测量可能被写入的比特数。这对于解决哪个消息要装载到你的数据包里，特别是当消息可以有任意复杂的序列化函数的情况时是特别有用的。?12345678910111213141516171819202122232425262728293031323334struct TestMessage : public Message{ uint32_t a,b,c; TestMessage() { a = 0; b = 0; c = 0; } template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) { serialize_bits( stream, a, 32 ); serialize_bits( stream, b, 32 ); serialize_bits( stream, c, 32 ); return true; } virtual SerializeInternal( WriteStream &amp; stream ) { return Serialize( stream ); } virtual SerializeInternal( ReadStream &amp; stream ) { return Serialize( stream ); } virtual SerializeInternal( MeasureStream &amp; stream ) { return Serialize( stream ); }};这里的技巧是桥接统一模板的序列化函数（所以你只需要写一次）与虚拟序列化方法，这通过从虚函数每个流类型中调入它。我通常用一个宏来打包这个引用，但它在上文的代码中这个宏已经被展开，所以你可以看到发生了什么。 现在，假设你有一个基于消息的指针可以让你做到这样并且它只是通过重载来工作：?12Message message = CreateSomeMessage();message-&gt;SerializeInternal( stream );另外一个就是如果你在编译时间知道了消息的完整组合，就可以为每个类型在被调入序列化函数之前实现一个关于消息类型转换为确切消息类型的大的switch语句。我在过去已经在控制台平台实现的这个消息系统这么做了（如PS3 SPUs），但对于现在（2016）的应用程序，虚函数的总开销是忽略不计的。 消息从一个基类派生，这个基类提供一个通用的接口例如序列化、消息的查询类型还有引用计数。引用计数是必要的，因为消息是通过指针传递的并且在应答之前不只是存储在消息发送队列，而且也存储在外发的数据包中，包本身是C++结构体。 这是一个策略，就是避免通过指针传递消息和数据包来复制数据。别的一些场景（理想的情况是在一个单独的线程）它们里面的数据包和消息会序列化到一个缓冲区。最终，当不再有对存在消息发送队列的消息的引用时（消息已经被应答）并且没有数据包包含保留在数据包发送队列里的消息，消息即是被销毁的。 我们也需要一种方式来创建消息。我用一个消息的工厂类来做这件事情，它有一个被复写的虚函数来根据类型创建一个消息。如果这个数据包工厂还知道消息类型的总数量就好了，那样我们就可以在网络上序列化一个消息类型，因为有严格的界限和在有效范围之外的消息类型值的包的恶意丢弃：?1234567891011121314151617181920212223242526272829enum TestMessageTypes{ TEST_MESSAGE_A, TEST_MESSAGE_B, TEST_MESSAGE_C, TEST_MESSAGE_NUM_TYPES}; // message definitions omitted class TestMessageFactory : public MessageFactory{ public: Message Create( int type ) { switch ( type ) { case TEST_MESSAGE_A: return new TestMessageA(); case TEST_MESSAGE_B: return new TestMessageB(); case TEST_MESSAGE_C: return new TestMessageC(); } } virtual int GetNumTypes() const { return TEST_MESSAGE_NUM_TYPES; }};再次重申，这是一个引用并且通常是被包裹在宏里面的，但下面要说明的就是它具体是怎么回事了。 可靠的有序消息算法 现在让我们来着手于如何在应答系统中实现可靠有序消息的细节。 发送可靠有序消息的算法如下： 对于消息发送： 使用测量流测量消息序列化后的大小 插入消息指针和它序列化的位数到一个序列缓冲区，它以消息id为索引。设置消息最后被发送的时间为-1 递增发送的消息的id 对于数据包发送： 从左-&gt;右（递增的消息id顺序）遍历在最早的未应答消息id和最新插入的消息id之间的发送消息序列缓冲区的这组消息。 超级重要的： 不要发送一个接收方不能缓冲的消息id，不然你会破坏消息的应答（由于这个消息不能被缓冲，但包含它的数据包会被应答，发送方就会认为这个消息已经被接收了，就不再重新发送它了）。这意味着你必须不能发送一个消息id等于或比最早的未应答消息的id加上消息接收缓冲区大小要新。 对于那些在最后0.1秒没有被发送的消息并且适合我们留在数据包的有效空间，就把它追加到消息列表去发送。根据迭代顺序得到优先级。 包括在外发数据包中的消息，并且要为每个消息添加一个引用。确保每个数据包的析构函数中减了引用计数。 在数据包n存储消息的数量并且消息的标识数组包含在一个序列缓冲区的数据包中，以外发数据包的序列号为索引。 把数据包添加到数据包发送队列。 对于数据包接收： 遍历包含在数据包中的消息组合并且把它们插入到消息接收队列缓冲区，以它们的消息id为索引。 前面的应答系统自动地应答我们刚刚收到的数据包序列号。 对于数据包应答： 用序列号查找包含在数据包中消息组合的标识部分。 从消息发送队列中移除那些已经存在的消息，并减少它们的引用计数。 通过从发送消息队列缓冲区中之前未应答消息的id的转寄来更新最后一个未应答的消息的id，直到发现一个有效的消息条目，或者你会到达当前发送消息的id。以先到者为准。 对于消息接收： 检查接受消息缓冲区确保当前收到消息的id对应的消息是否存在。 如果消息存在，将它从消息队列缓冲区中移除，递增接收消息的id并给这个消息返回一个指针。 如果否，就是没有有效的消息可接收。返回NULL。 总之，消息要保持被包含在数据包中直到这个数据包包含的消息得到应答。我们在发送者方使用一个数据结构来给消息标识的组合映射数据包序列号以便应答。当消息被应答时，要从发送队列中移除。对于接收方，以乱序到达的消息会被存储在一个序列缓冲区，并以消息id为索引，这个id会让我们以它们被发送的顺序接收它们。 最终的结果 在发送方，这为用户提供了一个像这样的接口：?12345678TestMessage message = (TestMessage) factory.Create( TEST_MESSAGE );if ( message ){ message-&gt;a = 1; message-&gt;b = 2; message-&gt;c = 3; connection.SendMessage( message );}还有在接收方：?1234567891011121314while ( true ){ Message message = connection.ReceiveMessage(); if ( !message ) break; if ( message-&gt;GetType() == TEST_MESSAGE ) { TestMessage testMessage = (TestMessage*) message; // process test message } factory.Release( message );}正如你所看到的，它已经是简单得不能再简单了。 如果这几个接口有引起你的兴趣，请看看我的新开源库 libyojimbo。 我希望你到现在为止对这个系列的文章是享受的请在 patreon上支持我的写作，并且我将更快写新的文章，再者你会在加州大学伯克利分校软件的开源许可证下获得这篇文章的示例源代码。谢谢你的支持！ 即将到来：客户端与服务器的连接 在“创建一个游戏网络协议”的下一篇文章会展示你如何在UDP之上创建你自己的客户端/服务器连接层，它会实现挑战/响应，会在服务器上分配客户端插槽，当服务器爆满或检测超时就拒绝客户端的连接。 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权；]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSL/TLS详解]]></title>
    <url>%2F2017%2F03%2F12%2Fssl_tls_illustrated%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[互联网的通信安全，建立在 SSL/TLS 协议之上。 本文简要介绍 SSL/TLS 协议的运行机制。文章的重点是设计思想和运行过程，不涉及具体的实现细节。如果想了解这方面的内容，请参阅 RFC 文档。 . . . 作用不使用 SSL/TLS 的 HTTP 通信，就是不加密的通信。所有信息明文传播，带来了三大风险。 （1） 窃听风险（eavesdropping）：第三方可以获知通信内容。 （2） 篡改风险（tampering）：第三方可以修改通信内容。 （3） 冒充风险（pretending）：第三方可以冒充他人身份参与通信。 SSL/TLS 协议是为了解决这三大风险而设计的，希望达到： （1） 所有信息都是加密传播，第三方无法窃听。 （2） 具有校验机制，一旦被篡改，通信双方会立刻发现。 （3） 配备身份证书，防止身份被冒充。 互联网是开放环境，通信双方都是未知身份，这为协议的设计带来了很大的难度。而且，协议还必须能够经受所有匪夷所思的攻击，这使得 SSL/TLS 协议变得异常复杂。 历史互联网加密通信协议的历史，几乎与互联网一样长。 1994 年，NetScape 公司设计了 SSL 协议（Secure Sockets Layer）的 1.0 版，但是未发布。 1995 年，NetScape 公司发布 SSL 2.0 版，很快发现有严重漏洞。 1996 年，SSL 3.0 版问世，得到大规模应用。 1999 年，互联网标准化组织 ISOC 接替 NetScape 公司，发布了 SSL 的升级版 TLS 1.0 版。 2006 年和 2008 年，TLS 进行了两次升级，分别为 TLS 1.1 版和 TLS 1.2 版。最新的变动是 2011 年 TLS 1.2 的修订版。 目前，应用最广泛的是 TLS 1.0，接下来是 SSL 3.0。但是，主流浏览器都已经实现了 TLS 1.2 的支持。 TLS 1.0 通常被标示为 SSL 3.1，TLS 1.1 为 SSL 3.2，TLS 1.2 为 SSL 3.3。 基本的运行过程SSL/TLS 协议的基本思路是采用公钥加密法，也就是说，客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。 但是，这里有两个问题。 （1）如何保证公钥不被篡改？ 解决方法：将公钥放在数字证书中。只要证书是可信的，公钥就是可信的。 （2）公钥加密计算量太大，如何减少耗用的时间？ 解决方法：每一次对话（session），客户端和服务器端都生成一个 “对话密钥”（session key），用它来加密信息。由于 “对话密钥” 是对称加密，所以运算速度非常快，而服务器公钥只用于加密 “对话密钥” 本身，这样就减少了加密运算的消耗时间。 因此，SSL/TLS 协议的基本过程是这样的： （1） 客户端向服务器端索要并验证公钥。 （2） 双方协商生成 “对话密钥”。 （3） 双方采用 “对话密钥” 进行加密通信。 上面过程的前两步，又称为 “握手阶段”（handshake）。 握手阶段的详细过程 “握手阶段” 涉及四次通信，我们一个个来看。需要注意的是，”握手阶段” 的所有通信都是明文的。 客户端发出请求ClientHello首先，客户端（通常是浏览器）先向服务器发出加密通信的请求，这被叫做 ClientHello 请求。 在这一步，客户端主要向服务器提供以下信息。 （1） 支持的协议版本，比如 TLS 1.0 版。 （2） 一个客户端生成的随机数，稍后用于生成 “对话密钥”。 （3） 支持的加密方法，比如 RSA 公钥加密。 （4） 支持的压缩方法。 这里需要注意的是，客户端发送的信息之中不包括服务器的域名。也就是说，理论上服务器只能包含一个网站，否则会分不清应该向客户端提供哪一个网站的数字证书。这就是为什么通常一台服务器只能有一张数字证书的原因。 对于虚拟主机的用户来说，这当然很不方便。2006 年，TLS 协议加入了一个 Server Name Indication 扩展，允许客户端向服务器提供它所请求的域名。 服务器回应（SeverHello）服务器收到客户端请求后，向客户端发出回应，这叫做 SeverHello。服务器的回应包含以下内容。 （1） 确认使用的加密通信协议版本，比如 TLS 1.0 版本。如果浏览器与服务器支持的版本不一致，服务器关闭加密通信。 （2） 一个服务器生成的随机数，稍后用于生成 “对话密钥”。 （3） 确认使用的加密方法，比如 RSA 公钥加密。 （4） 服务器证书。 除了上面这些信息，如果服务器需要确认客户端的身份，就会再包含一项请求，要求客户端提供 “客户端证书”。比如，金融机构往往只允许认证客户连入自己的网络，就会向正式客户提供 USB 密钥，里面就包含了一张客户端证书。 客户端回应客户端收到服务器回应以后，首先验证服务器证书。如果证书不是可信机构颁布、或者证书中的域名与实际域名不一致、或者证书已经过期，就会向访问者显示一个警告，由其选择是否还要继续通信。 如果证书没有问题，客户端就会从证书中取出服务器的公钥。然后，向服务器发送下面三项信息。 （1） 一个随机数。该随机数用服务器公钥加密，防止被窃听。 （2） 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。 （3） 客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时也是前面发送的所有内容的 hash 值，用来供服务器校验。 上面第一项的随机数，是整个握手阶段出现的第三个随机数，又称 “pre-master key”。有了它以后，客户端和服务器就同时有了三个随机数，接着双方就用事先商定的加密方法，各自生成本次会话所用的同一把 “会话密钥”。 至于为什么一定要用三个随机数，来生成 “会话密钥”，dog250 解释得很好： “ 不管是客户端还是服务器，都需要随机数，这样生成的密钥才不会每次都一样。由于 SSL 协议中证书是静态的，因此十分有必要引入一种随机因素来保证协商出来的密钥的随机性。 对于 RSA 密钥交换算法来说，pre-master-key 本身就是一个随机数，再加上 hello 消息中的随机，三个随机数通过一个密钥导出器最终导出一个对称密钥。 pre master 的存在在于 SSL 协议不信任每个主机都能产生完全随机的随机数，如果随机数不随机，那么 pre master secret 就有可能被猜出来，那么仅适用 pre master secret 作为密钥就不合适了，因此必须引入新的随机因素，那么客户端和服务器加上 pre master secret 三个随机数一同生成的密钥就不容易被猜出了，一个伪随机可能完全不随机，可是是三个伪随机就十分接近随机了，每增加一个自由度，随机性增加的可不是一。” 此外，如果前一步，服务器要求客户端证书，客户端会在这一步发送证书及相关信息。 服务器的最后回应服务器收到客户端的第三个随机数 pre-master key 之后，计算生成本次会话所用的 “会话密钥”。然后，向客户端最后发送下面信息。 （1）编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。 （2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时也是前面发送的所有内容的 hash 值，用来供客户端校验。 至此，整个握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用 “会话密钥” 加密内容。 参考 MicroSoft TechNet, SSL/TLS in Detail.aspx) Jeff Moser, The First Few Milliseconds of an HTTPS Connection Wikipedia, Transport Layer Security StackExchange, How does SSL work? （完） 转自: https://www.ruanyifeng.com/blog/2014/02/ssl_tls.html]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>noodle</tag>
        <tag>HTTPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KBE的UE4的demo大体解读]]></title>
    <url>%2F2017%2F03%2F11%2Fkbe_ue4_demo%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[写到一半发现论坛的热门帖子里官方写了个u3d的demo源码解析, 内容几乎重复, u3d跟ue4的demo框架流程几乎都是差不多的, 直接给出官方帖子的链接好了, 尴尬:http://bbs.kbengine.org/forum.php?mod=viewthread&amp;tid=166]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>KBE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议四之发送大块数据]]></title>
    <url>%2F2017%2F02%2F28%2Fsending_large_blocks_of_data%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[本篇自我总结有了本系列上篇文章中的分包和重组系统为何还要这个发送大块数据系统?是否是多余的?是雷同的吗?请看总结概要理清思路, 再细看文章. 为什么需要做这个发送大块数据系统第一眼看上去，这种替代性的技术似乎非常类似于数据包的分包和重组，但是它的实现是完全不同的。这种实现上的差异的目的是为了解决数据包分包和重组的一个关键弱点 : 一个片段的丢失就会导致整个数据包都要被丢弃掉然后重新分包重发。 你可能需要这样做的一些常见的例子包括：客户端在首次加入的时候，服务器需要下发一个大的数据块给客户端(可能是世界的初始状态)、一开始用来做增量编码的基线或者是在一个多人在线网络游戏里面客户端在加载界面所等待的大块数据。 在这些情况下非常重要的是不仅要优雅地处理数据包的丢失，还要尽可能的利用可用的带宽并尽可能快的发送大块数据。 这个发送大块数据系统大致可以理解为是一个在原来分包和重组系统的基础上增加了分包确认功能, 也就是说增加了可靠性的部分. 本篇基本术语In this new system blocks of data are called chunks. Chunks are split up into slices. This name change keeps the chunk system terminology (chunks/slices) distinct from packet fragmentation and reassembly (packets/fragments). 块 : 在这个新系统中，大块的数据被称为”块”(chunks) 片段 : 而块被分成的分包被称为”片段”(slices) 数据包的结构设计这个系统在网络上发送的数据包类型一共有两种类型： Slice packet片段数据包 : 这包括了一个块的片段，最多大小为1k。 1234567891011121314151617181920212223242526272829const int SliceSize = 1024;const int MaxSlicesPerChunk = 256;const int MaxChunkSize = SliceSize MaxSlicesPerChunk; struct SlicePacket : public protocol2::Packet&#123; uint16_t chunkId; int sliceId; int numSlices; int sliceBytes; uint8_t data[SliceSize]; template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, chunkId, 16 ); serialize_int( stream, sliceId, 0, MaxSlicesPerChunk - 1 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); if ( sliceId == numSlices - 1 ) &#123; serialize_int( stream, sliceBytes, 1, SliceSize ); &#125; else if ( Stream::IsReading ) &#123; sliceBytes = SliceSize; &#125; serialize_bytes( stream, data, sliceBytes ); return true; &#125;&#125;; Ack packet确认数据包 : 一个位域bitfield指示哪些片段已经收到, we just send the entire state of all acked slices in each ack packet. When the ack packet is received (including the slice that was just received). 1234567891011121314struct AckPacket : public protocol2::Packet &#123; uint16_t chunkId; int numSlices; bool acked[MaxSlicesPerChunk]; bool Serialize( Stream &amp; stream ) &#123; serialize_bits( stream, chunkId, 16 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); for ( int i = 0; i &lt; numSlices; ++i ) serialize_bool( stream, acked[i] ); return true; &#125; &#125;; &#125;&#125;; 发送方的实现与之前文章介绍的数据包的分包和重组系统不同，块系统在同一时间只能由一个块正在传输。发送方的策略是： 持续的发送片段数据包，直到所有的片段数据包都被确认。 不再对已经确认过的片段数据包进行发送。 对于发送方而言有一点比较微妙，实现一个片段数据包重新发送的最小延迟是一个很棒的主意，如果不这么做的话，就可能会出现这种一样情况，对于很小的块数据或者一个块的最后几个片段数据包，很容易不停的发送它们把整个网络都塞满。正是因为这一原因，我们使用了一个数组来记录每个片段数据包的上一次发送时间。重新发送延迟的一个选择是使用一个估计的网络往返时延，或者只有在超过上一次发送时间网络往返时延*1.25还没有收到确认数据包的情况才会重新发送。或者，你可以说“这根本就无所谓”，只要超过上一次发送时间100毫秒了就重新发送。我只是列举适合我自己的方案！ 我们使用以下的数据结构来描述发送方：123456789101112class ChunkSender&#123; bool sending; uint16_t chunkId; int chunkSize; int numSlices; int numAckedSlices; int currentSliceId; bool acked[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; double timeLastSent[MaxSlicesPerChunk];&#125;; 接收方的实现思路首先，接收方的设置会从块0开始。当一个片段数据包从网络上传递过来，并且能够匹配这个块id的话，“receiving”状态会从false翻转为true，第一个片段数据包的数据会插入” chunkData“变量的合适位置，片段数据包的数量会根据第一个片段数据包里面的数据进行正确的设置，已经接收到的片段数据包的数量会加一，也就是从0到1，针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 随着这个块数据的其他片段数据包的到来，会对每一个片段数据包进行检测，判断它们的id是否与当前块的id相同，如果不相同的话就会被丢弃。如果这个片段数据包已经收到过的话，那么这个包也会被丢弃。否则，这个片段数据包的数据会插入” chunkData“变量的合适位置、已经接收到的片段数据包的数量会加一、针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 这一过程会持续进行，直到接收到所有的片段数据包。一旦接收到所有的片段数据包（也就是已经接收到的片段数据包的数量等于片段数据包的数量的时候），接收方会把“receiving “状态改为false，而把”readyToRead“状态改为true。当”readyToRead”状态为true的时候，所有收到的片段数据包都会被丢弃。在这一点上，这个处理过程通常非常的短，会在收到片段数据包的同一帧进行处理，调用者会检查”我有一块数据要读取么？“并处理块数据。然后会重置数据块接收器的所有数据为默认值，除了块数据的id从0增加到1，这样我们就准备好接收下一个块了。1234567891011class ChunkReceiver&#123; bool receiving; bool readyToRead; uint16_t chunkId; int chunkSize; int numSlices; int numReceivedSlices; bool received[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize];&#125;; 防DDos如果你对每个收到的片段数据包都会回复一个确认数据包的话，那么发送方能够构造一个很小的片段数据包发送给你，而你会回复一个比发送给你的片段数据包还大的确认数据包，这样你的服务器就变成了一个可以被人利用来进行DDos放大攻击的工具。 永远不要设计一个包含对接收到的数据包进行一对一的映射响应的协议。让我们举个简单例子来说明一下这个问题。如果有人给你发送1000个片段数据包，永远不要给他回复1000个确认数据包。相反只发一个确认数据包，而且最多每50毫秒或者100毫秒才发送一个确认数据包。如果你是这样设计的话，那么DDos攻击完全不可能的。 原文原文出处 原文标题 : Sending Large Blocks of Data (How to send blocks quickly and reliably over UDP) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.In the previous article we implemented packet fragmentation and reassembly so we can send packets larger than MTU.This approach works great when the data block you&rsquo;re sending is time critical and can be dropped, but in other cases you need to send large blocks of quickly and reliably over packet loss, and you need the data to get through.In this situation, a different technique gives much better results.BackgroundIt&rsquo;s common for servers to send large block of data to the client on connect, for example, the initial state of the game world for late join.Let&rsquo;s assume this data is 256k in size and the client needs to receive it before they can join the game. The client is stuck behind a load screen waiting for the data, so obviously we want it to be transmitted as quickly as possible.If we send the data with the technique from the previous article, we get packet loss amplification because a single dropped fragment results in the whole packet being lost. The effect of this is actually quite severe. Our example block split into 256 fragments and sent over 1% packet loss now has a whopping 92.4% chance of being dropped!Since we just need the data to get across, we have no choice but to keep sending it until it gets through. On average, we have to send the block 10 times before it&rsquo;s received. You may laugh but this actually happened on a AAA game I worked on!To fix this, I implemented a new system for sending large blocks, one that handles packet loss by resends fragments until they are acked. Then I took the problematic large blocks and piped them through this system, fixing a bunch of players stalling out on connect, while continuing to send time critical data (snapshots) via packet fragmentation and reassembly.Chunks and SlicesIn this new system blocks of data are called chunks. Chunks are split up into slices. This name change keeps the chunk system terminology (chunks/slices) distinct from packet fragmentation and reassembly (packets/fragments).The basic idea is that slices are sent over the network repeatedly until they all get through. Since we are implementing this over UDP, simple in concept becomes a little more complicated in implementation because have to build in our own basic reliability system so the sender knows which slices have been received.This reliability gets quite tricky if we have a bunch of different chunks in flight, so we&rsquo;re going to make a simplifying assumption up front: we&rsquo;re only going to send one chunk over the network at a time. This doesn&rsquo;t mean the sender can&rsquo;t have a local send queue for chunks, just that in terms of network traffic there&rsquo;s only ever one chunk in flight at any time.This makes intuitive sense because the whole point of the chunk system is to send chunks reliably and in-order. If you are for some reason sending chunk 0 and chunk 1 at the same time, what&rsquo;s the point? You can&rsquo;t process chunk 1 until chunk 0 comes through, because otherwise it wouldn&rsquo;t be reliable-ordered.That said, if you dig a bit deeper you&rsquo;ll see that sending one chunk at a time does introduce a small trade-off, and that is that it adds a delay of RTT between chunk n being received and the send starting for chunk n+1 from the receiver&rsquo;s point of view.This trade-off is totally acceptable for the occasional sending of large chunks like data sent once on client connect, but it&rsquo;s definitely not acceptable for data sent 10 or 20 times per-second like snapshots. So remember, this system is useful for large, infrequently sent blocks of data, not for time critical data.Packet StructureThere are two sides to the chunk system, the sender and the receiver.The sender is the side that queues up the chunk and sends slices over the network. The receiver is what reads those slice packets and reassembles the chunk on the other side. The receiver is also responsible for communicating back to the sender which slices have been received via acks.The netcode I work on is usually client/server, and in this case I usually want to be able to send blocks of data from the server to the client and from the client to the server. In that case, there are two senders and two receivers, a sender on the client corresponding to a receiver on the server and vice-versa.Think of the sender and receiver as end points for this chunk transmission protocol that define the direction of flow. If you want to send chunks in a different direction, or even extend the chunk sender to support peer-to-peer, just add sender and receiver end points for each direction you need to send chunks.Traffic over the network for this system is sent via two packet types:Slice packet - contains a slice of a chunk up to 1k in size.Ack packet - a bitfield indicating which slices have been received so far.The slice packet is sent from the sender to the receiver. It is the payload packet that gets the chunk data across the network and is designed so each packet fits neatly under a conservative MTU of 1200 bytes. Each slice is a maximum of 1k and there is a maximum of 256 slices per-chunk, therefore the largest data you can send over the network with this system is 256k.const int SliceSize = 1024;const int MaxSlicesPerChunk = 256;const int MaxChunkSize = SliceSize MaxSlicesPerChunk;struct SlicePacket : public protocol2::Packet{ uint16_t chunkId; int sliceId; int numSlices; int sliceBytes; uint8_t data[SliceSize]; template &amp;lt;typename Stream&amp;gt; bool Serialize( Stream &amp;amp; stream ) { serialize_bits( stream, chunkId, 16 ); serialize_int( stream, sliceId, 0, MaxSlicesPerChunk - 1 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); if ( sliceId == numSlices - 1 ) { serialize_int( stream, sliceBytes, 1, SliceSize ); } else if ( Stream::IsReading ) { sliceBytes = SliceSize; } serialize_bytes( stream, data, sliceBytes ); return true; }};There are two points I&rsquo;d like to make about the slice packet. The first is that even though there is only ever one chunk in flight over the network, it&rsquo;s still necessary to include the chunk id (0,1,2,3, etc&hellip;) because packets sent over UDP can be received out of order.Second point. Due to the way chunks are sliced up we know that all slices except the last one must be SliceSize (1024 bytes). We take advantage of this to save a small bit of bandwidth sending the slice size only in the last slice, but there is a trade-off: the receiver doesn&rsquo;t know the exact size of a chunk until it receives the last slice.The other packet sent by this system is the ack packet. This packet is sent in the opposite direction, from the receiver back to the sender. This is the reliability part of the chunk network protocol. Its purpose is to lets the sender know which slices have been received.struct AckPacket : public protocol2::Packet{ uint16_t chunkId; int numSlices; bool acked[MaxSlicesPerChunk]; bool Serialize( Stream &amp;amp; stream ) { serialize_bits( stream, chunkId, 16 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); for ( int i = 0; i &amp;lt; numSlices; ++i ) { serialize_bool( stream, acked[i] ); return true; } }; } }};Acks are short for &lsquo;acknowledgments&rsquo;. So an ack for slice 100 means the receiver is acknowledging that it has received slice 100. This is critical information for the sender because not only does it let the sender determine when all slices have been received so it knows when to stop, it also allows the sender to use bandwidth more efficiently by only sending slices that haven&rsquo;t been acked.Looking a bit deeper into the ack packet, at first glance it seems a bit redundant. Why are we sending acks for all slices in every packet? Well, ack packets are sent over UDP so there is no guarantee that all ack packets are going to get through. You certainly don&rsquo;t want a desync between the sender and the receiver regarding which slices are acked.So we need some reliability for acks, but we don&rsquo;t want to implement an ack system for acks because that would be a huge pain in the ass. Since the worst case ack bitfield is just 256 bits or 32 bytes, we just send the entire state of all acked slices in each ack packet. When the ack packet is received, we consider a slice to be acked the instant an ack packet comes in with that slice marked as acked and locally that slice is not seen as acked yet.This last step, biasing in the direction of non-acked to ack, like a fuse getting blown, means we can handle out of order delivery of ack packets.Sender ImplementationLet&rsquo;s get started with the implementation of the sender.The strategy for the sender is:Keep sending slices until all slices are ackedDon&rsquo;t resend slices that have already been ackedWe use the following data structure for the sender:class ChunkSender{ bool sending; uint16_t chunkId; int chunkSize; int numSlices; int numAckedSlices; int currentSliceId; bool acked[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; double timeLastSent[MaxSlicesPerChunk];};As mentioned before, only one chunk is sent at a time, so there is a &lsquo;sending&rsquo; state which is true if we are currently sending a chunk, false if we are in an idle state ready for the user to send a chunk. In this implementation, you can&rsquo;t send another chunk while the current chunk is still being sent over the network. If you don&rsquo;t like this, stick a queue in front of the sender.Next, we have the id of the chunk we are currently sending, or, if we are not sending a chunk, the id of the next chunk to be sent, followed by the size of the chunk and the number of slices it has been split into. We also track, per-slice, whether that slice has been acked, which lets us count the number of slices that have been acked so far while ignoring redundant acks. A chunk is considered fully received from the sender&rsquo;s point of view when numAckedSlices == numSlices.We also keep track of the current slice id for the algorithm that determines which slices to send, which works like this. At the start of a chunk send, start at slice id 0 and work from left to right and wrap back around to 0 again when you go past the last slice. Eventually, you stop iterating across because you&rsquo;ve run out of bandwidth to send slices. At this point, remember our current slice index via current slice id so you can pick up from where you left off next time. This last part is important because it distributes sends across all slices, not just the first few.Now let&rsquo;s discuss bandwidth limiting. Obviously you don&rsquo;t just blast slices out continuously as you&rsquo;d flood the connection in no time, so how do we limit the sender bandwidth? My implementation works something like this: as you walk across slices and consider each slice you want to send, estimate roughly how many bytes the slice packet will take eg: roughly slice bytes + some overhead for your protocol and UDP/IP header. Then compare the amount of bytes required vs. the available bytes you have to send in your bandwidth budget. If you don&rsquo;t have enough bytes accumulated, stop. Otherwise, subtract the bytes required to send the slice and repeat the process for the next slice.Where does the available bytes in the send budget come from? Each frame before you update the chunk sender, take your target bandwidth (eg. 256kbps), convert it to bytes per-second, and add it multiplied by delta time (dt) to an accumulator.A conservative send rate of 256kbps means you can send 32000 bytes per-second, so add 32000 dt to the accumulator. A middle ground of 512kbit/sec is 64000 bytes per-second. A more aggressive 1mbit is 125000 bytes per-second. This way each update you accumulate a number of bytes you are allowed to send, and when you&rsquo;ve sent all the slices you can given that budget, any bytes left over stick around for the next time you try to send a slice.One subtle point with the chunk sender and is that it&rsquo;s a good idea to implement some minimum resend delay per-slice, otherwise you get situations where for small chunks, or the last few slices of a chunk that the same few slices get spammed over the network.For this reason we maintain an array of last send time per-slice. One option for this resend delay is to maintain an estimate of RTT and to only resend a slice if it hasn&rsquo;t been acked within RTT * 1.25 of its last send time. Or, you could just resend the slice it if it hasn&rsquo;t been sent in the last 100ms. Works for me!Kicking it up a notchDo the math you&rsquo;ll notice it still takes a long time for a 256k chunk to get across:1mbps = 2 seconds512kbps = 4 seconds256kbps = 8 seconds :(Which kinda sucks. The whole point here is quickly and reliably. Emphasis on quickly. Wouldn&rsquo;t it be nice to be able to get the chunk across faster? The typical use case of the chunk system supports this. For example, a large block of data sent down to the client immediately on connect or a block of data that has to get through before the client exits a load screen and starts to play. You want this to be over as quickly as possible and in both cases the user really doesn&rsquo;t have anything better to do with their bandwidth, so why not use as much of it as possible?One thing I&rsquo;ve tried in the past with excellent results is an initial burst. Assuming your chunk size isn&rsquo;t so large, and your chunk sends are infrequent, I can see no reason why you can&rsquo;t just fire across the entire chunk, all slices of it, in separate packets in one glorious burst of bandwidth, wait 100ms, and then resume the regular bandwidth limited slice sending strategy.Why does this work? In the case where the user has a good internet connection (some multiple of 10mbps or greater&hellip;), the slices get through very quickly indeed. In the situation where the connection is not so great, the burst gets buffered up and most slices will be delivered as quickly as possible limited only by the amount bandwidth available. After this point switching to the regular strategy at a lower rate picks up any slices that didn&rsquo;t get through the first time.This seems a bit risky so let me explain. In the case where the user can&rsquo;t quite support this bandwidth what you&rsquo;re relying on here is that routers on the Internet strongly prefer to buffer packets rather than discard them at almost any cost. It&rsquo;s a TCP thing. Normally, I hate this because it induces latency in packet delivery and messes up your game packets which you want delivered as quickly as possible, but in this case it&rsquo;s good behavior because the player really has nothing else to do but wait for your chunk to get through.Just don&rsquo;t go too overboard with the spam or the congestion will persist after your chunk send completes and it will affect your game for the first few seconds. Also, make sure you increase the size of your OS socket buffers on both ends so they are larger than your maximum chunk size (I recommend at least double), otherwise you&rsquo;ll be dropping slices packets before they even hit the wire.Finally, I want to be a responsible network citizen here so although I recommend sending all slices once in an initial burst, it&rsquo;s important for me to mention that I think this really is only appropriate, and only really borderline appropriate behavior for small chunks in the few 100s of k range in 2016, and only when your game isn&rsquo;t sending anything else that is time-critical.Please don&rsquo;t use this burst strategy if your chunk is really large, eg: megabytes of data, because that&rsquo;s way too big to be relying on the kindness of strangers, AKA. the buffers in the routers between you and your packet&rsquo;s destination. For this it&rsquo;s necessary to implement something much smarter. Something adaptive that tries to send data as quickly as it can, but backs off when it detects too much latency and/or packet loss as a result of flooding the connection. Such a system is outside of the scope of this article.Receiver ImplementationNow that we have the sender all sorted out let&rsquo;s move on to the reciever. As mentioned previously, unlike the packet fragmentation and reassembly system from the previous article, the chunk system only ever has one chunk in flight.This makes the reciever side of the chunk system much simpler:class ChunkReceiver{ bool receiving; bool readyToRead; uint16_t chunkId; int chunkSize; int numSlices; int numReceivedSlices; bool received[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize];};We have a state whether we are currently &lsquo;receiving&rsquo; a chunk over the network, plus a &rsquo;readyToRead&rsquo; state which indicates that a chunk has received all slices and is ready to be popped off by the user. This is effectively a minimal receive queue of length 1. If you don&rsquo;t like this, of course you are free to add a queue.In this data structure we also keep track of chunk size (although it is not known with complete accuracy until the last slice arrives), num slices and num received slices, as well as a received flag per-slice. This per-slice received flag lets us discard packets containing slices we have already received, and count the number of slices received so far (since we may receive the slice multiple times, we only increase this count the first time we receive a particular slice). It&rsquo;s also used when generating ack packets. The chunk receive is completed from the receiver&rsquo;s point of view when numReceivedSlices == numSlices.So what does it look like end-to-end receiving a chunk?First, the receiver sets up set to start at chunk 0. When the a slice packet comes in over the network matching the chunk id 0, &lsquo;receiving&rsquo; flips from false to true, data for that first slice is inserted into &lsquo;chunkData&rsquo; at the correct position, numSlices is set to the value in that packet, numReceivedSlices is incremented from 0 -&gt; 1, and the received flag in the array entry corresponding to that slice is set to true.As the remaining slice packets for the chunk come in, each of them are checked that they match the current chunk id and numSlices that are being received and are ignored if they don&rsquo;t match. Packets are also ignored if they contain a slice that has already been received. Otherwise, the slice data is copied into the correct place in the chunkData array, numReceivedSlices is incremented and received flag for that slice is set to true.This process continues until all slices of the chunk are received, at which point the receiver sets receiving to &lsquo;false&rsquo; and &lsquo;readyToRead&rsquo; to true. While &lsquo;readyToRead&rsquo; is true, incoming slice packets are discarded. At this point, the chunk receive packet processing is performed, typically on the same frame. The caller checks &lsquo;do I have a chunk to read?&rsquo; and processes the chunk data. All chunk receive data is cleared back to defaults, except chunk id which is incremented from 0 -&gt; 1, and we are ready to receive the next chunk.ConclusionThe chunk system is simple in concept, but the implementation is certainly not. I encourage you to take a close look at the source code for this article for further details. 译文 译文出处 翻译：张华栋 (wcby) 审校：王磊(未来的未来) 大家好，我是格伦·菲德勒。欢迎大家阅读系列教程《构建游戏网络协议》的第四篇文章。在之前的文章中，我们讨论了如何在游戏协议这一层实现对数据包的分包和重组。现在在这篇文章里面，我们将继续通过探索在UDP协议上发送大块数据的替代方案来继续我们构建一个专业级别的游戏网络协议的征程。 第一眼看上去，这种替代性的技术似乎非常类似于数据包的分包和重组，但是它的实现是完全不同的。这种实现上的差异的目的是为了解决数据包分包和重组的一个关键弱点-一个片段的丢失就会导致整个数据包都要被丢弃掉。这种行为是非常不好的，因为它会随着分包数量的增加而放大数据包丢失的概率。当你遇到大块数据包的时候，这种放大是如此的明显，加入256 k大小的分包丢失率是1%的话，那么原始数据包就有92.4%的概率被丢弃。平均来说，你需要发送原始数据包10次，它才能顺利的到达网络的另外一端！ 如果你需要在一个可能会有数据包丢失的网络上比如说互联网，快速和可靠地发送大量的数据，很明显，这样的方法是完全不可接受的。你可能需要这样做的一些常见的例子包括：客户端在首次加入的时候，服务器需要下发一个大的数据块给客户端(可能是世界的初始状态)、一开始用来做增量编码的基线或者是在一个多人在线网络游戏里面客户端在加载界面所等待的大块数据。 在这些情况下非常重要的是不仅要优雅地处理数据包的丢失，还要尽可能的利用可用的带宽并尽可能快的发送大块数据。这正是我要在这篇文章里面告诉你该如何做的内容。块和片段 让我们开始使用基本术语。在这个新系统中，大块的数据被称为”块“，而它们被分成的分包被称为”片段”。 这个名字上的改变使的块系统的术语(块和片段)不同于数据包分包和重组的术语(数据包和分包)。这是我认为很重要的一个事情，因为这些系统是在解决不同的问题，没有理由你不能在相同的网络协议中同时这两个系统。事实上，我经常把这两个结合起来，在时间比较关键的增量数据包里面使用数据包的分包和重组，当客户端加入游戏的时候，使用块系统来下发整个游戏世界的初始状态下(非常大的数据包)。 块系统的基本思想，真是一点都不复杂，是把块分成很多片段，然后通过网络多次发送片段，直到他们都顺利的到达网络的另外一端。当然，因为我们正在UDP协议上实现这个功能，同时还有可能数据包会丢失、数据包乱序到达以及数据包重复到达的情况，简单的概念在实现中也会变得非常复杂，因为我们必须在UDP协议上建立我们自己的具有基本可靠性的系统，这样发送方才能知道这个片段已经被网络的另外一端成功收到。 如果我们有一组不同的块正在传输过程中(就像我们在数据包的分包和重组中所做的那样)，那么可靠性的问题就会变得非常棘手，所以我们要做一个简化的假设。我们一次只会通过网络发送一个块的数据。这并不意味着发送者不能在本地有一个块的发送队列，这只是意味着在实际的网络传输中只有一个块的数据会正在传递。 这么做之所以有意义，是因为有了这一点假设以后就能保证块系统能够可靠有序的发送块。如果你因为某些原因在同一时间发送块0和块1，这会发生什么？你不能在块0到来之前处理块1，否则这个传输过程就不是有序可靠了。也就是说，如果你挖得深一些的话，你会发现一次只能发送一个块确实引入了一个小的权衡，它给正在接收的块Ｎ增加了一个网络往返延迟，以及从接收方的角度看块Ｎ＋１的发送开始时间也被延迟了一个网络往返延迟。 这个代价是完全可以接受的，因为发送大块数据是一个非常偶然的事情（举些简单的例子来说，当客户端连接上来的时候会发送大块数据，当新的关卡需要进行加载的时候才会发送大块数据。。。），但是如果1秒钟内10次或者20次发送块数据的话这就是绝对不能被接受的了。所以我希望你能看到这个系统是专为什么目的设计的以及不是为什么目的设计的。 数据包的结构 在块系统中有两方会参与，分别是发送方和接收方。 发送方是负责将块压入队列并通过网络发送片段。接收方是负责在网络的另外一端读取这些片段并进行重组。接收方还负责通过发送“确认”数据包给发送方来与发送方交流表明这个片段已经收到。 我工作过的网络模式通常是客户端与服务端之间的通信，在这种情况下，我通常希望能够从服务器往客户端发送大块数据，以及从客户端到服务器发送大块数据。所以在这种情况下，有两个发送方和两个接收方，一个发送方在客户端对应着在服务器那边有一个接收方，反过来也是如此。可以把发送方和接收方认为是块传输协议的终点，这样也就定义了网络流的方向。如果你想在不同的方向发送块，甚至是扩展块的发送方来支持点对点的发送，只需要在你需要发送块的每个方向添加一个发送方和一个接收方作为终点。 这个系统在网络上发送的数据包类型一共有两种类型： 1）片段数据包-这包括了一个块的片段，最多大小为1k。 2）确认数据包-一个位域指示哪些片段已经收到。 片段数据包是从发送方发送到接收器的。这是通过网络对块数据进行传递的有效载荷数据包，在设计的时候每个片段数据包的大小都贴近一个保守的最大传输单元的大小，也就是 1200字节。每个片段数据包最大是1 k，每个块最多有256个片段数据包，所以通过这个系统可以通过网络发送的最大的数据是256k（如果你愿意的话，你可以增加这个片段的最大数目）。我建议保持片段的大小为1k，这主要是基于最大传输单元方面的考虑。 ?1234567891011121314151617181920212223242526272829const int SliceSize = 1024;const int MaxSlicesPerChunk = 256;const int MaxChunkSize = SliceSize MaxSlicesPerChunk; struct SlicePacket : public protocol2::Packet{ uint16_t chunkId; int sliceId; int numSlices; int sliceBytes; uint8_t data[SliceSize]; template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) { serialize_bits( stream, chunkId, 16 ); serialize_int( stream, sliceId, 0, MaxSlicesPerChunk - 1 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); if ( sliceId == numSlices - 1 ) { serialize_int( stream, sliceBytes, 1, SliceSize ); } else if ( Stream::IsReading ) { sliceBytes = SliceSize; } serialize_bytes( stream, data, sliceBytes ); return true; }};在这里我想对片段数据包进行两点说明。第一点是即使只有一个块在网络上进行传输，仍然是有必要在数据包里面包含一个块的id(比如说，0、1、2、3、等等等)，这是,因为通过UDP协议发送的数据包可以是乱序到达的。通过这种方式的话，如果一个片段数据包到达的时候对应着一个已经接受过的块，举个简单的例子来说明，你正在接受块2的数据，但是块1的一个片段数据包现在到达了，你可以直接拒绝这个数据包，而不是接受它的数据包并把它的数据插入到块2从而把块2的数据给弄混了。 第二点。由于我们知道块分成片段的方法会把所有的片段除了最后一个以外都弄成必须SliceSize的大小(也就是1024字节)。我们利用这一点来节省一点带宽，我们只在最后一个片段里面发送片段的大小，但这是一种权衡:接收方不知道块的确切大小到底是多少字节，直到它接收到最后一个片段才能知道。 可以让这个系统继续往后发送新的数据包的机制是确认数据包。这个数据包是沿着另外一个方向进行发送的，也就是从接收方发回给发送方，这也是块网络协议中负责可靠性的部分。它存在的目的是让发送方知道这个片段已经被发送方收到。?1234567891011121314struct AckPacket : public protocol2::Packet { uint16_t chunkId; int numSlices; bool acked[MaxSlicesPerChunk]; bool Serialize( Stream &amp; stream ) { serialize_bits( stream, chunkId, 16 ); serialize_int( stream, numSlices, 1, MaxSlicesPerChunk ); for ( int i = 0; i &lt; numSlices; ++i ) serialize_bool( stream, acked[i] ); return true; } }; }};ack是“确认”的缩写。所以一个对片段100的确认数据包意味着接收方确认它已经接收到了片段100。这对于发送方来说是一条关键信息，因为它不仅让发送方知道什么时候所有的片段都已经被成功接收，这样发送方就可以停止发送了，它还允许发送方只重发那些还没有被确认的片段，这样就能让发送方更有效率的利用带宽。 让我们对确认数据包再深入一点思考，似乎在一开始看上去对于每个数据包的所有分片都发送确认包似乎有点多余。我们为什么要这么做?是的，这是因为确认数据包是通过UDP协议发送的，所以没有办法保证所有的确认数据包都会成功的到达网络的另外一端，你当然不会希望发送方和接收方之间对于目前确认到那个片段的信息都是不同步的。 所以我们需要一些确认数据包传输的可靠性，但是我们不希望实现一个确认数据包的确认系统，因为这将会是一个非常痛苦和麻烦的过程。因为在最坏的情况下，确认数据包的大小是256位或32字节，最简单的方法是也是最好的。we just send the entire state of all acked slices in each ack packet. When the ack packet is received, we consider a slice to be acked the instant an ack packet comes in with that slice marked as acked and locally that slice is not seen as acked yet. 基本的发送方实现 现在我们已经了解了这个系统背后的基本概念，让我们从发送方的实现开始实现整个系统。 发送方的策略是： 1）持续的发送片段数据包，直到所有的片段数据包都被确认。 2）不再对已经确认过的片段数据包进行发送。 我们使用以下的数据结构来描述发送方：?123456789101112class ChunkSender{ bool sending; uint16_t chunkId; int chunkSize; int numSlices; int numAckedSlices; int currentSliceId; bool acked[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize]; double timeLastSent[MaxSlicesPerChunk];};正如之前提到的那样，一次只会发送一个块的数据，如果我们正在发送一个块的数据的时候，那么关于“发送”的状态是true，假如我们处于闲置状态、正在准备发送一个块的数据的时候，那么关于“发送”的状态是false。在这个实现中，如果当前有一个块的数据仍在通过网络进行发送的话，你不能发送另外一个块的数据。你必须等待当前块的数据发送完毕之后才可以发送另外一个块的数据。如果你不喜欢的话，在块的发送器的前端按照你的意愿可以放置一个发送队列。 接下来，我们有我们正在发送的块数据的id，或者如果我们没有在发送块数据的话，那么我们有要发送的下一个块数据的id、以及这个块所分成的片段数据包的数量。我们也会跟踪每个片段数据包，来记录这个片段数据包是否已经被确认，这可以让我们避免重发那些已经收到的片段数据包，并且我们还会记录迄今为止已经确认收到的片段数据包的数量，这个数量会去掉冗余的确认，也就是每个片段数据包的确认只算一次。从发送方的观点来看，只有当确认的片段数据包的数量等于这个块所分成的片段数据包的数量的时候才会这个块数据已经被完全收到了。 我们还需要为这个算法记录当前发送的片段数据包的id，因为这将决定了哪些片段数据包将被发送。它的工作机制大致是这样：一个块数据开始发送的时候，是从id为0的片段数据包开始发送的，然后依次从左到右开始发送直到经过最后一个片段数据包(也就是id为分包数量的大小-1)的时候会回头从id为0的片段数据包继续发送。最终，你会停止这个迭代因为发送的片段数据包已经耗尽了带宽。在这一点上，我们通过记录当前发送的片段数据包的id就能记住我们当前遍历的片段数据包的索引，这样在下一次开始遍历的时候你就可以继续从这个位置开始发送片段数据包。最后一部分是非常重要的，这是因为它可以把发送一个块数据所有的片段数据包这个事情是分散开，而不是在一起就全部发出去。 现在让我们讨论下带宽限制。显然你不能把所有的片段数据包一次全部发完，因为如果这么做的话，会把整个链接堵住，那么，我们该如何限制发送方所使用的带宽?我的实现机制大概是这样的：当你对全部的片段数据包进行遍历并且考虑你想要发送的每个片段数据包的时候，大概估计下这个片段数据包会需要占据多少字节，比如可以用这种估计算法：大概这个片段的字节数+一些协议的开销和UDP / IP的报头。然后用所需的字节数和你带宽预算里面可用来进行发送的字节数进行比较。如果带宽预算里面没有足够可用的字节数，那么就停止发送。否则的话，从带宽预算里面减去发送这个片段数据包所需的字节数，然后对于下个片段数据包重复整个过程。 带宽预算里面可用的字节发送预算是从哪里计算得来的？在每一帧更新块的发送方之前，把你的目标带宽（比如说每秒256KB）转换成每秒可以发送的字节数，然后用它乘以更新时间来把记过放到一个累加器里面。每秒256KB是一个比较保守的发送速率，这意味你可以每秒发送32000个字节，所以把32000 dt这个值添加到累加器里面。每秒512KB是一个比较适中的估计，意味你可以每秒发送64000个字节。每秒1MB是一个比较激进的估计，意味你可以每秒发送125000个字节。通过这种方法，在每次更新的时候你就可以累加你被允许发送的字节数了，这样当你可以按照预算来发送最大数量的片段数据包，如果还有数据没有发完的话，会等到下一帧的时候再尝试发送。 对于发送方而言有一点比较微妙，实现一个片段数据包重新发送的最小延迟是一个很棒的主意，如果不这么做的话，就可能会出现这种一样情况，对于很小的块数据或者一个块的最后几个片段数据包，很容易不停的发送它们把整个网络都塞满。正是因为这一原因，我们使用了一个数组来记录每个片段数据包的上一次发送时间。重新发送延迟的一个选择是使用一个估计的网络往返时延，或者只有在超过上一次发送时间网络往返时延*1.25还没有收到确认数据包的情况才会重新发送。或者，你可以说“这根本就无所谓”，只要超过上一次发送时间100毫秒了就重新发送。我只是列举适合我自己的方案！ 把发送方实现的更完美一点 如果你仔细用数学计算一下的话，你会注意到对于一个256K 的数据块而言，它要在网络上发送完毕仍然需要发送很长的时间： 如果网络速率是每秒1M的话，就需要2秒钟的时间。 如果网络速率是每秒512KB的话，就需要4秒钟的时间。 如果网络速率是每秒256KB的话，就需要8秒钟的时间。 这可有点糟糕。我们实现系统的重点是快速和可靠性。再次强调下需要能够快速传递。如果块系统的传输不能做到快速的话，这是不是会不太好？块系统的一些典型用例会支持这一点。举个简单的例子来说明，当客户端第一次连接上服务器的时候，一大块数据需要立刻发送给客户端，或者在客户端退出加载界面开始游戏的时候需要能够大量数据快速下发给客户端。你想要尽快的传递完需要的数据，而且在这两种情况下，用户对于自己的带宽并没有什么太多其他的用途，那么为什么不使用尽可能多的带宽? 在过去我曾经尝试过一个方法，就是在一开始的时候尽量传递，这取得了很好的效果。假设你的块大小并不是那么大，而且你的块发送频率并不那么频繁，我没找到什么理由为什么你不能在一开始就把所有的片段数据包都发送出去，填充满贷款，然后等待100毫秒，在恢复成正常的带宽受限的片段数据包发送策略。 为什么这样会取得良好的效果？如果用户有一个良好的网络连接（可以每秒发送超过10MB的数据甚至更多。。。），事实上，片段数据包在网络上的传输非常的快速。如果是连接的情况并不是那么好的情况下，大部分的片段数据包会得到缓冲，大部分的片段数据包受限于带宽但是会尽可能快的发送出去。处理完这些数据包之后，就会切换到常规的策略，从那些第一次没有发送出去的片段数据包选择合适的进行发送。 这似乎有点冒险，所以让我来解释一下。如果出现大量数据需要传输但是已经超过带宽限制的情况，互联网上的路由器会倾向于缓冲这些数据包，而不是不惜代价的抛弃它们。这就是TCP协议会做的事情。通常情况下，我讨厌这个机制因为它会诱发延迟而且会弄乱那些你想要尽快交付的游戏数据包，但在这种情况下它是一个非常好的行为，这是因为玩家真的没有其他事情可以做，智能等待你的块数据赶紧传输完毕。只是在你的块数据传输完毕以后，会有一些垃圾数据或者交通拥堵，它会影响你的游戏开始的几秒钟。另外，请确保你增加了网络两端的加操作系统的套接字缓冲区的大小，这样它们才可以比你最大的块数据的大小要大(我建议至少增加一倍)，否则在超过网络带宽的限制之前你就会出现丢弃段数据包的情况。 最后，我想成为一个负责任的网络公民，虽然在这里我推荐在最开始连接的时候一次发送所有的片段数据包，所以对我来说介绍下我认为这真的是适当的是非常非常重要的，在2016年的网络环境下，发送几百个KB量级的数据包是没什么大不了的行为，而且只会发生在没有其他关键数据同时发送的情况下。让我们举个简单的例子来说明，如果用户正在玩你的游戏，那么当你发送大块数据的时候，使用保守的策略。如果不这么做的话，就会冒影响用户游戏体验的风险，这是因为你的发送行为可能会诱导额外的网络延迟或者出现数据包丢失的情况。 同样，如果你的块数据非常大的情况下，比如说是十几MB的情况，那么请不要使用这种野蛮发送的策略，这是因为这种方法太过于依赖陌生人的仁慈，也就是在你和你的数据包目的地之间的路由器缓冲区。如果要持续发送非常大的数据块保持一个高吞吐量有必要实施一些更聪明的方法。这是某种自适应的方法，它会试图尽快发送数据，但是一旦检测到因为连接上有太多的数据在传输导致太多的延迟或者数据包的丢失，就能切换回一个低速的方式。这样一个系统超出了本文的范围。 接收方的实现 现在我们已经解决了发送方实现的所有细节和小问题，那么让我们开始实现接收方。正如之前提到的那样，与之前文章介绍的数据包的分包和重组系统不同，块系统在同一时间只能由一个块正在传输。 这使得块系统的接收方可以实现的更加简单，你可以看下面的实现: ?1234567891011class ChunkReceiver{ bool receiving; bool readyToRead; uint16_t chunkId; int chunkSize; int numSlices; int numReceivedSlices; bool received[MaxSlicesPerChunk]; uint8_t chunkData[MaxChunkSize];}; 我们有一个状态来记录我们是否正在网络上“接收”一个块数据，加上“readyToRead’”状态来表明是否已经有一个块的所有片段数据包都已经收到、已经准备好被用户弹出进行读取处理了。接收队列的最小长度是1，这是非常有效的。如果你不喜欢这个的话，你当然可以立即从块数据接收器里面将这个数据弹出并把它插入实际的接收队列。 在这个数据结构中我们还记录了块数据的大小（尽管不是完全准确，直到收到最后一个片段数据包才能准确的计算块数据的大小）、片段数据包的数量、已经接收到的片段数据包的数量还有针对每个片段数据包的一个接收标记。针对每个片段数据包的接收标记可以让我们丢弃那些我们已经收到的片段数据包，并计算到目前为止我们已经收到的片段数据包的数量（因为我们可能会多次收到同一个片段数据包，但是我们只会在第一次收到这个片段数据包的才会增加计数器的值）。它也被用在生成确认数据包上。当已经接收到的片段数据包的数量等于片段数据包的数量的时候，从接收方的角度看这个块数据的接收才算完成。 首先，接收方的设置会从块0开始。当一个片段数据包从网络上传递过来，并且能够匹配这个块id的话，“receiving”状态会从false翻转为true，第一个片段数据包的数据会插入” chunkData“变量的合适位置，片段数据包的数量会根据第一个片段数据包里面的数据进行正确的设置，已经接收到的片段数据包的数量会加一，也就是从0到1，针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 随着这个块数据的其他片段数据包的到来，会对每一个片段数据包进行检测，判断它们的id是否与当前块的id相同，如果不相同的话就会被丢弃。如果这个片段数据包已经收到过的话，那么这个包也会被丢弃。否则，这个片段数据包的数据会插入” chunkData“变量的合适位置、已经接收到的片段数据包的数量会加一、针对每个片段数据包的接收标记里面对应这个片段数据包的项会变为true。 这一过程会持续进行，直到接收到所有的片段数据包。一旦接收到所有的片段数据包（也就是已经接收到的片段数据包的数量等于片段数据包的数量的时候），接收方会把“receiving “状态改为false，而把”readyToRead“状态改为true。当”readyToRead”状态为true的时候，所有收到的片段数据包都会被丢弃。在这一点上，这个处理过程通常非常的短，会在收到片段数据包的同一帧进行处理，调用者会检查”我有一块数据要读取么？“并处理块数据。然后会重置数据块接收器的所有数据为默认值，除了块数据的id从0增加到1，这样我们就准备好接收下一个块了。 浸泡测试的重要性和确认数据包 第一眼看上去，确认数据包这个系统似乎很简单： 1）记录已经接收到的片段数据包。 2）当一个片段数据包收到以后，回复一个包含所有确认收到的片段数据包信息的确认数据包。 这看上去实现起来似乎相当的简单，但是像大多数发生在UDP协议的事情一样，当涉及到数据包丢失的时候，就有一些微妙的点让它的处理有点棘手。 一个对于确认数据包比较天真的实现可能是这样子的。每次收到片段数据包，就回复一个包含所有确认收到的片段数据包信息的确认数据包（也会包括刚收到的片段数据包的信息）。这看上去非常符合逻辑，但是这使得块协议给恶意发送者一个漏洞使得它们可以块协议作为一个DDos的工具。如何作为一个DDos的工具?如果你对每个收到的片段数据包都会回复一个确认数据包的话，那么发送方能够构造一个很小的片段数据包发送给你，而你会回复一个比发送给你的片段数据包还大的确认数据包，这样你的服务器就变成了一个可以被人利用来进行DDos放大攻击的工具。 现在也许是因为我对DDos这个事情有一点偏执（我确实是有一点），但是一般来说你可以防止对DDos的放大，永远不要设计一个包含对接收到的数据包进行一对一的映射响应的协议。让我们举个简单例子来说明一下这个问题。如果有人给你发送1000个片段数据包，永远不要给他回复1000个确认数据包。相反只发一个确认数据包，而且最多每50毫秒或者100毫秒才发送一个确认数据包。如果你是这样设计的话，那么滥用你的UDP协议对DDos进行放大就是完全不可能的。 还有其他的方法让这个确认系统容易出错，而这些都往往表现为”发送挂起“。换句话说，接收方已经知道这个块已经发送完毕了，但是由于程序员的错误，发送方错过了一个确认数据包(可能是针对最后一个片段数据包的确认数据包)并且卡入到一个状态，会不停的反复重发这个片段数据包而没有得到一个确认数据包的响应。 在过去10年里，我可能至少5次从头开始实现这个块系统，每次我都找到新的和令人兴奋的方式来让发送方挂起。我开发和测试块系统的策略是首先进行编码确认它能够跑起来，然后设置一个测试工具在有大量的数据包丢失、数据包乱序和重复的情况下随机发送随机大小的块。这往往会清除任何挂起。我曾经实现过的块系统都至少有一个挂起存在，通常会有2到3个挂起。所以如果你是打算从头开始实现这个块系统的话，请不要轻敌。请设置一个浸泡测试。你会感谢我在这里的提醒的。 我通常遇到的第一次挂起是由于对同一个片段数据包的多次收到不会回复一个确认数据包。它有点像这样：” 哦，这个片段数据包已经收到过了么？已经收到过了就丢弃它”，然后忘记在确认数据包里面设置标记。这对于发送方来说是一个困扰，因为这样的话就不会有一个确认数据包，那么如果出现这种情况的话，又恰好遇到第一次收到这个片段数据包的时候发送的确认数据包出现丢包的情况，发送方根本就不知道这个他在反复发送的片段数据包其实已经被收到了。如果你就是这么不巧，遇上了第一次收到这个片段数据包的时候发送的确认数据包出现丢包的情况，那么就遇上了挂起的情况。如果你想在你的代码里面重现这个情况的话，可以在收到最后一个片段数据包的时候不发送确认数据包，那么出现的情况就是这种挂起了。 下一个挂起会发生在接收方在发送方知道之前就已经知道块发送完毕并切换它的状态变量“readyToRead”来丢弃后续传入的片段数据包。在这种状态下，即使接收方认为块已经完全接收完毕，但是发送方还不知道这一点，所以有必要设置确认数据包对应的标志位，即使块已经完全接收完毕，这样发送方才能一直接收到提示块已经全部发送完毕的确认数据包。 通常遇到的最后一个挂起情况是在读取完块数据以后的状态切换里面，那个时候状态变量“readyToRead”已经切回false而块的id也加一了。让我们举个简单例子来说明一下这个问题，块0已经完成接收，用户已经完成对块0的读取并且块id已经递增到1了，所以我们已经准备好接收块1的片段数据包了（我们会丢弃任何与我们当前正在接收块ID不同的片段数据包）。 再一次出现这种情况，就是这里的发送方因为确认数据包的丢失导致信息有一点滞后，可能是因为没有收到第一个确认数据包。在这种情况下，有必要关注片段数据包，如果我们正处于这么一个状态：我们尚未收到第n个片段数据包，但是前面n – 1个片段数据包都已经收到了，我们必须设置一个特殊的标记位然后我们会发送一个包含所有前面n – 1个片段数据包都已经收到信息的确认数据包，否则发送方不会意识到块数据已经收到并且发送方已经准备挂起了。 正如你所看到的那样，确认数据包的实现是有一点微妙的，这是一个有点奇怪的过程因为当片段数据包在网络的一端收到的时候，需要设置一个标记位来发送确认数据包直到发送方知道都有哪些发送的片段数据包被成功接收为止。如果你打破了片段数据包-&gt;确认数据包这个链接的话，那么整个系统就将挂起。我鼓励你仔细看看这篇文章的源代码搞清楚进一步的细节。 总结 块系统在概念上是很简单的，但是它的具体实现肯定不是微不足道的。在我看来，实现发送者设计这一块是一个很好的学习经验，当你从头开始实现这样的系统的时候一定有很多东西需要学习。 我希望你喜欢这个系统的设计，并试着自己动手从头开始实现它。这是一个很好的学习经历。此外，我鼓励你在patreon上支持我，作为回报，你会得到本文的示例源代码(以及本系列的其他文章的示例源代码)，还包括我在GDC 2015上关于网络物理的演讲的源代码。 如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议三之数据包的分包和重组]]></title>
    <url>%2F2017%2F02%2F26%2Fpacket_fragmentation_and_reassembly%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[本篇自我总结本篇主要讲了数据包的分包和重组问题, 到底数据包多大才好呢?是不是越大越好呢?包太大了怎么办呢?请看总结, 不明之处再看文中具体讲解. 为什么需要做这个分包和重组系统每台计算机(路由器)会沿着路由强制要求数据包的大小会有一个最大的上限，这个上限就是所谓的最大传输单元MTU。如果任意一个路由器收到一个数据包的大小超过这个最大传输单元的大小，它有这么两个选择，a)在IP层对这个数据包进行分包，并将分包后的数据包继续传递，b)丢弃这个数据包然后告诉你数据包被丢弃了，你自己负责摆平这个问题。 实例 : 这儿有一个我会经常遇到的情况。人们在编写多人在线游戏的时候，数据包的平均大小都会非常的小，让我们假设下，这些数据包的平均大小大概只有几百字节，但时不时会在他们的游戏中同时发生大量的事情并且发出去的数据包会出现丢失的情况，这个时候数据包会比通常的情况下要大。突然之间，游戏的数据包的大小就会超过最大传输单元的大小，这样就只有很少一部分玩家能够收到这个数据包，然后整个通信就崩溃了。 本篇基本术语 数据包packets 分包fragments 分包的数据结构我们将允许一个数据包最多可以分成256个数据包，并且每个分包后的数据包的大小不会超过1024个字节。这样的话，我们就可以通过这样一个系统来发送大小最大为256k的数据包 [protocol id] (32 bits) // not actually sent, but used to calc crc32 [crc32] (32 bits) [sequence] (16 bits) // 数据包序号 [packet type = 0] (2 bits) [fragment id] (8 bits) // 分包ID [num fragments] (8 bits) [pad zero bits to nearest byte index] // 用于字节对齐的bits &lt;fragment data&gt; 发送分包后的数据包发送分包以后的数据包是一件非常容易的事情。如果数据包的大小小于保守估计的最大传输单元的大小。那么就按正常的方法进行发送。否则的话，就计算这个数据包到底该分成多少个1024字节的数据包分包，然后构建这些分包并按照之前发送正常数据包的方法进行发送。 发送出去以后也不记录发送的数据包的内容，这种发送以后不记录发送的数据包的内容的方法有一个后果，就是数据包的任意一个分包如果丢失的话，那么整个数据包就都要丢弃。随着分包数量的增加，整个数据包被丢弃的概率也随之增加.由此可见，当你需要发送要给256K的数据包的时候要发送256个分包，如果有一个分包丢失的话，你就要重新把这个256k的数据包再分一次包然后再发送出去。 什么时候用这个分包和重组系统呢因为发送出去以后也不记录发送的数据包, 随着分包数量的增加，整个数据包被丢弃的概率也随之增加, 而一个片段的丢失就会导致整个数据包都要被丢弃掉.所以我建议你要小心分包以后的数量。 这个分包和重组系统最好是只对2-4个分包的情况进行使用，而且最好是针对那种对时间不怎么敏感的数据使用或者是就算分包lost了也无所谓的情况。绝对不要只是为了省事就把一大堆依赖顺序的事件打到一个大数据包里面然后依赖数据包的分包和重组机制进行发送。这会让事情变得更加麻烦。 数据包分包和重组系统的关键弱点是一个片段的丢失就会导致整个数据包都要被丢弃掉, 想要解决这个弱点得使用大块数据发送策略, 见下一篇文章 构建游戏网络协议四之发送大块数据. 接收分包后的数据包之所以对分包后的数据包进行接收很困难的原因是我们不仅需要给缓冲区建立一个数据结构还要把这些分包重组成原始的数据包，我们也要特别小心如果有人试图让我们的程序产生崩溃而给我们发送恶意的数据包。 要非常小心检查一切可能的情况。除此之外，还有一个非常简单的事情要注意：让分包保存在一个数据结构里面，当一个数据包的所有分包都到达以后（通过计数来判断是否全部到达），将这些分包重组成一个大的数据包，并把这个重组后的大数据包返回给接收方。 什么样的数据结构在这里是有意义的?这里并没有什么特别的数据结构!我使用的是一种我喜欢称之为序列缓冲区的东西。我想和你分享的最核心的技巧是如何让这个数据结构变得高效：12345678const int MaxEntries = 256; struct SequenceBuffer&#123; bool exists[MaxEntries]; uint16_t sequence[MaxEntries]; Entry entries[MaxEntries];&#125;; 原文原文出处 原文标题 : Packet Fragmentation and Reassembly (How to send and receive packets larger than MTU) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.In the previous article we discussed how to unify packet read and write into a single serialize function and added a bunch of safety features to packet read.Now we are ready to start putting interesting things in our packets and sending them over the network, but immediately we run into an interesting question: how big should our packets be?To answer this question properly we need a bit of background about how packets are actually sent over the Internet.BackgroundPerhaps the most important thing to understand about the internet is that there&rsquo;s no direct connection between the source and destination IP address. What actually happens is that packets hop from one computer to another to reach their destination.Each computer along this route enforces a maximum packet size called the maximum transmission unit, or MTU. According to the IP standard, if any computer recieves a packet larger than its MTU, it has the option of a) fragmenting that packet, or b) dropping the packet.So here&rsquo;s how this usually goes down. People write a multiplayer game where the average packet size is quite small, lets say a few hundred bytes, but every now and then when a lot of stuff is happening in their game and a burst of packet loss occurs, packets get a lot larger than usual, going above MTU for the route, and suddenly all packets start getting dropped!Just last year (2015) I was talking with Alex Austin at Indiecade about networking in his game Sub Rosa. He had this strange networking bug he couldn&rsquo;t reproduce. For some reason, players would randomly get disconnected from the game, but only when a bunch of stuff was going on. It was extremely rare and he was unable to reproduce it. Alex told me looking at the logs it seemed like packets just stopped getting through.This sounded exactly like an MTU issue to me, and sure enough, when Alex limited his maximum packet size to a reasonable value the bug went away.MTU in the real worldSo what&rsquo;s a reasonable maximum packet size?On the Internet today (2016, IPv4) the real-world MTU is 1500 bytes.Give or take a few bytes for UDP/IP packet header and you&rsquo;ll find that the typical number before packets start to get dropped or fragmented is somewhere around 1472.You can try this out for yourself by running this command on MacOS X:ping -g 56 -G 1500 -h 10 -D 8.8.4.4On my machine it conks out around just below 1500 bytes as expected:1404 bytes from 8.8.4.4: icmp_seq=134 ttl=56 time=11.945 ms1414 bytes from 8.8.4.4: icmp_seq=135 ttl=56 time=11.964 ms1424 bytes from 8.8.4.4: icmp_seq=136 ttl=56 time=13.492 ms1434 bytes from 8.8.4.4: icmp_seq=137 ttl=56 time=13.652 ms1444 bytes from 8.8.4.4: icmp_seq=138 ttl=56 time=133.241 ms1454 bytes from 8.8.4.4: icmp_seq=139 ttl=56 time=17.463 ms1464 bytes from 8.8.4.4: icmp_seq=140 ttl=56 time=12.307 ms1474 bytes from 8.8.4.4: icmp_seq=141 ttl=56 time=11.987 msping: sendto: Message too longping: sendto: Message too longRequest timeout for icmp_seq 142Why 1500? That&rsquo;s the default MTU for MacOS X. It&rsquo;s also the default MTU on Windows. So now we have an upper bound for your packet size assuming you actually care about packets getting through to Windows and Mac boxes without IP level fragmentation or a chance of being dropped: 1472 bytes.So what&rsquo;s the lower bound? Unfortunately for the routers in between your computer and the destination the IPv4 standard says 576. Does this mean we have to limit our packets to 400 bytes or less? In practice, not really.MacOS X lets me set MTU values in range 1280 to 1500 so considering packet header overhead, my first guess for a conservative lower bound on the IPv4 Internet today would be 1200 bytes. Moving forward, in IPv6 this is also a good value, as any packet of 1280 bytes or less is guaranteed to get passed on without IP level fragmentation.This lines up with numbers that I&rsquo;ve seen throughout my career. In my experience games rarely try anything complicated like attempting to discover path MTU, they just assume a reasonably conservative MTU and roll with that, something like 1000 to 1200 bytes of payload data. If a packet larger than this needs to be sent, it&rsquo;s split up into fragments by the game protocol and re-assembled on the other side.And that&rsquo;s exactly what I&rsquo;m going to show you how to do in this article.Fragment Packet StructureLet&rsquo;s get started with implementation.The first thing we need to decide is how we&rsquo;re going to represent fragment packets over the network so they are distinct from non-fragmented packets.Ideally, we would like fragmented and non-fragmented packets to be compatible with the existing packet structure we&rsquo;ve already built, with as little overhead as possible in the common case when we are sending packets smaller than MTU.Here&rsquo;s the packet structure from the previous article:[protocol id] (64 bits) // not actually sent, but used to calc crc32[crc32] (32 bits)[packet type] (2 bits for 3 distinct packet types)(variable length packet data according to packet type)[end of packet serialize check] (32 bits)In our protocol we have three packet types: A, B and C.Let&rsquo;s make one of these packet types generate really large packets:static const int MaxItems = 4096 * 4;struct TestPacketB : public Packet{ int numItems; int items[MaxItems]; TestPacketB() : Packet( TEST_PACKET_B ) { numItems = random_int( 0, MaxItems ); for ( int i = 0; i &lt; numItems; ++i ) items[i] = random_int( -100, +100 ); } template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_int( stream, numItems, 0, MaxItems ); for ( int i = 0; i &lt; numItems; ++i ) { serialize_int( stream, items[i], -100, +100 ); } return true; }};This may seem somewhat contrived but these situations really do occur. For example, if you have a strategy where you send all un-acked events from server to client and you hit a burst of packet loss, you can easily end up with packets larger than MTU, even though your average packet size is quite small.Another common case is delta encoded snapshots in a first person shooter. Here packet size is proportional to the amount of state changed between the baseline and current snapshots for each client. If there are a lot of differences between the snapshots the delta packet is large and there&rsquo;s nothing you can do about it except break it up into fragments and re-assemble them on the other side.Getting back to packet structure. It&rsquo;s fairly common to add a sequence number at the header of each packet. This is just a packet number that increases with each packet sent. I like to use 16 bits for sequence numbers even though they wrap around in about 15 minutes @ 60 packets-per-second, because it&rsquo;s extremely unlikely that a packet will be delivered 15 minutes late.Sequence numbers are useful for a bunch of things like acks, reliability and detecting and discarding out of order packets. In our case, we&rsquo;re going to use the sequence number to identify which packet a fragment belongs to:[protocol id] (64 bits) // not actually sent, but used to calc crc32[crc32] (32 bits)[sequence] (16 bits)[packet type] (2 bits)(variable length packet data according to packet type)[end of packet serialize check] (32 bits)Here&rsquo;s the interesting part. Sure we could just add a bit is_fragment to the header, but then in the common case of non-fragmented packets you&rsquo;re wasting one bit that is always set to zero.What I do instead is add a special fragment packet type:enum TestPacketTypes{ PACKET_FRAGMENT = 0, // RESERVED TEST_PACKET_A, TEST_PACKET_B, TEST_PACKET_C, TEST_PACKET_NUM_TYPES};And it just happens to be free because four packet types fit into 2 bits. Now when a packet is read, if the packet type is zero we know it&rsquo;s a fragment packet, otherwise we run through the ordinary, non-fragmented read packet codepath.Lets design what this fragment packet looks like. We&rsquo;ll allow a maximum of 256 fragments per-packet and have a fragment size of 1024 bytes. This gives a maximum packet size of 256k that we can send through this system, which should be enough for anybody, but please don&rsquo;t quote me on this.With a small fixed size header, UDP header and IP header a fragment packet be well under the conservative MTU value of 1200. Plus, with 256 max fragments per-packet we can represent a fragment id in the range [0,255] and the total number of fragments per-packet [1,256] with 8 bits.[protocol id] (32 bits) // not actually sent, but used to calc crc32[crc32] (32 bits)[sequence] (16 bits)[packet type = 0] (2 bits)[fragment id] (8 bits)[num fragments] (8 bits)[pad zero bits to nearest byte index]&lt;fragment data&gt;Notice that we pad bits up to the next byte before writing out the fragment data. Why do this? Two reasons: 1) it&rsquo;s faster to copy fragment data into the packet via memcpy than bitpacking each byte, and 2) we can now save a small amount of bandwidth by inferring the fragment size by subtracting the start of the fragment data from the total size of the packet.Sending Packet FragmentsSending packet fragments is easy. For any packet larger than conservative MTU, simply calculate how many 1024 byte fragments it needs to be split into, and send those fragment packets over the network. Fire and forget!One consequence of this is that if any fragment of that packet is lost then the entire packet is lost. It follows that if you have packet loss then sending a 256k packet as 256 fragments is not a very good idea, because the probability of dropping a packet increases significantly as the number of fragments increases. Not quite linearly, but in an interesting way that you can read more about here.In short, to calculate the probability of losing a packet, you must calculate the probability of all fragments being delivered successfully and subtract that from one, giving you the probability that at least one fragment was dropped.1 - probability_of_fragment_being_delivered ^ num_fragmentsFor example, if we send a non-fragmented packet over the network with 1% packet loss, there is naturally a 1&frasl;100 chance the packet will be dropped.As the number of fragments increase, packet loss is amplified:Two fragments: 1 - (99&frasl;100) ^ 2 = 2%Ten fragments: 1 - (99&frasl;100) ^ 10 = 9.5%100 fragments: 1 - (99&frasl;100) ^ 100 = 63.4%256 fragments: 1 - (99&frasl;100) ^ 256 = 92.4%So I recommend you take it easy with the number of fragments. It&rsquo;s best to use this strategy only for packets in the 2-4 fragment range, and only for time critical data that doesn&rsquo;t matter too much if it gets dropped. It&rsquo;s definitely not a good idea to fire down a bunch of reliable-ordered events in a huge packet and rely on packet fragmentation and reassembly to save your ass.Another typical use case for large packets is when a client initially joins a game. Here you usually want to send a large block of data down reliably to that client, for example, representing the initial state of the world for late join. Whatever you do, don&rsquo;t send that block of data down using the fragmentation and re-assembly technique in this article.Instead, check out the technique in next article which handles packet loss by resending fragments until they are all received.Receiving Packet FragmentsIt&rsquo;s time to implement the code that receives and processed packet fragments. This is a bit tricky because we have to be particularly careful of somebody trying to attack us with malicious packets.Here&rsquo;s a list of all the ways I can think of to attack the protocol:Try to send out of bound fragments ids trying to get you to crash memory. eg: send fragments [0,255] in a packet that has just two fragments.Send packet n with some maximum fragment count of say 2, and then send more fragment packets belonging to the same packet n but with maximum fragments of 256 hoping that you didn&rsquo;t realize I widened the maximum number of fragments in the packet after the first one you received, and you trash memory.Send really large fragment packets with fragment data larger than 1k hoping to get you to trash memory as you try to copy that fragment data into the data structure, or blow memory budget trying to allocate fragmentsContinually send fragments of maximum size (256&frasl;256 fragments) in hope that it I could make you allocate a bunch of memory and crash you out. Lets say you have a sliding window of 256 packets, 256 fragments per-packet max, and each fragment is 1k. That&rsquo;s 64 mb per-client.Can I fragment the heap with a bunch of funny sized fragment packets sent over and over? Perhaps the server shares a common allocator across clients and I can make allocations fail for other clients in the game because the heap becomes fragmented.Aside from these concerns, implementation is reasonably straightforward: store received fragments somewhere and when all fragments arrive for a packet, reassemble them into the original packet and return that to the user.Data Structure on Receiver SideThe first thing we need is some way to store fragments before they are reassembled. My favorite data structure is something I call a sequence buffer:const int MaxEntries = 256;struct SequenceBuffer{ uint32_t sequence[MaxEntries]; Entry entries[MaxEntries];};Indexing into the arrays is performed with modulo arithmetic, giving us a fast O(1) lookup of entries by sequence number:const int index = sequence % MaxEntries;A sentinel value of 0xFFFFFFFF is used to represent empty entries. This value cannot possibly occur with 16 bit sequence numbers, thus providing us with a fast test to see if an entry exists for a given sequence number, without an additional branch to test if that entry exists.This data structure is used as follows. When the first fragment of a new packet comes in, the sequence number is mapped to an entry in the sequence buffer. If an entry doesn&rsquo;t exist, it&rsquo;s added and the fragment data is stored in there, along with information about the fragment, eg. how many fragments there are, how many fragments have been received so far, and so on.Each time a new fragment arrives, it looks up the entry by the packet sequence number. When an entry already exists, the fragment data is stored and number of fragments received is incremented. Eventually, once the number of fragments received matches the number of fragments in the packet, the packet is reassembled and delivered to the user.Since it&rsquo;s possible for old entries to stick around (potentially with allocated blocks), great care must be taken to clean up any stale entries when inserting new entries in the sequence buffer. These stale entries correspond to packets that didn&rsquo;t receive all fragments.And that&rsquo;s basically it at a high level. For further details on this approach please refer to the example source code for this article. Click here to get the example source code for this article series.Test Driven DevelopmentOne thing I&rsquo;d like to close this article out on.Writing a custom UDP network protocol is hard. It&rsquo;s so hard that even though I&rsquo;ve done this from scratch at least 10 times, each time I still manage to fuck it up in a new and exciting ways. You&rsquo;d think eventually I&rsquo;d learn, but this stuff is complicated. You can&rsquo;t just write low-level netcode and expect it to just work.You have to test it!My strategy when testing low-level netcode is as follows:Code defensively. Assert everywhere. These asserts will fire and they&rsquo;ll be important clues you need when something goes wrong.Add functional tests and make sure stuff is working as you are writing it. Put your code through its paces at a basic level as you write it and make sure it&rsquo;s working as you build it up. Think hard about the essential cases that need to be property handled and add tests that cover them.But just adding a bunch of functional tests is not enough. There are of course cases you didn&rsquo;t think of! Now you have to get really mean. I call this soak testing and I&rsquo;ve never, not even once, have coded a network protocol that hasn&rsquo;t subsequently had problems found in it by soak testing.When soak testing just loop forever and just do a mix of random stuff that puts your system through its paces, eg. random length packets in this case with a huge amount of packet loss, out of order and duplicates through a packet simulator. Your soak test passes when it runs overnight and doesn&rsquo;t hang or assert.If you find anything wrong with soak testing. You may need to go back and add detailed logs to the soak test to work out how you got to the failure case. Once you know what&rsquo;s going on, stop. Don&rsquo;t fix it immediately and just run the soak test again.Instead, add a unit test that reproduces that problem you are trying to fix, verify your test reproduces the problem, and that it problem goes away with your fix. Only after this, go back to the soak test and make sure they run overnight. This way the unit tests document the correct behavior of your system and can quickly be run in future to make sure you don&rsquo;t break this thing moving forward when you make other changes.Add a bunch of logs. High level errors, info asserts showing an overview of what is going on, but also low-level warnings and debug logs that show what went wrong after the fact. You&rsquo;re going to need these logs to diagnose issues that don&rsquo;t occur on your machine. Make sure the log level can be adjusted dynamically.Implement network simulators and make sure code handles the worst possible network conditions imaginable. 99% packet loss, 10 seconds of latency and +/- several seconds of jitter. Again, you&rsquo;ll be surprised how much this uncovers. Testing is the time where you want to uncover and fix issues with bad network conditions, not the night before your open beta.Implement fuzz tests where appropriate to make sure your protocol doesn&rsquo;t crash when processing random packets. Leave fuzz tests running overnight to feel confident that your code is reasonably secure against malicious packets and doesn&rsquo;t crash.Surprisingly, I&rsquo;ve consistently found issues that only show up when I loop the set of unit tests over and over, perhaps these issues are caused by different random numbers in tests, especially with the network simulator being driven by random numbers. This is a great way to take a rare test that fails once every few days and make it fail every time. So before you congratulate yourself on your tests passing 100%, add a mode where your unit tests can be looped easily, to uncover such errors.Test simultaneously on multiple platforms. I&rsquo;ve never written a low-level library that worked first time on MacOS, Windows and Linux. There are always interesting compiler specific issues and crashes. Test on multiple platforms as you develop, otherwise it&rsquo;s pretty painful fixing all these at the end.This about how people can attack the protocol. Implement code to defend against these attacks. Add functional tests that mimic these attacks and make sure that your code handles them correctly.This is my process and it seems to work pretty well. If you are writing a low-level network protocol, the rest of your game depends on this code working correctly. You need to be absolutely sure it works before you build on it, otherwise it&rsquo;s basically a stack of cards.In my experience, game neworking is hard enough without having suspicions that that your low-level network protocol has bugs that only show up under extreme network conditions. That&rsquo;s exactly where you need to be able to trust your code works correctly. So test it! 译文 译文出处 译者：崔嘉艺(milan21) 审校：崔国军（星际迷航）大家好，我是格伦·菲德勒。欢迎大家阅读系列教程《构建游戏网络协议》的第三篇文章。在之前的文章中，我们讨论了如何将数据包的读取和写入用一个单独的序列化函数来实现。现在我们要开始把一些有趣的事情放到这些数据包中，,但正如你即将开始编码的令人惊叹的多人在线动作、第一人称射击、大型多人在线角色扮演游戏、多人在线战术竞技游戏会发生的那样，当你以每秒120次的频率发送8k大小的数据包，游戏网络中会传来一个声音呼喊着你:“不要发送超过1200字节大小的数据包!”但是都已经2016年了，你真的要注意最大传输单元这个东西么?不幸的是，答案是是的！最大传输单元MTU你可能已经听说过最大传输单元了。在网络程序员中流传着大量有关最大传输单元问题的故事。那么这到底是怎么回事呢？究竟什么是最大传输单元?为什么你要在乎最大传输单元这个事情？当你通过互联网来发送数据包的时候到底背后发生了什么？这些数据包要从一台计算机(路由器)跳到另一个计算机(路由器)上，如此往复多次才能到达自己的目的地。这是一个分组交换网络具体如何运作的方式。在大部分时间里，它的工作方式不像是在源和目的地之间存在一条直接的连接。但意外的是：每台计算机(路由器)会沿着路由强制要求数据包的大小会有一个最大的上限，这个上限就是所谓的最大传输单元。如果任意一个路由器收到一个数据包的大小超过这个最大传输单元的大小，它有这么两个选择，a)在IP层对这个数据包进行分包，并将分包后的数据包继续传递，b)丢弃这个数据包然后告诉你数据包被丢弃了，你自己负责摆平这个问题。这儿有一个我会经常遇到的情况。人们在编写多人在线游戏的时候，数据包的平均大小都会非常的小，让我们假设下，这些数据包的平均大小大概只有几百字节，但时不时会在他们的游戏中同时发生大量的事情并且发出去的数据包会出现丢失的情况，这个时候数据包会比通常的情况下要大。突然之间，游戏的数据包的大小就会超过最大传输单元的大小，这样就只有很少一部分玩家能够收到这个数据包，然后整个通信就崩溃了。就在去年(2015年)，我与亚历克斯·奥斯汀在Indiecade谈论他的游戏” Sub Rosa “中的网络部分。他遇到了一些奇怪的无法重现的网络bug。出于某种原因，一些客户端（在所有玩家里面总是有那么一个或者两个）会随机的从游戏中断开连接并且其他人都能够正常游戏。查看日志的话，亚历克斯又觉得一切都是正常的，只是看上去好像数据包突然停止进行传递了。对我来说，这听上去就像是一个最大传输单元所引起的问题，并且我非常确信。当亚历克斯把他最大的数据包大小限制在一个合理的值之内，这个bug就再也没有出现了。真实世界中的最大传输单元MTU所以什么是“一个合理的数据包的大小”？在今天的互联网上(2016年，还是基于IPv4)，典型的最大传输单元的大小是1500字节。在UDP/IP数据包的包头添加或者去掉几个字节，你会发现在数据包开始出现被丢弃或者被分包情况的一个典型的边界值是1472。你可以自己在MacOS X尝试运行下下面这个命令：ping-g 56 -G 1500 -h 10 -D 8.8.4.4 在我的机器上，这个结果在略微低于1500字节的大小上下浮动，符合预期：1404bytes from 8.8.4.4: icmp_seq=134 ttl=56 time=11.945 ms1414bytes from 8.8.4.4: icmp_seq=135 ttl=56 time=11.964 ms1424bytes from 8.8.4.4: icmp_seq=136 ttl=56 time=13.492 ms1434bytes from 8.8.4.4: icmp_seq=137 ttl=56 time=13.652 ms1444bytes from 8.8.4.4: icmp_seq=138 ttl=56 time=133.241 ms1454bytes from 8.8.4.4: icmp_seq=139 ttl=56 time=17.463 ms1464bytes from 8.8.4.4: icmp_seq=140 ttl=56 time=12.307 ms1474bytes from 8.8.4.4: icmp_seq=141 ttl=56 time=11.987 msping:sendto: Message too longping:sendto: Message too longRequesttimeout for icmp_seq 142 为什么是1500字节？这是MacOS X上默认的最大传输单元的大小。这也是Windows平台上默认的最大传输单元的大小。所以现在我们对数据包的大小有了一个上限（也就是不能超过这个默认的最大传输单元的大小），假如你真的关心数据包通过Windows平台或者Mac平台进行传播而不希望数据包在IP层进行分包或者有被丢弃的可能的话，那么就要保证数据包的大小不能超过这个上限：1472个字节。那么这个最大传输单元的大小的下限是多少呢？MacOS X允许设置的最大传输单元的大小的值是在1280到1500，所以我对现在互联网上通过IPv4进行传播的数据包的最大传输单元的大小的下限有一个比较保守的估计，就是1200字节。如果是在通过IPv4进行传播的情况下这个比较保守的大传输单元的大小的下限也会是一个很好的估计值，任意数据包只要大小小于1280字节都能保证在没有IP层分包的情况下顺利的传播。这个估计与我在职业生涯中感受到的情况是比较一致的。以我的经验来看，很少有游戏会试图做这些复杂的事情，诸如尝试发现路径上的最大传输单元的大小之类的，它们一般都是假定一个合理又保守的最大传输单元的大小然后一直遵循这个值。如果出现一个要发送的数据包比这个保守的最大传输单元的大小要大的情况，游戏协议会将这个数据包分成几个包然后在网络的另外一侧进行重组。而这正是我要在这篇文章里面要向你传授的内容。分包后的数据包的结构让我们从决定该对网络上传输的数据分包后采用什么的结构进行表示来开始构建我们的数据包分包和重组机制。在理想状态下，我们希望分包以后的数据包和未分包的数据包兼容我们现在已经建立好的数据包结构，这样当我们发送小于最大传输单元的大小的数据包的时候，网络协议没有任何多余的负载。下面是在前一篇文章结束的时候得到的数据包的结构：[protocol id] (64 bits) // not actually sent, but used to calc crc32[crc32] (32 bits)[packet type] (2 bits for 3 distinct packet types)(variable length packet data according to packet type)[end of packet serialize check] (32 bits)在我们这个例子之中，我们一共有三个数据包的类型，分别是Ａ、Ｂ和Ｃ。让我们用这三个数据包类型中的一个制造一个比最大传输单元的大小还要大一些的数据包：?1234567891011121314151617181920212223static const int MaxItems = 4096 * 4; struct TestPacketB : public protocol2::Packet{ int numItems; int items[MaxItems]; TestPacketB() : Packet( TEST_PACKET_B ) { numItems = random_int( 0, MaxItems ); for ( int i = 0; i &lt; numItems; ++i ) items[i] = random_int( -100, +100 ); } template &lt;typename stream&gt; bool Serialize( Stream &amp; stream ) { serialize_int( stream, numItems, 0, MaxItems ); for ( int i = 0; i &lt; numItems; ++i ) serialize_int( stream, items[i], -100, +100 ); return true; }};这可能看起来不怎么自然，但在现实世界中这些情况真的会发生。举个简单的例子来说，如果你有一个策略，这个策略是从服务器往客户端发送所有的未确认的事件，你会得到一组可信赖也有序的事件，但是也会遇到大量数据包的丢失的情况，你会轻易的遇到数据包比最大传输单元的大小还要大的情况，即使你的数据包的平均大小非常小。（译注：这是由于丢包重传导致的不停重发，而重发的数据包在UDP或者TCP上会进行合并。所以即使数据包的平均大小远小于最大传输单元的大小，但是由于大量这样的数据包的合并，还是很容易出现超过最大传输单元的大小的情况）。在大多数的情况下，通过实现这么一个策略：在一个数据包里面只包含很少一组事件或者状态更新来避免数据包的大小超过最大传输单元的大小，采用这种策略以后可以有效的规避上面的那种情况。这种规划在很多情况下都工作的很棒。。。但是有一种情况下这种策略也是存在问题的，这种情况就是增量编码。由一个增量编码器创建的数据包的大小是与当前状态与之前状态之间所发生的状态变化的数量成正比的。如果这两个状态之间有大量的不同的话，那么增量也将很大并且你对这种情况其实是没有什么办法的。如果出现一个增量恰好比最大传输单元的大小要大的情况，当然这是一个坏运气下才会出现的情况，但是你仍然要发送这个超过最大传输单元的大小的数据包！所以你可以看到，在增量编码的情况下，你真的不能限制数据包的最大大小一定小于最大传输单元的大小，在这种情况下，数据包的分包和重组策略就有了用武之地。让我们回到数据包的结构上面来。在每个数据包的包头的地方添加一个序号是一种非常常见的做法。这并没有什么复杂的。这只是一个数据包的序号，会在每个数据包进行发送的时候依次加一。举个例子来说，就是0、1、2、3这么简单。我喜欢用16比特来表示这个序号，即使在每秒发送60个数据包的情况下，只要15分钟序号就会被用完一遍，需要再次从头开始重用，但是这么做也没有什么关系，主要是因为你在网络上收到一个15分钟之前发送出去的数据包是一个非常罕见的事情，所以你很少会有机会困惑这到底是一个新的数据包还是一个旧的数据包（因为IP层在包头的地方有个跳转计数，超出一定跳转次数的数据包会被丢弃掉，所以基本不用担心这种情况）。如果你确实关心这个问题的话，请使用32比特的序号进行代替。无论你选择用多少比特来表示这个序号，它们对于很多事情都是有用的，比如说可依赖性、检测和丢弃乱序的数据包等等。除此之外，我们需要一个数据包序号的原因是在对数据包进行分包的时候，我们需要某个方法来确定这个数据包的分包到底是属于哪个数据包的。所以，让我们在我们的数据包的结构里面加上序号这个东西：[protocol id] (64 bits) // not actually sent, but used to calc crc32[crc32] (32 bits)[sequence] (16 bits)[packet type] (2 bits)(variable length packet data according to packet type)[end of packet serialize check] (32 bits)这是最有趣的部分。我们可以在数据包的头部只添加一个比特的标识 is_fragment，但是对于通常情况下根本不需要分包的数据包来说，你就浪费了一个比特，因为它总是要被置为0。这不是很好。相反，我的选择是在数据包的结构里面添加了一个特殊的”分包后的数据包“的类型：?12345678enum TestPacketTypes{ PACKET_FRAGMENT = 0, // RESERVED TEST_PACKET_A, TEST_PACKET_B, TEST_PACKET_C, TEST_PACKET_NUM_TYPES}; 这恰好不需要占据任何的空间，是因为四个数据包类型正好可以用两个比特来表示，而这两个比特的空间已经用于表示数据包类型了，我们只是在原来的枚举上新加了一个类型。这么处理以后，每次当我们读取一个数据包的时候，如果这个数据包的类型是0的话，我们就知道这个数据包是一个特殊的分包以后的数据包，它的内存布局可以通过数据包的类型得知，否则的话，我们就走回原来的通用的对非分包的数据包进行读取和处理的方法。让我们设计下这个分包后的数据包看起来应该是什么样子的。我们将允许一个数据包最多可以分成256个数据包，并且每个分包后的数据包的大小不会超过1024个字节。这样的话，我们就可以通过这样一个系统来发送大小最大为256k的数据包，这对于任意系统任意情况来说都应该是足够的，但是这只是我个人的一个看法，如果有特殊的情况，还请结合实际情况进行具体分析。有了这么一个不大的固定大小的数据包包头结构，再加上UDP的包头结构以及IP的包头结构，一个分包以后的数据包会小于之前我们保守估计的最大传输单元的大小：1200字节。除此之外，因为一个数据包最多可以分包成256个数据包，我们可以用【0，255】这个范围来表示分包的id和序号，这样每个分包里面还需要有8比特来表示这个序号。[protocol id] (32 bits) // not actually sent, but used to calc crc32[crc32] (32 bits)[sequence] (16 bits)[packet type = 0] (2 bits)[fragment id] (8 bits)[num fragments] (8 bits)[pad zero bits to nearest byte index]&lt;fragment data&gt;请注意，我们把这几个比特对齐到了下一个字节，然后才把对齐以后的数据写入到数据包里面。我们为什么要这么做？这么做其实是有两个原因的: 1) 这么处理以后可以通过memcpy函数更快的把分包的数据拷贝到数据包里面而不需要使用位打包器来对每个字节进行处理。2) 我们通过不发送分包数据的大小节省了一小部分带宽。我们可以通过从数据包的整体大小减去分包数据起始位置的字节序号来推断出这个分包的大小。发送分包以后的数据包发送分包以后的数据包是一件非常容易的事情。如果数据包的大小小于保守估计的最大传输单元的大小。那么就按正常的方法进行发送。否则的话，就计算这个数据包到底该分成多少个1024字节的数据包分包，然后构建这些分包并按照之前发送正常数据包的方法进行发送。发送出去以后也不记录发送的数据包的内容，这没什么困难的！这种发送以后不记录发送的数据包的内容的方法有一个后果，就是数据包的任意一个分包如果丢失的话，那么整个数据包就都要丢弃。由此可见，当你需要发送要给256K的数据包的时候要发送256个分包，如果有一个分包丢失的话，你就要重新把这个256k的数据包再分一次包然后再发送出去。这绝对不是一个好主意。这显然是一个很糟糕的办法，因为随着分包数目的增多，发生丢失的概率肯定是显著的增大。这种增长关系不是线性的，而是一种相当复杂的关系，如果你对这个计算感兴趣的话，你可以读下这篇文章。简而言之，如果你要计算一个数据包会被丢弃的概率，你必须要计算所有分包被成功发送到目的地的概率，然后从1中减去这个概率，得到的结果就是至少有一个分包丢失的概率。下面这个公式可以用来计算因为分包丢失导致整个数据包被丢弃的概率：＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼1- ( probability of fragment being delivered ) ^ num_fragments＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼＼ 让我们举个简单的例子对上面这个公式进行说明，如果我们发送的一个不需要分包的数据包，如果它在网络上传说的时候丢失的概率是1%，那么只有百分之一的概率会出现这个数据包被丢弃的情况，或者我们不要嫌麻烦，把这些数据代入到上面这个公式里面: 1 – (99/100) ^ 1 = 1/100 = 1%。随着分包数量的增加，整个数据包被丢弃的概率也随之增加：两个分包的情况： 1 – (99/100) ^ 2 = 2%。十个分包的情况： 1 – (99/100) ^ 10 = 9.5%。一百个分包的情况： 1 – (99/100) ^ 100 = 63.4%。二百五十六个分包的情况： 1 – (99/100) ^ 256 = 92.4%。所以我建议你要小心分包以后的数量。这个策略最好是只对2-4个分包的情况进行使用，而且最好是针对那种对时间不怎么敏感的数据使用或者是就算分包lost了也无所谓的情况。绝对不要只是为了省事就把一大堆依赖顺序的事件打到一个大数据包里面然后依赖数据包的分包和重组机制进行发送。这会让事情变得更加麻烦。大数据包的另外一种典型的用途是当客户端新连入一个游戏服务器的时候。游戏服务器通常会把大块的数据以一种可靠的方式下发给客户端。对于后期才加入的客户端而言，这些大块的数据也许代表了世界的初始状态。无论这些数据包含了怎么样的信息，请不要使用本篇文章的分包技巧来给客户端下发大块的数据。相反，请查阅这个系列教程的下篇文章，在那篇文章里面将讲解如何在有数据包可能发生丢失的情况下，快速可靠的发送大块的数据直到这些大块的数据完全被确认接收。对分包后的数据包的接收尽管发送分包以后的数据包是一件相同简单的事情，但是对分包后的数据包进行接收就相对需要技巧了。之所以对分包后的数据包进行接收很困难的原因是我们不仅需要给缓冲区建立一个数据结构还要把这些分包重组成原始的数据包，我们也要特别小心如果有人试图让我们的程序产生崩溃而给我们发送恶意的数据包。[protocol id] (32 bits) // not actually sent, but used to calc crc32[crc32] (32 bits)[sequence] (16 bits)[packet type = 0] (2 bits)[fragment id] (8 bits)[num fragments] (8 bits)[pad zero bits to nearest byte index]&lt;fragment data&gt;下面列出的是我能想到的如何攻击你的协议以便让你的服务器崩溃的所有方法：尝试发送分包ID在一个限制范围内的分包，看看能不能让你的程序崩溃。举个例子来说，也许这个数据包只有两个分包，但是我会发送多个分包ID在【０，２５５】之内的分包。发送一个数据包，让我们假设这个书包的最大分包数量是2，然后发送多个属于这个数据包的分包，但是在分包的数据包报头里面把最大分包的数量改为256，来希望你在接收完第一个分包以后，不会发现分包的最大数量信息被改变了，这样就有可能造成你的内存崩溃。发送非常大的分包，让分包里面的数据超过1k，测试下你在试图把分包数据拷贝到数据结构的时候是否有良好的判断，如果没有良好的判断的话，这也许会让你的内存崩溃，或者占用大量的内容让你在分配新的分包的时候没有空间。 持续的给你的程序发送最大分包数目的分包（也就是如果最大分包数目是256的话，就持续不断的发送分包ID是256的数据包），希望你会分配大量的内容来容纳这些分包然后让你的内存崩溃。让我们假设你的程序中有一个滑动窗口，这个滑动窗口有256个数据包，每个数据包最多可以有256个分包，每个分包预留的空间是1k。那么也就是会给每个客户端预留67,108,864字节或者64mb的空间。我可以通过这种方法让服务器崩溃么？我能用一堆大小有趣的分包来耗尽的你地址空间的heap空间么？因为服务器的程序是你来设计实现的，所以只有你才知道这个问题的确切答案。它取决于你的内存预算以及如何分配内存来存储分包。如果你考虑过了觉得这会是一个问题，那么就限制下缓冲区中分包的数目或考虑减少每个数据包的分包的最大数目。考虑给每个分包静态分配数据结果或者使用一个内存池来减少内存碎片。所以你可以看到，在接收端代码是非常脆弱的，要非常小心检查一切可能的情况。除此之外，还有一个非常简单的事情要注意：让分包保存在一个数据结构里面，当一个数据包的所有分包都到达以后（通过计数来判断是否全部到达），将这些分包重组成一个大的数据包，并把这个重组后的大数据包返回给接收方。什么样的数据结构在这里是有意义的?这里并没有什么特别的数据结构!我使用的是一种我喜欢称之为序列缓冲区的东西。我想和你分享的最核心的技巧是如何让这个数据结构变得高效：?12345678const int MaxEntries = 256; struct SequenceBuffer{ bool exists[MaxEntries]; uint16_t sequence[MaxEntries]; Entry entries[MaxEntries];};这里面有几件事情值得注意。首先，使用类似数组的结构可以允许对给定条目是否存在于一个序列缓冲区的存在性进行快速测试并且将测试结果进行缓存，即使每个数据包的条目结构是非常大的（而且这种结构对于数据包的分包和重组来说是非常棒的）。那么我们该如何使用这个数据结构？当你接收到一个分包以后的数据包以后，这个数据包会携带一个序号指定它是属于哪个数据包的分包。这个序号会随着发送而不停的增大(所有序号全部用完导致发生环绕除外)， 所以最关键的技巧是你要对序号进行散列让散列后的序号进入一个数组中某个给定的位置，具体处理过程如下所示：int index = sequence % MaxEntries; 现在你可以用O(1)的时间复杂度进行一个快速测试，来通过序号看下一个给定的条目是否存在，并且判断下这个条目是否和你想要读取或者写入的数据包序号相匹配。也就是说，你既需要测试存在性又需要测试序号是否是预期的序号，这是因为所有的序号都是有效的数字(比如说是0)，还有就是因为根据一个特定的数据包序号查找到的条目可能是存在的，但是它属于过去的一个序号（比如说，这是某个其他的序号，但是恰巧通过取模的计算得到相同的序号）。所以当一个新的数据包的第一个分包到达的时候，你需要把数据包的序号散列成一个索引，如果发现这个索引对应的内容还不存在的话，你需要设置exists[index] = 1，并设置sequence[index]来匹配你正在处理的数据包，并把这个分包储存在序号缓冲区对应的条目里面。这样当下一次有分包实际到达的时候，你会得到相同的序号，然后得到一个相当的索引，在查找的时候会发现对应这个索引的内容已经存在了，并且这个条目的序号正好能和刚刚接收到的数据包的序号匹配，所以你就可以把这个分包累加到这个条目上，这个过程会一直重复直到这个数据包的所有分包都被接收到为止。如果从比较抽象的层次来解释这个事情的话，基本原理大概就是这样的。这种方法的一个更完整的解释请参阅本文的示例源代码。在地址https://www.patreon.com/gafferongames可以获取本系列文章示例的源代码。网络协议的测试驱动开发还有一件事情我想在文章的末尾进行补充说明。我感觉如果我不向我的读者提及这个方法的话，就是对他们的一个伤害。编写一个定制的网络协议是非常困难的。这个过程是如此的苦难以至于我从头开始至少编写了10次网络协议，但是每次我都觉得我在用一种全新的有趣的方法在做这个事情。也许你会认为是我在挑战自己，用一些新奇的方法来实现这个过程，但其实是这个过程太复杂了，完全没有办法按照预期的那样写完代码就期待它们能够正确的工作。你需要对写出来的代码进行测试！当编写底层网络协议层的代码的时候，我的策略是:1、防御性编程。在一切可能的地方进行断言。当某些地方出现错误的时候这些断言会起作用，并且将成为查找问题的重要线索。2、添加函数的功能测试，确保它们是如你的预期那样工作的。把你的代码放到一个可以运行和测试的环境下，这样可以不时地对它们进行测试以便可以确保它一直会像你起初创建它们时候那样良好的工作。仔细考虑有哪些情况需要正确的处理并给这些情况添加测试。3、虽然函数测试非常有必要，但是只是添加一些函数测试是远远不够的。无论如何都有遇到你没有预料到的情况! 现在你必须把它们放到一个真实的环境下看看到底会发生什么。我把这个称之为浸泡测试，并且在我之前编写网络协议的过程中还从来没有过在浸泡测试的过程中没有发现问题的情况。浸泡测试只是在不停的循环，并会随机的做一些事情让你的系统在它的空间中处理一些情况，让我们举些简单的例子对它进行一些简单的说明，比如说构造出随机长度的数据包，而且这些数据包有一个非常高的丢失率，通过数据包模拟器发出的大量乱序并且重复的数据包等等。如果你的程序能够在一晚上的时间里面不挂起或者遇到断言，那么就算你的程序通过了浸泡测试。4、如果你的程序在浸泡测试的过程中发现了某些问题。你可能需要在代码里面添加一些详细的日志以便下次在浸泡测试的时候如果遇到了同样的问题你可以找到出现问题的原因。一旦你知道发生了什么，就可以停止了。不要立即的修复这个问题并且再次运行浸泡测试。这种做法非常的愚蠢。相反，利用单元测试来不停的重现你需要修复的问题，确保单元测试能够重现问题，而且这个问题因为你的修复已经彻底修好了。只有在这样的处理流程之后，才能回到浸泡测试并确保程序在浸泡测试能正常运转一整夜。通过这种方式，单元测试能够记录你的系统的正确的行为并且在以后需要的时候可以快速的运行起来，确保当你做其他改变的时候不会导致一些原来修过的问题重复的出现。这就是我的数据包分包和重组的处理流程了，它似乎工作的不错。如果你在进行一些底层的网络协议的设计，你的游戏的其他的部分将依赖于底层的网络协议的设计。在你继续构建其他的功能之前，你需要绝对的确认底层的网络协议是否能够正常的工作，否则就像一堆胡乱堆积的卡片，很容易就散架了。多人在线游戏的网络部分是非常困难的，如果不小心设计的话，很容易就会出现底层网络协议可能无法正常的工作或者存在缺陷。所以请确保你是知道你的底层网络协议是如何工作的！即将到来的文章的预告下一篇文章是: 《发送大块的数据》请继续阅读本系列的下一篇文章，在哪篇文章里面我将向你展示如何通过数据包快速可信赖的发送大块的数据，如果其中一块数据丢失了也不需要丢弃整个数据包！如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议二之序列化策略]]></title>
    <url>%2F2017%2F02%2F25%2Fserialization_strategies%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[自我总结本篇概要 读取数据的时候要特别小心， 因为可能会有攻击者发送过来的恶意的数据包以及错误的包， 在写入数据的时候你可能会轻松很多，因为如果有任何事情出错了，那几乎肯定是你自己导致的错误 统一的数据包序列化功能 ：诀窍在于让流类型的序列化函数模板化。在我的系统中有两个流类型：ReadStream类和WriteStream类。每个类都有相同的一套方法，但实际上它们没有任何关系。一个类负责从比特流读取值到变量中，另外一个类负责把变量的值写到流中。在模板里类似这样写, 通过 Stream::IsWriting 和 Stream::IsReading 模板会自动区分,然后帮你生产你想要的代码, 简洁漂亮 1234567891011121314151617181920212223242526272829303132333435363738class WriteStream&#123;public: enum &#123; IsWriting = 1 &#125;; enum &#123; IsReading = 0 &#125;; // ...&#125;;class ReadStream&#123;public: enum &#123; IsWriting = 0 &#125;; enum &#123; IsReading = 1 &#125;; // ..&#125;template &lt;typename Stream&gt;bool serialize( Stream &amp; stream, float &amp; value )&#123; union FloatInt &#123; float float_value; uint32_t int_value; &#125;; FloatInt tmp; if ( Stream::IsWriting ) tmp.float_value = value; bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) value = tmp.float_value; return result;&#125; 边界检查和终止读取 ： 把允许的大小范围也传给序列化函数而不仅仅是所需的比特数量。 序列化浮点数和向量 ： 计算机根本不知道内存中的这个32位的值到底是一个整数还是一个浮点数还是一个字符串的部分。它知道的就是这仅仅是一个32位的值。代码如下(可以通过一个联合体来访问看上去是整数的浮点数).有些时候，你并不想把一个完整精度的浮点数进行传递。那么该如何压缩这个浮点值？第一步是将它的值限制在某个确定的范围内然后用一个整数表示方式来将它量化。举个例子来说，如果你知道一个浮点类型的值是在区间[-10,+10]，对于这个值来说可以接受的精确度是0.01，那么你可以把这个浮点数乘以100.0让它的值在区间[-1000,+1000]并在网络上将其作为一个整数进行序列化。而在接收的那一端，仅仅需要将它除以100.0来得到最初的浮点值. 123456789union FloatInt&#123; float float_value; uint32_t int_value;&#125;; FloatInt tmp;tmp.float_value= 10.0f;printf(“float value as an integer: %x\n”, tmp.int_value ); 序列化字符串和数组 : 为什么要费那么大精力把一个字节数组按比特打包到你的比特流里?为什么不在序列化写入之前进行按字节进行对齐？Why not align to byte so you can memcpy the array of bytes directly into the packet?如何将比特流按字节对齐？只需要在流的当前位置做些计算就可以了，找出还差写入多少个比特就能让当前比特流的比特数量被8整除，然后按照这个数字插入填充比特（比如当前比特流的比特数量是323，那么323+5才能被8整除，所以需要插入5个填充比特）。对于填充比特来说，填充的比特值都是0，这样当你序列化读取的时候你可以进行检测，如果检测的结果是正确的，那么就确实是在读取填充的部分，并且填充的部分确实是0。一直读取到下一个完整字节的比特起始位置（可以被8整除的位置）。如果检测的结果是在应该填充的地方发现了非0的比特值，那么就中止序列化读取并丢弃这个数据包。 序列化数组的子集 : 当实现一个游戏网络协议的时候，或早或晚总会需要序列化一个对象数组然后在网络上传递。比如说服务器也许需要把所有的物体发送给客户端，或者有时候需要发送一组事件或者消息。如果你要发送所有的物体到客户端，这是相当简单直观的，但是如果你只是想发送一个数组的一个子集怎么办？最先想到也是最容易的办法是遍历数组的所有物体然后序列化一个bool数组，这个bool数组标记的是对应的物体是否通过网络发送。如果bool值为1那么后面会跟着物体的数据，否则就会被忽略然后下一个物体的bool值取决于流的下一个值。如果有大量的物体需要发送，举个例子来说，整个场景中有4000个物体，有一半的物体也就是2000个需要通过网络进行发送。每个物体需要一个序号，那么就需要2000个序号，每个序号需要12比特。。。。这就是说数据包里面24000比特或者说接近30000比特（几乎是30000，不是严格是，译注：原文如此）的数据被序号浪费掉了.可以把序号的编码方式修改下来节省数据，序号不再是全局序号，而是相对上一个物体的相对序号。 如何应对恶意数据包和错误包 : 如果某些人发送一些包含随机信息的恶意数据包给你的服务器。你会不会在解析的时候把服务器弄崩溃掉？有三种技术应对 : 协议ID : 在你的数据包里面包含协议ID。一般典型的做法是，头4个字节你可以设定一些比较罕见而且独特的值，你可以通过这３２比特的数据判断出来根本就不是你的应用程序的包，然后就可以直接丢弃了。 CRC32 : 对你的数据包整体做一个CRC32的校验，并把这个校验码放到数据包的包头。可以不发送这个协议ID，但是发送方和接收方提前确认过这个协议ID是什么，并在计算数据包CRC32值的时候装作这个数据包带上了这个协议ID的前缀来参与计算。这样如果发送方使用的协议ID与接收方不一致的时候，CRC32的校验就会失败，这将为每个数据包节省4个字节. 序列化检测 : 是在包的中间，在一段复杂的序列化写入之前或者之后写上一个已知的32比特整数，并在另外一端序列化读取的时候用相同的值进行检测判断。如果序列化检查值是不正确的，那么就中止序列化读取并丢弃这个数据包。. . . 原文原文出处 原文标题 : Serialization Strategies (Smart tricks that unify packet read and write) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.In the previous article, we created a bitpacker but it required manual checking to make sure reading a packet from the network is safe. This is a real problem because the stakes are particularly high - a single missed check creates a vulnerability that an attacker can use to crash your server.In this article, we&rsquo;re going to transform the bitpacker into a system where this checking is automatic. We&rsquo;re going to do this with minimal runtime overhead, and in such a way that we don&rsquo;t have to code separate read and write functions, performing both read and write with a single function.This is called a serialize function.Serializing BitsLet&rsquo;s start with the goal. Here&rsquo;s where we want to end up:struct PacketA{ int x,y,z; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_bits( stream, x, 32 ); serialize_bits( stream, y, 32 ); serialize_bits( stream, z, 32 ); return true; }};Above you can see a simple serialize function. We serialize three integer variables x,y,z with 32 bits each.struct PacketB{ int numElements; int elements[MaxElements]; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_int( stream, numElements, 0, MaxElements ); for ( int i = 0; i &lt; numElements; ++i ) { serialize_bits( buffer, elements[i], 32 ); } return true; }};And now something more complicated. We serialize a variable length array, making sure that the array length is in the range [0,MaxElements].Next, we serialize a rigid body with an simple optimization while it&rsquo;s at rest, serializing only one bit in place of linear and angular velocity:struct RigidBody{ vec3f position; quat4f orientation; vec3f linear_velocity; vec3f angular_velocity; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_vector( stream, position ); serialize_quaternion( stream, orientation ); bool at_rest = Stream::IsWriting ? ( velocity.length() == 0 ) : 1; serialize_bool( stream, at_rest ); if ( !at_rest ) { serialize_vector( stream, linear_velocity ); serialize_vector( stream, angular_velocity ); } else if ( Stream::IsReading ) { linear_velocity = vec3f(0,0,0); angularvelocity = vec3f(0,0,0); } return true; }};Notice how we&rsquo;re able to branch on Stream::IsWriting and Stream::IsReading to write code for each case. These branches are removed by the compiler when the specialized read and write serialize functions are generated.As you can see, serialize functions are flexible and expressive. They&rsquo;re also safe, with each serialize call performing checks and aborting read if anything is wrong (eg. a value out of range, going past the end of the buffer). Most importantly, this checking is automatic, so you can&rsquo;t forget to do it!Implementation in C++The trick to making this all work is to create two stream classes that share the same interface: ReadStream and WriteStream.The write stream implementation writes values using the bitpacker:class WriteStream{public: enum { IsWriting = 1 }; enum { IsReading = 0 }; WriteStream( uint8_t buffer, int bytes ) : m_writer( buffer, bytes ) {} bool SerializeInteger( int32_t value, int32_t min, int32_t max ) { assert( min &lt; max ); assert( value &gt;= min ); assert( value &lt;= max ); const int bits = bits_required( min, max ); uint32_t unsigned_value = value - min; m_writer.WriteBits( unsigned_value, bits ); return true; } // …private: BitWriter m_writer;};And the read stream implementation reads values in:class ReadStream{public: enum { IsWriting = 0 }; enum { IsReading = 1 }; ReadStream( const uint8_t buffer, int bytes ) : m_reader( buffer, bytes ) {} bool SerializeInteger( int32_t &amp; value, int32_t min, int32_t max ) { assert( min &lt; max ); const int bits = bits_required( min, max ); if ( m_reader.WouldReadPastEnd( bits ) ) { return false; } uint32_t unsigned_value = m_reader.ReadBits( bits ); value = (int32_t) unsigned_value + min; return true; } // …private: BitReader mreader;};With the magic of C++ templates, we leave it up to the compiler to specialize the serialize function to the stream class passed in, producing optimized read and write functions.To handle safety serialize calls are not actually functions at all. They&rsquo;re actually macros that return false on error, thus unwinding the stack in case of error, without the need for exceptions.For example, this macro serializes an integer in a given range:#define serialize_int( stream, value, min, max ) \ do \ { \ assert( min &lt; max ); \ int32_t int32_value; \ if ( Stream::IsWriting ) \ { \ assert( value &gt;= min ); \ assert( value &lt;= max ); \ int32_value = (int32_t) value; \ } \ if ( !stream.SerializeInteger( int32_value, min, max ) ) \ { \ return false; \ } \ if ( Stream::IsReading ) \ { \ value = int32_value; \ if ( value &lt; min || value &gt; max ) \ { \ return false; \ } \ } \ } while (0)If a value read in from the network is outside the expected range, or we read past the end of the buffer, the packet read is aborted.Serializing Floating Point ValuesWe&rsquo;re used to thinking about floating point numbers as being different to integers, but in memory they&rsquo;re just a 32 bit value like any other.The C++ language lets us work with this fundamental property, allowing us to directly access the bits of a float value as if it were an integer:union FloatInt{ float float_value; uint32_t int_value;};FloatInt tmp;tmp.float_value = 10.0f;printf( “float value as an integer: %x\n”, tmp.int_value );You may prefer to do this with an aliased uint32_t pointer, but this breaks with GCC -O2. Friends of mine point out that the only truly standard way to get the float as an integer is to cast a pointer to the float value to char and reconstruct the integer from the bytes values accessed through the char pointer.Meanwhile in the past 5 years I&rsquo;ve had no problems in the field with the union trick. Here&rsquo;s how I use it to serialize an uncompressed float value:template &lt;typename Stream&gt;bool serialize_float_internal( Stream &amp; stream, float &amp; value ){ union FloatInt { float float_value; uint32_t int_value; }; FloatInt tmp; if ( Stream::IsWriting ) { tmp.float_value = value; } bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) { value = tmp.float_value; } return result;}This is of course wrapped with a serialize_float macro for error checking:#define serialize_float( stream, value ) \ do \ { \ if ( !serialize_float_internal( stream, value ) ) \ { \ return false; \ } } while (0)We can now transmit full precision floating point values over the network.But what about situations where you don&rsquo;t need full precision? What about a floating point value in the range [0,10] with an acceptable precision of 0.01? Is there a way to send this over the network using less bits?Yes there is. The trick is to simply divide by 0.01 to get an integer in the range [0,1000] and send that value over the network. On the other side, convert back to a float by multiplying by 0.01.Here&rsquo;s a general purpose implementation of this basic idea:template &lt;typename Stream&gt;bool serialize_compressed_float_internal( Stream &amp; stream, float &amp; value, float min, float max, float res ){ const float delta = max - min; const float values = delta / res; const uint32_t maxIntegerValue = (uint32_t) ceil( values ); const int bits = bits_required( 0, maxIntegerValue ); uint32_t integerValue = 0; if ( Stream::IsWriting ) { float normalizedValue = clamp( ( value - min ) / delta, 0.0f, 1.0f ); integerValue = (uint32_t) floor( normalizedValue maxIntegerValue + 0.5f ); } if ( !stream.SerializeBits( integerValue, bits ) ) { return false; } if ( Stream::IsReading ) { const float normalizedValue = integerValue / float( maxIntegerValue ); value = normalizedValue delta + min; } return true;}Of course we need error checking, so we wrap this with a macro:#define serialize_compressed_float( stream, value, min, max ) \ do \ { \ if ( !serialize_float_internal( stream, value, min, max ) ) \ { \ return false; \ } \ } while (0)And now the basic interface is complete. We can serialize both compressed and uncompressed floating point values over the network.Serializing Vectors and QuaternionsOnce you can serialize float values it&rsquo;s trivial to serialize vectors over the network. I use a modified version of the vectorial library in my projects and implement serialization for its vector type like this:template &lt;typename Stream&gt;bool serialize_vector_internal( Stream &amp; stream, vec3f &amp; vector ){ float values[3]; if ( Stream::IsWriting ) { vector.store( values ); } serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); if ( Stream::IsReading ) { vector.load( values ); } return true;}#define serialize_vector( stream, value ) \ do \ { \ if ( !serialize_vector_internal( stream, value ) ) \ { \ return false; \ } \ } \ while(0)If your vector is bounded in some range, then you can compress it:template &lt;typename Stream&gt;bool serialize_compressed_vector_internal( Stream &amp; stream, vec3f &amp; vector, float min, float max, float res ){ float values[3]; if ( Stream::IsWriting ) { vector.store( values ); } serialize_compressed_float( stream, values[0], min, max, res ); serialize_compressed_float( stream, values[1], min, max, res ); serialize_compressed_float( stream, values[2], min, max, res ); if ( Stream::IsReading ) { vector.load( values ); } return true;}Notice how we are able to build more complex serialization using the primitives we&rsquo;re already created. Using this approach you can easily extend the serialization to support anything you need.Serializing Strings and ArraysWhat if you need to serialize a string over the network?Is it a good idea to send a string over the network with null termination? Not really. You&rsquo;re just asking for trouble! Instead, serialize the string as an array of bytes with the string length in front. Therefore, in order to send a string over the network, we have to work out how to send an array of bytes.First observation. Why waste effort bitpacking an array of bytes into your bit stream just so they are randomly shifted by [0,7] bits? Why not align to byte so you can memcpy the array of bytes directly into the packet?To align a bitstream just work out your current bit index in the stream and how many bits of padding are needed until the current bit index divides evenly into 8, then insert that number of padding bits. For bonus points, pad up with zero bits to add entropy so that on read you can verify that yes, you are reading a byte align and yes, it is indeed padded up with zero bits to the next whole byte bit index. If a non-zero bit is discovered in the padding, abort serialize read and discard the packet.Here&rsquo;s my code to align a bit stream to byte:void BitWriter::WriteAlign(){ const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) { uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); }}bool BitReader::ReadAlign(){ const int remainderBits = m_bitsRead % 8; if ( remainderBits != 0 ) { uint32_t value = ReadBits( 8 - remainderBits ); assert( m_bitsRead % 8 == 0 ); if ( value != 0 ) return false; } return true;}#define serialize_align( stream ) \ do \ { \ if ( !stream.SerializeAlign() ) \ return false; \ } while (0)Now we can align to byte prior to writing an array of bytes, letting us use memcpy for the bulk of the array data. The only wrinkle is because the bitpacker works at the word level, it&rsquo;s necessary to have special handling for the head and tail portions. Because of this, the code is quite complex and is omitted for brevity. You can find it in the sample code for this article.The end result of all this is a serialize_bytes primitive that we can use to serialize a string as a length followed by the string data, like so:template &lt;typename Stream&gt;bool serialize_string_internal( Stream &amp; stream, char string, int buffer_size ){ uint32_t length; if ( Stream::IsWriting ) { length = strlen( string ); assert( length &lt; buffer_size - 1 ); } serialize_int( stream, length, 0, buffer_size - 1 ); serialize_bytes( stream, (uint8_t)string, length ); if ( Stream::IsReading ) { string[length] = ‘\0’; }}#define serialize_string( stream, string, buffer_size ) \do \{ \ if ( !serialize_string_internal( stream, \ string, \ buffer_size ) ) \ { \ return false; \ } \} while (0)This is an ideal string format because it lets us quickly reject malicious data, vs. having to scan through to the end of the packet searching for &lsquo;\0&rsquo; before giving up. This is important because otherwise protocol level attacks could be crafted to degrade your server&rsquo;s performance by making it do extra work.Serializing Array SubsetsWhen implemeting a game network protocol, sooner or later you need to serialize an array of objects over the network. Perhaps the server needs to send object state down to the client, or there is an array of messages to be sent.This is straightforward if you are sending all objects in the array - just iterate across the array and serialize each object in turn. But what if you want to send a subset of the array?The simplest approach is to iterate across all objects in the array and serialize a bit per-object if that object is to be sent. If the value of the bit is 1 then the object data follows in the bit stream, otherwise it&rsquo;s ommitted:template &lt;typename Stream&gt;bool serialize_scene_a( Stream &amp; stream, Scene &amp; scene ){ for ( int i = 0; i &lt; MaxObjects; ++i ) { serialize_bool( stream, scene.objects[i].send ); if ( !scene.objects[i].send ) { if ( Stream::IsReading ) { memset( &amp;scene.objects[i], 0, sizeof( Object ) ); } continue; } serialize_object( stream, scene.objects[i] ); } return true;}This approach breaks down as the size of the array gets larger. For example, for an array size of size 4096, then 4096 / 8 = 512 bytes spent on skip bits. That&rsquo;s not good. Can we switch it around so we take overhead propertional to the number of objects sent instead of the total number of objects in the array?We can but now, we&rsquo;ve done something interesting. We&rsquo;re walking one set of objects in the serialize write (all objects in the array) and are walking over a different set of objects in the serialize read (subset of objects sent).At this point the unified serialize function concept starts to breaks down, and in my opinion, it&rsquo;s best to separate the read and write back into separate functions, because they have so little in common:bool write_scene_b( WriteStream &amp; stream, Scene &amp; scene ){ int num_objects_sent = 0; for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( scene.objects[i].send ) num_objects_sent++; } write_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) { continue; } write_int( stream, i, 0, MaxObjects - 1 ); write_object( stream, scene.objects[i] ); } return true;}bool read_scene_b( ReadStream &amp; stream, Scene &amp; scene ){ memset( &amp;scene, 0, sizeof( scene ) ); int num_objects_sent; read_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; num_objects_sent; ++i ) { int index; read_int( stream, index, 0, MaxObjects - 1 ); read_object( stream, scene.objects[index] ); } return true;}One more point. The code above walks over the set of objects twice on serialize write. Once to determine the number of changed objects and a second time to actually serialize the set of changed objects. Can we do it in one pass instead? Absolutely! You can use another trick, rather than serializing the # of objects in the array up front, use a sentinel value to indicate the end of the array:bool write_scene_c( WriteStream &amp; stream, Scene &amp; scene ){ for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) { continue; } write_int( stream, i, 0, MaxObjects ); write_object( stream, scene.objects[i] ); } write_int( stream, MaxObjects, 0, MaxObjects ); return true;}bool read_scene_c( ReadStream &amp; stream, Scene &amp; scene ){ memset( &amp;scene, 0, sizeof( scene ) ); while ( true ) { int index; read_int( stream, index, 0, MaxObjects ); if ( index == MaxObjects ) { break; } read_object( stream, scene.objects[index] ); } return true;}The above technique works great if the objects sent are a small percentage of total objects. But what if a large number of objects are sent, lets say half of the 4000 objects in the scene. That&rsquo;s 2000 object indices with each index costing 12 bits&hellip; that&rsquo;s 24000 bits or 3000 bytes (almost 3k!) in your packet wasted on indexing.You can reduce this overhead by encoding each object index relative to the previous object index. Think about it, you&rsquo;re walking from left to right along an array, so object indices start at 0 and go up to MaxObjects - 1. Statistically speaking, you&rsquo;re quite likely to have objects that are close to each other and if the next index is +1 or even +10 or +30 from the previous one, on average, you&rsquo;ll need quite a few less bits to represent that difference than an absolute index.Here&rsquo;s one way to encode the object index as an integer relative to the previous object index, while spending less bits on statistically more likely values:template &lt;typename Stream&gt;bool serialize_object_index_internal( Stream &amp; stream, int &amp; previous, int &amp; current ){ uint32_t difference; if ( Stream::IsWriting ) { assert( previous &lt; current ); difference = current - previous; assert( difference &gt; 0 ); } // +1 (1 bit) bool plusOne; if ( Stream::IsWriting ) { plusOne = difference == 1; } serialize_bool( stream, plusOne ); if ( plusOne ) { if ( Stream::IsReading ) { current = previous + 1; } previous = current; return true; } // [+2,5] -&gt; [0,3] (2 bits) bool twoBits; if ( Stream::IsWriting ) { twoBits = difference &lt;= 5; } serialize_bool( stream, twoBits ); if ( twoBits ) { serialize_int( stream, difference, 2, 5 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [6,13] -&gt; [0,7] (3 bits) bool threeBits; if ( Stream::IsWriting ) { threeBits = difference &lt;= 13; } serialize_bool( stream, threeBits ); if ( threeBits ) { serialize_int( stream, difference, 6, 13 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [14,29] -&gt; [0,15] (4 bits) bool fourBits; if ( Stream::IsWriting ) { fourBits = difference &lt;= 29; } serialize_bool( stream, fourBits ); if ( fourBits ) { serialize_int( stream, difference, 14, 29 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [30,61] -&gt; [0,31] (5 bits) bool fiveBits; if ( Stream::IsWriting ) { fiveBits = difference &lt;= 61; } serialize_bool( stream, fiveBits ); if ( fiveBits ) { serialize_int( stream, difference, 30, 61 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [62,125] -&gt; [0,63] (6 bits) bool sixBits; if ( Stream::IsWriting ) { sixBits = difference &lt;= 125; } serialize_bool( stream, sixBits ); if ( sixBits ) { serialize_int( stream, difference, 62, 125 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true; } // [126,MaxObjects+1] serialize_int( stream, difference, 126, MaxObjects + 1 ); if ( Stream::IsReading ) { current = previous + difference; } previous = current; return true;}template &lt;typename Stream&gt;bool serialize_scene_d( Stream &amp; stream, Scene &amp; scene ){ int previous_index = -1; if ( Stream::IsWriting ) { for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) { continue; } write_object_index( stream, previous_index, i ); write_object( stream, scene.objects[i] ); } write_object_index( stream, previous_index, MaxObjects ); } else { while ( true ) { int index; read_object_index( stream, previous_index, index ); if ( index == MaxObjects ) { break; } read_object( stream, scene.objects[index] ); } } return true;}But what about the worst case? Won&rsquo;t we spent more bits when indices are &gt;= +126 apart than on an absolute index? Yes we do, but how many of these worst case indices fit in an array of size 4096? Just 32. It&rsquo;s nothing to worry about.Protocol IDs, CRC32 and Serialization ChecksWe are nearly at the end of this article, and you can see by now that we are sending a completely unattributed binary stream. It&rsquo;s essential that read and write match perfectly, which is of course why the serialize functions are so great, it&rsquo;s hard to desync something when you unify read and write.But accidents happen, and when they do this system can seem like a stack of cards. What if you somehow desync read and write? How can you debug this? What if somebody tries to connect to your latest server code with an old version of your client?One technique to protect against this is to include a protocol id in your packet. For example, it could be a combination of a unique number for your game, plus the hash of your protocol version and a hash of your game data. Now if a packet comes in from an incompatible game version, it&rsquo;s automatically discarded because the protocol ids don&rsquo;t match:[protocol id] (64bits)(packet data)The next level of protection is to pass a CRC32 over your packet and include that in the header. This lets you pick up corrupt packets (these do happen, remember that the IP checksum is just 16 bits&hellip;). Now your packet header looks like this:[protocol id] (64bits)[crc32] (32bits)(packet data)At this point you may be wincing. Wait. I have to take 8+4 = 12 bytes of overhead per-packet just to implement my own checksum and protocol id? Well actually, you don&rsquo;t. You can take a leaf out of how IPv4 does their checksum, and make the protocol id a magical prefix.This means you don&rsquo;t actually send it, and rely on the fact that if the CRC32 is calculated as if the packet were prefixed by the protocol id, then the CRC32 will be incorrect if the sender does not have the same protocol id as the receiver, thus saving 8 bytes per-packet:[protocol id] (64bits) // not actually sent, but used to calc crc32[crc32] (32bits)(packet data)One final technique, perhaps as much a check against programmer error on your part and malicious senders (although redundant once you encrypt and sign your packet) is the serialization check. Basically, somewhere mid-packet, either before or after a complicated serialization section, just write out a known 32 bit integer value, and check that it reads back in on the other side with the same value. If the serialize check value is incorrect abort read and discard the packet.I like to do this between sections of my packet as I write them, so at least I know which part of my packet serialization has desynced read and write as I&rsquo;m developing my protocol. Another cool trick I like to use is to always serialize a protocol check at the very end of the packet, to detect accidental packet truncation (which happens more often than you would think).Now the packet looks something like this:[protocol id] (64bits) // not actually sent, but used to calc crc32[crc32] (32bits)(packet data)[end of packet serialize check] (32 bits)This is great packet structure to use during development. 译文 注意 ：这篇译文对应的是下面的原作者原文旧版本。 译文出处 译者：崔嘉艺（milan21） 审校：陈敬凤(nunu) 在这个系列文章中，我将完全从头开始构建一个专业级别的客户端/服务器游戏网络协议，只使用了C++编译器和一组UDP套接字。如果你正在寻找一个关于如何实现你自己的游戏网络协议方面的详细、实用实现，那么这个系列的文章对你来说就再适合不过了。大家好，我是Glenn Fiedler，欢迎阅读《构建游戏网络协议》系列教程的第二篇文章。在前面的文章里，我们讨论了在多人在线网络游戏里面读取和写入网络包的不同方法。我们很快就否决了通过文本的格式比如XML和JSON来发送游戏状态的办法因为它们确实在效率上存在比较大的问题，因此我们决定用自定义的二进制格式进行代替。我们实现了一个位打包器(bitpacker)，所以我们无需手动将几个布尔变量聚成一个8位比特值(以便为了节省空间)，也无需考虑大端小端问题，可以每次写入一个完整的单词而不需要将单词拆成一个个字符，再考虑如何用字节表示它们，这使得位打包器既非常简单也工作的非常快，也无需考虑与平台有关的细节。 但是我们仍然遗留了以下这些问题需要解决：1. 我们需要实现一个方法来判断整数值是否超出预期范围，如果超出了就要中止网络包的读取和解析，因为会有一些不怀好意的人给我们发送恶意网络包希望我们的程序和内存崩溃掉。网络包的读取和解析的中止必须是自动化的，而且不能使用异常处理，因为异常处理太慢了会拖累我们的程序。2. 如果独立的读取和写入函数是手动编解码的，那么维护它们真的是一个噩梦。我们希望能够为包一次性的编写好序列化代码并且没有任何运行时的性能消耗（主要是额外的分支、虚化等等）。我们应该如何实现上面的这些目标? 请继续阅读，我将向你展示如何用Ｃ＋＋来实现这些功能。开发和完善这些技术花费了我不少时间，所以我希望这些内容对你来说是有帮助的，至少是一个很好的选择值得考虑是否要替换你目前采用的方案，或者可以与你在其他游戏看到的这个问题解决方案相结合，看是否能得到更好的解决方案。 统一的数据包序列化功能让我们从我们的目标开始。这就是我们在本文结束的时候希望得到的东西：?1234567891011121314151617181920212223242526272829303132333435363738394041struct PacketA{ int x,y,z; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_bits( stream, x, 32 ); serialize_bits( stream, y, 32 ); serialize_bits( stream, z, 32 ); return true; }}; struct PacketB{ int numElements; int elements[MaxElements]; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_int( stream, numElements, 0, MaxElements ); for ( int i = 0; i &lt; numElements; ++i ) serialize_bits( buffer, elements[i], 32 ); return true; }}; struct PacketC{ bool x; short y; int z; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_int( stream, x, 8 ); serialize_int( stream, y, 16 ); serialize_int( stream, z, 32 ); return true; }};看下上面的代码片段，可以看到每个数据包结构里面都只有一个单独的序列化函数，而不是有互相独立的序列化读取和序列化写入函数。这非常的棒！它把整个序列化代码一分为二，你可能需要做很多努力来实现序列化读取和写入（因为读取的过程是数据解析并装入本地内存，写入的的过程是将本地的数据写到消息体，会有比较大的差异，所以代码基本是一分为二，一半用于读取，一半用于写入）。如果要让这项工作变得有效，诀窍在于让流类型的序列化函数模板化。在我的系统中有两个流类型：ReadStream类和WriteStream类。每个类都有相同的一套方法，但实际上它们没有任何关系。一个类负责从比特流读取值到变量中，另外一个类负责把变量的值写到流中。ReadStream和WriteStream只是上一篇文章中BitReader和BitWriter类的一个高层次封装。当然也有其他方法可以用来代替。如果你不喜欢用模板的话，你可以使用一个纯虚的基类作为流的接口，然后分别实现读取和写入类来实现这个流接口。但是如果你这么做的话，就要在每个序列化调用的时候发生了一次虚函数调用。这种方法对我来说开销似乎比较大。实现这个功能的另外一种方法是实现一个超级棒的流类型，可以通过配置而在运行时进入读取或者写入模式。这种方法会比虚函数方法快一些，但是仍然会在每次序列化调用的时候存在分支判断到底应该是读还是写，所以它不如硬编码读取和写入函数那么快。我更喜欢模板方法，因为它可以让编译器为项目产生经过优化的读取/写入函数。你甚至可以把序列化代码也这样实现以便让编译器为特定的读取和写入优化一大堆东西：?123456789101112131415161718192021222324252627282930struct RigidBody{ vec3f position; quat3f orientation; vec3f linear_velocity; vec3f angular_velocity; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_vector( stream, position ); serialize_quaternion( stream, orientation ); bool at_rest = Stream::IsWriting ? velocity.length() == 0 : 1; serialize_bool( stream, at_rest ); if ( !at_rest ) { serialize_vector( stream, linear_velocity ); serialize_vector( stream, angular_velocity ); } else if ( Stream::IsReading ) { linear_velocity = vec3f(0,0,0); angular_velocity = vec3f(0,0,0); } return true; }};虽然这看起来很没有效率，但是实际上并不是！这个函数经过模板特化以后会根据流的类型优化了所有分支。这很整齐漂亮吧？而ReadStream和WriteStream是这样的 ：1234567891011121314151617class WriteStream&#123;public: enum &#123; IsWriting = 1 &#125;; enum &#123; IsReading = 0 &#125;; // ...&#125;;class ReadStream&#123;public: enum &#123; IsWriting = 0 &#125;; enum &#123; IsReading = 1 &#125;; // ..&#125;边界检查和终止读取现在我们通过编译器实现了生成优化过的序列化读取/写入函数，我们还需要一些方法来做序列化读取时候的自动错误检测以便让我们不受恶意网络包的影响。我们要做的第一件事情是把允许的大小范围也传给序列化函数而不仅仅是所需的比特数量。试想一下如果有了最小和最大的范围，序列化函数就能自己算出所需的比特数量：serialize_int(stream, numElements, 0, MaxElements );这种做法开辟了一类新的方法，可以非常容易地支持带符号整数的序列化并且序列化函数可以检测从网络中读取的值并确保这个值一定在期望的范围内。如果这个值超出范围了，就立即中止序列化读取并丢弃这个数据包。因为我们没有办法使用异常来处理这种中止（因为异常太慢了），所以上面的方式是我比较喜欢的处理方式。在我的环境中，serialize_int其实并不是一个函数，它实际是一个如下面代码所示的宏：?1234567891011121314151617181920#define serialize_int( stream, value, min, max) \ do \ { \ assert( min &lt; max); \ int32_tint32_value; \ if ( Stream::IsWriting) \ { \ assert( value &gt;= min); \ assert( value &lt;= max); \ int32_value = (int32_t)value; \ } \ if ( !stream.SerializeInteger( int32_value, min, max ) ) \ return false; \ if ( Stream::IsReading) \ { \ value =int32_value; \ if ( value &lt; min || value &gt; max) \ return false; \ } \ } while (0)我让人觉得恐怖害怕的原因是我竟然使用了宏来插入代码来检测SerializeInteger函数的结果以及在发生错误的时候返回false。这会让人感觉到这种行为和异常处理很像，它会在出错的时候回溯堆栈到序列化调用堆栈的最顶上，但是这种处理不会带来任何的问题比如性能的消耗。在回溯的时候出现分支是非常罕见的（序列化错误非常少见）所以分支预测应该不会带来什么性能上的问题。还有一种情况我们也需要中止序列化读取：如果流读取超出了结尾。这种情况其实也是非常罕见的，但是我们必须在每次序列化操作都进行这个检查，这是因为流读取超出结尾会造成的影响是未定义的（也就是说我们对于它能造成什么样子的结果完全是未知的，最糟糕的情况并不是代码崩溃，而是把我们的内容数据完全搞乱了，相信我，你会无比痛恨这件事情）。如果我们没有做这个检测，可能会出现程序无限循环的情况，因为读取的位置超出了缓冲区的结尾。虽然在读取的时候如果发现超出比特流结尾的时候返回0值是很常见的做法（如以前的文章提到的那样），但是返回0值也不能保证序列化函数在有循环的时候能够正确的中止。如果要确保程序是有良好定义的行为，那么这种缓冲溢出检测总是必须的。最后一点，在序列化写入的时候如果遇到范围检测失败或者写入的地址超出流的结尾的时候，我并没有采用中止这种做法。在写入数据的时候你可能会轻松很多，因为如果有任何事情出错了，那几乎肯定是你自己导致的错误。在序列化写入的时候我们只是对每个序列化写入做了断言来确保一切是符合预期的（在范围内、写入的地址没有超出流的结尾），其他的一切都任由你来发挥。序列化浮点数和向量这个比特流现在只序列化类型整数的值。如果我们要序列化一个类型为浮点数的值，我们该怎么做？我们的做法虽然看上去有点投机取巧但实际上并不是。在内存中浮点数也是像整数那样保存成一个32位的值。你的计算机根本不知道内存中的这个32位的值到底是一个整数还是一个浮点数还是一个字符串的部分。它知道的就是这仅仅是一个32位的值。幸运的是，C++语言使得我们可以直接对这个基础属性进行控制（其他语言不行，因为底层被封装掉了，这也是C++被认为不好的地方之一，很多现代语言都禁止了这种做法）。你可以通过一个联合体来访问看上去是整数的浮点数：?123456789union FloatInt{ float float_value; uint32_t int_value;}; FloatInt tmp;tmp.float_value= 10.0f;printf(“float value as an integer: %x\n”, tmp.int_value );你也可以通过别名uint32_t的指针来做到这一点，但是因为GCC -O2会导致这种做法有些性能问题，所以我更倾向于使用联合体这种做法。我的朋友们指出（很有可能是正确的）从一个整数类型的值转换到浮点值的唯一真正标准的做法是将浮点数指针转换成uint8_t指针然后通过这个字节指针来分别引用4个字节来对这个值进行重建。虽然这对我来说似乎是一个非常愚蠢的做法。女士们，先生们。。。这毕竟是C++啊！（作者的意思是C++提供了很多接触底层的方法，我们可以尽情利用这一优势，只要能保证结果是正确的就可以了，哪怕使用一些取巧的办法也无所谓！）。 与此同时，在过去的5年里，在使用联合体这个技巧方面我还没有遇到过什么问题。下面是我如何序列化一个未压缩的浮点值：?123456789101112131415161718192021template &lt;typename Stream&gt;bool serialize_float_internal( Stream &amp; stream, float &amp; value ){ union FloatInt { float float_value; uint32_t int_value; }; FloatInt tmp; if ( Stream::IsWriting ) tmp.float_value = value; bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) value = tmp.float_value; return result;}通过一个serialize_float宏来包装这个部分以方便在序列化读取的时候方便进行一致的错误检测：?123456#define serialize_float( stream, value) \ do \ { \ if ( !protocol2::serialize_float_internal( stream, value )) \ return false; \ } while(0)有些时候，你并不想把一个完整精度的浮点数进行传递。那么该如何压缩这个浮点值？第一步是将它的值限制在某个确定的范围内然后用一个整数表示方式来将它量化。举个例子来说，如果你知道一个浮点类型的值是在区间[-10,+10]，对于这个值来说可以接受的精确度是0.01，那么你可以把这个浮点数乘以100.0让它的值在区间[-1000,+1000]并在网络上将其作为一个整数进行序列化。而在接收的那一端，仅仅需要将它除以100.0来得到最初的浮点值。下面是这个概念用序列化实现的版本：?12345678910111213141516171819202122232425262728293031323334template &lt;typename Stream&gt;boolserialize_compressed_float_internal( Stream &amp; stream, float &amp; value, float min, float max, float res ){ const float delta = max - min; const float values = delta / res; const uint32_t maxIntegerValue = (uint32_t) ceil( values ); const int bits = bits_required( 0, maxIntegerValue ); uint32_t integerValue = 0; if ( Stream::IsWriting ) { float normalizedValue = clamp( ( value - min ) / delta, 0.0f, 1.0f ); integerValue = (uint32_t) floor( normalizedValue maxIntegerValue + 0.5f ); } if ( !stream.SerializeBits( integerValue, bits ) ) return false; if ( Stream::IsReading ) { const float normalizedValue = integerValue / float( maxIntegerValue ); value = normalizedValue delta + min; } return true;}一旦你实现了对浮点数的序列化，那么将方法拓展下通过网络序列化向量和四元数就非常容易了。我在我自己的项目中使用了这个超赞的针对向量数学的向量库（https://github.com/scoopr/vectorial）的一个修改版本，并且我对这些类型实现的序列化方法如下所示：?12345678910111213141516171819202122232425262728293031323334353637383940414243444546template &lt;typename Stream&gt;boolserialize_vector_internal( Stream &amp; stream, vec3f &amp; vector ){ float values[3]; if ( Stream::IsWriting ) vector.store( values ); serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); if ( Stream::IsReading ) vector.load( values ); return true;} template &lt;typename Stream&gt;boolserialize_quaternion_internal( Stream &amp; stream, quat4f &amp; quaternion ){ float values[4]; if ( Stream::IsWriting ) quaternion.store( values ); serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); serialize_float( stream, values[3] ); if ( Stream::IsReading ) quaternion.load( values ); return true;} #defineserialize_vector( stream, value) \ do \ { \ if ( !serialize_vector_internal( stream, value )) \ return false; \ } \ while(0) #defineserialize_quaternion( stream, value) \ do \ { \ if ( !serialize_quaternion_internal( stream, value ) ) \ return false; \ } \ while(0)如果你知道你的向量的取值会限制在某个范围内，你可以像下面这样对它进行压缩：?123456789101112131415161718192021222324252627282930313233template &lt;typename Stream&gt; &lt;typename stream=“”&gt; bool serialize_compressed_vector_internal( Stream &amp; stream, vec3f &amp; vector, float min, float max, float res ) { float values[3]; if ( Stream::IsWriting ) vector.store( values ); serialize_compressed_float( stream, values[0], min, max, res ); serialize_compressed_float( stream, values[1], min, max, res ); serialize_compressed_float( stream, values[2], min, max, res ); if ( Stream::IsReading ) vector.load( values ); return true; }&lt;/typename&gt;你如果想要在网络上压缩一个方向，不要把它视为四个取值范围在[-1,+1]的成员变量的结构。如果使用这个四元数的三个最小值来表示它效果会好的多，请看下这篇文章的示例代码(地址在https://www.patreon.com/gafferongames?ty=h)来得到一个这方面的实现。四元数是简单的超复数。 复数是由实数加上虚数单位 i 组成，其中i^2 = -1。 相似地，四元数都是由实数加上三个虚数单位 i、j、k 组成，而且它们有如下的关系： i^2 = j^2 = k^2 = -1， i^0 = j^0 = k^0 = 1 , 每个四元数都是 1、i、j 和 k 的线性组合，即是四元数一般可表示为a + bk+ cj + di，其中a、b、c 、d是实数。对于i、j、k本身的几何意义可以理解为一种旋转，其中i旋转代表X轴与Y轴相交平面中X轴正向向Y轴正向的旋转，j旋转代表Z轴与X轴相交平面中Z轴正向向X轴正向的旋转，k旋转代表Y轴与Z轴相交平面中Y轴正向向Z轴正向的旋转，-i、-j、-k分别代表i、j、k旋转的反向旋转。 序列化字符串和数组如果你想序列化字符串并通过网络传输该怎么办？在网络上发送字符串的时候用Null作为终止符是个好主意么？我不这么认为。我认为这么做只是在自找麻烦！我们应该把字符串作为带长度作为前缀的字符数组。所以，要通过网络发送字符串，我们必须解决如何有效的发送字符数组的问题。观察到的第一个事情：为什么要费那么大精力把一个字节数组按比特打包到你的比特流里?只是为了让它们随机的偏移[0,7]比特？为什么不在序列化写入之前进行按字节进行对齐？Why not align to byte so you can memcpy the array of bytes directly into the packet?如果这么处理的话，数据包里面的字节数组数据就很对齐的很准，数组的每个字节都对应着数据包里面的一个实际字节。对于每个要序列化的字节数组，你只损失了[0,7]个比特，这取决于对齐的方式，但是以我的观点来看这没什么好在意的。如何将比特流按字节对齐？只需要在流的当前位置做些计算就可以了，找出还差写入多少个比特就能让当前比特流的比特数量被8整除，然后按照这个数字插入填充比特（比如当前比特流的比特数量是323，那么323+5才能被8整除，所以需要插入5个填充比特）。对于填充比特来说，填充的比特值都是0，这样当你序列化读取的时候你可以进行检测，如果检测的结果是正确的，那么就确实是在读取填充的部分，并且填充的部分确实是0。一直读取到下一个完整字节的比特起始位置（可以被8整除的位置）。如果检测的结果是在应该填充的地方发现了非0的比特值，那么就中止序列化读取并丢弃这个数据包。下面是我用来将比特流按比特对齐的代码：?123456789101112131415161718192021222324252627282930void BitWriter::WriteAlign(){ const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) { uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); }} bool BitReader::ReadAlign(){ const int remainderBits = m_bitsRead % 8; if ( remainderBits != 0 ) { uint32_t value = ReadBits( 8 - remainderBits ); assert( m_bitsRead % 8 == 0 ); if ( value != 0 ) return false; } return true;} #define serialize_align( stream) \ do \ { \ if ( !stream.SerializeAlign() ) \ return false; \ } while(0)现在我们可以使用这个对齐操作来有效率的将字节数组写入比特流：因为我们已经将比特流按照字节对齐了，所以我们可以使用memcpy方法来做大部分的工作。唯一的问题在于比特读取器和比特写入器是按照双字进行工作的，所以需要一些特殊的代码来处理字节数组的头部和尾部，以确保头部的零散比特会被写入内存，并且在头部处理完毕以后，读取的位置会被正确设置到下一个字节。?1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798void BitWriter::WriteBytes( const uint8_t data, int bytes ){ assert( GetAlignBits() == 0 ); assert( m_bitsWritten + bytes 8 &lt;= m_numBits ); assert( ( m_bitsWritten % 32 ) == 0 || ( m_bitsWritten % 32 ) == 8|| ( m_bitsWritten % 32 ) == 16 || ( m_bitsWritten % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsWritten % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) WriteBits( data[i], 8 ); if ( headBytes == bytes ) return; assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) { assert( ( m_bitsWritten % 32 ) == 0 ); memcpy( &amp;m_data[m_wordIndex], data+headBytes, numWords4 ); m_bitsWritten += numWords 32; m_wordIndex += numWords; m_scratch = 0; } assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 &amp;&amp; tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) WriteBits( data[tailStart+i], 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords 4 + tailBytes == bytes );} void ReadBytes( uint8_t data, int bytes ){ assert( GetAlignBits() == 0 ); assert( m_bitsRead + bytes 8 &lt;= m_numBits ); assert( ( m_bitsRead % 32 ) == 0 || ( m_bitsRead % 32 ) == 8 || ( m_bitsRead % 32 ) == 16 || ( m_bitsRead % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsRead % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) data[i] = ReadBits( 8 ); if ( headBytes == bytes ) return; assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) { assert( ( m_bitsRead % 32 ) == 0 ); memcpy( data + headBytes, &amp;m_data[m_wordIndex], numWords 4 ); m_bitsRead += numWords 32; m_wordIndex += numWords; m_scratchBits = 0; } assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 &amp;&amp; tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) data[tailStart+i] = ReadBits( 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords 4 + tailBytes == bytes );} template &lt;typename Stream&gt;bool serialize_bytes_internal( Stream &amp; stream, uint8_t data, int bytes ){ return stream.SerializeBytes( data, bytes );} #define serialize_bytes( stream, data, bytes) \ do \ { \ if ( !serialize_bytes_internal( stream, data, bytes ) ) \ return false; \ } while(0) 现在，我们可以通过先序列化字符串长度然后序列化字符串数据的方法来序列化一个字符串：?123456789101112131415161718192021222324template &lt;typename Stream&gt;bool serialize_string_internal(Stream &amp; stream, char string, int buffer_size ){ uint32_t length; if ( Stream::IsWriting ) { length = strlen( string ); assert( length &lt; buffer_size - 1 ); } serialize_int( stream, length, 0, buffer_size - 1 ); serialize_bytes( stream, (uint8_t*)string, length ); if ( Stream::IsReading ) string[length] = ‘\0’;} #define serialize_string( stream, string, buffer_size) \do \{ \ if ( !serialize_string_internal(stream, \ string,buffer_size ) ) \ return false; \} while (0) 正如你看到的那样，可以从基本元素的序列化开始构建一个相当复杂的序列化体系。序列化数组的子集当实现一个游戏网络协议的时候，或早或晚总会需要序列化一个对象数组然后在网络上传递。比如说服务器也许需要把所有的物体发送给客户端，或者有时候需要发送一组事件或者消息。如果你要发送所有的物体到客户端，这是相当简单直观的，但是如果你只是想发送一个数组的一个子集怎么办？最先想到也是最容易的办法是遍历数组的所有物体然后序列化一个bool数组，这个bool数组标记的是对应的物体是否通过网络发送。如果bool值为1那么后面会跟着物体的数据，否则就会被忽略然后下一个物体的bool值取决于流的下一个值。?12345678910111213141516171819template &lt;typename Stream&gt;bool serialize_scene_a( Stream &amp; stream, Scene &amp; scene ){ for ( int i = 0; i &lt; MaxObjects; ++i ) { serialize_bool( stream, scene.objects[i].send ); if ( !scene.objects[i].send ) { if ( Stream::IsReading ) memset( &amp;scene.objects[i], 0, sizeof( Object ) ); continue; } serialize_object( stream, scene.objects[i] ); } return true;}但是如果物体的数组很大怎么办？举个例子，比如场景中有4000个物体。4000 / 8 = 500。光是标记物体是否发送的BOOL数组就要500个字节的开销，即使你只发送了一两个物体也是这样！这种方法。。。。不是太好。所以我们是否能够找到一种办法来让额外的开销正比于发送的物体数目而不是正比于数组中的物体数目？我们可以找到这么一个方法，但是现在我们已经做了一些有意思的事情。我们在序列化写入的时候遍历一个物体的集合（数组里面的所有物体）但是序列化读取的时候遍历的是一个不同的物体集合（发送物体数组的子集）。在这一点上统一的序列化函数概念就不能维系了。对于这种情况最好是把读取和写入分解成单独的函数：?123456789101112131415161718192021222324252627282930313233343536373839bool write_scene_b( protocol2::WriteStream &amp; stream, Scene &amp; scene ){ int num_objects_sent = 0; for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( scene.objects[i].send ) num_objects_sent++; } write_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_int( stream, i, 0, MaxObjects - 1 ); write_object( stream, scene.objects[i] ); } return true;} bool read_scene_b( protocol2::ReadStream &amp; stream, Scene &amp; scene ){ memset( &amp;scene, 0, sizeof( scene ) ); int num_objects_sent; read_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; num_objects_sent; ++i ) { int index; read_int( stream, index, 0, MaxObjects - 1 ); read_object( stream, scene.objects[index] ); } return true;}此外，你可以用发生变化的对象集合来生成一个单独的数据结构，并且针对发生变化的对象集合实现序列化。但是对每个你期望能够序列化的数据结构都产生C++代码对应的数据结构体是一件非常痛苦的事情。最终你可能想要同时遍历几个数据结构然后高效的将一个动态数据结构写入比特流。这在写一些更高级的序列化方法比如增量编码的时候是一种非常平常的做法。只要你采用了这种做法，统一序列化这种做法就不再有什么意义。我对此的建议是如果任何时候你想这么做，那么请不要担心，就把序列化读取和序列化写入分开好了。将序列化读取和序列化写入统一起来是一种非常简单的方式，但是这种方式带来的简单易用与序列化写入时动态生成数据结构的痛苦相比是不划算的。我的经验是复杂的序列化功能有时候可能会需要单独的序列化读取和序列化写入功能，但是如果可能的话，尽量让具体的序列化函数是统一读取和写入的（举个例子来说，实际的物体和事件无论何时序列化都尽量保持序列化读取和序列化写入是统一的）。多说一点。上面的代码在一次序列化写入的时候对物体集合进行了两次遍历。一次遍历用来确定发生变化的物体数目，第二次遍历用来对发生变化的物体集合进行实际的序列化。我们是否能只用一次遍历就能处理好发生变化的物体集合的序列化？当然可以！你可以使用另外一个技巧，用一个哨兵值（sentinel value）来标记数组的结尾位置，而不是一直序列化数组中的物体直到遇到#。使用这种方法你可以在发送的时候只遍历整个数组一遍，当没有更多物体需要发送的时候，就把哨兵值序列化进数据包以表示数组结束了：?1234567891011121314151617181920212223242526272829bool write_scene_c( protocol2::WriteStream &amp; stream, Scene &amp; scene ){ for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_int( stream, i, 0, MaxObjects ); write_object( stream, scene.objects[i] ); } write_int( stream, MaxObjects, 0, MaxObjects ); return true;} bool read_scene_c( protocol2::ReadStream &amp; stream, Scene &amp; scene ){ memset( &amp;scene, 0, sizeof( scene ) ); while ( true ) { int index; read_int( stream, index, 0, MaxObjects ); if ( index == MaxObjects ) break; read_object( stream, scene.objects[index] ); } return true;} 这种做法非常的简单，并且在发送的物体集合相比较全部物体集合比例非常小的时候工作的很棒。但是如果有大量的物体需要发送，举个例子来说，整个场景中有4000个物体，有一半的物体也就是2000个需要通过网络进行发送。每个物体需要一个序号，那么就需要2000个序号，每个序号需要12比特。。。。这就是说数据包里面24000比特或者说接近30000比特（几乎是30000，不是严格是，译注：原文如此）的数据被序号浪费掉了。可以把序号的编码方式修改下来节省数据，序号不再是全局序号，而是相对上一个物体的相对序号。想下这个问题，我们从左到右遍历一个数组，所以数组中物体的序号从0开始并且逐步增大到MaxObjects – 1。从统计学的角度来说，要发送的物体有可能是挨着很近的，这样下一个序号可能就是+1或者+10再或者是+30这样的小数字，因为我们这里用的序号是相对上一个发送的物体的，所以数字从统计意义上来说都会比较小，所以平均来讲，相比较之前的解决方案你可能需要更少的比特来表示物体的序号。（其实最差情况下我们所需的比特位也只是和前一个方案相同而已，可以证明每个序号，后一方案都比前一方案的要小，那么每个序号花费的比特位无疑不会更多，但是这种方案的主要问题在于健壮性，需要确保关于数据集合的数据包中间都不能丢，一旦中间某个包被丢掉了，那么后面的解析就完全乱掉了，实现起来更加困难一些)。下面就是这么一种编码物体序号的方式，每个序号都是相对上一个物体序号而言的，不再是全局序号，从统计的角度来讲它们会消耗更少的比特位（但是如果非常大的集合，但是发送的数组所占的比例很小，那么两种方法的差异其实是比较小的）：?123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133template &lt;typename Stream&gt;bool serialize_object_index_internal( Stream &amp; stream, int &amp; previous, int &amp; current ){ uint32_t difference; if ( Stream::IsWriting ) { assert( previous &lt; current ); difference = current - previous; assert( difference &gt; 0 ); } // +1 (1 bit) bool plusOne; if ( Stream::IsWriting ) plusOne = difference == 1; serialize_bool( stream, plusOne ); if ( plusOne ) { if ( Stream::IsReading ) current = previous + 1; previous = current; return true; } // [+2,5] -&gt; [0,3] (2 bits) bool twoBits; if ( Stream::IsWriting ) twoBits = difference &lt;= 5; serialize_bool( stream, twoBits ); if ( twoBits ) { serialize_int( stream, difference, 2, 5 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [6,13] -&gt; [0,7] (3 bits) bool threeBits; if ( Stream::IsWriting ) threeBits = difference &lt;= 13; serialize_bool( stream, threeBits ); if ( threeBits ) { serialize_int( stream, difference, 6, 13 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [14,29] -&gt; [0,15] (4 bits) bool fourBits; if ( Stream::IsWriting ) fourBits = difference &lt;= 29; serialize_bool( stream, fourBits ); if ( fourBits ) { serialize_int( stream, difference, 14, 29 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } //[30,61] -&gt; [0,31] (5 bits) bool fiveBits; if ( Stream::IsWriting ) fiveBits = difference &lt;= 61; serialize_bool( stream, fiveBits ); if ( fiveBits ) { serialize_int( stream, difference, 30, 61 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [62,125] -&gt; [0,63] (6 bits) bool sixBits; if ( Stream::IsWriting ) sixBits = difference &lt;= 125; serialize_bool( stream, sixBits ); if ( sixBits ) { serialize_int( stream, difference, 62, 125 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [126,MaxObjects+1] serialize_int( stream, difference, 126, MaxObjects + 1 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true;} template &lt;typename Stream&gt;bool serialize_scene_d( Stream &amp; stream, Scene &amp; scene ){ int previous_index = -1; if ( Stream::IsWriting ) { for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_object_index( stream, previous_index, i ); write_object( stream, scene.objects[i] ); } write_object_index( stream, previous_index, MaxObjects ); } else { while ( true ) { int index; read_object_index( stream, previous_index, index ); if ( index == MaxObjects ) break; read_object( stream, scene.objects[index] ); } } return true;}通常情况下，这将节省大量的带宽，因为要发送的物体的序号往往倾向于聚在一起。在这种情况下，如果下一个物体也要被发送，那么它的的序号就是+1只需要一个比特大小。如果是+2到+5的情况每个序号需要5个比特。平均下来序号所占的数据大小方面可以降低2-3倍。但是要注意的是如果是间隔序号比较大的序号的消耗将比不相关序号编码方案（每个序号都是占12比特空间）要大。这看上去非常糟糕，但是实际上并不会这么差，试想一下，即使你遇到了“最差情况”（要发送的物体的序号间隔均匀都是相差128），那么在一个4000物体的大数组里面你实际才发送几个物体？只有32个而已，所以不用担心这个问题!协议ID和CRC32和序列化检测阅读到这里，你可能会有一个疑惑。“哦哦哦，整个体系看上去非常脆弱啊，只有一个完全不带任何属性信息的二进制流。流里面只有一个个数据协议。你该怎么对这些信息进行反序列化读取和写入？如果某些人发送一些包含随机信息的数据包给你的服务器。你会不会在解析的时候把服务器弄崩溃掉？”确实大部分游戏服务器就是这样工作的，但是我有个好消息告诉你和其他之前是这么做服务器的人，存在这样的技术可以减少或者几乎杜绝由于序列化层传过来的数据导致的崩溃可能性。第一种技术是在你的数据包里面包含协议ID。一般典型的做法是，头4个字节你可以设定一些比较罕见而且独特的值，比如0x12345678，反正是这种其他人不会想着去使用的值就好了。但是说真的，把你的序列ID和协议版本的数字用散列得到一个散列值放到每个数据包的前面32比特的位置，这种方法真的工作的很好。至少如果是其他应用程序的数据包发送到了你的端口（要记住，UDP的数据包可以从任何IP任何端口在任何时间发送过来），你可以通过这３２比特的数据判断出来根本就不是你的应用程序的包，然后就可以直接丢弃了。12[protocol id] (32bits)(packet data)下一个级别的防护是对你的数据包整体做一个CRC32的校验，并把这个校验码放到数据包的包头。这可以让你在接收的时候偶然会放过一些错误的数据包进来处理（这确实是会发生，IP的校验和是16位的，所以一堆东西不会使用16位的校验和。。。其实是通过协议ID来避免这种小概率事件的）。现在你的数据包头文件看起来像下面这样：12345[protocol id](32bits)[crc32](32bits)(packet data)如果你按着这个顺序做下来的话，现在你可能会有点畏惧。”请等一下，我需要为每个数据包花费8个额外的字节来实现我自己的校验和以及协议ID么？“事实上，你可以不这么做。你可以学习下看看IPv4是如何进行校验的，并让协议ID变成一个魔术前缀(Magical Prefix)。也就是说你可以不发送这个协议ID，但是发送方和接收方提前确认过这个协议ID是什么，并在计算数据包CRC32值的时候装作这个数据包带上了这个协议ID的前缀来参与计算。这样如果发送方使用的协议ID与接收方不一致的时候，CRC32的校验就会失败，这将为每个数据包节省4个字节：12345[protocol id] (32bits) // not actually sent, but used to calc crc32[crc32](32bits)(packet data)当然，CRC32只是防止有些随机的数据包误打误撞的情况，但是对于可以轻易修改或者构建恶意数据包的头4个字节以便修正CRC32值的那些恶意发送者来说它起不到什么防护作用。要防止那些恶意发送者，你需要使用一个保密性更好的密码哈希函数，同时还需要一个密钥，这个密钥最好是在客户端尝试登陆游戏服务器之前就通过HTTPS协议在客户端和服务器之间统一好(而且要确保每个客户端的密钥都不一样，只有服务器和对应的客户端才知道密钥是什么)。最后一项技术，也可能是最有效的阻止恶意发送者的技术了（虽然会导致数据包的加密和签名有很多冗余信息），这就是序列化检查（serialization check）。这个技术基本上来说是在包的中间，在一段复杂的序列化写入之前或者之后写上一个已知的32比特整数，并在另外一端序列化读取的时候用相同的值进行检测判断。如果序列化检查值是不正确的，那么就中止序列化读取并丢弃这个数据包。我喜欢在我的数据包每个部分之间写入一些序列化检查值，这样我至少知道我的数据包那部分已经被成功的序列化读取和写入（有些问题无论你如何努力避免都很难完全避免的）。我喜欢使用的另外一个很酷的技巧是在数据包的结尾序列化一个协议检查值，这非常非常的有用，因为它能够帮我判断是否遇到了数据包截断（非常像上一篇文章最后提到的臭名昭著的大端截断和小端截断，在开发的时候也是很让人头疼的地方）。所以现在网络包看起来应该是像这样：1234567[protocol id] (32bits) // not actually sent, but used to calc crc32[crc32](32bits)(packet data)[end of packet serialize check] (32 bits)如果你喜欢的话，你可以把这些协议编译出来然后在你的发布版本中检查这些数据包的内容，特别是在有非常棒的数据包加密和数据包签名支持的情况下，不过不编译也没关系，反正不再需要它们了。下一篇预告：数据包的分包和重组请继续阅读这个系列的下一篇文章，在这篇文章里我将向大家介绍如何拓展本章中实现的网络协议来实现数据包的分包和重组以确保你的网络包的大小在MTU限制以下。MTU：最大传输单元，Maximum Transmission Unit，是指一种通信协议的某一层上面所能通过的最大数据包大，以字节为单位。最大传输单元这个参数通常与通信接口有关，比如网络接口卡、串口等。因为协议数据单元的包头和包尾的长度是固定的，MTU越大，则一个协议数据单元的承载的有效数据就越长，通信效率也越高。MTU越大，传送相同的用户数据所需的数据包个数也越低。MTU也不是越大越好，因为MTU越大， 传送一个数据包的延迟也越大；并且MTU越大，数据包中 bit位发生错误的概率也越大。MTU越大，通信效率越高而传输延迟增大，所以要权衡通信效率和传输延迟选择合适的MTU。以以太网传送IPv4报文为例。MTU表示的长度包含IP包头的长度，如果IP层以上的协议层发送的数据报文的长度超过了MTU，则在发送者的IP层将对数据报文进行分片，在接收者的IP层对接收到的分片进行重组。 如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！ 【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。 原文旧版本 Hi, I’mGlenn Fiedler and welcome to the second article in Building a GameNetwork Protocol.In the previous article we discussed different ways to read and write packets in multiplayer games. Wequickly shot down sending game state via text formats like XML and JSON becausethey’re really inefficient and decided to write own binary protocolinstead. We implemented a bitpacker so we don’t have to round boolsup to 8 bits, solved endianness issues, wrote words at a time instead of bytesand pretty much made the bitpacker as simple and as fast as possible withoutplatform specific tricks.Where weleft off we still had the following problems to solve:1. Weneed a way to check if integer values are outside the expected rangeand abort packet read because people will send malicious packets tryingto make us trash memory. The packet read abort must be automatic and notuse exceptions because they’re really slow.2. Separateread and write functions are a maintainance nightmare if those functions arecoded manually. We’d like to write the serialization code for a packet once but not pay any runtime cost (in terms of additional branching, virtuals and soon) when doing so.How can wedo this? Read on and I’ll show you how exactly I do it in C++. It’s taken awhile for me to develop and refine this technique so I hope you’ll find ituseful and at least a good alternative to consider vs. the way youcurrently do it or how you’ve seen it done in other game engines.Unified Packet Serialize FunctionLets startwith the goal. Here’s where we want to end up:struct PacketA{ int x,y,z; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_bits( stream, x, 32 ); serialize_bits( stream, y, 32 ); serialize_bits( stream, z, 32 ); return true; }}; struct PacketB{ int numElements; int elements[MaxElements]; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_int( stream, numElements, 0, MaxElements ); for ( int i = 0; i &lt; numElements; ++i ) serialize_bits( buffer, elements[i], 32 ); return true; }}; struct PacketC{ bool x; short y; int z; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_int( stream, x, 8 ); serialize_int( stream, y, 16 ); serialize_int( stream, z, 32 ); return true; }};Noticethere is a single serialize function per-packet struct instead of separate readand write functions. This is great! It halves the amount of serialization codeand now you have put in some serious effort in order to desync read andwrite.The trickto making this work efficiently is having thestream class templated in the serialize function. There are two stream types inmy system: ReadStream and WriteStream. Each class has the same set of methods,but otherwise are not related in any way. One class reads values in from a bitstream to variables, and the other writes variables values out to a bit stream.ReadStream and WriteStream are just wrappers on top of BitReader andBitWriter classes from the previous article.There areof course alternatives to this approach. If you dislike templates you couldhave a pure virtual base stream interface and implement that interfacewith read and write stream classes. But now you’re taking a virtualfunction for each serialize call. Seems like an excessive amount of overhead tome.Anotheroption is to have an uber-stream class that can be configured to act in read orwrite mode at runtime. This can be faster than the virtual functionmethod, but you still have to branch per-serialize call to decide if you shouldread or write so it’s not going to be as fast as hand-coded read and write.I preferthe templated method because it lets the compiler do the work of generatingoptimized read/write functions for you. You can even code serializefunctions like this and let the compiler optimize out a bunch of stuff whenspecializing read and write:structRigidBody{ vec3f position; quat3f orientation; vec3f linear_velocity; vec3f angular_velocity; template &lt;typename Stream&gt; bool Serialize( Stream &amp; stream ) { serialize_vector( stream, position ); serialize_quaternion( stream, orientation ); bool at_rest = Stream::IsWriting ? velocity.length() == 0 : 1; serialize_bool( stream, at_rest ); if ( !at_rest ) { serialize_vector( stream, linear_velocity ); serialize_vector( stream, angular_velocity ); } else if ( Stream::IsReading ) { linear_velocity = vec3f(0,0,0); angular_velocity = vec3f(0,0,0); } return true; }}; While thismay look inefficient, it’s actually not! The templatespecialization of this function optimizes out all of the branchesaccording to the stream type. Pretty neat huh?Bounds Checking and Abort ReadNow thatwe’ve twisted the compiler’s arm to generate optimized read/writefunctions, we need some way to automate error checking on readso we’re not vulnerable to malicious packets.The firststep is to pass in the range of the integer to the serialize functioninstead of just the number of bits required. Think about it. The serializefunction can work out the number of bits required from the min/max values:serialize_int(stream, numElements, 0, MaxElements );This opensup the interface to support easy serialization of signed integer quantities andthe serialize function can check the value read in from the networkand make sure it’s within the expected range. If the value is outsiderange, abort serialize read immediately and discard the packet.Since wecan’t use exceptions to handle this abort (too slow), here’s how I like to doit.In mysetup serialize_int is not actually a function, it’s a sneaky macro likethis:#defineserialize_int( stream, value, min, max) \ do \ { \ assert( min &lt; max); \ int32_tint32_value; \ if ( Stream::IsWriting) \ { \ assert( value &gt;= min); \ assert( value &lt;= max); \ int32_value = (int32_t)value; \ } \ if ( !stream.SerializeInteger( int32_value, min, max ) ) \ return false; \ if ( Stream::IsReading) \ { \ value =int32_value; \ if ( value &lt; min || value &gt; max) \ return false; \ } \ } while (0) The reasonI’m being a terrible person here is that I’m using the macro to insert codethat checks the result of SerializeInteger and returns false onerror. This gives you exception-like behavior in the sense that itunwinds the stack back to the top of the serialization callstack on error, butyou don’t pay anything like the cost of exceptions to do this. The branch tounwind is super uncommon (serialization errors are rare) so branchprediction should have no trouble at all.Anothercase where we need to abort is if the stream reads past the end. This is also arare branch but it’s one we do have to check on each serialization operationbecause reading past the end is undefined. If we fail to do this check, weexpose ourselves to infinite loops as we read past the end of the buffer.While it’s common to return 0 values when reading past the end of a bit stream(as per-the previous article) there is no guarantee that reading zerovalues will always result in the serialize function terminating correctlyif it has loops. This overflow check is necessary for well defined behavior.One finalpoint. On serialize write I don’t do any abort on range checksor write past the end of the stream. You can be a lot more relaxed on thewrite since if anything goes wrong it’s pretty much guaranteed to be yourfault. Just assert that everything is as expected (in range, not past theend of stream) for each serialize write and you’re good to go.Serializing Floats and VectorsThe bitstream only serializes integer values. How can we serialize a float value?Seemstrickly but it’s not actually. A floating point number stored inmemory is just a 32 bit value like any other. Your computer doesn’t knowif a 32 bit word in memory is an integer, a floating point value or partof a string. It’s just a 32 bit value. Luckily, the C++ language (unlikea few others) lets us work with this fundamental property.You canaccess the integer value behind a floating point number with a union:union FloatInt{ float float_value; uint32_t int_value;}; FloatInt tmp;tmp.float_value= 10.0f;printf(“float value as an integer: %x\n”, tmp.int_value ); You canalso do it via an aliased uint32_t pointer, but I’ve experienced thisbreak with GCC -O2, so I prefer the union trick instead. Friends of minepoint out (likely correctly) that the only truly standard way to get the float as an integer is to cast a pointer to the float value touint8_t and reconstruct the integer value from the four bytevalues accessed individually through the byte pointer. Seems a prettydumb way to do it to me though. Ladies and gentlemen… C++!Meanwhilein the past 5 years I’ve had no actual problems in the field with the uniontrick. Here’s how I serialize an uncompressed float value:template &lt;typename Stream&gt; boolserialize_float_internal( Stream &amp; stream, float &amp; value ){ union FloatInt { float float_value; uint32_t int_value; }; FloatInt tmp; if ( Stream::IsWriting ) tmp.float_value = value; bool result = stream.SerializeBits( tmp.int_value, 32 ); if ( Stream::IsReading ) value = tmp.float_value; return result;}Wrap thiswith a serialize_float macro for convenient error checking on read:#define serialize_float( stream, value) \ do \ { \ if ( !protocol2::serialize_float_internal( stream, value )) \ return false; \ } while(0)Sometimesyou don’t want to transmit a full precision float. How can you compress a floatvalue? The first step is to bound that value in some known rangethen quantize it down to an integer representation.Forexample, if you know a floating point number in is range [-10,+10] and anacceptable resolution for that value is 0.01, then you can just multiply thatfloating point number by 100.0 to get it in the range [-1000,+1000] andserialize that as an integer over the network. On the other side, justdivide by 100.0 to get back to the floating point value.Here is ageneralized version of this concept:template &lt;typename Stream&gt; boolserialize_compressed_float_internal( Stream &amp; stream, float &amp; value, float min, float max, float res ){ const float delta = max - min; const float values = delta / res; const uint32_t maxIntegerValue = (uint32_t) ceil( values ); const int bits = bits_required( 0, maxIntegerValue ); uint32_t integerValue = 0; if ( Stream::IsWriting ) { float normalizedValue = clamp( ( value - min ) / delta, 0.0f, 1.0f ); integerValue = (uint32_t) floor( normalizedValue maxIntegerValue + 0.5f ); } if ( !stream.SerializeBits( integerValue, bits ) ) return false; if ( Stream::IsReading ) { const float normalizedValue = integerValue / float( maxIntegerValue ); value = normalizedValue delta + min; } return true;}Once youcan serialize float values it’s trivial extend to serialize vectorsand quaternions over the network. I use a modified version of the awesome vectorial library for vector math in my projects and I implement serialization for thosetypes like this:template &lt;typename Stream&gt; boolserialize_vector_internal( Stream &amp; stream, vec3f &amp; vector ){ float values[3]; if ( Stream::IsWriting ) vector.store( values ); serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); if ( Stream::IsReading ) vector.load( values ); return true;} template &lt;typename Stream&gt; boolserialize_quaternion_internal( Stream &amp; stream, quat4f &amp; quaternion ){ float values[4]; if ( Stream::IsWriting ) quaternion.store( values ); serialize_float( stream, values[0] ); serialize_float( stream, values[1] ); serialize_float( stream, values[2] ); serialize_float( stream, values[3] ); if ( Stream::IsReading ) quaternion.load( values ); return true;} #defineserialize_vector( stream, value) \ do \ { \ if ( !serialize_vector_internal( stream, value )) \ return false; \ } \ while(0) #defineserialize_quaternion( stream, value) \ do \ { \ if ( !serialize_quaternion_internal( stream, value ) ) \ return false; \ } \ while(0)If youknow your vector is bounded in some range, you can compress it like this:template &lt;typename Stream&gt; boolserialize_compressed_vector_internal( Stream &amp; stream, vec3f &amp; vector, float min, float max, float res ){ float values[3]; if ( Stream::IsWriting ) vector.store( values ); serialize_compressed_float( stream, values[0], min, max, res ); serialize_compressed_float( stream, values[1], min, max, res ); serialize_compressed_float( stream, values[2], min, max, res ); if ( Stream::IsReading ) vector.load( values ); return true;}If youwant to compress an orientation over the network, don’t just compress it as avector with 8.8.8.8 bounded in the range [-1,+1]. You can do much better if youuse the smallest three representation of the quaternion. See the sample code for this article for an implementation.Serializing Strings and ArraysWhat ifyou want to serialize a string over the network?Is it agood idea to send a string over the network with null termination? I don’tthink so. You’re just asking for trouble! Instead, treat the string as an arrayof bytes with length prefixed. So, in order to send a string over thenetwork, we have to work out how to efficiently send an array of bytes.Firstobservation: why waste effort bitpacking an array of bytes into your bit streamjust so they are randomly shifted by shifted by [0,7] bits? Why not just alignto byte before writing the array, so the array data sits in the packetnicely aligned, each byte of the array corresponding to an actual byte in thepacket. You lose only [0,7] bits for each array of bytes serialized,depending on the alignment, but that’s nothing to be too concerned about in myopinion.How toalign the bit stream to byte? Just work out your current bit index in thestream and how many bits are left to write until the current bit number in thebit stream divides evenly into 8, then insert that number of paddingbits. For bonus points, pad up with zero bits to add entropy so that onread you can verify that yes, you are reading a byte align and yes, it isindeed padded up with zero bits to the next whole byte bit index. If a non-zerobit is discovered in the pad bits, abort serialize read and discard thepacket.Here’s mycode to align a bit stream to byte:void BitWriter::WriteAlign(){ const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) { uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); }} bool BitReader::ReadAlign(){ const int remainderBits = m_bitsRead % 8; if ( remainderBits != 0 ) { uint32_t value = ReadBits( 8 - remainderBits ); assert( m_bitsRead % 8 == 0 ); if ( value != 0 ) return false; } return true;} #define serialize_align( stream) \ do \ { \ if ( !stream.SerializeAlign() ) \ return false; \ } while(0)Now we canuse this align operation to write an array of bytes into the bit streamefficiently: since we are aligned to bytes we can do most of the workusing memcpy. The only wrinkle is because the bit reader and bit writer work atthe word level, so it’s neccessary to have special code to handle the head andtail portion of the byte array, to make sure any previous scratch bits areflushed to memory at the head, and the scratch is properly setup for the nextbytes after the array in the tail section. void BitWriter::WriteBytes( const uint8_t data, int bytes ){ assert( GetAlignBits() == 0 ); assert( m_bitsWritten + bytes 8 &lt;= m_numBits ); assert( ( m_bitsWritten % 32 ) == 0 || ( m_bitsWritten % 32 ) == 8|| ( m_bitsWritten % 32 ) == 16 || ( m_bitsWritten % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsWritten % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) WriteBits( data[i], 8 ); if ( headBytes == bytes ) return; assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) { assert( ( m_bitsWritten % 32 ) == 0 ); memcpy( &amp;m_data[m_wordIndex], data+headBytes, numWords4 ); m_bitsWritten += numWords 32; m_wordIndex += numWords; m_scratch = 0; } assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 &amp;&amp; tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) WriteBits( data[tailStart+i], 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords 4 + tailBytes == bytes );} void ReadBytes( uint8_t data, int bytes ){ assert( GetAlignBits() == 0 ); assert( m_bitsRead + bytes 8 &lt;= m_numBits ); assert( ( m_bitsRead % 32 ) == 0 || ( m_bitsRead % 32 ) == 8 || ( m_bitsRead % 32 ) == 16 || ( m_bitsRead % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsRead % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) data[i] = ReadBits( 8 ); if ( headBytes == bytes ) return; assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) { assert( ( m_bitsRead % 32 ) == 0 ); memcpy( data + headBytes, &amp;m_data[m_wordIndex], numWords 4 ); m_bitsRead += numWords 32; m_wordIndex += numWords; m_scratchBits = 0; } assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 &amp;&amp; tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) data[tailStart+i] = ReadBits( 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords 4 + tailBytes == bytes );} template &lt;typename Stream&gt; bool serialize_bytes_internal( Stream &amp; stream, uint8_t data, int bytes ){ return stream.SerializeBytes( data, bytes );} #define serialize_bytes( stream, data, bytes) \ do \ { \ if ( !serialize_bytes_internal( stream, data, bytes ) ) \ return false; \ } while(0) Now we canserialize a string by by serializing its length followed by the stringdata:template &lt;typename Stream&gt; bool serialize_string_internal(Stream &amp; stream, char string, int buffer_size ){ uint32_t length; if ( Stream::IsWriting ) { length = strlen( string ); assert( length &lt; buffer_size - 1 ); } serialize_int( stream, length, 0, buffer_size - 1 ); serialize_bytes( stream, (uint8_t*)string, length ); if ( Stream::IsReading ) string[length] = ‘\0’;} #define serialize_string( stream, string, buffer_size) \do \{ \ if ( !serialize_string_internal(stream, \ string,buffer_size ) ) \ return false; \} while (0) As you cansee, you can build up quite complicated serialization from basicprimitives.Serializing Array SubsetsWhenimplemeting a game network protocol, sooner or later you need to serialize anarray of objects over the network. Perhaps the server needs to send all objectsdown to the client, or an array of events or messages to be sent. This isfairly straightforward if you are sending all objects in the array downto the client, but what if you want to send only a subset of the array?The firstand simplest approach is to iterate across all objects in the array andserialize a bool per-object if that object is to be sent. If the value of thatbool 1 then the object data follows, otherwise it’s ommitted and the bool forthe next object is up next in the stream.template &lt;typename Stream&gt; bool serialize_scene_a( Stream &amp; stream, Scene &amp; scene ){ for ( int i = 0; i &lt; MaxObjects; ++i ) { serialize_bool( stream, scene.objects[i].send ); if ( !scene.objects[i].send ) { if ( Stream::IsReading ) memset( &amp;scene.objects[i], 0, sizeof( Object ) ); continue; } serialize_object( stream, scene.objects[i] ); } return true;}But whatif the array of objects is very large, like 4000 objects in the scene? 4000 / 8= 500. Ruh roh. That’s an overhead of 500 bytes, even if you only send oneor two objects! That’s… not good. Can we switch it around so we take overheadpropertional to the number of objects sent instead of the total number ofobjects in the array?Wecan but now, we’ve done something interesting. We’re walking one set of objectsin the serialize write (all objects in the array) and are walking over adifferent set of objects in the serialize read (subset of objects sent). Atthis point the unified serialize function concept breaks down. It’s best toseparate the read and write back into separate functions in cases like this:bool write_scene_b( protocol2::WriteStream &amp; stream, Scene &amp; scene ){ int num_objects_sent = 0; for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( scene.objects[i].send ) num_objects_sent++; } write_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_int( stream, i, 0, MaxObjects - 1 ); write_object( stream, scene.objects[i] ); } return true;} bool read_scene_b( protocol2::ReadStream &amp; stream, Scene &amp; scene ){ memset( &amp;scene, 0, sizeof( scene ) ); int num_objects_sent; read_int( stream, num_objects_sent, 0, MaxObjects ); for ( int i = 0; i &lt; num_objects_sent; ++i ) { int index; read_int( stream, index, 0, MaxObjects - 1 ); read_object( stream, scene.objects[index] ); } return true;} Alternativelyyou could generate a separate data structure with the set of changed objects,and implement a serialize for that array of changed objects. But having togenerate a C++ data structure for each data structure you want serialized is ahuge pain in the ass. Eventually you want to walk several datastructures at the same time and effectively write out a dynamic data structureto the bit stream. This is a really common thing to do when writing moreadvanced serialization methods like delta encoding. As soon as you do it thisway, unified serialize no longer makes sense.My adviceis that when you want to do this, don’t worry, just separate read and write.Unifying read and write are simply not worth the hassle when dynamicallygenerating a data structure on write. My rule of thumb is that complicatedserialization probably justifies separate read and writefunctions, but if possible, try to keep the leaf nodes unified if you can (eg.the actual objects / events, whatever being serialized).One morepoint. The code above walks over the set of objects twice onserialize write. Once to determine the number of changed objects and a secondtime to actually serialize the set of changed objects. Can we do it in onepass instead? Absolutely! You can use another trick, a sentinel value toindicate the end of the array, rather than serializing the # of objects in thearray up front. This way you can iterate over the array only once onsend, and when there are no more objects to send, serialize the sentinalvalue to indicate the end of the array:bool write_scene_c( protocol2::WriteStream &amp; stream, Scene &amp; scene ){ for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_int( stream, i, 0, MaxObjects ); write_object( stream, scene.objects[i] ); } write_int( stream, MaxObjects, 0, MaxObjects ); return true;} bool read_scene_c( protocol2::ReadStream &amp; stream, Scene &amp; scene ){ memset( &amp;scene, 0, sizeof( scene ) ); while ( true ) { int index; read_int( stream, index, 0, MaxObjects ); if ( index == MaxObjects ) break; read_object( stream, scene.objects[index] ); } return true;} This ispretty simple and it works great if the set of objects sent is a smallpercentage of total objects. But what if a large number of objects are sent,lets say half of the 4000 objects in the scene. That’s 2000 object indices witheach index costing 12 bits… that’s 24000 bits or 3000 bytes (almost 3k!) inyour packet wasted indexing objects.You canreduce this by encoding each object index relative to the previous objectindex. Think about it, we’re walking left to right along an array, so objectindices start at 0 and go up to MaxObjects – 1. Statistically speaking, you’relikely to have objects that are close to each other and if the next index is +1or even +10 or +30 from the previous one, on average, you’ll need quite a fewless bits to represent that difference than youneed to represent an absolute index.Here’s oneway to encode the object index as an integer relative to the previous objectindex, while spending less bits on statistically more likely values (eg. smalldifferences between successive object indices, vs. large ones):template &lt;typename Stream&gt; bool serialize_object_index_internal( Stream &amp; stream, int &amp; previous, int &amp; current ){ uint32_t difference; if ( Stream::IsWriting ) { assert( previous &lt; current ); difference = current - previous; assert( difference &gt; 0 ); } // +1 (1 bit) bool plusOne; if ( Stream::IsWriting ) plusOne = difference == 1; serialize_bool( stream, plusOne ); if ( plusOne ) { if ( Stream::IsReading ) current = previous + 1; previous = current; return true; } // [+2,5] -&gt; [0,3] (2 bits) bool twoBits; if ( Stream::IsWriting ) twoBits = difference &lt;= 5; serialize_bool( stream, twoBits ); if ( twoBits ) { serialize_int( stream, difference, 2, 5 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [6,13] -&gt; [0,7] (3 bits) bool threeBits; if ( Stream::IsWriting ) threeBits = difference &lt;= 13; serialize_bool( stream, threeBits ); if ( threeBits ) { serialize_int( stream, difference, 6, 13 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [14,29] -&gt; [0,15] (4 bits) bool fourBits; if ( Stream::IsWriting ) fourBits = difference &lt;= 29; serialize_bool( stream, fourBits ); if ( fourBits ) { serialize_int( stream, difference, 14, 29 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } //[30,61] -&gt; [0,31] (5 bits) bool fiveBits; if ( Stream::IsWriting ) fiveBits = difference &lt;= 61; serialize_bool( stream, fiveBits ); if ( fiveBits ) { serialize_int( stream, difference, 30, 61 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [62,125] -&gt; [0,63] (6 bits) bool sixBits; if ( Stream::IsWriting ) sixBits = difference &lt;= 125; serialize_bool( stream, sixBits ); if ( sixBits ) { serialize_int( stream, difference, 62, 125 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true; } // [126,MaxObjects+1] serialize_int( stream, difference, 126, MaxObjects + 1 ); if ( Stream::IsReading ) current = previous + difference; previous = current; return true;} template &lt;typename Stream&gt; bool serialize_scene_d( Stream &amp; stream, Scene &amp; scene ){ int previous_index = -1; if ( Stream::IsWriting ) { for ( int i = 0; i &lt; MaxObjects; ++i ) { if ( !scene.objects[i].send ) continue; write_object_index( stream, previous_index, i ); write_object( stream, scene.objects[i] ); } write_object_index( stream, previous_index, MaxObjects ); } else { while ( true ) { int index; read_object_index( stream, previous_index, index ); if ( index == MaxObjects ) break; read_object( stream, scene.objects[index] ); } } return true;} In thecommon case this saves a bunch of bandwidth because object indices tend to beclustered together. In the case where the next object is sent, that’s just onebit for the next index being +1 and 5 bits per-index for +2 to +5. On averagethis gives somewhere between a 2-3X reduction in indexing overhead. But noticethat larger indices far apart cost a lot more for each index than thenon-relative encoding (12 bits per index). This seems bad but it’snot because think about it, even if you hit the ‘worst case’ (objectsindices spaced apart evenly with by +128 apart) how many of these can youactually fit into an object array 4000 large? Just 32. No worries!Protocol IDs, CRC32 and Serialization ChecksAt thispoint you may wonder. Wow. This whole thing seems reallyfragile. It’s a totally unattributed binary stream. A stack of cards. Whatif you somehow desync read and write? What if somebody just sent packetscontaining random bytes to your server. How long until you hit a sequence ofbytes that crashes you out?I havegood news for you and the rest of the game industry since most game serversbasically work this way. There are techniques you can use to reduce orvirtually eliminate the possibility of corrupt data getting past theserialization layer.The firsttechnique is to include a protocol id in your packet. Typically, the first 4bytes you can set to some reasonable rare and unique value, maybe 0x12345678because nobody else will ever think to use that. But seriously, put in a hashof your protocol id and your protocol version number in the first 32 bits ofeach packet and you’re doing pretty good. At least if a random packet gets sentto your port from some other application (remember UDP packets can come in fromany IP/port combination at any time) you can trivially discard it:[protocol id] (32bits)(packet data)The nextlevel of protection is to pass a CRC32 over your packet and include that in theheader. This lets you pick up corrupt packets (these do happen, rememberthat the IP checksum is just 16 bits, and a bunch of stuff will not get pickedup by a checksum of 16bits…). Now your packet header looks ilke this:protocol idcrc32(packet data)At thispoint you may be wincing. Wait. I have to take 8 bytes of overhead per-packetjust to implement my own checksum and protocol id? Well actually, you don’t.You can take a leaf out of how IPv4 does their checksum, and make the protocolid a magical prefix. eg: you don’t actually send it, but if bothsender and receiver knows the protocol id and the CRC32 iscalculated as if the packet were prefixed by the protocol id, the CRC32will be incorrect if the sender does not have the same protocol id as thereceiver, saving 4 bytes per-packet:[protocol id] (32bits) // not actually sent, but used to calc crc32crc32(packet data)Of courseCRC32 is only protection against random packet correction, and is no actualprotection against a malicious sender who can easily modify or construct amalicious packet and then properly adjust the CRC32 in the first four bytes. Toprotect against this you need to use a more cryptographically secure hashfunction combined with a secret key perhaps exchanged between client andserver over HTTPS by the matchmaker prior to the client attempting to connectto the game server (different key for each client, known only by the server andthat particular client).One finaltechnique, perhaps as much a check against programmer error on your part andmalicious senders (although redundant once you encrypt and sign your packet) isthe serialization check. Basically, somewhere mid-packet, either beforeor after a complicated serialization section, just write out a known 32 bitinteger value, and check that it reads back in on the other side with the samevalue. If the serialize check value is incorrect abort read and discardthe packet.I like todo this between sections of my packet as I write them, so at least I know whichpart of my packet serialization has desynced read and write as I’m developingmy protocol (it’s going to happen no matter how hard you try to avoid it…).Another cool trick I like to use is to serialize a protocol check at the veryend of the packet, this is super, super useful because it helps pick up packettruncations (like the infamous, little endian vs. big endian truncation of thelast word from the previous article).So now thepacket looks something like this:[protocol id] (32bits) // not actually sent, but used to calc crc32crc32(packet data)[end of packetserialize check] (32 bits)You canjust compile these protocol checks out in your retail build if you like,especially if you have a good encryption and packet signature, as they shouldno longer be necessary.Up next: Packet Fragmentation and ReassemblyRead onfor the next article in this series where I show you how to extend your networkprotocol to perform packet fragmentation and reassembly so you can keep yourpacket payload under MTU.Pleasesupport my writing on patreon, and I’llwrite new articles faster, plus you get access to example source code for thisarticle under BSD 3.0 licence. Thanks for your support!]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建游戏网络协议一之数据包的读取和写入]]></title>
    <url>%2F2017%2F02%2F24%2Freading_and_writing_packets%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[自我总结这篇文章只是介绍, 之后的文章才是正题. 此篇文章大体介绍了 : 文本格式传输的低效率问题， 为了可读性而产生了太多冗余无用数据 为什么不用目前已经有了的库比如Protocol Buffers：因为我们不需要版本信息，也不需要什么跨语言的支持。所以让我们直接忽略掉这些功能并用我们自己的不带属性的二进制流进行代替，在这个过程中我们可以获得更多的控制性和灵活性 要注意大小端的问题 实现一个位打包器， 工作在32位或者64位的级别， 而不是是工作在字节这个级别。因为现代机器对这个长度进行了专门的优化而不应该像1985年那样在字节的级别对缓冲区进行处理。 要注意防止恶意数据包的问题 ： 我们需要实现一个方法来判断整数值是否超出预期范围，如果超出了就要中止网络包的读取和解析，因为会有一些不怀好意的人给我们发送恶意网络包希望我们的程序和内存崩溃掉。网络包的读取和解析的中止必须是自动化的，而且不能使用异常处理，因为异常处理太慢了会拖累我们的程序。 如果独立的读取和写入函数是手动编解码的，那么维护它们真的是一个噩梦。我们希望能够为包一次性的编写好序列化代码并且没有任何运行时的性能消耗（主要是额外的分支、虚化等等）。 我们为了不想自己手动检查各种可能会被攻击的地方， 需要实现检查自动化， 在下一篇文章 构建游戏网络协议二之序列化策略 里将会说。. . . 原文原文出处 原文标题 : Reading and Writing Packets (Best practices for reading and writing packets) Introduction Hi, I’m Glenn Fiedler and welcome to Building a Game Network Protocol.In this article we&rsquo;re going to explore how AAA multiplayer games like first person shooters read and write packets. We&rsquo;ll start with text based formats then move into binary hand-coded binary formats and bitpacking.At the end of this article and the next, you should understand exactly how to implement your own packet read and write the same way the pros do it.BackgroundConsider a web server. It listens for requests, does some work asynchronously and sends responses back to clients. It’s stateless and generally not real-time, although a fast response time is great. Web servers are most often IO bound.Game server are different. They&rsquo;re a headless version of the game running in the cloud. As such they are stateful and CPU bound. The traffic patterns are different too. Instead of infrequent request/response from tens of thousands of clients, a game server has far fewer clients, but processes a continuous stream of input packets sent from each client 60 times per-second, and broadcasts out the state of the world to clients 10, 20 or even 60 times per-second.And this state is huge. Thousands of objects with hundreds of properties each. Game network programmers spend a lot of their time optimizing exactly how this state is sent over the network with crazy bit-packing tricks, hand-coded binary formats and delta encoding.What would happen if we just encoded this world state as XML?&lt;world_update world_time=”0.0”&gt; &lt;object id=”1” class=”player”&gt; &lt;property name=”position” value=”(0,0,0)”&lt;/property&gt; &lt;property name=”orientation” value=”(1,0,0,0)”&lt;/property&gt; &lt;property name=”velocity” value=”(10,0,0)”&lt;/property&gt; &lt;property name=”health” value=”100”&gt;&lt;/property&gt; &lt;property name=”weapon” value=”110”&gt;&lt;/property&gt; … 100s more properties per-object … &lt;/object&gt; &lt;object id=”100” class=”grunt”&gt; &lt;property name=”position” value=”(100,100,0)”&lt;/property&gt; &lt;property name=”health” value=”10”&lt;/property&gt; &lt;/object&gt; &lt;object id=”110” class=”weapon”&gt; &lt;property type=”semi-automatic”&gt;&lt;/property&gt; &lt;property ammo_in_clip=”8”&gt;&lt;/property&gt; &lt;property round_in_chamber=”true”&gt;&lt;/property&gt; &lt;/object&gt; … 1000s more objects …&lt;/world_update&gt;Pretty verbose&hellip; it&rsquo;s hard to see how this would be practical for a large world.JSON is a bit more compact:{ “world_time”: 0.0, “objects”: { 1: { “class”: “player”, “position”: “(0,0,0)”, “orientation”: “(1,0,0,0)”, “velocity”: “(10,0,0)”, “health”: 100, “weapon”: 110 } 100: { “class”: “grunt”, “position”: “(100,100,0)”, “health”: 10 } 110: { “class”: “weapon”, “type: “semi-automatic” “ammo_in_clip”: 8, “round_in_chamber”: 1 } // etc… }}But it still suffers from the same problem: the description of the data is larger than the data itself. What if instead of fully describing the world state in each packet, we split it up into two parts?A schema that describes the set of object classes and properties per-class, sent only once when a client connects to the server.Data sent rapidly from server to client, which is encoded relative to the schema.The schema could look something like this:{ “classes”: { 0: “player” { “properties”: { 0: { “name”: “position”, “type”: “vec3f” } 1: { “name”: “orientation”, “type”: “quat4f” } 2: { “name”: “velocity”, “type”: “vec3f” } 3: { “name”: “health”, “type”: “float” } 4: { “name”: “weapon”, “type”: “object”, } } } 1: “grunt”: { “properties”: { 0: { “name”: “position”, “type”: “vec3f” } 1: { “name”: “health”, “type”: “float” } } } 2: “weapon”: { “properties”: { 0: { “name”: “type”, “type”: “enum”, “enum_values”: [ “revolver”, “semi-automatic” ] } 1: { “name”: “ammo_in_clip”, “type”: “integer”, “range”: “0..9”, } 2: { “name”: “round_in_chamber”, “type”: “integer”, “range”: “0..1” } } } }}The schema is quite big, but that&rsquo;s beside the point. It&rsquo;s sent only once, and now the client knows the set of classes in the game world and the number, name, type and range of properties per-class.With this knowledge we can make the rapidly sent portion of the world state much more compact:{ “world_time”: 0.0, “objects”: { 1: [0,”(0,0,0)”,”(1,0,0,0)”,”(10,0,0)”,100,110], 100: [1,”(100,100,0)”,10], 110: [2,1,8,1] }}And we can compress it even further by switching to a custom text format:0.01:0,0,0,0,1,0,0,0,10,0,0,100,110100:1,100,100,0,10110:2,1,8,1As you can see, it’s much more about what you don’t send than what you do.The Inefficiencies of TextWe’ve made good progress on our text format so far, moving from a highly attributed stream that fully describes the data (more description than actual data) to an unattributed text format that&rsquo;s an order of magnitude more efficient.But there are inherent inefficiencies when using text format for packets:We are most often sending data in the range A-Z, a-z and 0-1, plus a few other symbols. This wastes the remainder of the 0-255 range for each character sent. From an information theory standpoint, this is an inefficient encoding.The text representation of integer values are in the general case much less efficient than the binary format. For example, in text format the worst case unsigned 32 bit integer 4294967295 takes 10 bytes, but in binary format it takes just four.In text, even the smallest numbers in 0-9 range require at least one byte, but in binary, smaller values like 0, 11, 31, 100 can be sent with fewer than 8 bits if we know their range ahead of time.If an integer value is negative, you have to spend a whole byte on &rsquo;-&rsquo; to indicate that.Floating point numbers waste one byte specifying the decimal point.The text representation of numerical values are variable length: “5”, “12345”, “3.141593”. Because of this we need to spend one byte on a separator after each value so we know when it ends.Newlines &lsquo;\n&rsquo; or some other separator are required to distinguish between the set of variables belonging to one object and the next. When you have thousands of objects, this really adds up.In short, if we wish to optimize any further, it&rsquo;s necessary to switch to a binary format.Switching to a Binary FormatIn the web world there are some really great libraries that read and write binary formats like BJSON, Protocol Buffers, Flatbuffers, Thrift, Cap’n Proto and MsgPack.In manay cases, these libraries are great fit for building your game network protocol. But in the fast-paced world of first person shooters where efficiency is paramount, a hand-tuned binary protocol is still the gold standard.There are a few reasons for this. Web binary formats are designed for situations where versioning of data is extremely important. If you upgrade your backend, older clients should be able to keep talking to it with the old format. Data formats are also expected to be language agnostic. A backend written in Golang should be able to talk with a web client written in JavaScript and other server-side components written in Python or Java.Game servers are completely different beasts. The client and server are almost always written in the same language (C++), and versioning is much simpler. If a client with an incompatible version tries to connect, that connection is simply rejected. There&rsquo;s simply no need for compatibility across different versions.So if you don’t need versioning and you don’t need cross-language support what are the benefits for these libraries? Convenience. Ease of use. Not needing to worry about creating, testing and debugging your own binary format.But this convenience is offset by the fact that these libraries are less efficient and less flexible than a binary protocol we can roll ourselves. So while I encourage you to evaluate these libraries and see if they suit your needs, for the rest of this article, we&rsquo;re going to move forward with a custom binary protocol.Getting Started with a Binary FormatOne option for creating a custom binary protocol is to use the in-memory format of your data structures in C/C++ as the over-the-wire format. People often start here, so although I don’t recommend this approach, lets explore it for a while before we poke holes in it.First define the set of packets, typically as a union of structs:struct Packet{ enum PacketTypeEnum { PACKET_A, PACKET_B, PACKET_C }; uint8_t packetType; union { struct PacketA { int x,y,z; } a; struct PacketB { int numElements; int elements[MaxElements]; } b; struct PacketC { bool x; short y; int z; } c; };};When writing the packet, set the first byte in the packet to the packet type number (0, 1 or 2). Then depending on the packet type, memcpy the appropriate union struct into the packet. On read do the reverse: read in the first byte, then according to the packet type, copy the packet data to the corresponding struct.It couldn’t get simpler. So why do most games avoid this approach?The first reason is that different compilers and platforms provide different packing of structs. If you go this route you’ll spend a lot of time with #pragma pack trying to make sure that different compilers and different platforms lay out the structures in memory exactly the same way.The next one is endianness. Most computers are mostly little endian these days but historically some architectures like PowerPC were big endian. If you need to support communication between little endian and big endian machines, the memcpy the struct in and out of the packet approach simply won’t work. At minimum you need to write a function to swap bytes between host and network byte order on read and write for each variable in your struct.There are other issues as well. If a struct contains pointers you can’t just serialize that value over the network and expect a valid pointer on the other side. Also, if you have variable sized structures, such as an array of 32 elements, but most of the time it’s empty or only has a few elements, it&rsquo;s wasteful to always send the array at worst case size. A better approach would support a variable length encoding that only sends the actual number of elements in the array.But ultimately, what really drives a stake into the heart of this approach is security. It’s a massive security risk to take data coming in over the network and trust it, and that&rsquo;s exactly what you do if you just copy a block of memory sent over the network into your struct. Wheee! What if somebody constructs a malicious PacketB and sends it to you with numElements = 0xFFFFFFFF?You should, no you must, at minimum do some sort of per-field checking that values are in range vs. blindly accepting what is sent to you. This is why the memcpy struct approach is rarely used in professional games.Read and Write FunctionsThe next level of sophistication is read and write functions per-packet.Start with the following simple operations:void WriteInteger( Buffer &amp; buffer, uint32_t value );void WriteShort( Buffer &amp; buffer, uint16_t value );void WriteChar( Buffer &amp; buffer, uint8_t value );uint32_t ReadInteger( Buffer &amp; buffer );uint16_t ReadShort( Buffer &amp; buffer );uint8_t ReadByte( Buffer &amp; buffer );These operate on a structure which keeps track of the current position:struct Buffer{ uint8_t data; // pointer to buffer data int size; // size of buffer data (bytes) int index; // index of next byte to be read/written};The write integer function looks something like this:void WriteInteger( Buffer &amp; buffer, uint32_t value ){ assert( buffer.index + 4 &lt;= size );#ifdef BIG_ENDIAN ((uint32_t)(buffer.data+buffer.index)) = bswap( value );#else // #ifdef BIG_ENDIAN ((uint32_t)(buffer.data+buffer.index)) = value;#endif // #ifdef BIG_ENDIAN buffer.index += 4;}And the read integer function looks like this:uint32_t ReadInteger( Buffer &amp; buffer ){ assert( buffer.index + 4 &lt;= size ); uint32_t value;#ifdef BIG_ENDIAN value = bswap( ((uint32_t)(buffer.data+buffer.index)) );#else // #ifdef BIG_ENDIAN value = ((uint32_t)(buffer.data+buffer.index));#endif // #ifdef BIG_ENDIAN buffer.index += 4; return value;}Now, instead of copying across packet data in and out of structs, we implement read and write functions for each packet type:struct PacketA{ int x,y,z; void Write( Buffer &amp; buffer ) { WriteInteger( buffer, x ); WriteInteger( buffer, y ); WriteInteger( buffer, z ); } void Read( Buffer &amp; buffer ) { ReadInteger( buffer, x ); ReadInteger( buffer, y ); ReadInteger( buffer, z ); }};struct PacketB{ int numElements; int elements[MaxElements]; void Write( Buffer &amp; buffer ) { WriteInteger( buffer, numElements ); for ( int i = 0; i &lt; numElements; ++i ) WriteInteger( buffer, elements[i] ); } void Read( Buffer &amp; buffer ) { ReadInteger( buffer, numElements ); for ( int i = 0; i &lt; numElements; ++i ) ReadInteger( buffer, elements[i] ); }};struct PacketC{ bool x; short y; int z; void Write( Buffer &amp; buffer ) { WriteByte( buffer, x ); WriteShort( buffer, y ); WriteInt( buffer, z ); } void Read( Buffer &amp; buffer ) { ReadByte( buffer, x ); ReadShort( buffer, y ); ReadInt( buffer, z ); }};When reading and writing packets, start the packet with a byte specifying the packet type via ReadByte/WriteByte, then according to the packet type, call the read/write on the corresponding packet struct in the union.Now we have a system that allows machines with different endianness to communicate and supports variable length encoding of elements.BitpackingWhat if we have a value in the range [0,1000] we really only need 10 bits to represent all possible values. Wouldn&rsquo;t it be nice if we could write just 10 bits, instead of rounding up to 16? What about boolean values? It would be nice to send these as one bit instead of 8!One way to implement this is to manually organize your C++ structures into packed integers with bitfields and union tricks, such as grouping all bools together into one integer type via bitfield and serializing them as a group. But this is tedious and error prone and there’s no guarantee that different C++ compilers pack bitfields in memory exactly the same way.A much more flexible way that trades a small amount of CPU on packet read and write for convenience is a bitpacker. This is code that reads and writes non-multiples of 8 bits to a buffer.Writing BitsMany people write bitpackers that work at the byte level. This means they flush bytes to memory as they are filled. This is simpler to code, but the ideal is to read and write words at a time, because modern machines are optimized to work this way instead of farting across a buffer at byte level like it’s 1985.If you want to write 32 bits at a time, you&rsquo;ll need a scratch word twice that size, eg. uint64_t. The reason is that you need the top half for overflow. For example, if you have just written a value 30 bits long into the scratch buffer, then write another value that is 10 bits long you need somewhere to store 30 + 10 = 40 bits.uint64_t scratch;int scratch_bits;int word_index;uint32_t buffer;When we start writing with the bitpacker, all these variables are cleared to zero except buffer which points to the start of the packet we are writing to. Because we&rsquo;re accessing this packet data at a word level, not byte level, make sure packet buffers lengths are a multiple of 4 bytes.Let’s say we want to write 3 bits followed by 10 bits, then 24. Our goal is to pack this tightly in the scratch buffer and flush that out to memory, 32 bits at a time. Note that 3 + 10 + 24 = 37. We have to handle this case where the total number of bits don’t evenly divide into 32. This is actually the common case.At the first step, write the 3 bits to scratch like this:xxxscratch_bits is now 3.Next, write 10 bits:yyyyyyyyyyxxxscratch_bits is now 13 (3+10).Next write 24 bits:zzzzzzzzzzzzzzzzzzzzzzzzyyyyyyyyyyxxxscratch_bits is now 37 (3+10+24). We’re straddling the 32 bit word boundary in our 64 bit scratch variable and have 5 bits in the upper 32 bits (overflow). Flush the lower 32 bits of scratch to memory, advance word_index by one, shift scratch right by 32 and subtract 32 from scratch_bits.scratch now looks like this:zzzzzWe&rsquo;ve finished writing bits but we still have data in scratch that&rsquo;s not flushed to memory. For this data to be included in the packet we need to make sure to flush any remaining bits in scratch to memory at the end of writing.When we flush a word to memory it is converted to little endian byte order. To see why this is important consider what happens if we flush bytes to memory in big endian order:DCBA000ESince we fill bits in the word from right to left, the last byte in the packet E is actually on the right. If we try to send this buffer in a packet of 5 bytes (the actual amount of data we have to send) the packet catches 0 for the last byte instead of E. Ouch!But when we write to memory in little endian order, bytes are reversed back out in memory like this:ABCDE000And we can write 5 bytes to the network and catch E at the end. Et voilà!Reading BitsTo read the bitpacked data, start with the buffer sent over the network:ABCDEThe bit reader has the following state:uint64_t scratch;int scratch_bits;int total_bits;int num_bits_read;int word_index;uint32_t buffer;To start all variables are cleared to zero except total_bits which is set to the size of the packet as bytes 8, and buffer which points to the start of the packet.The user requests a read of 3 bits. Since scratch_bits is zero, it’s time to read in the first word. Read in the word to scratch, shifted left by scratch_bits (0). Add 32 to scratch_bits.The value of scratch is now:zzzzzzzzzzzzzzzzzzzyyyyyyyyyyxxxRead off the low 3 bits, giving the expected value of:xxxShift scratch to the right 3 bits and subtract 3 from scratch_bits:zzzzzzzzzzzzzzzzzzzyyyyyyyyyyRead off another 10 bits in the same way, giving the expected value of:yyyyyyyyyyScratch now looks like:zzzzzzzzzzzzzzzzzzzThe next read asks for 24 bits but scratch_bits is only 19 (=32-10-3).It’s time to read in the next word. Shifting the word in memory left by scratch_bits (19) and or it on top of scratch.Now we have all the bits necessary for z in scratch:zzzzzzzzzzzzzzzzzzzzzzzzRead off 24 bits and shift scratch right by 24. scratch is now all zeros.We&rsquo;re done!Beyond BitpackingReading and writing integer values into a packet by specifying the number of bits to read/write is not the most user friendly option.Consider this example:const int MaxElements = 32;struct PacketB{ int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) { WriteBits( writer, numElements, 6 ); for ( int i = 0; i &lt; numElements; ++i ) WriteBits( writer, elements[i] ); } void Read( BitReader &amp; reader ) { ReadBits( reader, numElements, 6 ); for ( int i = 0; i &lt; numElements; ++i ) ReadBits( reader, elements[i] ); }};This code looks fine at first glance, but let’s assume that some time later you, or somebody else on your team, increases MaxElements from 32 to 200 but forget to update the number of bits required to 7（注意看WriteBits( writer, numElements, 6 )的6， 现在需要7了）. Now the high bit of numElements are being silently truncated on send. It&rsquo;s pretty hard to track something like this down after the fact.The simplest option is to just turn it around and define the maximum number of elements in terms of the number of bits sent:const int MaxElementBits = 7;const int MaxElements = ( 1 &lt;&lt; MaxElementBits ) - 1;Another option is to get fancy and work out the number of bits required at compile time:template &lt;uint32_t x&gt; struct PopCount{ enum { a = x - ( ( x &gt;&gt; 1 ) &amp; 0x55555555 ), b = ( ( ( a &gt;&gt; 2 ) &amp; 0x33333333 ) + ( a &amp; 0x33333333 ) ), c = ( ( ( b &gt;&gt; 4 ) + b ) &amp; 0x0f0f0f0f ), d = c + ( c &gt;&gt; 8 ), e = d + ( d &gt;&gt; 16 ), result = e &amp; 0x0000003f };};template &lt;uint32_t x&gt; struct Log2{ enum { a = x | ( x &gt;&gt; 1 ), b = a | ( a &gt;&gt; 2 ), c = b | ( b &gt;&gt; 4 ), d = c | ( c &gt;&gt; 8 ), e = d | ( d &gt;&gt; 16 ), f = e &gt;&gt; 1, result = PopCount&lt;f&gt;::result };};template &lt;int64_t min, int64_t max&gt; struct BitsRequired{ static const uint32_t result = ( min == max ) ? 0 : ( Log2&lt;uint32_t(max-min)&gt;::result + 1 );};#define BITS_REQUIRED( min, max ) BitsRequired&lt;min,max&gt;::resultNow you can’t mess up the number of bits, and you can specify non-power of two maximum values and it everything works out.const int MaxElements = 32;const int MaxElementBits = BITS_REQUIRED( 0, MaxElements );But be careful when array sizes aren&rsquo;t a power of two! In the example above MaxElements is 32, so MaxElementBits is 6. This seems fine because all values in [0,32] fit in 6 bits. The problem is that there are additional values within 6 bits that are outside our array bounds: [33,63]. An attacker can use this to construct a malicious packet that corrupts memory!This leads to the inescapable conclusion that it’s not enough to just specify the number of bits required when reading and writing a value, we must also check that it is within the valid range: [min,max]. This way if a value is outside of the expected range we can detect that and abort read.I used to implement this using C++ exceptions, but when I profiled, I found it to be incredibly slow. In my experience, it’s much faster to take one of two approaches: set a flag on the bit reader that it should abort, or return false from read functions on failure. But now, in order to be completely safe on read you must to check for error on every read operation.const int MaxElements = 32;const int MaxElementBits = BITS_REQUIRED( 0, MaxElements );struct PacketB{ int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) { WriteBits( writer, numElements, MaxElementBits ); for ( int i = 0; i &lt; numElements; ++i ) { WriteBits( writer, elements[i], 32 ); } } void Read( BitReader &amp; reader ) { ReadBits( reader, numElements, MaxElementBits ); if ( numElements &gt; MaxElements ) { reader.Abort(); return; } for ( int i = 0; i &lt; numElements; ++i ) { if ( reader.IsOverflow() ) break; ReadBits( buffer, elements[i], 32 ); } }};If you miss any of these checks, you expose yourself to buffer overflows and infinite loops when reading packets. Clearly you don’t want this to be a manual step when writing a packet read function. You want it to be automatic, just read the NEXT ARTICLE. 译文 译文出处 译者：陈敬凤（nunu） 审校：崔国军（飞扬971） 导论大家好，我是格伦·菲德勒。欢迎大家阅读我新开的这个系列教程：《构建游戏网络协议》。 在这个系列文章中，我将完全从头开始为动作游戏（比如说第一人称射击游戏、近身格斗和实时多人在线战斗竞技场游戏都是动作游戏）基于UDP协议构建一个专业级别的游戏网络协议。我所使用的工具只包括我的Macbook Pro、 Sublime Text 3（这是一个很好用的编辑器，非常值得一试）、我信赖的C++编译器和一组UDP套接字。 我写这个系列文章的目的是分享在过去十年里我在这个领域学到的有关游戏网络方面的专业知识，因为似乎没有人写过这些方面的东西。所以其他的网络程序员到底是如何想如何做的，我想通过这个系列文章来进行一定的分享。如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我可以有机会来写更多的文章。如果你对我的工作进行支持和捐赠的话，你还可以访问到这个系列文章的示例源代码(这些源代码都是开源的,所以您可以使用任何你想用的地方甚至是商业内容上！)以及一个密码以便访问这个网站上还没有发表的一些文章。 关于我的情况，已经说得足够多了，现在让我们开始这个系列文章把！ 对数据包的读取和写入你是否有想过多人在线游戏是如何读取和写入数据包的？ 如果你有web开发的背景话，你可能会使用XML、JSON或YAML以文本格式来通过网络发送数据。但大多数游戏网络程序员会嘲笑这种对游戏数据进行编码的建议。他们可能会说：”哈哈哈，这真的很有趣。你不是认真的对吧？“哦。你…是认真的。你被解雇了。你可以收拾东西回家了。 我只是开了一个玩笑，但是玩笑归玩笑，为什么这种方法不好？ 一个网页服务器位于网络上的某个位置，监听请求并发送响应。这些请求和响应都是无状态的并且对实时性要求非常非常低（当然有个快速的响应是非常重要的，但是如果不是这种情况也没什么关系）。因为无状态和请求/响应的频率非常低，所以具有非常良好的扩展性。但是多人在线游戏的服务器与这种网页服务器完全是不同的。它是以每秒60次的速度来对游戏世界的状态进行模拟。它是实时的并且有状态的，并且需要把所有的状态以每秒20次或者30次的频率下发给客户端。因为整个游戏世界有几千个物体，每个物体可能会有几百个状态，所以下发给客户端的状态可能是海量的。 如果你使用文本格式(如XML或JSON)对这个游戏状态进行编码的话，有可能这种方法是非常低效的。 让我们举个简单的例子，看下下面这个XML文档： ?1234567891011121314151617181920&lt;world_update world_time=“0.0”&gt; &lt;object id=“1” class=“player”&gt; &lt;property name=“position” value=“(0,0,0)” &lt;=”” property=“”&gt; &lt;property name=“orientation” value=“(1,0,0,0)” &lt;=”” property=“”&gt; &lt;property name=“velocity” value=“(10,0,0)” &lt;=”” property=“”&gt; &lt;property name=“health” value=“100”&gt;&lt;/property&gt; &lt;property name=“weapon” value=“110”&gt;&lt;/property&gt; … 100s more properties per-object … &lt;/property&gt;&lt;/property&gt;&lt;/property&gt;&lt;/object&gt; &lt;object id=“100” class=“grunt”&gt; &lt;property name=“position” value=“(100,100,0)” &lt;=”” property=“”&gt; &lt;property name=“health” value=“10” &lt;=”” property=“”&gt; &lt;/property&gt;&lt;/property&gt;&lt;/object&gt; &lt;object id=“110” class=“weapon”&gt; &lt;property type=“semi-automatic”&gt;&lt;/property&gt; &lt;property ammo_in_clip=“8”&gt;&lt;/property&gt; &lt;property round_in_chamber=“true”&gt;&lt;/property&gt; &lt;/object&gt; … 1000s more objects …&lt;/world_update&gt; 这看上去真的很让人讨厌。我们可以做得更好吗?当然。。。使用JSON来编码的话文本量可以少一些： ?123456789101112131415161718192021222324{ “world_time”: 0.0, “objects”: { 1: { “class”: “player”, “position”: “(0,0,0)”, “orientation”: “(1,0,0,0)”, “velocity”: “(10,0,0)”, “health”: 100, “weapon”: 110 } 100: { “class”: “grunt”, “position”: “(100,100,0)”, “health”: 10 } 110: { “class”: “weapon”, “type: “semi-automatic“ “ammo_in_clip“: 8, “round_in_chamber”: 1 } }}但要注意的是描述数据的属性比实际要发送的数据还大。这糟透了。 但是如果我们把数据分成两个部分呢? 1. 一个模式来描述物体类的几何以及每个类的属性。 2. 数据相对于该模式进行编码来快速下发给各个客户端。 下面是JSON的一个模式。它只会被下发一次： ?1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859{ “classes”: { 0: “player” { “properties”: { 0: { “name”: “position”, “type”: “vec3f” } 1: { “name”: “orientation”, “type”: “quat4f” } 2: { “name”: “velocity”, “type”: “vec3f” } 3: { “name”: “health”, “type”: “float” } 4: { “name”: “weapon”, “type”: “object”, } } } 1: “grunt”: { “properties”: { 0: { “name”: “position”, “type”: “vec3f” } 1: { “name”: “health”, “type”: “float” } } } 2: “weapon”: { “properties”: { 0: { “name”: “type”, “type”: “enum”, “enum_values”: [ “revolver”, “semi-automatic” ] } 1: { “name”: “ammo_in_clip”, “type”: “integer”, “range”: “0..9”, } 2: { “name”: “round_in_chamber”, “type”: “integer”, “range”: “0..1” } } } }} 现在我们可以更加紧凑的进行状态更新(每秒进行20次或30次状态更新…)： ?12345678{ “world_time”: 0.0, “objects”: { 1: [0,“(0,0,0)”,“(1,0,0,0)”,“(10,0,0)”,100,110], 100: [1,“(100,100,0)”,10], 110: [2,1,8,1] }} 这确实更好一点了。但是我们为什么不能直接下发一个简单的文本格式而没有任何废话吗? ?12340.01:0,0,0,0,1,0,0,0,10,0,0,100,110100:1,100,100,0,10110:2,1,8,1 这比原来紧凑多了。当然它在没有模式的情况下是完全不可读的，但是这也是整个问题的所在。 文本格式存在的问题我们确实取得了一些进展，但是我们仍然在使用文本格式，所以就继承了文本格式固有的低效。 为什么？ 让我们以一个浮点数为例来分析这个问题。在二进制格式中一个浮点数需要占据4个字节，但是在文本格式下一个浮点数需要多的多的空间。让我们举个简单的例子：“3.141592653589793”一共用了17个字节（并没有包括终止符‘\0’）。当然你可以说让我们把这个浮点数缩短到6位有效数字的情况。但是就算我们这样处理了，所得到的“3.141593”所占据的空间仍然是二进制表示法的2倍大小。更糟糕的是如果你有一个很大的表示位置的数值，比如”12134.534112”。 你仍然需要6位精度的准确性所以现在你需要使用12个字节来表示这个浮点数。而这样所占据的空间是二进制表示法的3倍大小。你还会为每一个‘,’分隔符浪费一个字节。 我希望你能看到，在多人在线游戏中使用文本格式进行通信是完全不可行的。“但是格伦，整个互联网都是构建于文本格式之上的，为什么在多人在线游戏中使用文本格式进行通信完全不可行？”。恩，我认为这是那些创造互联网的人在这个过程中犯下的一些错误。(译注：原作者的这个结论下的太武断了，我觉得主要是因为互联网和游戏通信协议的面向对象不同，游戏通信协议面向的目标是非常特定而且固定的，这使得他们可以提前沟通到底以什么格式来传输协议，或者说不需要传输任何解读的标签就能够互相理解，而这种互相理解的基础是通过其他方式已经进行了沟通，但是互联网的通信则是完全不同的，它要非常的通用，兼顾更多的需求，而且根本就不知道是谁来解读这些信息，所以信息的可读性就非常非常的重要，因为我们没有其他的管道进行通信，只能凭借传输过去的信息本身来解读，所以互联网的协议才是设计成这样。)。可读性可能在这些早期协议很重要，但我可以向你保证，让人们很容易来阅读这些协议和修改你的游戏的网络协议正好是硬币的两面，根本没有办法兼得。 而且因为带宽是非常关键的要素，并且你希望在每秒用尽可能少的比特数来传递尽可能多的游戏内容，所以你完全没有办法使用文本格式，必须切换成二进制格式。 切换成二进制格式在网络世界中存在一些用来读取和写入二进制格式的库，比如BJSON,、Protocol Buffers、 Flatbuffers、 Thrift、 Cap’n Proto 和 MsgPack。这几乎都是常识了。 这些库其实都还不错。借助接口描述语言和一些代码生成工具的帮助能够读取和写入（这通常被称为序列化）你的游戏数据结构成为二进制格式，而这些二进制数据会在网络上进行传递。在网络的另外一侧，这些数据会被反序列化并且转换回原始的数据结构。 使用二进制格式进行传说网络数据的优点有：1) 你不必自己动手写一个序列化层。2）这些库往往是语言无关的，所以你可以在程序的前端（也就是客户端）使用一种语言而在程序的后端（也就是服务端）使用另外一种语言，而在这种情况下手，程序的前端和程序的后端仍然是可以进行通信的。3)提供了版本信息，这样的话，如果你的客户端使用的是一个比较旧版本的协议，而服务器使用的是一个比较新版本的协议的话，它们仍然是可以进行通信的。反过来也是一样。 这些库看上去很棒，但是它们是实现你的游戏的网络协议的一种很棒的方法么？ 其实情况并不总是如此。如果你有一个协议，这个协议是负责向网页服务器进行请求和接受响应，但是需要支持多种语言并且版本信息对你来说非常的重要，那么在这种情况下，因为有了可以支持这些功能的库的存在，你再自己去实现自己的带版本信息的序列化层和多语言支持就是一件特别傻的事情。如果你的游戏需要从一台机器上远程调用另外一台机器的函数。那么可能只用这些已有的库就是完全没问题的。。。 但是在性能至关重要的网络通信的情况下，比如我们在这篇文章中讨论的这种情况（对于第一人称射击游戏游戏、动作游戏等等），游戏网络的基本单位是状态而不是远程函数调用。同时,跨语言支持二进制格式所能提供的好处很小，这是因为客户端和服务器通常情况下都是使用一种语言进行开发的（比如C++）。这些游戏也不需要复杂的版本信息和版本验证机制，因为如何客户端试图以一个不同的协议版本与服务器进行连接的话，那么服务器可以直接拒绝这个连接就好。有一种特别简单但是有效的办法可以解决版本的问题，就是绕过客户端永远用和服务器相同的协议版本进行连接。 所以如果你的协议不是大部分使用远程函数调用的话，你其实根本就不需要版本信息，也不需要什么跨语言的支持，到底这么做有什么好处？从我的观点来看好处其实并不多。所以让我们直接忽略掉这些功能并用我们自己的不带属性的二进制流进行代替，在这个过程中我们可以获得更多的控制性和灵活性。 如何开始使用二进制格式如果你在用C或C++对你的游戏进行编码，你可能想知道为什么不能直接使用memcpy（这是一个函数，可以直接进行内存拷贝, 将n字节长的内容从一个内存地址复制到另一个地址）把我的结构拷贝到数据包里面？很多人会经常从这里开始，因此尽管我不推荐这种方法，还是让我们看下这个方法，看看如果用这个方法来进行网络包传递的话会有哪些问题。 首先要定义你的数据包的集合，通常情况下这是结构的”联合“（C语言的一种语法名字）： ?123456789101112131415161718192021222324252627struct Packet{ enum PacketTypeEnum { PACKET_A, PACKET_B, PACKET_C }; uint8_t packetType; union { struct PacketA { int x,y,z; } a; struct PacketB { int numElements; int elements[MaxElements]; } b; struct PacketC { bool x; short y; int z; } c; }; }; 当对数据包进行写入的时候，设置数据包的第一个字节为数据包的类型（0或者1或者2）。然后依据数据包的类型，使用memcpy函数将合适的联合结构拷贝到数据包里面。在读取数据包的时候进行完全相反的操作：读取的数据包的第一个字节，然后根据数据包的类型就能判断出这个数据包还有多少字节没有读取。然后使用memcpy函数将数据包的数据拷贝到合适的联合结构里面。 这种方法没有办法更加简化了，所以这就完了么？不完全是这样的！我不推荐这种方法，因为它有一些很讨厌的问题。 首先，不同的编译器和平台对于数据结构的打包方式是完全不同的。如果你走这条路的话，你会花很多时间来使用#pragma来努力确保在不同平台的不同的编译器上布局结构在内存中使用完全相同的方式。让这种方式可以在32位和64位架构上都能顺利工作是一个很有“意思”的过程。这并不是说这是不可能的，但它肯定不是一件容易的事情。 下一个比较大的问题是字节顺序。现代计算机大多是使用小端这种字节顺序（英特尔的中央处理器都是如此），但PowerPC的内核使用大端这种字节顺序。历史上的网络数据通过用大端这种字节顺序（网络字节顺序）来进行发送，但没有理由让你遵循这一传统。在我的代码里面，我使用的是小端这种字节顺序，这么做的原因是使用这个字节顺序在最常见的平台（英特尔）上在打包和解包中所要做的工作量最少。 如果你需要支持使用小端字节顺序的机器和使用大端字节顺序的机器之间进行通信的话，只是使用memcpy函数来将结构体拷贝到数据包的方法根本行不通。你至少需要编写一个函数来在结构被读取以后对它的每个属性的字节顺序进行调整，然后才能顺利的读取和写入。 还有一些其他的小问题。很显然，如果你的结构中包含指针，你不能直接把指针进行序列化然后在网络上进行传递然后期望在网络的另外一端反序列化这个指针以后，这个指针在那边还能够正常的使用。另外，如果你有可变大小的结构，比如说一个可以多达32个元素的数组，但是它在大多数时间里面都是空的或者只有很少一些元素，但是为了防止最差的情况你总是需要假设它有32个元素并且进行序列化和反序列化，这非常非常的浪费。一个更好的办法是让你可以对你的可变长度的结构进行编码，能够把长度信息编码到数据结构本身，这样在序列化和反序列化的时候都能妥善处理这个问题。 我觉得真正影响这个方法的可用性的最后一个问题是安全性。如果使用这种方法，你相当于直接把整个C++结构中的数据包直接拷贝，然后在网络上进行发送。你到底在想什么？!!如果有人构造了一个恶意的数据包，并把这个包的长度标记为0xFFFFFFFF发送给你的话，这样你在处理这个数据包的时候，将导致你耗费尽所有的内存空间。 这是一个巨大的安全风险，让你的原始数据直接在网络上进行传输并且选择相信这些在网络上接收到的数据。。你应该，至少，对这些值进行一个取值范围的检查，让这些值确保落在你期望的范围之内，而不是盲目的相信所有发送给你的数据。 读取和写入函数这个复杂度导致的下一个问题就是每个数据包的读取和写入函数。让我们先从以下几个简单的操作开始： ?1234567void WriteInteger( Buffer &amp; buffer, uint32_t value );void WriteShort( Buffer &amp; buffer, uint16_t value );void WriteChar( Buffer &amp; buffer, uint8_t value ); uint32_t ReadInteger( Buffer &amp; buffer );uint16_t ReadShort( Buffer &amp; buffer );uint8_t ReadByte( Buffer &amp; buffer ); 这几个函数是对一个缓冲区结构进行操作，有点类似这样： ?123456struct Buffer{ uint8_t data; // pointer to buffer data int size; // size of buffer data (bytes) int index; // index of next byte to be read/written};举个简单的例子来说，写入整数的函数会像是如下这样： ?12345678910void WriteInteger( Buffer &amp; buffer, uint32_t value ){ assert( buffer.index + 4 &lt;= size );#ifdef BIG_ENDIAN ((uint32_t)(buffer.data+buffer.index)) = bswap( value ); #else // #ifdef BIG_ENDIAN ((uint32_t)(buffer.data+buffer.index)) = value; #endif // #ifdef BIG_ENDIAN buffer.index += 4;} 而读取整数的函数会像是如下这样： ?123456789101112uint32_t ReadInteger( Buffer &amp; buffer ){ assert( buffer.index + 4 &lt;= size ); uint32_t value;#ifdef BIG_ENDIAN value = bswap( ((uint32_t)(buffer.data+buffer.index)) );#else // #ifdef BIG_ENDIAN value = ((uint32_t)(buffer.data+buffer.index));#endif // #ifdef BIG_ENDIAN buffer.index += 4; return value;} 现在就不仅仅是直接使用memcpy函数把内存中的数据结构直接拷贝到数据包里面，而是为每个数据包类型使用了单独的读取和写入函数： ?1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859struct PacketA{ int x,y,z; void Write( Buffer &amp; buffer ) { WriteInteger( buffer, x ); WriteInteger( buffer, y ); WriteInteger( buffer, z ); } void Read( Buffer &amp; buffer ) { ReadInteger( buffer, x ); ReadInteger( buffer, y ); ReadInteger( buffer, z ); }}; struct PacketB{ int numElements; int elements[MaxElements]; void Write( Buffer &amp; buffer ) { WriteInteger( buffer, numElements ); for ( int i = 0; i &lt; numElements; ++i ) WriteInteger( buffer, elements[i] ); } void Read( Buffer &amp; buffer ) { ReadInteger( buffer, numElements ); for ( int i = 0; i &lt; numElements; ++i ) ReadInteger( buffer, elements[i] ); }}; struct PacketC{ bool x; short y; int z; void Write( Buffer &amp; buffer ) { WriteByte( buffer, x ); WriteShort( buffer, y ); WriteInt( buffer, z ); } void Read( Buffer &amp; buffer ) { ReadByte( buffer, x ); ReadShort( buffer, y ); ReadInt( buffer, z ); }}; 当对数据包进行读取和写入的时候，通过ReadByte / WriteByte函数来在数据包的数据之前加上一个字节，用来表明数据包的类型，然后根据数据包的类型，调用联合体里面对应数据包结构里面读取或者写入函数。 所以，现在我们有了一个简单的系统，允许使用不同的字节顺序的机器可以进行通信，并支持可变长度的编码。举个简单的例子来说，现在可以对数组的长度进行序列化并且只对存在的数据进行遍历和序列化，而不用像以前那样，总是要按照最坏情况来发送整个队列。 读取和写入函数存在的问题有了读取和写入函数，是相比较之前的memcpy方法进行数据包的打包和解包前进了一大步，但是这种方法也存在一些问题。让我们一起看下具体都有些什么问题。 第一个存在的问题：如果有一个值，它的取值范围是【0，100】，那么它只需要10个比特就能表示所有可能的值，但是因为我们只能序列化一个字节的值【0，255】，一个short类型的整数【0,655325】或者是一个32比特的整数【0，2^32-1】，所以我们必须凑够16位。这样的话就浪费了6位！ 同样的，如果是一个布尔值（只有真和假两种情况）只需要一位就能表示，但是它们必须取整为一个字节，这就浪费了7位！现在你可以通过用C++结构来实现你的数据包以及对C++的位域进行序列化并使用一些联合方面的技巧来对这个值进行取整，但是你真的能保证两个完全不同的C ++编译器用完全相同的方式来在内存对位域进行打包？据我所知，应该是不可能的。 第二个存在的问题：怀有恶意的人仍然可以构造一个恶意的数据包。举个简单的例子来说，让我们假设MaxElements是32，所以数据包PacketB的元素的数目肯定是在范围【0，32】之间。由于我们是在字节级别对数据包进行读取和吸入，所以元素的数目是存在一个字节里面，可能的取值范围是【0，255】。这导致字节里面【33,255】这个范围的值是完全没有定义的。 如果有人构造了一个elementCount = 255的恶意的数据包，会发生什么？会发生内存崩溃！当然你可以手动设置一个阈值，对读入的数据的大小进行限制，或者手动对它们进行检查，如果出现问题就中止读取，但是你真正想要的一个知道每个要读取的域的最小/最大值到底是多少的系统，并且在任何数据包中对应的值如果超出预期的范围就会自动中止读取，这样在你的代码看到这些不合法的值之前，这些值就已经被丢弃了。在我看来，最后一个存在的问题是，维护这些单独的读取和写入函数真的很让人讨厌。随着这些函数变得越来越复杂，它很容易对其中一个函数进行改变而忘记相应的改变另外一个函数（读取和写入函数是成对出现的，如果要改变一个的话，需要同时改变另外一个，要么就会出现问题，发送方和写入方根本就没法正确的得到数据包里面的信息）。导致之间的读写不同步。而这些不同步可能非常难以追查。 我们要解决所有这些问题，但是首先我们要通过实现一个位打包器来朝这个方向努力，这样我们才不会继续浪费位来存储一些没必要的数据。 实现一个位打包器如果我们要在数据包写入一个布尔值的话，它应该只需要在数据包里面占据一个位的大小。如果我们要在数据包写入一个取值范围在【0，31】的值，它应该在数据包里面占据五个位的大小，而不是八个位的大小。如果我们要在数据包写入一个取值范围在【0，100000】的值，它应该在数据包里面占据十七位的大小，而不是二十四位的大小或者三十二位的大小。要做到这一点，我们需要写一个位打包器。 很多人所写的位打包器是工作在字节这个级别上的，举个例子来说明的话就是他们会把生成的字节刷新到流里面，但是我不喜欢这种方法，因为如果它们是工作在word这个级别的话会快的多。我的目标是每次是写下很多word的时候（32位或者64位），然后每次读取32位或者64位，因为现代机器对这个长度进行了专门的优化而不应该像1985年那样在字节的级别对缓冲区进行处理。 我的位打包技术的原理是类似这样的：如果你想在一次在数据包写入32位的话，你需要一个两倍大小的临时word，比如说uint64_t。如果你想在一次在数据包写入64位的话，你需要128位。这样做的原因是你需要整个区域的上半部分进行溢出，因为你可以只写一个30位大小的值到临时缓冲区，然后需要写入一个10位大小的值到临时缓冲区中。如果这个临时字的上半部分没有额外的空间的话，你需要额外的分支和逻辑来处理溢出情况。既然你想在位打包里面的循环里面产生尽可能少的分支，那么这种方法将有很大的意义。 以位来写入数据包对于位打包器的数据吸入，你需要一些缓冲区以及一个变量来记录目前在缓冲区里面的位的数目。在这个例子之中，让我们把字长选为32位，这样，位打包器的变量看上去就会像是这样： ?1234uint64_t scratch;int scratch_bits;int word_index;uint32_t buffer; 当你开始启动你的位打包器进行写入的时候，所有这些变量都将被清零，缓冲区的指针会指向缓冲区的起始位置，这个指针用于数据包实际开始写入的位置指示。这个缓冲区的长度是以字节为单位的，必须要是4的整数倍，因为我们工作的字长是32位。 比方说，我们要写入3位数据，然后是10位数据，再然后是24位数据。你的目标是在临时缓冲区对这块数据使用一个比较紧密的打包方式并把整理好的数据刷新到内存中去，一次是32位（一个字长）。需要注意的是3 + 10 + 24 = 37，这是故意设计的。你必须处理这种情况。 在第一步中，向临时缓冲区写入3位数据就像下面这样： xxx 临时缓冲区的长度现在就是3位。 接下来，向临时缓冲区写入10位数据就像下面这样： yyyyyyyyyyxxx 临时缓冲区的长度现在就是13位（10+3）。为什么要以这种方式进行打包而不是使用xxxyyyyyyyyyy这种方式？我用这种字节顺序写入的原因是因为我用小端字节顺序来存储网络数据。如果我没有按照这个方向对位数据进行存储的话，那么在发送数据包的时候我将不得不将数据包的大小对齐到下一个双字的长度那里，或者我只能对我的数据包的尾端进行截断。如果你想以大端字节顺序来发送刷新的数据的话，你应该按照另外一个方向对数据包的数据进行打包。 接下来，向临时缓冲区写入24位数据就像下面这样： zzzzzzzzzzzzzzzzzzzzzzzzyyyyyyyyyyxxx 临时缓冲区的长度现在就是37位（10+3+24）。我们正在跨越32位字的边界，并有5位会写入到uint64_t结构的上半32位中（所以这实际是一个溢出）。现在bit_index &gt;= 32，刷新低32位的数据到内存中去，并对word_index加1，然后从临时缓冲区的长度减去32并向对临时缓冲区向右偏移32位。 临时缓冲区现在看上去就像是下面这样： zzzzz 我们可以继续下去，但是我们决定在停下来对这一点进行解释。我们必须在整个数据包的最后放置的是一个32位的字。对字这个级别进行处理的位打包器的一个微妙的一点是你需要在整体写入的最后进行一个刷新处理，这样才能保证最后的这些位会写入到内存中去。当我们把一个字刷新到内存中去的时候，我们要确保这个字会被正确的转换成小端字节顺序。举个简单的例子来说：ＡＢＣＤ当被写入内存中的时候需要被有效的转换成ＤＣＢＡ这个顺序。如果要想明白为什么这种做法非常重要，需要考虑下如果我们用大端字节顺序来把字刷新到内存中去会发生什么事情？最后一个字会截断，因为这个字会以在整数相同的字节顺序写入到内存中去。因为我们会把开始的那些位写到字的低的那个字节中去，我们的内存中的顺序看上去是这样的：对于刚才那串比特值，写入内存的时候会是ＡＢＣＤＥ这个顺序。DCBA000E现在，如果我们尝试以５个字节的大小来用一个数据包来发送缓冲区（我们要发送的数据的实际大小）它捕获的最后一个字节会是０而不是Ｅ。（作者在这里打了一个笑脸）。 但是，当我们以小端这种字节顺序把上面的内容写入内存的时候，字节在内存中的布局是这样的： ABCDE000 我们可以只在网络上写5个字节，这样就节省了3个字节，并且仍然是以Ｅ来作为数据包的结尾。 实际上，我们所做的是开关字周围的字节，因为我们通过这种方法进行构造来避免小端字节顺序的重新排序，所以我们希望是以它们在内存中的顺序直接写入到网络的数据包中，字节顺序是很难处理的。 其代码类似于 ：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144class BitWriter&#123;public: BitWriter( void* data, int bytes ) : m_data( (uint32_t*)data ), m_numWords( bytes / 4 ) &#123; assert( data ); assert( ( bytes % 4 ) == 0 ); // buffer size must be a multiple of four m_numBits = m_numWords * 32; m_bitsWritten = 0; m_wordIndex = 0; m_scratch = 0; m_scratchBits = 0; &#125; void WriteBits( uint32_t value, int bits ) &#123; assert( bits &gt; 0 ); assert( bits &lt;= 32 ); assert( m_bitsWritten + bits &lt;= m_numBits ); value &amp;= ( uint64_t(1) &lt;&lt; bits ) - 1; m_scratch |= uint64_t( value ) &lt;&lt; m_scratchBits; m_scratchBits += bits; if ( m_scratchBits &gt;= 32 ) &#123; assert( m_wordIndex &lt; m_numWords ); m_data[m_wordIndex] = host_to_network( uint32_t( m_scratch &amp; 0xFFFFFFFF ) ); m_scratch &gt;&gt;= 32; m_scratchBits -= 32; m_wordIndex++; &#125; m_bitsWritten += bits; &#125; void WriteAlign() &#123; const int remainderBits = m_bitsWritten % 8; if ( remainderBits != 0 ) &#123; uint32_t zero = 0; WriteBits( zero, 8 - remainderBits ); assert( ( m_bitsWritten % 8 ) == 0 ); &#125; &#125; void WriteBytes( const uint8_t* data, int bytes ) &#123; assert( GetAlignBits() == 0 ); assert( m_bitsWritten + bytes * 8 &lt;= m_numBits ); assert( ( m_bitsWritten % 32 ) == 0 || ( m_bitsWritten % 32 ) == 8 || ( m_bitsWritten % 32 ) == 16 || ( m_bitsWritten % 32 ) == 24 ); int headBytes = ( 4 - ( m_bitsWritten % 32 ) / 8 ) % 4; if ( headBytes &gt; bytes ) headBytes = bytes; for ( int i = 0; i &lt; headBytes; ++i ) WriteBits( data[i], 8 ); if ( headBytes == bytes ) return; FlushBits(); assert( GetAlignBits() == 0 ); int numWords = ( bytes - headBytes ) / 4; if ( numWords &gt; 0 ) &#123; assert( ( m_bitsWritten % 32 ) == 0 ); memcpy( &amp;m_data[m_wordIndex], data + headBytes, numWords * 4 ); m_bitsWritten += numWords * 32; m_wordIndex += numWords; m_scratch = 0; &#125; assert( GetAlignBits() == 0 ); int tailStart = headBytes + numWords * 4; int tailBytes = bytes - tailStart; assert( tailBytes &gt;= 0 &amp;&amp; tailBytes &lt; 4 ); for ( int i = 0; i &lt; tailBytes; ++i ) WriteBits( data[tailStart+i], 8 ); assert( GetAlignBits() == 0 ); assert( headBytes + numWords * 4 + tailBytes == bytes ); &#125; void FlushBits() &#123; if ( m_scratchBits != 0 ) &#123; assert( m_wordIndex &lt; m_numWords ); m_data[m_wordIndex] = host_to_network( uint32_t( m_scratch &amp; 0xFFFFFFFF ) ); m_scratch &gt;&gt;= 32; m_scratchBits -= 32; m_wordIndex++; &#125; &#125; int GetAlignBits() const &#123; return ( 8 - ( m_bitsWritten % 8 ) ) % 8; &#125; int GetBitsWritten() const &#123; return m_bitsWritten; &#125; int GetBitsAvailable() const &#123; return m_numBits - m_bitsWritten; &#125; const uint8_t* GetData() const &#123; return (uint8_t*) m_data; &#125; int GetBytesWritten() const &#123; return ( m_bitsWritten + 7 ) / 8; &#125; int GetTotalBytes() const &#123; return m_numWords * 4; &#125;private: uint32_t* m_data; uint64_t m_scratch; int m_numBits; int m_numWords; int m_bitsWritten; int m_wordIndex; int m_scratchBits;&#125;;以位来读取数据包我们该如何在网络的另外一侧读取已经通过位打包器打包好的数据？ 要我们从要在网络上进行发送的缓冲区开始，假设我们刚刚从recvfrom.函数返回。它里面的内容有５位这么长。 ABCDE 因为我们是按照字这个等级进行读取，我们必须要把数据的长度截断到双字这个长度，比如说８个字节： ABCDE000 现在，这里有一点很微妙的地方。当我在网络上发送一个数据包的时候，我真的不知道它到底包含了多少位的数据（否则的话我将不得不将这个数据包的大小直接记录在数据包的包头里面），而且这是一个带宽的浪费。但是通过在网络的另外一侧使用recvfrom函数我确实知道到底这个数据包的内容的长度是多少，因此当５个字节大小的数据包从网络上到达的时候，我可以直接认为缓冲区中的位的大小是数据包字节大小乘以８。因此，实际上，位读取器认为这个分组中要被读取的比特数为5 8 = 40，而不是37。 你真的想在这里做的事情是确保如果位读取器读取的位置已经超过缓冲区实际位数的结尾，在这个例子中，就是发送的３７位数据，那么它将读取的是零而不是未定义数据。这会自动发生在数据包最后一个字节的最后3位上，因为它们是由位写入器写入的零，但是对于在缓冲区中的最后3个比特你必须确保它们被读为零，以及未来的任何可能位的读取也是返回零。这是一个非常重要的安全步骤以防止你读取的时候超过缓冲区或者数据流的结尾。 现在让我们开始吧。你将在位读取器里面有如下这些变量： ?123456uint64_t scratch;int scratch_bits;int total_bits;int num_bits_read;int word_index;uint32_t buffer;这比位写入器要复杂一些，因为你需要做更多的检测。但是请记住，永远不要相信来自客户端的数据。 要开始进行读取的时候，字的序号是０，临时缓冲区的内容和临时缓冲区的位数都是０。 然后用户请求３位的空间。因为临时缓冲区的位数现在是０，现在是时候来读取第一个字了。读取第一个字然后把它放到临时缓冲区，并对临时缓冲区的位数（目前是0）向右进行偏移。把临时缓冲区的位数添加32。 现在通过把临时缓冲区的内容拷贝到另外一个变量里面来读取前面的3位并通过&amp; ( (1&lt;&lt;3 ) – 1 )进行遮罩处理，来给出最后输出的结果： xxx 现在将临时缓冲区的内容向右偏移3位，并从临时缓冲区的位数中减去3： zzzzzzzzzzzzzzzzzzzyyyyyyyyyy 现在用完全相同的办法读取后面的10位。临时缓冲区看上去就是下面这样的： zzzzzzzzzzzzzzzzzzz 恩。接下来的读取调用请求24位数据的内容，但是临时缓冲区的位数只有19了（19 = 32-10-3 ）。。。现在是时候来读取下一个字了。这个字多半是零因为我们已经对它们进行了清除，但是它还有多余的5位我们需要在接下来进行读取。现在我们准备读取临时缓冲区里面有关z的比特位了： zzzzzzzzzzzzzzzzzzzzzzzz 接下来读取24位并向右位移24位。临时缓冲区现在全部都是0了。 理论上位读取器认为还有27位数据留在缓冲区没有进行读取，如果我们继续进行读取的话，这些位会被作为零弹出来，因为最后一个字节的前3位是零，并且我们把临时缓冲区的最后3个字节全部清位0，因为我们为了按照字进行对齐。（为了对齐，所以填充了3个全部是0的字节。所以位读取器这时候看的话还有3个完整的字节没有读。） 但是让我们假设，由于某种原因，越过了这个点以后用户还是一直尝试进行读取。为了处理这种情况，检查缓冲区中你需要读取的字的数目，以及每次你需要从缓冲区实际读取的字的情况。如果你已经读完了要读的字以后，不要继续增加word_index并继续读取数据这会让你的内存崩溃的，给你的缓冲区填充0来对齐并在缓冲区的位数添加32在每次给缓冲区的末尾添加一个新的字的情况下。通过这种方法，在读取位数的每次内部循环调用的时候能确保分支最后总是返回零，并把它放在你每次需要读取一个新字的地方，但是你读取的位置已经超过了缓冲区结尾的地方你仍然是安全的并且返回0个比特。 这似乎有点过于谨慎了，但是在对网络数据进行读取的时候这种谨慎是特别重要的。这是通过网络传输过来的数据。不要相信它们！如果你的读取和写入是不同步的，或者有人给你发送了一个恶意的缓冲区数据，你可能会被困在一个循环里面并不断尝试读取数据。确保内部循环里面所有的遍历里面的读取都会检测缓冲区是否溢出或者有损坏，如果用户读取的位置已经超出了缓冲区的结尾这种策略总是返回定义的值（0），通过这种行为你可以确保大多数情况都是符合预期的。 其代码类似于：123456789101112131415161718192021222324252627282930313233343536class BitReader&#123;public: // ... uint32_t ReadBits( int bits ) &#123; assert( bits &gt; 0 ); assert( bits &lt;= 32 ); assert( m_bitsRead + bits &lt;= m_numBits ); m_bitsRead += bits; assert( m_scratchBits &gt;= 0 &amp;&amp; m_scratchBits &lt;= 64 ); if ( m_scratchBits &lt; bits ) &#123; assert( m_wordIndex &lt; m_numWords ); m_scratch |= uint64_t( network_to_host( m_data[m_wordIndex] ) ) &lt;&lt; m_scratchBits; m_scratchBits += 32; m_wordIndex++; &#125; assert( m_scratchBits &gt;= bits ); const uint32_t output = m_scratch &amp; ( (uint64_t(1)&lt;&lt;bits) - 1 ); m_scratch &gt;&gt;= bits; m_scratchBits -= bits; return output; &#125; //...&#125;如何使位打包器更棒位打包器这种方法非常的棒，但是直接用于读取和写入数据包时，这并不是最有用的方法。我见过有团队直接使用位打包器进行读取和写入数据包，但是这并不是最佳的方法。你会拥有很多复杂的代码，并且非常容易出错。 让我们一起看下这个例子： ?123456789101112131415161718192021const int MaxElements = 32; struct PacketB{ int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) { WriteBits( writer, numElements, 6 ); for ( int i = 0; i &lt; numElements; ++i ) WriteBits( writer, elements[i] ); } void Read( BitReader &amp; reader ) { ReadBits( reader, numElements, 6 ); for ( int i = 0; i &lt; numElements; ++i ) ReadBits( reader, elements[i] ); }};第一种方法很容易出错，让我们假设在一段时间以后，你把MaxElements的值从32提高到100，但是你没有修改需要序列化的比特的数目，也就是需要序列化的比特的数目还是6（注意看WriteBits( writer, numElements, 6 )的6， 现在需要7了）。哎呦。因为你忘记了在读取和写入函数里面更新比特的数目，那么现在当你在发送数据的时候你会把高位进行截断。事后追查这样的事情是相当困难的。让我们通过增加一些编译时候的检测来计算出所需要的比特的位数： ?12345678910111213141516171819202122232425262728template &lt;uint32_t x&gt; struct PopCount&#123; enum &#123; a = x - ( ( x &gt;&gt; 1 ) &amp; 0x55555555 ), b = ( ( ( a &gt;&gt; 2 ) &amp; 0x33333333 ) + ( a &amp; 0x33333333 ) ), c = ( ( ( b &gt;&gt; 4 ) + b ) &amp; 0x0f0f0f0f ), d = c + ( c &gt;&gt; 8 ), e = d + ( d &gt;&gt; 16 ), result = e &amp; 0x0000003f &#125;; &#125;;template &lt;uint32_t x&gt; struct Log2&#123; enum &#123; a = x | ( x &gt;&gt; 1 ), b = a | ( a &gt;&gt; 2 ), c = b | ( b &gt;&gt; 4 ), d = c | ( c &gt;&gt; 8 ), e = d | ( d &gt;&gt; 16 ), f = e &gt;&gt; 1, result = PopCount&lt;f&gt;::result &#125;;&#125;;template &lt;int64_t min, int64_t max&gt; struct BitsRequired&#123; static const uint32_t result = ( min == max ) ? 0 : ( Log2&lt;uint32_t(max-min)&gt;::result + 1 );&#125;;#define BITS_REQUIRED( min, max ) BitsRequired&lt;min,max&gt;::result哦，太好了。模板元编程和宏。感谢格伦！ 但是这么做真的真棒，相信我！因为你现在没有办法弄乱需要的比特的数目了： ?12345678910111213141516171819202122232425const int MaxElements = 32;const int MaxElementBits = BITS_REQUIRED( 0, MaxElements ); struct PacketB{ int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) { WriteBits( writer, numElements, MaxElementBits ); for ( int i = 0; i &lt; numElements; ++i ) WriteBits( writer, elements[i], 32 ); } void Read( BitReader &amp; reader ) { ReadBits( reader, numElements, MaxElementBits ); for ( int i = 0; i &lt; numElements; ++i ) ReadBits( buffer, elements[i], 32 ); }}; 当然现在也有机会犯错。MaxElements的值是32所以BITS_REQUIRED(0,32) 返回的是6，因为5比特所能得到的取值范围只有【0，31】。这没什么问题，但是现在我们有概率把未定义的值进行插入，如果有一个恶意发送者发送了一个取值范围在【33,63】之间的值的话。当你在长度为32的数组里面读入63个整数的时候会发生什么？当然，你可以对取值范围进行限制来修正这个问题，但是仔细想想。。。如果你从位读取器得到了一个值但是它超出了取值范围，你要么就是让读取和写入操作完全不同步（这显然是你自己的错误）或者有人试图坑你。所以，不要对取值范围进行限制。如果遇到这种情况，直接停止对数据包的读取并且丢弃这个数据包。我还想在这里提到一个陷阱，因为这个陷阱看上去很方便，但是其实它会让代码运行的很慢。我有一次使用异常实现了数据包的读取中断。它看上去很棒，因为在一个递归的位打包器读取函数里面你可能会有28层调用堆栈，而你想要做的不过是展开堆栈然后回到数据包读取函数调用的地方，但是异常真的太慢太慢了。为了取代异常，有两种方法可以运行的快得多：1)在位读取器那里设置一个值表明这个数据包应该被丢弃，2）升级数据包的读取函数让它在读取失败的时候返回false。但是现在，你可以使用这里面的任意一种方法，为了实现读取时候的安全性，你需要检测上面说的标记或者在每一次读取的时候返回一个值，否则如果遇到读取失败的情况，你还是会一直继续读取直到把内存全部耗光。?123456789101112131415161718192021222324252627282930313233343536const int MaxElements = 32;const int MaxElementBits = BITS_REQUIRED( 0, MaxElements ); struct PacketB{ int numElements; int elements[MaxElements]; void Write( BitWriter &amp; writer ) { WriteBits( writer, numElements, MaxElementBits ); for ( int i = 0; i &lt; numElements; ++i ) WriteBits( writer, elements[i], 32 ); } void Read( BitReader &amp; reader ) { ReadBits( reader, numElements, MaxElementBits ); if ( numElements &gt; MaxElements ) { reader.Abort(); return; } for ( int i = 0; i &lt; numElements; ++i ) { if ( reader.IsOverflow() ) break; ReadBits( buffer, elements[i], 32 ); } }}; 但是这么做了以后，整个读取函数就开始变得非常的复杂，而且如果有什么地方你漏过了这些检查的话，你就把自己置于缓冲区溢出和无线循环这种危险的境地。你不会希望在写数据包读取函数的时候全部变成一个手动的过程，你肯定是希望这个过程是自动化的。因为这给了程序员太多犯错误的机会。 请继续关注下一篇文，在那篇文章里面我将向你展现如何使用C++来用非常简洁的方式实现。 这个系列的下一篇文章是《序列化策略》。 在这个系列的下一篇文章，我将向你展示如何将读取和写入函数统一到一个单独的序列化函数里面，它可以在不增加任何运行时损耗的情况同时处理读取和写入。 如果你觉得这篇文章有价值的话，请在patreon上支持我的写作，这样我会写的更快。你可以在BSD 3.0许可下访问到这篇文章里面的代码。非常感谢你的支持！ 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kbe之ubuntu下的编译]]></title>
    <url>%2F2017%2F02%2F10%2Fkbe%E4%B9%8Bubuntu%E4%B8%8B%E7%9A%84%E7%BC%96%E8%AF%91%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[感觉之前的博客已经整理了大多数之前的关于基础的私人笔记, 现在应该可以讨论一下实操的东西了.先来一发之前的kbe在ubuntu下的编译笔记吧, 因为官方对于ubuntu下的kbe编译文档是有问题的. . . . 编译步骤 安装openssl : sudo apt-get install libssl-dev 安装mysql : sudo apt-get install libmysqld-dev sudo apt-get install mysql-server 编译kbe : cd kbengine/kbe/src chmod -R 755 . make 编译出错解决方法查看是否是以下问题导致的 内存不足 硬盘容量不足(编译完要占2个G左右的硬盘空间)]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>KBE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kbe之1分钟完成安装]]></title>
    <url>%2F2017%2F02%2F09%2Fkbe_installation_tutorial%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[KBEngine概绍根据之前的博文 游戏服务端常用架构 属于第三代服务端框架，可能类似于图10。（这个理解不确定）Kbengine引擎应该是对图10中的Gate服务器和NODE和OBJ进行了细分。在功能上大体划分为与位置有关（在Kbengine中称为Cellapp）和与位置无关（在Kbengine中称为Baseapp）。类似于下面的示图架构。 KBE安装介绍 官方是有自动化的安装py脚本的, 不过还是有很多小坑的.不过其实脚本主要也就是只做两件事, 其他都是可选的: 配置环境变量 安装mysql . . . 安装步骤 安装kbe之前请提前在mysql里 建一个数据库(比如建一个数据库kbe_database) 一个拥有所有权限(免得多事…)的用户(比如这个用户是kbe_user) (具体详情请谷歌, 本篇文章是讲kbe的安装的, 不讨论mysql, 弄完mysql之后就可以开始下面的1分钟kbe安装教程啦) 找到你的kbe根目录, 然后进入根目录, 比如你的kbe根目录是kbengine, 则 : cd kbengine sudo python kbengine/kbe/tools/server/install/installer.py install 然后它就会问你 :Install KBEngine to Linux-account(No input is kbe): 为了简单起见, 建议直接填写你当前的linux用户名称, 比如我的是”b” 然后就是开始配置环境变量了, 它就会显示 12345678Check the dependences:- kbe_environment: checking...ERROR: KBE_ROOT: is error! The directory or file not found:KBE_ROOT//kbeKBE_ROOT=KBE_ROOT current: reset KBE_ROOT(No input is [/home/b/kbengine-0.9.18/]): KBE_ROOT这里填写你的kbe根目录所在路径, 比如像我的是~/kbengine-0.9.18, 那就填~/kbengine-0.9.18 他之后显示的都直接敲回车, 用默认的就可以, 如果直到他开始问你mysql的东西都没有弹出Check to some problems, if you are sure this is not a problem please skip: [yes|no]yes, 说明基本没填错 到mysql他会问 12- MySQL: checking...- MySQL is installed on the remote machine?[yes/no] 这里我们直接填yes, 然后就直接填我们之前建立好的数据库kbe_database和用户kbe_user即可, 它会显示 : 12345678- Enter mysql ip-address:127.0.0.1- Enter mysql ip-port:3306- Enter mysql-account:kbe_user- Enter mysql-password:123456- Enter mysql-databaseName:kbe_database- MySQL: yesModified: /home/b/kbengine-0.9.18//kbe/res/server/kbengine_defs.xmlKBEngine has been successfully installed! 是否安装成功 找到你的kbe根目录, 然后进入根目录, 比如你的kbe根目录是kbengine, 则1. 进入kbe根目录下的assets目录 : cd kbengine/assets 2. 运行启动脚本 : sh ./start_server.sh 用ps检查一下是否有以下进程再跑 : 12345678910b@b-VirtualBox:~/kbengine-0.9.18/assets$ ps -ef | grep -v grep | grep -i kbeb 15504 1372 0 04:28 pts/1 00:00:01 /home/b/kbengine-0.9.18/kbe/bin/server//machine --cid=2129652375332859700 --gus=1b 15505 1372 0 04:28 pts/1 00:00:05 /home/b/kbengine-0.9.18/kbe/bin/server//logger --cid=1129653375331859700 --gus=2b 15506 1372 0 04:28 pts/1 00:00:02 /home/b/kbengine-0.9.18/kbe/bin/server//interfaces --cid=1129652375332859700 --gus=3b 15507 1372 0 04:28 pts/1 00:00:06 /home/b/kbengine-0.9.18/kbe/bin/server//dbmgr --cid=3129652375332859700 --gus=4b 15508 1372 0 04:28 pts/1 00:00:07 /home/b/kbengine-0.9.18/kbe/bin/server//baseappmgr --cid=4129652375332859700 --gus=5b 15509 1372 0 04:28 pts/1 00:00:07 /home/b/kbengine-0.9.18/kbe/bin/server//cellappmgr --cid=5129652375332859700 --gus=6b 15510 1372 0 04:28 pts/1 00:00:03 /home/b/kbengine-0.9.18/kbe/bin/server//baseapp --cid=6129652375332859700 --gus=7b 15511 1372 0 04:28 pts/1 00:00:03 /home/b/kbengine-0.9.18/kbe/bin/server//cellapp --cid=7129652375332859700 --gus=8b 15512 1372 0 04:28 pts/1 00:00:06 /home/b/kbengine-0.9.18/kbe/bin/server//loginapp --cid=8129652375332859700 --gus=9 检查我们mysql中的kbe_database数据库里是否多了几个表 : 1234567891011mysql&gt; show tables;+---------------------------+| Tables_in_b_test_database |+---------------------------+| kbe_accountinfos || kbe_email_verification || kbe_entitylog || kbe_serverlog || tbl_Account |+---------------------------+5 rows in set (0.00 sec) 好, 如果都有基本安装完成!]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>KBE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多级指针与多维数组详解]]></title>
    <url>%2F2017%2F02%2F08%2F%E5%A4%9A%E7%BA%A7%E6%8C%87%E9%92%88%E4%B8%8E%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84%E8%AF%A6%E8%A7%A3%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[指针与数组是 C/C++ 编程中非常重要的元素，同时也是较难以理解的。其中，多级指针与 “多维” 数组更是让很多人云里雾里，其实，只要掌握一定的方法，理解多级指针和 “多维” 数组完全可以像理解一级指针和一维数组那样简单。 基础知识首先，先声明一些常识，如果你对这些常识还不理解，请先去弥补一下基础知识： 实际上并不存在多维数组，所谓的多维数组本质上是用一维数组模拟的。 数组名是一个常量（意味着不允许对其进行赋值操作），其代表数组首元素的地址。 数组与指针的关系是因为数组下标操作符[]，比如，int a[3][2]相当于((a+3)+2) 。 指针是一种变量，也具有类型，其占用内存空间大小和系统有关，一般32位系统下，sizeof(指针变量)=4。 指针可以进行加减算术运算，加减的基本单位是sizeof(指针所指向的数据类型)。 对数组的数组名进行取地址(&amp;)操作，其类型为整个数组类型。 对数组的数组名进行sizeof运算符操作，其值为整个数组的大小(以字节为单位)。 数组作为函数形参时会退化为指针。 指针一个指针包含两方面： 地址值； 所指向的数据类型。 解引用操作符（dereference operator）会根据指针当前的地址值，以及所指向的数据类型，访问一块连续的内存空间（大小由指针所指向的数据类型决定），将这块空间的内容转换成相应的数据类型，并返回左值。 有时候，两个指针的值相同，但数据类型不同，解引用取到的值也是不同的，例如，12345char str[] =&#123;0, 1, 2, 3&#125;; /* 以字符的 ASCII 码初始化 */ char * pc = &amp;str[0]; /* pc 指向 str[0]，即 0 */ int * pi = (int *) pc; /* 指针的 “值” 是个地址，32 位。 */ 此时，pc 和 pi 同时指向 str[0]，但 pc 的值为 0（即，ASCII 码值为 0 的字符）；而 pi 的值为 50462976。或许把它写成十六进制会更容易理解：0x03020100（4 个字节分别为 3,2,1,0）。我想你已经明白了，因为小端字节序, 且指针 pi 指向的类型为 int，因此在解引用时，需要访问 4 个字节的连续空间，并将其转换为 int 返回。 一维数组与数组指针假如有一维数组如下： char a[3]; 该数组一共有 3 个元素，元素的类型为 char，如果想定义一个指针指向该数组，也就是如果想把数组名 a 赋值给一个指针变量，那么该指针变量的类型应该是什么呢？前文说过，一个数组的数组名代表其首元素的地址，也就是相当于 &amp; a[0]，而 a[0] 的类型为 char，因此 &amp; a[0] 类型为 char *，因此，可以定义如下的指针变量： char * p = a;//相当于char * p = &amp;a[0] 以上文字可用如下内存模型图表示。 大家都应该知道，a 和 &amp; a[0] 代表的都是数组首元素的地址，而如果你将 &amp; a 的值打印出来，会发现该值也等于数组首元素的地址。请注意我这里的措辞，也就是说，&amp;a 虽然在数值上也等于数组首元素地址的值，但是其类型并不是数组首元素地址类型，也就是char *p = &amp;a是错误的。 前文第 6 条常识已经说过，对数组名进行取地址操作，其类型为整个数组，因此，&amp;a 的类型是 char (*)[3]，所以正确的赋值方式如下: char (*p)[3] = &amp;a; 注意： 很多人对类似于a+1,&amp;a+1,&amp;a[0]+1,sizeof(a),sizeof(&amp;a)等感到迷惑，其实只要搞清楚指针的类型就可以迎刃而解。比如在面对 a+1 和 &amp; a+1 的区别时，由于 a 表示数组首元素地址，其类型为 char *，因此 a+1 相当于数组首地址值 + sizeof(char)；而 &amp; a 的类型为char (*)[3]，代表整个数组，因此 &amp; a+1 相当于数组首地址值 + sizeof(a)。 sizeof(a) 代表整个数组大小，前文第 7 条说明，但是无论数组大小如何，sizeof(&amp;a) 永远等于一个指针变量占用空间的大小，具体与系统平台有关 二维数组与数组指针假如有如下二维数组： char a[3][2]; 由于实际上并不存在多维数组，因此，可以将 a[3][2] 看成是一个具有 3 个元素的一维数组，只是这三个元素分别又是一个一维数组。实际上，在内存中，该数组的确是按照一维数组的形式存储的，存储顺序为 (低地址在前)：a[0][0]、a[0][1]、a[1][0]、a[1][1]、a[2][0]、a[2][1]。(此种方式也不是绝对，也有按列优先存储的模式) 为了方便理解，我画了一张逻辑上的内存图，之所以说是逻辑上的，是因为该图只是便于理解，并不是数组在内存中实际的存储模型（实际模型为前文所述）。 如上图所示，我们可以将数组分成两个维度来看，首先是第一维，将 a[3][2] 看成一个具有三个元素的一维数组，元素分别为：a[0]、a[1]、a[2]，其中，a[0]、a[1]、a[2] 又分别是一个具有两个元素的一维数组 (元素类型为 char)。从第二个维度看，此处可以将 a[0]、a[1]、a[2] 看成自己代表” 第二维” 数组的数组名，以 a[0]为例，a[0](数组名)代表的一维数组是一个具有两个 char 类型元素的数组，而 a[0]是这个数组的数组名 (代表数组首元素地址)，因此 a[0] 类型为 char *，同理 a[1]和 a[2]类型都是 char *。而 a 是第一维数组的数组名，代表首元素地址，而首元素是一个具有两个 char 类型元素的一维数组，因此 a 就是一个指向具有两个 char 类型元素数组的数组指针，也就是 char(*)[2]。 也就是说，如下的赋值是正确的: 123char (*p)[2] = a; //a为第一维数组的数组名，类型为char (*)[2]char * p = a[0]; //a[0]维第二维数组的数组名，类型为char * 同样，对 a 取地址操作代表整个数组的首地址，类型为数组类型 (请允许我暂且这么称呼)，也就是 char (*)[3][2]，所以如下赋值是正确的： char (*p)[3][2] = &amp;a; 若做如下定义： 12345int a[3][4] = &#123;0,1,2,3,4,5,6,7,8,9,10,11&#125;; int ** p; p = (int**)a; // 不做强制类型转换会报错 说明： p 是一个二级指针，它首先是一个指针，指向一个 int*； a 是二维数组名，它首先是一个指针，指向一个含有 4 个元素的 int 数组； 由此可见，a 和 p 的类型并不相同，如果想将 a 赋值给 p，需要强制类型转换。 为什么二维数组名传递给二级指针是不安全的？假如我们将 a 强制转换之后赋值给 p : p = (int**)a; 既然 p 是二级指针，那么 当 **p 时会出什么问题呢？ 首先看一下 p 的值，p 指向 a[0][0]，即 p 的值为 a[0][0] 的地址； 再看一下 p 的值，p 所指向的类型是 int，占 4 字节，根据前面所讲的解引用操作符的过程：从 p 指向的地址开始，取连续 4 个字节的内容。 * p得到的正式 a[0][0] 的值，即 0。 再看一下 **p 的值，诶，报错了？当然报错了，因为你访问了地址为 0 的空间，而这个空间你是没有权限访问的。 二维数组和二级指针相关的参数匹配 三维数组与数组指针假设有三维数组： char a[3][2][2]; 同样，为了便于理解，特意画了如下的逻辑内存图。分析方法和二维数组类似，首先，从第一维角度看过去，a[3][2][2] 是一个具有三个元素 a[0]、a[1]、a[2] 的一维数组，只是这三个元素分别又是一个 “二维” 数组, a 作为第一维数组的数组名，代表数组首元素的地址，也就是一个指向一个二维数组的数组指针，其类型为 char ()[2][2]。从第二维角度看过去，a[0]、a[1]、a[2] 分别是第二维数组的数组名，代表第二维数组的首元素的地址，也就是一个指向一维数组的数组指针，类型为 char()[2]；同理，从第三维角度看过去，a[0][0]、a[0][1]、a[1][0]、a[1][1]、a[2][0]、a[2][1] 又分别是第三维数组的数组名，代表第三维数组的首元素的地址，也就是一个指向 char 类型的指针，类型为 char *。 由上可知，以下的赋值是正确的： 1234char (*p)[3][2][2] = &amp;a;//对数组名取地址类型为整个数组char (*p)[2][2] = a;char (*p) [2] = a[0];//或者a[1]、a[2]char *p = a[0][0];//或者a[0][1]、a[1][0]... 多级指针所谓的多级指针，就是一个指向指针的指针，比如: 12345char *p = "my name is chenyang.";char **pp = &amp;p;//二级指针char ***ppp = &amp;pp;//三级指针 假设以上语句都位于函数体内，则可以使用下面的简化图来表达多级指针之间的指向关系。 多级指针通常用来作为函数的形参，比如常见的 main 函数声明如下: int main(int argc,char ** argv) 因为当数组用作函数的形参的时候，会退化为指针来处理，所以上面的形式和下面是一样的。 int main(int argc,char* argv[]) argv 用于接收用户输入的命令参数，这些参数会以字符串数组的形式传入，类似于: 12345//模拟用户传入的参数char * parm[] = &#123;"parm1","parm2","parm3","parm4"&#125;;//模拟调用main函数，实际中main函数是由入口函数调用的(glibc中的入口函数默认为_start)main(sizeof(parm)/sizeof(char *),parm); 多级指针的另一种常见用法是，假设用户想调用一个函数分配一段内存，那么分配的内存地址可以有两种方式拿到：第一种是通过函数的返回值，该种方式的函数声明如下： 12345void * get_memery(int size)&#123; void *p = malloc(size); return p;&#125; 第二种获取地址的方法是使用二级指针，代码如下： 12345678910int get_memery(int** buf,int size)&#123; *buf = (int *)malloc(size); if(*buf == NULL) return -1; else return 0;&#125;int *p = NULL;get_memery(&amp;p,10);]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程间的通信与同步]]></title>
    <url>%2F2017%2F01%2F27%2Fipc%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[概绍IPC 即 Inter Process Communication, 大概有以下几种方式(排序已打乱) : 6.共享内存( shared memory, 非常实用, 后文将说一下比较常用的两种方式, 分别是 mmap 和 System V共享内存 ) ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。 3.信号量( semophore, 主要用来进程/线程间同步, 后文将会说 System V信号量) ：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 7.套接字( socket ) ：套接字也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。 1.匿名管道( 英文为pipe, 这种IPC很原始 )：匿名管道是一种半双工的通信方式，通常是在父子进程间使用。 2.命名管道 ( named pipe或FIFO, 这种IPC很原始 ) ：命名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。 4.消息队列( message queue, 正在被淘汰 ) ：消息队列是消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 5.信号 ( sinal ) ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 共享内存概绍采用共享内存通信的一个显而易见的好处是效率高，因为进程可以直接读写内存，而不需要任何数据的拷贝。对于像管道和消息队列等通信方式，则需要在内核和用户空间进行四次的数据拷贝，而共享内存则只拷贝两次数据 [1]：一次从输入文件到共享内存区，另一次从共享内存区到输出文件。实际上，进程之间在共享内存时，并不总是读写少量数据后就解除映射，有新的通信时，再重新建立共享内存区域。而是保持共享区域，直到通信完毕为止，这样，数据内容一直保存在共享内存中，并没有写回文件。共享内存中的内容往往是在解除映射时才写回文件的。因此，采用共享内存的通信方式效率是非常高的。 Linux 的 2.2.x 内核支持多种共享内存方式，如 mmap() 系统调用，Posix 共享内存，以及系统 V 共享内存。linux 发行版本如 Redhat 8.0 支持 mmap() 系统调用及系统 V 共享内存，但还没实现 Posix 共享内存，本文将主要介绍 mmap() 系统调用及系统 V 共享内存 API 的原理及应用。 内核怎样保证各个进程寻址到同一个共享内存区域的内存页面 1、page cache 及 swap cache 中页面的区分：一个被访问文件的物理页面都驻留在 page cache 或 swap cache 中，一个页面的所有信息由 struct page 来描述。struct page 中有一个域为指针 mapping ，它指向一个 struct address_space 类型结构。page cache 或 swap cache 中的所有页面就是根据 address_space 结构以及一个偏移量来区分的。 2、文件与 address_space 结构的对应：一个具体的文件在打开后，内核会在内存中为之建立一个 struct inode 结构，其中的 i_mapping 域指向一个 address_space 结构。这样，一个文件就对应一个 address_space 结构，一个 address_space 与一个偏移量能够确定一个 page cache 或 swap cache 中的一个页面。因此，当要寻址某个数据时，很容易根据给定的文件及数据在文件内的偏移量而找到相应的页面。 3、进程调用 mmap() 时，只是在进程空间内新增了一块相应大小的缓冲区，并设置了相应的访问标识，但并没有建立进程空间到物理页面的映射。因此，第一次访问该空间时，会引发一个缺页异常。 4、对于共享内存映射情况，缺页异常处理程序首先在 swap cache 中寻找目标页（符合 address_space 以及偏移量的物理页），如果找到，则直接返回地址；如果没有找到，则判断该页是否在交换区 (swap area)，如果在，则执行一个换入操作；如果上述两种情况都不满足，处理程序将分配新的物理页面，并把它插入到 page cache 中。进程最终将更新进程页表。注：对于映射普通文件情况（非共享映射），缺页异常处理程序首先会在 page cache 中根据 address_space 以及数据偏移量寻找相应的页面。如果没有找到，则说明文件数据还没有读入内存，处理程序会从磁盘读入相应的页面，并返回相应地址，同时，进程页表也会更新。 5、所有进程在映射同一个共享内存区域时，情况都一样，在建立线性地址与物理地址之间的映射之后，不论进程各自的返回地址如何，实际访问的必然是同一个共享内存区域对应的物理页面。注：一个共享内存区域可以看作是特殊文件系统 shm 中的一个文件，shm 的安装点在交换区上。 上面涉及到了一些数据结构，围绕数据结构理解问题会容易一些。 共享内存的优缺点使用共享内存的优缺点如下所述 。 优点：使用共享内存进行进程间的通信非常方便，而且函数的接口也简单，数据的共享还使进程间的数据不用传送，而是直接访问内存，也加快了程序的效率。 同时，它也不像无名管道那样要求通信的进程有一定的父子关系 。 缺点：共享 内存没有提供同步的机制，这使得在使用共享 内存进行进程间通信时，往往要借助其他的手段来进行进程间的同步工作 。 mmapmmap() 系统调用使得进程之间通过映射同一个普通文件实现共享内存。普通文件被映射到进程地址空间后，进程可以向访问普通内存一样对文件进行访问，不必再调用 read()，write（）等操作。 注：实际上，mmap() 系统调用并不是完全为了用于共享内存而设计的。它本身提供了不同于一般对普通文件的访问方式，进程可以像读写内存一样对普通文件的操作。而 Posix 或系统 V 的共享内存 IPC 则纯粹用于共享目的，当然 mmap() 实现共享内存也是其主要应用之一。 mmap() 系统调用形式如下void* mmap (void * addr , size_t len , int prot , int flags , int fd , off_t offset) 参数 fd 为即将映射到进程空间的文件描述字，一般由 open() 返回，同时，fd 可以指定为 - 1，此时须指定 flags 参数中的 MAP_ANON，表明进行的是匿名映射（不涉及具体的文件名，避免了文件的创建及打开，很显然只能用于具有亲缘关系的进程间通信）。 len 是映射到调用进程地址空间的字节数，它从被映射文件开头 offset 个字节开始算起。 prot 参数指定共享内存的访问权限。可取如下几个值的或：PROT_READ（可读） , PROT_WRITE （可写）, PROT_EXEC （可执行）, PROT_NONE（不可访问）。 flags 由以下几个常值指定：MAP_SHARED , MAP_PRIVATE , MAP_FIXED，其中，MAP_SHARED , MAP_PRIVATE 必选其一，而 MAP_FIXED 则不推荐使用。 offset 参数一般设为 0，表示从文件头开始映射。 参数 addr 指定文件应被映射到进程空间的起始地址，一般被指定一个空指针，此时选择起始地址的任务留给内核来完成。 函数的返回值为最后文件映射到进程空间的地址，进程可直接操作起始地址为该值的有效地址。这里不再详细介绍 mmap() 的参数，读者可参考 mmap() 手册页获得进一步的信息。 系统调用 mmap() 用于共享内存的两种方式 （1）使用普通文件提供的内存映射：适用于任何进程之间； 此时，需要打开或创建一个文件，然后再调用 mmap()；典型调用代码如下： fd=open(name, flag, mode); ptr=mmap(NULL, len , PROT_READ|PROT_WRITE, MAP_SHARED , fd , 0); 通过 mmap() 实现共享内存的通信方式有许多特点和要注意的地方，我们将在范例中进行具体说明。 （2）使用特殊文件提供匿名内存映射：适用于具有亲缘关系的进程之间； 由于父子进程特殊的亲缘关系，在父进程中先调用 mmap()，然后调用 fork()。 那么在调用 fork() 之后，子进程继承父进程匿名映射后的地址空间，同样也继承 mmap() 返回的地址，这样，父子进程就可以通过映射区域进行通信了。 注意，这里不是一般的继承关系。 一般来说，子进程单独维护从父进程继承下来的一些变量。 而 mmap() 返回的地址，却由父子进程共同维护。 对于具有亲缘关系的进程实现共享内存最好的方式应该是采用匿名内存映射的方式。 此时，不必指定具体的文件，只要设置相应的标志即可，参见范例 2。 系统调用 munmap()int munmap(void * addr, size_t len)该调用在进程地址空间中解除一个映射关系，addr 是调用 mmap() 时返回的地址，len 是映射区的大小。当映射关系解除后，对原来映射地址的访问将导致段错误发生。 系统调用 msync()int msync (void * addr , size_t len, int flags)一般说来，进程在映射空间的对共享内容的改变并不直接写回到磁盘文件中，往往在调用 munmap（）后才执行该操作。可以通过调用 msync() 实现磁盘上文件内容与共享内存区的内容一致。 mmap() 范例下面将给出使用 mmap() 的两个范例： 范例 1 给出两个进程通过映射普通文件实现共享内存通信； 范例 2 给出父子进程通过匿名映射实现共享内存。 系统调用 mmap() 有许多有趣的地方，下面是通过 mmap（）映射普通文件实现进程间的通信的范例，我们通过该范例来说明 mmap() 实现共享内存的特点及注意事项。 范例1两个进程通过映射普通文件实现共享内存通信范例1 包含两个子程序：map_normalfile1.c 及 map_normalfile2.c。编译两个程序，可执行文件分别为 map_normalfile1 及 map_normalfile2。两个程序通过命令行参数指定同一个文件来实现共享内存方式的进程间通信。 map_normalfile2 试图打开命令行参数指定的一个普通文件，把该文件映射到进程的地址空间，并对映射后的地址空间进行写操作。map_normalfile1 把命令行参数指定的文件映射到进程地址空间，然后对映射后的地址空间执行读操作。这样，两个进程通过命令行参数指定同一个文件来实现共享内存方式的进程间通信。 下面是两个程序代码： map_normalfile1.c12345678910111213141516171819202122232425262728293031323334353637/*-------------map_normalfile1.c-----------*/#include &lt;sys/mman.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;typedef struct&#123; char name[4]; int age;&#125;people;main(int argc, char** argv) // map a normal file as shared mem:&#123; int fd,i; people *p_map; char temp; fd=open(argv[1],O_CREAT|O_RDWR|O_TRUNC,00777); lseek(fd,sizeof(people)*5-1,SEEK_SET); write(fd,"",1); p_map = (people*) mmap( NULL,sizeof(people)*10,PROT_READ|PROT_WRITE, MAP_SHARED,fd,0 ); close( fd ); temp = 'a'; for(i=0; i&lt;10; i++) &#123; temp += 1; memcpy( ( *(p_map+i) ).name, &amp;temp,2 ); ( *(p_map+i) ).age = 20+i; &#125; printf(" initialize over \n "); sleep(10); munmap( p_map, sizeof(people)*10 ); printf( "umap ok \n" );&#125; map_normalfile2.c12345678910111213141516171819202122232425/*-------------map_normalfile2.c-----------*/#include &lt;sys/mman.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;typedef struct&#123; char name[4]; int age;&#125;people;main(int argc, char** argv) // map a normal file as shared mem:&#123; int fd,i; people *p_map; fd=open( argv[1],O_CREAT|O_RDWR,00777 ); p_map = (people*)mmap(NULL,sizeof(people)*10,PROT_READ|PROT_WRITE, MAP_SHARED,fd,0); for(i = 0;i&lt;10;i++) &#123; printf( "name: %s age %d;\n",(*(p_map+i)).name, (*(p_map+i)).age ); &#125; munmap( p_map,sizeof(people)*10 );&#125; map_normalfile1.c 首先定义了一个 people 数据结构，（在这里采用数据结构的方式是因为，共享内存区的数据往往是有固定格式的，这由通信的各个进程决定，采用结构的方式有普遍代表性）。map_normfile1 首先打开或创建一个文件，并把文件的长度设置为 5 个 people 结构大小。然后从 mmap() 的返回地址开始，设置了 10 个 people 结构。然后，进程睡眠 10 秒钟，等待其他进程映射同一个文件，最后解除映射。 map_normfile2.c 只是简单的映射一个文件，并以 people 数据结构的格式从 mmap() 返回的地址处读取 10 个 people 结构，并输出读取的值，然后解除映射。 分别把两个程序编译成可执行文件 map_normalfile1 和 map_normalfile2 后，在一个终端上先运行./map_normalfile2 /tmp/test_shm，程序输出结果如下： initialize over umap ok 在 map_normalfile1 输出 initialize over 之后，输出 umap ok 之前，在另一个终端上运行 map_normalfile2 /tmp/test_shm，将会产生如下输出 (为了节省空间，输出结果为稍作整理后的结果)： name: b age 20; name: c age 21; name: d age 22; name: e age 23; name: f age 24; name: g age 25; name: h age 26; name: I age 27; name: j age 28; name: k age 29; 在 map_normalfile1 输出 umap ok 后，运行 map_normalfile2 则输出如下结果： name: b age 20; name: c age 21; name: d age 22; name: e age 23; name: f age 24; name: age 0; name: age 0; name: age 0; name: age 0; name: age 0; 从程序的运行结果中可以得出的结论 1、 最终被映射文件的内容的长度不会超过文件本身的初始大小，即映射不能改变文件的大小； 2、 可以用于进程通信的有效地址空间大小大体上受限于被映射文件的大小，但不完全受限于文件大小。打开文件被截短为 5 个 people 结构大小，而在 map_normalfile1 中初始化了 10 个 people 数据结构，在恰当时候（map_normalfile1 输出 initialize over 之后，输出 umap ok 之前）调用 map_normalfile2 会发现 map_normalfile2 将输出全部 10 个 people 结构的值，后面将给出详细讨论。 注：在 linux 中，内存的保护是以页为基本单位的，即使被映射文件只有一个字节大小， 内核也会为映射分配一个页面大小的内存。当被映射文件小于一个页面大小时， 进程可以对从 mmap() 返回地址开始的一个页面大小进行访问， 而不会出错；但是， 如果对一个页面以外的地址空间进行访问， 则导致错误发生， 后面将进一步描述。因此， 可用于进程间通信的有效地址空间大小不会超过文件大小及一个页面大小的和。 3、 文件一旦被映射后，调用 mmap() 的进程对返回地址的访问是对某一内存区域的访问，暂时脱离了磁盘上文件的影响。所有对 mmap() 返回地址空间的操作只在内存中有意义，只有在调用了 munmap() 后或者 msync() 时，才把内存中的相应内容写回磁盘文件，所写内容仍然不能超过文件的大小。 范例2父子进程通过匿名映射实现共享内存并用semaphore同步主要介绍下在多进程中使用信号量semaphore的方法。在上一文中，我们已经知道semaphore和mutex对临界区访问控制的一个最主要区别就是semaphore可以跨进程使用，而mutex只能在一个进程中使用。我们再来看下sem_init的原型，熟悉决定进程共享或者线程共享的方法：12#include &lt;semaphore.h&gt;int sem_init(sem_t *sem, int pshared, unsigned int value); 通过设置pshared的值来控制信号量是属于进程间共享还是线程间共享，若pshared为0表明是多线程共享，否则就是多进程间共享。 接下来我们实验思路是：创建两个进程，一个进程负责读取用户在界面输入的数据，然后存入本地的test.txt文件；另一个进程负责读取该文件，然后在标准输出上显示读取的内容。 为此，我们需要创建两个个支持两个进程访问的信号量sem1和sem2，读文件时需要获取sem1信号，读取结束后释放sem2信号；写文件需要获取sem2信号，写文件结束后方式sem1信号。 sem2的初始值为1，sem1的初始值为0，以保证先写入再进行读取，源代码如下，稍后挑关键内容进行解释： mmap_fork_sync.c12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;pthread.h&gt;#include&lt;semaphore.h&gt;#include&lt;string.h&gt;#include&lt;sys/mman.h&gt;#include&lt;unistd.h&gt;#include&lt;sys/types.h&gt;#include&lt;sys/stat.h&gt;#include&lt;fcntl.h&gt;#define BUF_SIZE 30 void readfile(sem_t* psem1,sem_t* psem2)&#123; FILE* fp; char buf[BUF_SIZE]; int str_len,str_seek=0; while(1) &#123; sem_wait(psem1); fp=fopen("data.txt","r+"); if(fp==NULL) return ; memset(buf,0,sizeof(BUF_SIZE)); fseek(fp,str_seek,SEEK_SET); str_len=fread(buf,sizeof(char),BUF_SIZE-1,fp); buf[str_len]=0; str_seek+=str_len; fputs("output:",stdout); puts(buf); fclose(fp); sem_post(psem2); &#125;&#125;void writefile(sem_t* psem1,sem_t* psem2)&#123; FILE* fp; char buf[BUF_SIZE]; while(1) &#123; sem_wait(psem2); fp=fopen("data.txt","a"); if(fp==NULL) return; memset(buf,0,BUF_SIZE); fputs("Input:",stdout); fgets(buf,BUF_SIZE,stdin); fwrite(buf,sizeof(char),strlen(buf),fp); fclose(fp); sem_post(psem1); &#125;&#125; int main()&#123; int pid; int fd1,fd2; void* pv; sem_t* psem1; sem_t* psem2; fd1=open("data1",O_CREAT|O_RDWR|O_TRUNC,0666); fd2=open("data2",O_CREAT|O_RDWR|O_TRUNC,0666);\ ftruncate(fd1,8192); ftruncate(fd2,8192); //lseek(fd,5000,SEEK_SET); psem1=(sem_t*)mmap(NULL,sizeof(sem_t),PROT_READ|PROT_WRITE,MAP_SHARED,fd1,0); psem2=(sem_t*)mmap(NULL,sizeof(sem_t),PROT_READ|PROT_WRITE,MAP_SHARED,fd2,0); sem_init(psem1,1,0); sem_init(psem2,1,1); pid=fork(); if(pid==0) &#123; puts("进入子进程"); writefile(psem1,psem2); &#125; else &#123; puts("进入父进程"); readfile(psem1,psem2); &#125; sem_destroy(psem1); sem_destroy(psem2); munmap(psem1,sizeof(sem_t)); munmap(psem2,sizeof(sem_t)); close(fd1); close(fd2); return 0; &#125; 为了能够跨进程使用 semaphore ，我们引入了跨进程的技术mmap,第61、第62行分别打开了两个mmap需要映射的文件，和我们平时用的open函数不同，这里面为程序赋予了该文件的666权限。这点很重要，因为mmap需要映射的本地文件必须明确赋予其可读写的权限，否则无法通信。 第63行和第64行分别设置两个本地映射文件的大小，以保证有充分的空间在mmap中映射并容纳我们定义的sem_t变量。这点也很重要，如果空间不够会造成总线错误。 第66行和第67行分别利用mmap在共享内存中映射了两个sem_t类型的指针，这就是我们需要sem_init的信号量。 第68、69行开始初始化信号量。 70行fork了两个进程，在子进程中我们进行写操作，在主进程中我们进行读操作。读写操作的代码比较简单，在这里不再多说。 第81到86行在使用完信号量后分别是销毁信号量、释放共享内存、关闭文件操作符。 程序写到这里基本上完成了这个实验，可以考察程序的输出结果，编译命令 : gcc mmap_fork_sync.c -o mmap_fork_sync -pthread , 体会父子进程匿名共享内存： b@b-VirtualBox:~/tc/mmap_test$ ./mmap_fork_sync 进入父进程 进入子进程 Input:4 output:4 Input:5 output:5 Input:6 output:6 Input:7 output:7 Input:7 output:7 ... 我们可以顺便可以简单总结下在多进程中使用信号量的步骤： （1）open()用于进行mmap映射的文件，得到文件操作符fd；（2）把映射文件用ftruncate或者fseek重新设置大小，以保证有足够的空间容纳我们需要传递的sem_t变量；（3）利用mmap函数在共享内存中创建sen_t类型的指针。（4）用sem_init()函数初始化第（3）步中创建的指针，也就得到了我们需要的信号量。（5）用sem_wait()和sem_post()函数进行信号量的等待和释放。（6）用sem_destroy()销毁信号量。（7）用munmap()释放共享内存以及用close()函数关闭文件操作符。 理解页式管理机制前面对范例运行结构的讨论中已经提到，linux 采用的是页式管理机制。对于用 mmap() 映射普通文件来说，进程会在自己的地址空间新增一块空间，空间大小由 mmap() 的 len 参数指定，注意，进程并不一定能够对全部新增空间都能进行有效访问。进程能够访问的有效地址大小取决于文件被映射部分的大小。简单的说，能够容纳文件被映射部分大小的最少页面个数决定了进程从 mmap() 返回的地址开始，能够有效访问的地址空间大小。超过这个空间大小，内核会根据超过的严重程度返回发送不同的信号给进程。可用如下图示说明： 注意：文件被映射部分而不是整个文件决定了进程能够访问的空间大小，另外，如果指定文件的偏移部分，一定要注意为页面大小的整数倍。下面是对进程映射地址空间的访问范例： 12345678910111213141516171819202122232425262728293031323334353637#include &lt;sys/mman.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;typedef struct&#123; char name[4]; int age;&#125;people;main(int argc, char** argv)&#123; int fd,i; int pagesize,offset; people *p_map; pagesize = sysconf(_SC_PAGESIZE); printf("pagesize is %d\n",pagesize); fd = open(argv[1],O_CREAT|O_RDWR|O_TRUNC,00777); lseek(fd,pagesize*2-100,SEEK_SET); write(fd,"",1); offset = 0; //此处offset = 0编译成版本1；offset = pagesize编译成版本2 p_map = (people*)mmap(NULL,pagesize*3,PROT_READ|PROT_WRITE,MAP_SHARED,fd,offset); close(fd); for(i = 1; i&lt;10; i++) &#123; (*(p_map+pagesize/sizeof(people)*i-2)).age = 100; printf("access page %d over\n",i); (*(p_map+pagesize/sizeof(people)*i-1)).age = 100; printf("access page %d edge over, now begin to access page %d\n",i, i+1); (*(p_map+pagesize/sizeof(people)*i)).age = 100; printf("access page %d over\n",i+1); &#125; munmap(p_map,sizeof(people)*10);&#125; 如程序中所注释的那样，把程序编译成两个版本，两个版本主要体现在文件被映射部分的大小不同。文件的大小介于一个页面与两个页面之间（大小为：pagesize2-99），版本 1 的被映射部分是整个文件，版本 2 的文件被映射部分是文件大小减去一个页面后的剩余部分，不到一个页面大小 (大小为：pagesize-99)。程序中试图访问每一个页面边界，两个版本都试图在进程空间中映射 pagesize3 的字节数。 版本 1 的输出结果如下： pagesize is 4096 access page 1 over access page 1 edge over, now begin to access page 2 access page 2 over access page 2 over access page 2 edge over, now begin to access page 3 Bus error //被映射文件在进程空间中覆盖了两个页面，此时，进程试图访问第三个页面 版本 2 的输出结果如下： pagesize is 4096 access page 1 over access page 1 edge over, now begin to access page 2 Bus error //被映射文件在进程空间中覆盖了一个页面，此时，进程试图访问第二个页面 结论：采用系统调用 mmap() 实现进程间通信是很方便的，在应用层上接口非常简洁。内部实现机制区涉及到了 linux 存储管理以及文件系统等方面的内容，可以参考一下相关重要数据结构来加深理解。在本专题的后面部分，将介绍系统 v 共享内存的实现。 System V共享内存说一下System V共享内存. 顾名思义，共享内存就是允许两个不相关的进程访问同一个逻辑内存。 共享内存是在两个正在运行的进程之间共享和传递数据的一种非常有效的方式 。 不同进程之间共享的内存通常安排在同－段物理内存中 。 进程可以将同一段共享内存连接到它们 自己 的地址空间中，所有进程都可以访问共享内存中的地址，就好像它们是由用 C 语言 函数 malloc 分配的内存一样。 而如果某个进程向共享内存写入数据，所做的改动将立即影响到可以访问同一段共享内存的任何其他进程 。 不过，共享内存并未提供同步机制，也就是说，在第一个进程对共享内存的写操作结束之前，并无自动机制可以阻止第二个进程对它进行读取。 所以通常需要用其他的机制来同步对共享内存的访问 。 shmget在 Linux 中也提供了一组函数接口用于使用共享 内存， 首先常用的函数是 shmget ， 该函数用来创建共享内存，它用到的头文件是 ： #include &lt;sys/shm .h&gt; 函数原型是：int shmget(key_ t key, int size , int flag) ; 第一个参数，程序需要提供一个参数 key （非 0 整数），它有效地为共享内存段命名，shmget 函数运行成功时会返回一个与 key 相关的共享内存标识符（非负整数），用于后续的共享内存函数；调用失败返回－ 1 。不相关的进程可以通过该函数的返回值访问同一共享内存，它代表程序可能要使用的某个资源，程序对所有共享内存的访问都是间接的 。 程序先通过调用 shmget 函数并提供一个键，再由系统生成一个相应的共享内存标识符（ shmget 函数的返回值） 。 第二个参数， size 以字节为单位指定需要共享的内存容量。 第三个参数， shmfl.g 是权限标志，它的作用与 open 函数的 mode 参数一样，如果要想在key 标识的共享 内存不存在的条件下创建它的话，可以与 IPC_CREAT 做或操作 。 共享内存的权限标志与文件的读写权限一样，举例来说， 0644 表示允许一个进程创建的共享内存被内存创建者所拥有的进程向共享内存读取和写人数据，同时其他用户创建的进程只能读取共享内存 。 shmat当共享 内存创建后，其余进程可以调用 shmat 将其连接到自身的地址空间中，它的函数原型是 ：void *shmat(int shmid , void *addr , int flag) ; shmid 为 shmget 函数返回的共享存储标识符， addr 和 flag 参数决定了以什么方式来确定连接的地址，函数的返回值即是该进程数据段所连接的实际地址， 其他进程可以对此进程进行读写操作 。 shmdtshmdt 函数用于将共享 内存从当前进程中分离 。 注意，将共享内存分离并不是删除它，只是使该共享内存对当前进程不再可用 。 它的原型如下：int shmdt(const void *shmaddr) ; 参数 shmaddr 是 shmat 函数返回的地址指针，调用成功时返回 0 ，失败时返回－ 1 。 例子程序共享 内存是进程间通信的最快的方式，但是共享 内存的同步问题自身无法解决（即进程该何时去共享内存取得数据，而何时不能取），但用信号量即可轻易解决这个问题 。 下面使用例来说明如何使用信号量解决共享内存的同步问题 。 这个例子的主要功能是writer 向 reader 传递数据，并且只有在 writer 发送完毕后， reader 才取数据，否则阻塞等待 。 reader.cpp12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/sem.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/shm.h&gt;#include &lt;errno.h&gt;#define SEM_KEY 4001#define SHM_KEY 5678union semun &#123; int val;&#125;;int main(void)&#123; /*create a shm*/ int semid,shmid; shmid = shmget(SHM_KEY,sizeof(int),IPC_CREAT|0666); if(shmid&lt;0)&#123; printf("create shm error\n"); return -1; &#125; void * shmptr; shmptr =shmat(shmid,NULL,0); if(shmptr == (void *)-1)&#123; printf("shmat error:%s\n",strerror(errno)); return -1; &#125; int * data = (int *)shmptr; semid = semget(SEM_KEY,2,IPC_CREAT|0666);/*这里是创建一个semid，并且有两个信号量*/ union semun semun1;/*下面这四行就是初始化那两个信号量，一个val=0,另一个val=1*/ semun1.val=0; semctl(semid,0,SETVAL,semun1); semun1.val=1; semctl(semid,1,SETVAL,semun1); struct sembuf sembuf1; while(1)&#123; sembuf1.sem_num=0;/*sem_num=0指的是下面操作指向第一个信号量，上面设置可知其 val=0*/ sembuf1.sem_op=-1; /*初始化值为0，再-1的话就会等待*/ sembuf1.sem_flg=SEM_UNDO; semop(semid,&amp;sembuf1,1);/*reader在这里会阻塞,直到收到信号*/ printf("the NUM:%d\n",*data);/*输出结果*/ sembuf1.sem_num=1;/*这里让writer再次就绪，就这样循环*/ sembuf1.sem_op=1; sembuf1.sem_flg=SEM_UNDO; semop(semid,&amp;sembuf1,1); &#125; return 0;&#125; 然后是writer writer.cpp12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;sys/types.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/sem.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/shm.h&gt;#include &lt;errno.h&gt;#define SEM_KEY 4001#define SHM_KEY 5678union semun &#123; int val;&#125;;int main(void)&#123; /*create a shm*/ int semid,shmid; shmid = shmget(SHM_KEY,sizeof(int),IPC_CREAT|0666); if(shmid&lt;0) &#123; printf("create shm error\n"); return -1; &#125; void * shmptr; shmptr =shmat(shmid,NULL,0); if(shmptr == (void *)-1) &#123; printf("shmat error:%s\n",strerror(errno)); return -1; &#125; int * data = (int *)shmptr; semid = semget(SEM_KEY,2,0666); struct sembuf sembuf1; union semun semun1; while(1) &#123; sembuf1.sem_num=1;//这里指向第2个信号量（sem_num=1） sembuf1.sem_op=-1;//操作是-1，因为第2个信号量初始值为1，所以下面不会阻塞 sembuf1.sem_flg=SEM_UNDO; semop(semid,&amp;sembuf1,1);/*继续*/ scanf("%d",data); /*用户在终端输入数据*/ sembuf1.sem_num=0;/*这里指向第一个信号量*/ sembuf1.sem_op=1;/*操作加1*/ sembuf1.sem_flg=SEM_UNDO; semop(semid,&amp;sembuf1,1); //执行+1后，我们发现，reader阻塞正是由于第一个信号量为0， //无法减一，而现在writer先为其加1，那reader就绪！writer继续循环， //发现第二个信号量已经减为0，则阻塞了，我们回到reader*/ &#125; return 0;&#125; 输出多打开几个终端，同时执行 writer 程序，看是否 reader 能够正确地读到数据 writer : [b@host 1105]$ ./writer 51 09 977 writer : [b@host 1105]$ ./writer 22 11 55 55 5 reader : [b@host 1105]$ ./reader the NUM:22 the NUM:11 the NUM:55 the NUM:55 the NUM:5 the NUM:51 the NUM:9 the NUM:977 要想让程序安全地执行，就要有一种进程同步的进制，保证在进入临界区的操作是原子操作 。例如，使用信号量来进行进程的同步 。 因为对信号量的操作都是原子性的 。 System V信号量在 Linux 中提供了一组函数接口用于使用System V信号量 ，首先常用的函数是 semget，该函数用来创建和打开信号量 ，它用到的头文件是： 123#include &lt;sys / types . h&gt;#include &lt; sys / ipc . h &gt;#include &lt;sys/sem. h &gt; semget函数原型是：int semget( key_ t key , int nsems , int semflg) ; 该函数执行成功返回信号量标示符，失败则返回－ 1 。 参数 key 是函数通过调用负ok 函数得到的键值， nsems 代表创建信号量的个数，如果只是访问而不创建则可以指定该参数为0 ；但一旦创建了该信号量 ，就不能更改其信号量个数。 只要不删除该信号量 ，就可以重新调用该函数创建该键值的信号量 ，该函数只是返回以前创建的值，而不会重新创建。 semflg指定该信号茸的读写权限， 当创建信号量时不许加 IPCC阻AT ，若指定 IPC CREAT IIPCEXCL 后创建时发现存在该信号量 ，创建失败 。 semopsemop 函数，用于改变信号量的值，原型是：int semop(int semid, struct sembuf *sops , unsigned nsops) ; sem_id 是 由 semget 返回的信号量标识符， sembuf 结构的定义如下：1234567struct sembuf &#123; short sem_num; // 除非使用一组信号量，否则它为 O short sem_op ; // 信号量在一次操作中需要改变的数据，通常是两个数， // 一个是－ 1 ，即 p （等待）操作，一个是＋ 1 ，即 v （发送信号）操作 。 short sem_flg; // 通常为 SEM_UNDO ， 使操作系统跟踪信号， // 并在进程没有释放该信号量而终止时 ， 操作系统释放信号量&#125; semctlsemctl 函数，该函数用来直接控制信号量信息，它的原型是：int semctl (int semid, int semnum, int cmd , ... ) ; 如果有第 4 个参数，它通常是一个 union semum 结构，定义如下：12345union semun&#123; int val ; struct semid_ds *buf; unsigned short *arry ;&#125; 前两个参数与前面一个函数中的一样， cmd 通常是 SETVAL 或 IPC RMID 。 SETVAL用来把信号量初始化为一个己知的值 。 p 值通过 union semun 中的 val 成员设置，其作用是在信号量第一次使用前对它进行设置 。 IPC_RMID 用于删除一个已经无须继续使用的信号量标识符 ipcs命令ipcs 是一个 UINX/Linux 的命令 ，用于报告系统的消息队列、信号量、共享内存等 。 下面列举一些常用命令。 ipcs -a 用于列出本用户所有相关的 ipcs 参数，结果如下所示 : [b@host ~]$ ipcs -a ------ Shared Memory Segments -------- key shmid owner perms bytes nattch status 0x000004d1 32768 b 666 2052 0 0x000004d2 65537 b 666 2052 0 ------ Semaphore Arrays -------- key semid owner perms nsems ------ Message Queues -------- key msqid owner perms used-bytes messages ipcs -l 用于列出系统的限额 [b@host ~]$ ipcs -l ------ Shared Memory Limits -------- max number of segments = 4096 max seg size (kbytes) = 4194303 max total shared memory (kbytes) = 1073741824 min seg size (bytes) = 1 ------ Semaphore Limits -------- max number of arrays = 32000 max semaphores per array = 32000 max semaphores system wide = 1024000000 max ops per semop call = 500 semaphore max value = 32767 ------ Messages: Limits -------- max queues system wide = 32000 max size of message (bytes) = 65536 default max size of queue (bytes) = 65536 ipcs -u 用于列出当前的使用情况 [b@host ~]$ ipcs -u ------ Shared Memory Status -------- segments allocated 2 pages allocated 2 pages resident 2 pages swapped 0 Swap performance: 0 attempts 0 successes ------ Semaphore Status -------- used arrays = 3 allocated semaphores = 3 ------ Messages: Status -------- allocated queues = 0 used headers = 0 used space = 0 bytes ipcs -t 用于列出最后的访问时间 [b@host ~]$ ipcs -t ------ Shared Memory Attach/Detach/Change Times -------- shmid owner attached detached changed 32768 b May 18 06:46:54 May 18 06:47:43 May 18 06:45:48 65537 b May 18 06:45:57 May 18 06:46:08 May 18 06:45:57 ------ Semaphore Operation/Change Times -------- semid owner last-op last-changed ------ Message Queues Send/Recv/Change Times -------- msqid owner send recv change]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>noodle</tag>
        <tag>IPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟六之状态同步]]></title>
    <url>%2F2017%2F01%2F26%2Fstate_synchronization%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[自我总结状态同步的要点为 : input+state : 既通过网络发送输入信息又会发送状态信息来进行同步 发送端 优先级累加器 : 只发送一些重要的实体状态更新, 而不是所有都发. 如果遇到一个物体的状态更新信息不合适放到这个数据包里面，那么就跳过这个物体并尝试下一个。当你序列化完这个数据包以后，将那些已经在这帧更新过的物体在优先级累加器里面的值重置为0，但是那些没有在这帧更新过的物体在优先级累加器里面的值则保持不变。 接收端 抗网络抖动 : 做一个jitter buffer来缓冲数据, 然后以相同时间的间隔均匀取出 应用状态更新 : 一旦你的数据包从抖动缓冲器里面出来，你该在状态更新直接应用这些信息进行仿真。 对两边都量化(这里的量化指的是&lt;&lt;网络物理模拟五之快照压缩&gt;&gt;说的量化压缩技术) : 如果只有接收端用了量化的数据, 那接收端模拟的结果很可能与发送端不同, 所以要对两边都量化来避免发送端和接收端模拟的差异 长时间丢包的平滑处理 : 对于不同的网络断开时间用不同的平滑因子, 来自适应误差 增量压缩 : 相对编码 : 在数据包的包头里面发送最近确认的数据包的序列号（这个数据是从可靠的确认系统里面得到的）然后对每个物体编码相对这个基准帧的偏移量 绝对编码 原文原文出处 原文标题 : State Synchronization (Keeping simulations in sync by sending state) Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networked Physics.In the previous article we discussed techniques for compressing snapshots.In this article we round out our discussion of networked physics strategies with state synchronization, the third and final strategy in this article series.State SynchronizationWhat is state synchronization? The basic idea is that, somewhat like deterministic lockstep, we run the simulation on both sides but, unlike deterministic lockstep, we don&rsquo;t just send input, we send both input and state.This gives state synchronization interesting properties. Because we send state, we don&rsquo;t need perfect determinism to stay in sync, and because the simulation runs on both sides, objects continue moving forward between updates.This lets us approach state synchronization differently to snapshot interpolation. Instead of sending state updates for every object in each packet, we can now send updates for only a few, and if we&rsquo;re smart about how we select the objects for each packet, we can save bandwidth by concentrating updates on the most important objects.So what&rsquo;s the catch? State synchronization is an approximate and lossy synchronization strategy. In practice, this means you&rsquo;ll spend a lot of time tracking down sources of extrapolation divergence and pops. But other than that, it&rsquo;s a quick and easy strategy to get started with.ImplementationHere&rsquo;s the state sent over the network per-object:struct StateUpdate{ int index; vec3f position; quat4f orientation; vec3f linear_velocity; vec3f angular_velocity;};Unlike snapshot interpolation, we&rsquo;re not just sending visual quantities like position and orientation, we&rsquo;re also sending non-visual state such as linear and angular velocity. Why is this?The reason is that state synchronization runs the simulation on both sides, so it&rsquo;s always extrapolating from the last state update applied to each object. If linear and angular velocity aren&rsquo;t synchronized, this extrapolation is done with incorrect velocities, leading to pops when objects are updated.While we must send the velocities, there&rsquo;s no point wasting bandwidth sending (0,0,0) over and over while an object is at rest. We can fix this with a trivial optimization, like so:void serialize_state_update( Stream &amp; stream, int &amp; index, StateUpdate &amp; state_update ){ serialize_int( stream, index, 0, NumCubes - 1 ); serialize_vector( stream, state_update.position ); serialize_quaternion( stream, state_update.orientation ); bool at_rest = stream.IsWriting() ? state_update.AtRest() : false; serialize_bool( stream, at_rest ); if ( !at_rest ) { serialize_vector( stream, state_update.linear_velocity ); serialize_vector( stream, state_update.angular_velocity ); } else if ( stream.IsReading() ) { state_update.linear_velocity = vec3f(0,0,0); state_update.angular_velocity = vec3f(0,0,0); }}What you see above is a serialize function. It&rsquo;s a trick I like to use to unify packet read and write. I like it because it&rsquo;s expressive while at the same time it&rsquo;s difficult to desync read and write. You can read more about them here.Packet StructureNow let&rsquo;s look at the overall structure of packets being sent:const int MaxInputsPerPacket = 32;const int MaxStateUpdatesPerPacket = 64;struct Packet{ uint32_t sequence; Input inputs[MaxInputsPerPacket]; int num_object_updates; StateUpdate state_updates[MaxStateUpdatesPerPacket];};First we include a sequence number in each packet so we can determine out of order, lost or duplicate packets. I recommend you run the simulation at the same framerate on both sides (for example 60HZ) and in this case the sequence number can work double duty as the frame number.Input is included in each packet because it&rsquo;s needed for extrapolation. Like deterministic lockstep we send multiple redundant inputs so in the case of packet loss it&rsquo;s very unlikely that an input gets dropped. Unlike deterministic lockstep, if don&rsquo;t have the next input we don&rsquo;t stop the simulation and wait for it, we continue extrapolating forward with the last input received.Next you can see that we only send a maximum of 64 state updates per-packet. Since we have a total of 901 cubes in the simulation so we need some way to select the n most important state updates to include in each packet. We need some sort of prioritization scheme.To get started each frame walk over all objects in your simulation and calculate their current priority. For example, in the cube simulation I calculate priority for the player cube as 1000000 because I always want it to be included in every packet, and for interacting (red cubes) I give them a higher priority of 100 while at rest objects have priority of 1.Unfortunately if you just picked objects according to their current priority each frame you&rsquo;d only ever send red objects while in a katamari ball and white objects on the ground would never get updated. We need to take a slightly different approach, one that prioritizes sending important objects while also distributing updates across all objects in the simulation.Priority AccumulatorYou can do this with a priority accumulator. This is an array of float values, one value per-object, that is remembered from frame to frame. Instead of taking the immediate priority value for the object and sorting on that, each frame we add the current priority for each object to its priority accumulator value then sort objects in order from largest to smallest priority accumulator value. The first n objects in this sorted list are the objects you should send that frame.You could just send state updates for all n objects but typically you have some maximum bandwidth you want to support like 256kbit/sec. Respecting this bandwidth limit is easy. Just calculate how large your packet header is and how many bytes of preamble in the packet (sequence, # of objects in packet and so on) and work out conservatively the number of bytes remaining in your packet while staying under your bandwidth target.Then take the n most important objects according to their priority accumulator values and as you construct the packet, walk these objects in order and measure if their state updates will fit in the packet. If you encounter a state update that doesn&rsquo;t fit, skip over it and try the next one. After you serialize the packet, reset the priority accumulator to zero for objects that fit but leave the priority accumulator value alone for objects that didn&rsquo;t. This way objects that don&rsquo;t fit are first in line to be included in the next packet.The desired bandwidth can even be adjusted on the fly. This makes it really easy to adapt state synchronization to changing network conditions, for example if you detect the connection is having difficulty you can reduce the amount of bandwidth sent (congestion avoidance) and the quality of state synchronization scales back automatically. If the network connection seems like it should be able to handle more bandwidth later on then you can raise the bandwidth limit.Jitter BufferThe priority accumulator covers the sending side, but on the receiver side there is much you need to do when applying these state updates to ensure that you don&rsquo;t see divergence and pops in the extrapolation between object updates.The very first thing you need to consider is that network jitter exists. You don&rsquo;t have any guarantee that packets you sent nicely spaced out 60 times per-second arrive that way on the other side. What happens in the real world is you&rsquo;ll typically receive two packets one frame, 0 packets the next, 1, 2, 0 and so on because packets tend to clump up across frames. To handle this situation you need to implement a jitter buffer for your state update packets. If you fail to do this you&rsquo;ll have a poor quality extrapolation and pops in stacks of objects because objects in different state update packets are slightly out of phase with each other with respect to time.All you do in a jitter buffer is hold packets before delivering them to the application at the correct time as indicated by the sequence number (frame number) in the packet. The delay you need to hold packets for in this buffer is a much smaller amount of time relative to interpolation delay for snapshot interpolation but it&rsquo;s the same basic idea. You just need to delay packets just enough (say 4-5 frames @ 60HZ) so that they come out of the buffer properly spaced apart.Applying State UpdatesOnce the packet comes out of the jitter how do you apply state updates? My recommendation is that you should snap the physics state hard. This means you apply the values in the state update directly to the simulation.I recommend against trying to apply some smoothing between the state update and the current state at the simulation level. This may sound counterintuitive but the reason for this is that the simulation extrapolates from the state update so you want to make sure it extrapolates from a valid physics state for that object rather than some smoothed, total bullshit made-up one. This is especially important when you are networking large stacks of objects.Surprisingly, without any smoothing the result is already pretty good:Your browser does not support the video tag.As you can see it&rsquo;s already looking quite good and barely any bandwidth optimization has been performed. Contrast this with the first video for snapshot interpolation which was at 18mbit/sec and you can see that using the simulation to extrapolate between state updates is a great way to use less bandwidth.Of course we can do a lot better than this and each optimization we do lets us squeeze more state updates in the same amount of bandwidth. The next obvious thing we can do is to apply all the standard quantization compression techniques such as bounding and quantizing position, linear and angular velocity value and using the smallest three compression as described in snapshot compression.But here it gets a bit more complex. We are extrapolating from those state updates so if we quantize these values over the network then the state that arrives on the right side is slightly different from the left side, leading to a slightly different extrapolation and a pop when the next state update arrives for that object.Your browser does not support the video tag.Quantize Both SidesThe solution is to quantize the state on both sides. This means that on both sides before each simulation step you quantize the entire simulation state as if it had been transmitted over the network. Once this is done the left and right side are both extrapolating from quantized state and their extrapolations are very similar.Because these quantized values are being fed back into the simulation, you&rsquo;ll find that much more precision is required than snapshot interpolation where they were just visual quantities used for interpolation. In the cube simulation I found it necessary to have 4096 position values per-meter, up from 512 with snapshot interpolation, and a whopping 15 bits per-quaternion component in smallest three (up from 9). Without this extra precision significant popping occurs because the quantization forces physics objects into penetration with each other, fighting against the simulation which tries to keep the objects out of penetration. I also found that softening the constraints and reducing the maximum velocity which the simulation used to push apart penetrating objects also helped reduce the amount of popping.Your browser does not support the video tag.With quantization applied to both sides you can see the result is perfect once again. It may look visually about the same as the uncompressed version but in fact we&rsquo;re able to fit many more state updates per-packet into the 256kbit/sec bandwidth limit. This means we are better able to handle packet loss because state updates for each object are sent more rapidly. If a packet is lost, it&rsquo;s less of a problem because state updates for those objects are being continually included in future packets.Be aware that when a burst of packet loss occurs like 1&frasl;4 a second with no packets getting through, and this is inevitable that eventually something like this will happen, you will probably get a different result on the left and the right sides. We have to plan for this. In spite of all effort that we have made to ensure that the extrapolation is as close as possible (quantizing both sides and so on) pops can and will occur if the network stops delivering packets.Visual SmoothingWe can cover up these pops with smoothing.Remember how I said earlier that you should not apply smoothing at the simulation level because it ruins the extrapolation? What we&rsquo;re going to do for smoothing instead is calculating and maintaining position and orientation error offsets that we reduce over time. Then when we render the cubes in the right side we don&rsquo;t render them at the simulation position and orientation, we render them at the simulation position + error offset, and orientation * orientation error.Over time we work to reduce these error offsets back to zero for position error and identity for orientation error. For error reduction I use an exponentially smoothed moving average tending towards zero. So in effect, I multiply the position error offset by some factor each frame (eg. 0.9) until it gets close enough to zero for it to be cleared (thus avoiding denormals). For orientation, I slerp a certain amount (0.1) towards identity each frame, which has the same effect for the orientation error.The trick to making this all work is that when a state update comes in you take the current simulation position and add the position error to that, and subtract that from the new position, giving the new position error offset which gives an identical result to the current (smoothed) visual position.The same process is then applied to the error quaternion (using multiplication by the conjugate instead of subtraction) and this way you effectively calculate on each state update the new position error and orientation error relative to the new state such that the object appears to have not moved at all. Thus state updates are smooth and have no immediate visual effect, and the error reduction smoothes out any error in the extrapolation over time without the player noticing in the common case.I find that using a single smoothing factor gives unacceptable results. A factor of 0.95 is perfect for small jitters because it smooths out high frequency jitter really well, but at the same time it is too slow for large position errors, like those that happen after multiple seconds of packet loss:Your browser does not support the video tag.The solution I use is two different scale factors at different error distances, and to make sure the transition is smooth I blend between those two factors linearly according to the amount of positional error that needs to be reduced. In this simulation, having 0.95 for small position errors (25cms or less) while having a tighter blend factor of 0.85 for larger distances (1m error or above) gives a good result. The same strategy works well for orientation using the dot product between the orientation error and the identity matrix. I found that in this case a blend of the same factors between dot 0.1 and 0.5 works well.The end result is smooth error reduction for small position and orientation errors combined with a tight error reduction for large pops. As you can see above you don&rsquo;t want to drag out correction of these large pops, they need to be fast and so they&rsquo;re over quickly otherwise they&rsquo;re really disorienting for players, but at the same time you want to have really smooth error reduction when the error is small hence the adaptive error reduction approach works really well.Your browser does not support the video tag.Delta CompressionEven though I would argue the result above is probably good enough already it is possible to improve the synchronization considerably from this point. For example to support a world with larger objects or more objects being interacted with. So lets work through some of those techniques and push this technique as far as it can go.There is an easy compression that can be performed. Instead of encoding absolute position, if it is within a range of the player cube center, encode position as a relative offset to the player center position. In the common cases where bandwidth is high and state updates need to be more frequent (katamari ball) this provides a large win.Next, what if we do want to perform some sort of delta encoding for state synchronization? We can but it&rsquo;s quite different in this case than it is with snapshots because we&rsquo;re not including every cube in every packet, so we can&rsquo;t just track the most recent packet received and say, OK all these state updates in this packet are relative to packet X.What you actually have to do is per-object update keep track of the packet that includes the base for that update. You also need to keep track of exactly the set of packets received so that the sender knows which packets are valid bases to encode relative to. This is reasonably complicated and requires a bidirectional ack system over UDP. Such a system is designed for exactly this sort of situation where you need to know exactly which packets definitely got through. You can find a tutorial on how to implement this in this article.So assuming that you have an ack system you know with packet sequence numbers get through. What you do then is per-state update write one bit if the update is relative or absolute, if absolute then encode with no base as before, otherwise if relative send the 16 bit sequence number per-state update of the base and then encode relative to the state update data sent in that packet. This adds 1 bit overhead per-update as well as 16 bits to identify the sequence number of the base per-object update. Can we do better?Yes. In turns out that of course you&rsquo;re going to have to buffer on the send and receive side to implement this relative encoding and you can&rsquo;t buffer forever. In fact, if you think about it you can only buffer up a couple of seconds before it becomes impractical and in the common case of moving objects you&rsquo;re going to be sending the updates for same object frequently (katamari ball) so practically speaking the base sequence will only be from a short time ago.So instead of sending the 16 bit sequence base per-object, send in the header of the packet the most recent acked packet (from the reliability ack system) and per-object encode the offset of the base sequence relative to that value using 5 bits. This way at 60 packets per-second you can identify an state update with a base half a second ago. Any base older than this is unlikely to provide a good delta encoding anyway because it&rsquo;s old, so in that case just drop back to absolute encoding for that update.Now lets look at the type of objects that are going to have these absolute encodings rather than relative. They&rsquo;re the objects at rest. What can we do to make them as efficient as possible? In the case of the cube simulation one bad result that can occur is that a cube comes to rest (turns grey) and then has its priority lowered significantly. If that very last update with the position of that object is missed due to packet loss, it can take a long time for that object to have its at rest position updated.We can fix this by tracking objects which have recently come to rest and bumping their priority until an ack comes back for a packet they were sent in. Thus they are sent at an elevated priority compared with normal grey cubes (which are at rest and have not moved) and keep resending at that elevated rate until we know that update has been received, thus &ldquo;committing&rdquo; that grey cube to be at rest at the correct position.ConclusionAnd that&rsquo;s really about it for this technique. Without anything fancy it&rsquo;s already pretty good, and on top of that another order of magnitude improvement is available with delta compression, at the cost of significant complexity! 译文 译文出处 译者：陈敬凤（nunu） 审校：崔国军（飞扬971） 介绍大家好，我是格伦·菲德勒。欢迎阅读《网络物理模拟》的系列文章，这个系列文章的主题是关于如何将一个物理模拟通过网络通信进行同步。 在这篇文章，我们将讨论第三种也是最后一种同步的策略：状态同步。 状态同步概念在我看来，这是最简单的同步策略也是最容易理解的同步策略。事实上，这是我开始实现《雇佣兵2：战火纷飞》的网络物理部分的时候我首先尝试的同步策略。这个同步策略的基本想法是我们在网络的两侧同时运行仿真，但是与具有确定性的帧同步不同的是帧同步会通过网络发送输入信息并且依赖完美的确定性来保持同步，这种同步策略是既通过网络发送输入信息又会发送状态信息来进行同步。 这就赋予了状态同步与之前的同步策略完全不同的属性。与具有确定性的帧同步不同，这种同步策略不需要要求确定性来保持同步，因为我们可以迅速的通过网络发送状态来纠正任何的偏差。这种同步策略也跟快照信息的插值不同，如果一个对象不在数据包里面的话，这个物体还是会继续移动，因为网络两侧的仿真都在持续的运行。 正是由于这一特性，状态同步的实现方法才会与快照信息的插值有差别。不再是在每个数据包里面发送每个物体的状态更新信息，我们可以只对几个对象进行更新。如果我们在每个数据包选择要同步的物体的时候方案比较聪明的话，我们可以更有效地利用带宽，把注意力主要集中在最重要的物体的更新上，而那些不那么重要的物体，他们的更新信息可以以一个较低的速率进行发送。这样的话，相比较快照信息插值这种要在一个快照里面包括所有物体的方法，状态同步这种方法使用的带宽可以减少一个数量级。此外，状态同步这种方法不会在网络延迟之上还要附加插值带来的延迟，因为它相比较于快照信息插值这种方法，延迟也更低。 这样做的代价是状态同步是一个近似和有损的同步策略。如果推送信息的时候出现了一些问题导致大量的数据包丢失的话，远程的模拟仿真使用的是过期的数据进行预测。根据我的经验，如果使用这个同步策略的话，你会花很多时间追踪由于进行预测所带来的差异。如果使用这个同步策略的话，在大量物体堆叠的时候，会看到很多物体的移动不正常，并且很难精确地追查。在这篇文章中，我会告诉你如何追踪并通过网络发送量化和压缩的物理状态来减少分歧的根源。 实现让我们从实际的实现来看下这个同步策略。这里是每个发送的对象的网络状态：12345678struct StateUpdate &#123; int index; vec3f position; quat4f orientation; vec3f linear_velocity; vec3f angular_velocity;&#125;;需要注意的是，我们发送的不仅仅是一些像位置、方向这样的视觉信息，这个地方与快照信息插值那种方法相同，我们还发送了很多非视觉的物体状态信息，比如线性速度和角速度，这是与快照信息插值那种方法不同的地方。这么做是必要的是因为物理仿真需要对每个物体最近的状态进行外推。因此，状态更新需要提供所有进行外推所需的信息，以便能够正确的进行推测。如果一个物体的速度信息没有发送的话，在预测物体前进的时候，就会使用一个不正确的速度信息，这将导致下一次物体信息进行更新的时候有一个拉扯。 当我们在网络上对状态更新进行序列化的时候，没有必要对不动的物体浪费网络带宽，为这些不动的物体发送什么(0,0,0)来表示线性速度和角速度。我们可以做一个简单的优化，通过把物体的静止状态包含在内来给每个静止的物体节省24字节的带宽：123456789101112131415161718192021void serialize_state_update( Stream &amp; stream, int &amp; index, StateUpdate &amp; state_update )&#123; serialize_int( stream, index, 0, NumCubes - 1 ); serialize_vector( stream, state_update.position ); serialize_quaternion( stream, state_update.orientation ); bool at_rest = stream.IsWriting() ? state_update.AtRest() : false; serialize_bool( stream, at_rest ); if ( !at_rest ) &#123; serialize_vector( stream, state_update.linear_velocity ); serialize_vector( stream, state_update.angular_velocity ); &#125; else if ( stream.IsReading() ) &#123; state_update.linear_velocity = vec3f(0,0,0); state_update.angular_velocity = vec3f(0,0,0); &#125;&#125;上面的代码就是我所谓的序列化功能。这里面有一个我喜欢的小技巧来统一位打包器的读取和写入函数，它们通常是分开实现的。这个函数会在两种不同的上下文中进行调用：写入的时候和读取的时候。你可以通过IsReading/IsWriting函数来知道自己目前处在哪个上下文。我喜欢这个技巧的原因是如何读取和写入功能统一在一个函数的时候，读取和写入的不同步就会很少发生。如果你希望读取和写入功能统一在一起并且像我这样进行数据包的数据，请参考这里。 数据包结构体当把状态更新写入的时候，如果这个物体是静止不动的话，这个函数其实只序列化了一比特的信息而不会更新线性速度和角速度的信息。如果这个物体不是静止不动的话，会把线性速度和角速度的信息写入之前先写入一比特的信息。在从数据包进行读取的时候，代码会读取这个比特位，如果这个比特位是0的话，会从这个比特流里面读取线性速度和角速度的信息，否则的话，会把物体的线性速度和角速度全部清为（0,0,0）。这是一个非常简单而有效的无损带宽压缩策略能够针对静止不动的物体进行数据的压缩，能够节省将近一半的带宽。接下来让我们看一下被发送的数据包的结构：1234567891011const int MaxInputsPerPacket = 32;const int MaxStateUpdatesPerPacket = 64; struct Packet&#123; uint32_t sequence; Input inputs[MaxInputsPerPacket]; int num_object_updates; StateUpdate state_updates[MaxStateUpdatesPerPacket];&#125;;从上面的数据包结构中，你可以看到，首先登场的是我们在每个数据包包含的序列号，通过这个数据信息我们可以判断数据包是否出故障、丢失或者重复。我强烈建议你在网络两侧的运行都按照相同的帧速率（比如说60fps）进行仿真。在这种情况下，你还可以给序列号赋予另外一重任务：作为状态更新的帧号。 输入信息被包含在每个数据包里面，这是因为仿真需要输入信息才能进行外推。当仿真在网络的另外一侧运行的时候，我们希望通过状态更新的信息以及运行玩家相同的输入信息来往前预测后续状态的信息，并且希望预测出来的状态能够和真实的状态尽可能的接近。就跟具有确定性的帧同步一样，我们发送了多个冗余输入信息，这样即使在有包丢失的情况下，输入信息也不太可能完全被丢弃而不能到达网络的另外一端，但是跟具有确定性的帧同步不一样的是，就算是最坏的情况下（也就是我们没有收到后续的输入信息的情况），我们本地的仿真也不会停止并且等待后续的输入信息的到来，我们还是会根据最后收到的输入信息继续往前模拟。接下来，你可以看到，在一个数据包里面我们最多发送64个状态更新。我们在仿真的场景中一共有901个立方体，所以我们需要一些方法来从这901个立方体里面选出一些最重要的立方体，在每一个数据包进行数据更新。我们需要某种优先级方案，这样我们才能找到最重要的物体，允许我们只会很频繁的发送最重要的物体的状态信息，而那些不怎么重要的物体的更新就会不那么的频繁，零零散散的更新，这样保证所有对象都有机会进行更新和发送，但是又能让最重要的那些物体始终得到更新。这样就要求在仿真的每一帧开始的时候遍历所有的物体并且计算它们当前的优先级。让我们举个简单的例子来说，在立方体模拟这个场景中，我把玩家立方体的优先级设为100000，因为我希望玩家立方体的更新信息能够包含在每个数据包里面，而对于发生交互的立方体（那些红色的立方体），我赋予它们的优先级为100，而所有静止不动的立方体，优先级为1。非常遗憾的是如果只有这一个机制的话，是不足以公平分配对象的更新的，这是因为如果你刚刚仅仅是在每一帧对物体的当前优先级进行了排序，这样的话，如果人物立方体和红色立方体有交互的话，那么就永远只有红色立方体的信息会被发送，而地面上的白色立方体则永远不会更新。我们需要一个稍微不同的方法，优先发送重要的对象，同时也会在仿真的过程让那些不重要的物体也有更新的机会。优先级累加器你可以通过优先级累加器做到这一点。这是一组浮点数值，每个对象都会有一个对应的浮点数值，在帧与帧之间会一直保留。有了这个值以后，不再是根据当前帧中这个物体的重要性进行排序，而是在每一帧中将每个物体的重要性加到这个优先级累加器中，然后对优先级累加器的值进行从大到小进行排序。这个排序的顺序中前面的物体就是这一帧中你应该发送的物体。你可以为所有的N个物体发送状态更新信息，但是通常情况下你的带宽会有一些限制，比如说你需要控制在256k比特每秒。尊重这个带宽限制是很容易的。只要计算出你的数据包Header有多大，并且计算下数据包的preamble部分有多大（这主要是指序号、标记哪些物体在这个数据包等信息），这样就能计算出来你的数据包还剩下多少字节，可以通过计划传递多少个物体的更新信息来确保带宽小于约定的最大带宽。然后根据它们的优先级累加器里面的值选取数目和上面计算相符合的n个最重要的物体，然后用这n个最重要的物体的更新信息来构建你的数据包。对这n个最重要的物体进行依次遍历并测试它们的数据更新信息是否应该放在这个数据包里面。如果遇到一个物体的状态更新信息不合适放到这个数据包里面，那么就跳过这个物体并尝试下一个。当你序列化完这个数据包以后，将那些已经在这帧更新过的物体在优先级累加器里面的值重置为0，但是那些没有在这帧更新过的物体在优先级累加器里面的值则保持不变。通过使用这个办法，刚才检测不合适放到数据包的物体的更新信息会被首先包含在下一帧的数据包里面。使用这种同步策略，所需的带宽甚至可以动态调整。这使得这种同步策略可以很容易的根据不断变化的网络条件来调整状态同步的信息量，让我们举个简单的例子来说，如果你发现连接有困难，就可以减少发送所占的带宽（拥塞避免），这样状态同步的规模会逐步的自动回复回来。如果网络连接似乎可以处理更多的带宽，那么就可以把带宽限制提高。我们这里所做的处理主要是在网络发送这一端。抖动缓冲区对物体做了优先级的排序，并且在每帧里面更新物体在优先级累加器里面的值，并在每次发送数据包的时候只发送n个最重要的物体。但是在网络接收这一端，还有很多需要做的事情，比如当应用接收过来的状态更新信息的时候，如何避免与之前预测的物体状态信息之间的差异会被玩家感觉到。你需要考虑的第一件事情是，网络抖动的存在。你没有任何办法来确保你发送的数据包就是完美的是每秒60次的频率抵达网络的另外一侧。在现实世界中会发生的事情是你可能在一帧中收到两个数据包，然后在下一帧一个数据包也收不到，然后下面几帧可能是1个，2个或者0个。为了处理这种情况，你需要实现一个抖动缓冲器来保存你的状态更新数据包。如果你不这样做，所做的推测质量就很难保证而且会出现对象堆叠的情况，这是因为在不同的状态更新数据包里面，每一个物体的信息都会有些轻微的变化。 在抖动缓冲器你所要做的事情就是保存这些数据包，然后根据数据包里面的序号（其实也就是帧号了）来在正确的时间将数据包发给应用程序处理。你需要在这个缓冲区来保存数据包所导致的延迟相比较快照信息插值所带来的延迟是一段非常微小的时间，但是这两种同步策略的基本思想是一致的。你只要稍微延迟一下数据包让时间刚刚好就好（比如说每秒60次更新的情况延迟个4-5帧），这样数据包就能够以合适的间隔从缓冲区里面出来到达应用程序。应用状态更新一旦你的数据包从抖动缓冲器里面出来，你该如何使用数据包的信息进行状态更新呢？我的建议是，你要努力对齐这种状态。在状态更新直接应用这些信息进行仿真。我反对在状态更新和当前仿真的目前状态之间做一些平滑的更替。这听起来可能有悖常理，但这样做的原因是，当前有些物体的状态可能是根据之前状态推测出来的，所以你要保证这个物体的预测信息是从物体有效的物理状态出发，而不是一些平滑出来的完全是虚假的数据出发。这在你有大量的物体对象的时候会格外的重要。到目前为止，我们已迅速建立了一个实用的同步策略而没做有太多的工作。事实上，这种同步策略已经足够好了，已经完全可以在互联网上进行游戏对战了，并且它对丢包、抖动和带宽限制都处理的相当不错。【视频1：cube_state_sync_uncompressed】正如你可以看到的那样，这种同步策略已经看上去相当不错了，并且几乎没有做任何的带宽优化。与快照信息插值那种策略的第一次18m每秒的信息量相比，你可以看到在状态更新之间进行状态的推测是使用更少的带宽的好方法。当然，我们可以做得比现在的状态好的多，每次我们做优化的时候我们都可以使用相同的带宽来传递更多的物体状态更新信息。我们可以做的下一个明显有效果的事情是应用所有的标准量化压缩技术，比如压缩边界和量化位置、线性速度和角速度的值以及如同《网络物理模拟五之快照压缩》里面的描述的“最小的三个变量”方法。但在这里它变得更复杂一点。我们从这些状态更新向前进行推测，所以如果我们量化这些通过网络传递的值的话，那么到达右侧的状态将与左侧的状态稍有不同，这会让推测变得更加不准，并且当下一个状态更新到达的时候会出现一些拉扯现象。【视频2： cube_state_sync_compressed】对两边都量化解决的办法是量化两侧的状态。这意味着，在每一个仿真进行一次更新之前，你要对网络的两侧同时量化整个模拟状态，就好像它们都已经在网络上传输了一样。一旦这样做了的话，左侧和右侧都是从量化的状态进行推测，这样它们的推测结果就会非常的接近。由于这些量化以后的值会被反馈到仿真中去，你会发现这种方法对精确度的要求比快照信息插值的方法所要求的精确度要高的多，因为在快照信息插值的方法里面，使用的数据只是用来插值的视觉信息。在立方体模拟这个情况下，我发现有必要对于每米要有4096个位置的精度，而在快照信息插值的方法里每米只要有512个位置的精度就可以了，所以四元数最小的三个分量每个要15比特位（在快照信息插值的方法里四元数最小的三个分量每个只要9个比特位的信息就行）。如果没有这种额外的精度出现，就非常容易出现物体的拉扯的情况，这是因为量化以后的数据会迫使物理对象相互渗透，这与模拟要所求的尽量保持物理对象不相互渗透的努力是背道而驰的。我还发现，软化约束以及减少模拟用于推开相互渗透的物理对象的最大速度也有助于减少出现物体拉扯的情况。【视频3： cube_state_sync_quantize_both_sides】在量化应用于网络两侧的模拟以后，就可以再次看到结果是比较完美的。这种处理以后，看起来视觉效果与未压缩版本差不多，但事实上通过这种方案我们能够适应每个包进行更多的状态更新，同时还能满足每秒256比特的带宽限制。这意味着我们能够更好地处理数据包的丢失，因为每一个对象的状态更新可以更迅速的发送。如果出现数据包丢失的情况，对整个模拟来说也会引发更少的问题，这是因为通过未来到来的书包正在持续不断地对这些物体进行状态更新。 请注意如果出现数据包的集中丢弃的情况，比如说在四分之一秒的时间没有数据包通过，这种情况是不可避免的，总是会发生一些这样的事情，你可能会在网络的两侧得到完全不同的结果。我们必须为这种情况进行规划。我们会尽一切努力来确保外推是尽可能与实际结果相接近的（采用在网络的两侧进行量化以及其他一些方案），但是由于网络停止传输数据包，还是会发生各种拉扯和不准确的情况。 视觉平滑还记得我之前说过的那个事情么？你不应该对模拟这一侧使用平滑算法，因为它会对外推有不好的影响吗? 我们要做的不是平滑而是计算和维护位置和方向的误差补偿，这个量会随着时间而减少。然后当我们在网络的右侧渲染立方体的时候，我们并不是用模拟的位置和方向对这些立方体进行渲染，我们是用模拟的位置和方向再加上误差补偿来对这些立方体进行渲染。位置信息是模拟位置信息加上误差补偿，方向信息是模拟的方向信息再乘以方向的误差补偿。 随着时间的推移，我们努力减少这些误差补偿，让位置的误差补偿尽量趋近于0，而方向的误差补偿尽量趋近于一致。为了减少误差，我使用了一个指数平滑的移动，平均线趋近零。所以实际上，我用每一帧的位置误差乘以某个系数（比如说是0.9），直到它接近于零而被清除(这样就避免了突变)。对于方向而言，我用某一个固定的量（比如说是0.1）来对每一帧的标准向量进行球面插值，这个可以达到方向误差相同的效果。 让所有事情都能够正常运行的诀窍在于当一个状态更新数据包到达的时候，你获取当前的模拟位置信息，并把位置误差添加上去，然后再从新的位置里面减去这个值，这样就可以让新位置的位置误差和当前的视觉位置比较一致（平滑）。然后把相同的过程应用于四元数误差（使用乘法的共轭而不是减法来与基准方向信息进行叠加），通过这种方法你就可以有效的计算在每个状态更新数据包到达的时候，相对于新的状态下新的位置误差和方向误差，这样处理的话物体看上去就根本没有进行任何的移动。因此状态更新的非常平滑，没有任何突然移动的视觉效果，而且可以随着时间慢慢减少由于推断带来的误差而通常情况下这么处理不会让玩家注意到。 我发现只使用一个单独的平滑因子会产生不可接受的结果。平滑因子0.95对于那些小的抖动来说是非常完美的，因为它对那些高频抖动的平滑是非常完美的，但是它对于大的位置误差来说平滑的太慢了，比如说发生了好几秒数据包丢失以后，物体的位置和实际位置差的比较大，这时候用这个因子来平滑就太慢了： 【视频4：cube_state_sync_basic_smoothing】我使用的解决方案是针对不同的误差距离使用两个不同的平滑因子，并且我会根据需要减少的位置误差的大小来对这两个平滑因子进行线性的混合来让过渡非常的平滑。在这个模拟中，我使用的是0.95来平滑小的位置误差(针对25厘米或者误差更小的情况)，而对于大一点的距离而言会使用一个更严格的混合系数0.85(针对1米或者误差更大的情况)，这给出了一个非常好的结果。对于方向而言，相同的策略适用于对方向误差和单位矩阵使用点积的情况。我发现在这种情况下，混合系数分别采用0.1和0.5的效果就非常的好。 最终的结果是对小的位置误差和方向误差的平滑操作与对大的位置误差和方向误差的快速收敛很好的结合在了一起。正如你在上面看到的那样，你不想拖着一直不处理这些大的位置误差和方向误差，这些大的位置误差和方向误差需要被快速的解决否则它们会给玩家造成非常大的困扰，但是同时当位置误差和方向误差很小的时候你希望这个误差减少的过程能够非常的平滑，因此自适应误差减少方法效果很好。 【视频5：cube_state_sync_adaptive_smoothing.mp4】增量压缩尽管我认为上述结果可能已经足够好了，从这一点上来看可以大大提高同步的质量。让我们举些简单的例子来说明，比如支持一个有大量对象的世界或者有更多的对象与之交互。所以让我们通过一些技术上的改进，来推动这项技术尽可能的完美。 有一种简单的压缩，可以立刻执行。不再是编码绝对位置，如果位置是在玩家立方体中心的某个范围之内的话，就会以玩家的中心位置的偏移量来进行编码。如果是常见情况下，带宽很高而且状态更新需要非常的频繁（katamari球），通过这种方法就能节省下很多带宽。 接下来，如果我们想对状态同步执行某种增量编码怎么办? 我们可以做到但是具体的方法会和快照里面的增量编码方法差别很大，这是因为在这种情况下我们的每个数据包不会包含每一个立方体的信息，所以我们不能跟踪最新收到的数据包，并且自以为地觉得这个数据包的所有这些状态更新都是相对于X这个数据包的。 你实际要做的就是逐对象的进行更新，对数据包进行跟踪包括更新的基础值。你还需要跟踪收到的数据包的准备的数量，这样发送方才能知道哪些数据包可以作为增量编码有效的基础值。这是相当复杂的，并且是需要通过UDP协议进行双向确认的系统。这样一个系统是专为这种情况设计的，因为你肯定需要知道哪些数据包确定是到达了另外一侧。你可以在这个教程里面找到具体如何实现这个功能的指南。 所以假设你有一个确认系统，这样你就知道已经发送到网络另外一侧的数据包的序列号。你所要做的就是在每个状态更新的时候，用一位数据来记录下这个更新到底是相对更新还是绝对更新，如果是绝对更新就没有针对基础编码这回事，否则就是一个相对更新，所以要发送16位序列号来标记每个状态相对应的基础状态，然后相对于基础状态对更新数据进行编码并通过数据包进行发送。这为每次更新增加了1比特开销，以及需要增加16位序列号的开销来标记每个物体更新的基准帧。我们可以做得更好吗? 是的。确实可以做的更好。你要在发送和接收端进行缓冲来实现这个相对编码机制，但是你不可能永远缓冲。事实上，如果你仔细想想，你只能缓冲几秒钟然后整个缓冲就变得不切实际，对于物体在移动这个常见的情况，你会经常发送相同对象的更新信息（比如说katamari球），所以实际上基准帧只能是很短时间之前的一帧状态。 所以对每个物体发送16位的序列号来表明基准帧，在数据包的包头里面发送最近确认的数据包的序列号（这个数据是从可靠的确认系统里面得到的）然后对每个物体编码相对这个基准帧的偏移量，这个偏移量使用5位信息。通过这种方式在每秒60个数据包的情况下，你可以识别相对于基准帧半秒前的状态更新。任何比这个值更老的基准帧不太可能提供一个良好的增量编码的基准，主要是因为它们太老了，所以在这种情况下就要切回到绝对编码进行状态更新。 现在让我们看看会使用这些绝对编码而不是相对编码的对象的类型。他们是静止的对象。我们能做什么来让他们的更新尽可能的高效?在这种立方体模拟的情况，一个可能发生的很糟糕的结果是一个立方体进行停止状态（变成灰色）然后它的优先级显著降低。如果由于数据包的丢失，导致最后对象的位置更新信息被错过的话，可能需要很长时间才会轮到这个物体来更新它的停止位置信息。 我们可以通过跟踪哪些最近变成停止状态的对象来解决这个问题，并且会提高这些对象的优先级直到一个确认包返回来标记这些对象的位置更新信息已经被成功的发送了。因此他们的发送优先级会相对于正常的灰色立方体（那些处于静止状态没有移动的立方体）的发送优先级有一定的提高，并且会在这个提高后的优先级上一直发送，直到我们知道对这些立方体的更新信息已经收到，也就是网络的另外一侧会“承诺”把这些灰色的立方体放在正确的位置上停止。 最后这就是有关于这种技术的全部内容。它非常的有趣，不需要任何花哨的内容就已经足够好了，然后在此基础上可以做一个数量级的带宽节省（通过增量编码），但是这个方案的复杂性非常的高。 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟五之快照压缩]]></title>
    <url>%2F2017%2F01%2F24%2Fsnapshot_compression%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[自我总结快照压缩技术的要点为 : 压缩Orientation数据 : 利用四元数的”最小的三个分量”性质:x^2+y^2+z^2+w^2 = 1 来在传输的时候丢弃一个分量并在网络的另外一端对整个四元数进行重建 压缩线性速度和Postion数据 : 把他们限制在某个范围内, 就可以用这个范围的最大值所占用的比特位数来保存这两种数据了, 而不用一个超大的数来保证可以保存他们的最大值了(占用超多bit位) 增量压缩 . . . 原文原文出处 原文标题 : Snapshot Compression (Advanced techniques for optimizing bandwidth) Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networked Physics.In the previous article we sent snapshots of the entire simulation 10 times per-second over the network and interpolated between them to reconstruct a view of the simulation on the other side.The problem with a low snapshot rate like 10HZ is that interpolation between snapshots adds interpolation delay on top of network latency. At 10 snapshots per-second, the minimum interpolation delay is 100ms, and a more practical minimum considering network jitter is 150ms. If protection against one or two lost packets in a row is desired, this blows out to 250ms or 350ms delay.This is not an acceptable amount of delay for most games, but when the physics simulation is as unpredictable as ours, the only way to reduce it is to increase the packet send rate. Unfortunately, increasing the send rate also increases bandwidth. So what we&rsquo;re going to do in this article is work through every possible bandwidth optimization (that I can think of at least) until we get bandwidth under control.Our target bandwidth is 256 kilobits per-second.Starting Point @ 60HZLife is rarely easy, and the life of a network programmer, even less so. As network programmers we&rsquo;re often tasked with the impossible, so in that spirit, let&rsquo;s increase the snapshot send rate from 10 to 60 snapshots per-second and see exactly how far away we are from our target bandwidth.That&rsquo;s a LOT of bandwidth: 17.37 megabits per-second!Let&rsquo;s break it down and see where all the bandwidth is going.Here&rsquo;s the per-cube state sent in the snapshot: struct CubeState { bool interacting; vec3f position; vec3f linear_velocity; quat4f orientation; };And here&rsquo;s the size of each field:quat orientation: 128 bitsvec3 linear_velocity: 96 bitsvec3 position: 96 bitsbool interacting: 1 bitThis gives a total of 321 bits bits per-cube (or 40.125 bytes per-cube).Let&rsquo;s do a quick calculation to see if the bandwidth checks out. The scene has 901 cubes so 901x40.125 = 36152.625 bytes of cube data per-snapshot. 60 snapshots per-second so 36152.625 x 60 = 2169157.5 bytes per-second. Add in packet header estimate: 2169157.5 + 32x60 = 2170957.5. Convert bytes per-second to megabits per-second: 2170957.5 x 8 / ( 1000 x 1000 ) = 17.38mbps.Everything checks out. There&rsquo;s no easy way around this, we&rsquo;re sending a hell of a lot of bandwidth, and we have to reduce that to something around 1-2% of it&rsquo;s current bandwidth to hit our target of 256 kilobits per-second.Is this even possible? Of course it is! Let&rsquo;s get started :)Optimizing OrientationWe&rsquo;ll start by optimizing orientation because it&rsquo;s the largest field. (When optimizing bandwidth it&rsquo;s good to work in the order of greatest to least potential gain where possible&hellip;)Many people when compressing a quaternion think: &ldquo;I know. I&rsquo;ll just pack it into 8.8.8.8 with one 8 bit signed integer per-component!&rdquo;. Sure, that works, but with a bit of math you can get much better accuracy with fewer bits using a trick called the &ldquo;smallest three&rdquo;.How does the smallest three work? Since we know the quaternion represents a rotation its length must be 1, so x^2+y^2+z^2+w^2 = 1. We can use this identity to drop one component and reconstruct it on the other side. For example, if you send x,y,z you can reconstruct w = sqrt( 1 - x^2 - y^2 - z^2 ). You might think you need to send a sign bit for w in case it is negative, but you don&rsquo;t, because you can make w always positive by negating the entire quaternion if w is negative (in quaternion space (x,y,z,w) and (-x,-y,-z,-w) represent the same rotation.)Don&rsquo;t always drop the same component due to numerical precision issues. Instead, find the component with the largest absolute value and encode its index using two bits [0,3] (0=x, 1=y, 2=z, 3=w), then send the index of the largest component and the smallest three components over the network (hence the name). On the other side use the index of the largest bit to know which component you have to reconstruct from the other three.One final improvement. If v is the absolute value of the largest quaternion component, the next largest possible component value occurs when two components have the same absolute value and the other two components are zero. The length of that quaternion (v,v,0,0) is 1, therefore v^2 + v^2 = 1, 2v^2 = 1, v = 1/sqrt(2). This means you can encode the smallest three components in [-0.707107,+0.707107] instead of [-1,+1] giving you more precision with the same number of bits.With this technique I&rsquo;ve found that minimum sufficient precision for my simulation is 9 bits per-smallest component. This gives a result of 2 + 9 + 9 + 9 = 29 bits per-orientation (down from 128 bits).Your browser does not support the video tag.This optimization reduces bandwidth by over 5 megabits per-second, and I think if you look at the right side, you&rsquo;d be hard pressed to spot any artifacts from the compression.Optimizing Linear VelocityWhat should we optimize next? It&rsquo;s a tie between linear velocity and position. Both are 96 bits. In my experience position is the harder quantity to compress so let&rsquo;s start here.To compress linear velocity we need to bound its x,y,z components in some range so we don&rsquo;t need to send full float values. I found that a maximum speed of 32 meters per-second is a nice power of two and doesn&rsquo;t negatively affect the player experience in the cube simulation. Since we&rsquo;re really only using the linear velocity as a hint to improve interpolation between position sample points we can be pretty rough with compression. 32 distinct values per-meter per-second provides acceptable precision.Linear velocity has been bounded and quantized and is now three integers in the range [-1024,1023]. That breaks down as follows: [-32,+31] (6 bits) for integer component and multiply 5 bits fraction precision. I hate messing around with sign bits so I just add 1024 to get the value in range [0,2047] and send that instead. To decode on receive just subtract 1024 to get back to signed integer range before converting to float.11 bits per-component gives 33 bits total per-linear velocity. Just over 1&frasl;3 the original uncompressed size!We can do even better than this because most cubes are stationary. To take advantage of this we just write a single bit &ldquo;at rest&rdquo;. If this bit is 1, then velocity is implicitly zero and is not sent. Otherwise, the compressed velocity follows after the bit (33 bits). Cubes at rest now cost just 127 bits, while cubes that are moving cost one bit more than they previously did: 159 + 1 = 160 bits.Your browser does not support the video tag.But why are we sending linear velocity at all? In the previous article we decided to send it because it improved the quality of interpolation at 10 snapshots per-second, but now that we&rsquo;re sending 60 snapshots per-second is this still necessary? As you can see below the answer is no.Your browser does not support the video tag.Linear interpolation is good enough at 60HZ. This means we can avoid sending linear velocity entirely. Sometimes the best bandwidth optimizations aren&rsquo;t about optimizing what you send, they&rsquo;re about what you don&rsquo;t send.Optimizing PositionNow we have only position to compress. We&rsquo;ll use the same trick we used for linear velocity: bound and quantize. I chose a position bound of [-256,255] meters in the horizontal plane (xy) and since in the cube simulation the floor is at z=0, I chose a range of [0,32] meters for z.Now we need to work out how much precision is required. With experimentation I found that 512 values per-meter (roughly 2mm precision) provides enough precision. This gives position x and y components in [-131072,+131071] and z components in range [0,16383]. That&rsquo;s 18 bits for x, 18 bits for y and 14 bits for z giving a total of 50 bits per-position (originally 96).This reduces our cube state to 80 bits, or just 10 bytes per-cube.This is approximately 1&frasl;4 of the original cost. Definite progress!Your browser does not support the video tag.Now that we&rsquo;ve compressed position and orientation we&rsquo;ve run out of simple optimizations. Any further reduction in precision results in unacceptable artifacts.Delta CompressionCan we optimize further? The answer is yes, but only if we embrace a completely new technique: delta compression.Delta compression sounds mysterious. Magical. Hard. Actually, it&rsquo;s not hard at all. Here&rsquo;s how it works: the left side sends packets to the right like this: &ldquo;This is snapshot 110 encoded relative to snapshot 100&rdquo;. The snapshot being encoded relative to is called the baseline. How you do this encoding is up to you, there are many fancy tricks, but the basic, big order of magnitude win comes when you say: &ldquo;Cube n in snapshot 110 is the same as the baseline. One bit: Not changed!&rdquo;To implement delta encoding it is of course essential that the sender only encodes snapshots relative to baselines that the other side has received, otherwise they cannot decode the snapshot. Therefore, to handle packet loss the receiver has to continually send &ldquo;ack&rdquo; packets back to the sender saying: &ldquo;the most recent snapshot I have received is snapshot n&rdquo;. The sender takes this most recent ack and if it is more recent than the previous ack updates the baseline snapshot to this value. The next time a packet is sent out the snapshot is encoded relative to this more recent baseline. This process happens continuously such that the steady state becomes the sender encoding snapshots relative to a baseline that is roughly RTT (round trip time) in the past.There is one slight wrinkle: for one round trip time past initial connection the sender doesn&rsquo;t have any baseline to encode against because it hasn&rsquo;t received an ack from the receiver yet. I handle this by adding a single flag to the packet that says: &ldquo;this snapshot is encoded relative to the initial state of the simulation&rdquo; which is known on both sides. Another option if the receiver doesn&rsquo;t know the initial state is to send down the initial state using a non-delta encoded path, eg. as one large data block, and once that data block has been received delta encoded snapshots are sent first relative to the initial baseline in the data block, then eventually converge to the steady state of baselines at RTT.Your browser does not support the video tag.As you can see above this is a big win. We can refine this approach and lock in more gains but we&rsquo;re not going to get another order of magnitude improvement past this point. From now on we&rsquo;re going to have to work pretty hard to get a number of small, cumulative gains to reach our goal of 256 kilobits per-second.Incremental ImprovementsFirst small improvement. Each cube that isn&rsquo;t sent costs 1 bit (not changed). There are 901 cubes so we send 901 bits in each packet even if no cubes have changed. At 60 packets per-second this adds up to 54kbps of bandwidth. Seeing as there are usually significantly less than 901 changed cubes per-snapshot in the common case, we can reduce bandwidth by sending only changed cubes with a cube index [0,900] identifying which cube it is. To do this we need to add a 10 bit index per-cube to identify it.There is a cross-over point where it is actually more expensive to send indices than not-changed bits. With 10 bit indices, the cost of indexing is 10xn bits. Therefore it&rsquo;s more efficient to use indices if we are sending 90 cubes or less (900 bits). We can evaluate this per-snapshot and send a single bit in the header indicating which encoding we are using: 0 = indexing, 1 = changed bits. This way we can use the most efficient encoding for the number of changed cubes in the snapshot.This reduces the steady state bandwidth when all objects are stationary to around 15 kilobits per-second. This bandwidth is composed entirely of our own packet header (uint16 sequence, uint16 base, bool initial) plus IP and UDP headers (28 bytes).Next small gain. What if we encoded the cube index relative to the previous cube index? Since we are iterating across and sending changed cube indices in-order: cube 0, cube 10, cube 11, 50, 52, 55 and so on we could easily encode the 2nd and remaining cube indices relative to the previous changed index, e.g.: +10, +1, +39, +2, +3. If we are smart about how we encode this index offset we should be able to, on average, represent a cube index with less than 10 bits.The best encoding depends on the set of objects you interact with. If you spend a lot of time moving horizontally while blowing cubes from the initial cube grid then you hit lots of +1s. If you move vertically from initial state you hit lots of +30s (sqrt(900)). What we need then is a general purpose encoding capable of representing statistically common index offsets with less bits.After a small amount of experimentation I came up with this simple encoding:[1,8] =&gt; 1 + 3 (4 bits)[9,40] =&gt; 1 + 1 + 5 (7 bits)[41,900] =&gt; 1 + 1 + 10 (12 bits)Notice how large relative offsets are actually more expensive than 10 bits. It&rsquo;s a statistical game. The bet is that we&rsquo;re going to get a much larger number of small offsets so that the win there cancels out the increased cost of large offsets. It works. With this encoding I was able to get an average of 5.5 bits per-relative index.Now we have a slight problem. We can no longer easily determine whether changed bits or relative indices are the best encoding. The solution I used is to run through a mock encoding of all changed cubes on packet write and count the number of bits required to encode relative indices. If the number of bits required is larger than 901, fallback to changed bits.Here is where we are so far, which is a significant improvement:Your browser does not support the video tag.Next small improvement. Encoding position relative to (offset from) the baseline position. Here there are a lot of different options. You can just do the obvious thing, eg. 1 bit relative position, and then say 8-10 bits per-component if all components have deltas within the range provided by those bits, otherwise send the absolute position (50 bits).This gives a decent encoding but we can do better. If you think about it then there will be situations where one position component is large but the others are small. It would be nice if we could take advantage of this and send these small components using less bits.It&rsquo;s a statistical game and the best selection of small and large ranges per-component depend on the data set. I couldn&rsquo;t really tell looking at a noisy bandwidth meter if I was making any gains so I captured the position vs. position base data set and wrote it to a text file for analysis.I wrote a short ruby script to find the best encoding with a greedy search. The best bit-packed encoding I found for the data set works like this: 1 bit small per delta component followed by 5 bits if small [-16,+15] range, otherwise the delta component is in [-256,+255] range and is sent with 9 bits. If any component delta values are outside the large range, fallback to absolute position. Using this encoding I was able to obtain on average 26.1 bits for changed positions values.Delta Encoding Smallest ThreeNext I figured that relative orientation would be a similar easy big win. Problem is that unlike position where the range of the position offset is quite small relative to the total position space, the change in orientation in 100ms is a much larger percentage of total quaternion space.I tried a bunch of stuff without good results. I tried encoding the 4D vector of the delta orientation directly and recomposing the largest component post delta using the same trick as smallest 3. I tried calculating the relative quaternion between orientation and base orientation, and since I knew that w would be large for this (rotation relative to identity) I could avoid sending 2 bits to identify the largest component, but in turn would need to send one bit for the sign of w because I don&rsquo;t want to negate the quaternion. The best compression I could find using this scheme was only 90% of the smallest three. Not very good.I was about to give up but I run some analysis over the smallest three representation. I found that 90% of orientations in the smallest three format had the same largest component index as their base orientation 100ms ago. This meant that it could be profitable to delta encode the smallest three format directly. What&rsquo;s more I found that there would be no additional precision loss with this method when reconstructing the orientation from its base. I exported the quaternion values from a typical run as a data set in smallest three format and got to work trying the same multi-level small/large range per-component greedy search that I used for position.The best encoding found was: 5-8, meaning [-16,+15] small and [-128,+127] large. One final thing: as with position the large range can be extended a bit further by knowing that if the component value is not small the value cannot be in the [-16,+15] range. I leave the calculation of how to do this as an exercise for the reader. Be careful not to collapse two values onto zero.The end result is an average of 23.3 bits per-relative quaternion. That&rsquo;s 80.3% of the absolute smallest three.That&rsquo;s just about it but there is one small win left. Doing one final analysis pass over the position and orientation data sets I noticed that 5% of positions are unchanged from the base position after being quantized to 0.5mm resolution, and 5% of orientations in smallest three format are also unchanged from base.These two probabilities are mutually exclusive, because if both are the same then the cube would be unchanged and therefore not sent, meaning a small statistical win exists for 10% of cube state if we send one bit for position changing, and one bit for orientation changing. Yes, 90% of cubes have 2 bits overhead added, but the 10% of cubes that save 20+ bits by sending 2 bits instead of 23.3 bit orientation or 26.1 bits position make up for that providing a small overall win of roughly 2 bits per-cube.Your browser does not support the video tag.As you can see the end result is pretty good.ConclusionAnd that&rsquo;s about as far as I can take it using traditional hand-rolled bit-packing techniques. You can find source code for my implementation of all compression techniques mentioned in this article here.It&rsquo;s possible to get even better compression using a different approach. Bit-packing is inefficient because not all bit values have equal probability of 0 vs 1. No matter how hard you tune your bit-packer a context aware arithmetic encoding can beat your result by more accurately modeling the probability of values that occur in your data set. This implementation by Fabian Giesen beat my best bit-packed result by 25%.It&rsquo;s also possible to get a much better result for delta encoded orientations using the previous baseline orientation values to estimate angular velocity and predict future orientations rather than delta encoding the smallest three representation directly.Chris Doran from Geomerics wrote also wrote an excellent article exploring the mathematics of relative quaternion compression that is worth reading. 译文 译文出处 译者：张大伟（卡卡是我）/ 许春(conan) 审校：崔国军（飞扬971）介绍大家好，我是格伦·菲德勒。欢迎阅读《网络物理模拟》的系列文章，这个系列文章的主题是关于如何将一个物理模拟通过网络通信进行同步。在前面的文章中，我们会通过网络以每秒10次的速度发送整个模拟状态的快照，并在网络通信的另外一侧对这些快照进行插值来重建整个模拟的世界状态。因为我们发送的快照的频率比较低，这样会带来的一个问题就是对快照进行插值的话会在网络延迟的基础上还要增加插值带来的延迟。如果是每秒10次快照这个情况，最小的插值延迟是100毫秒，考虑到网络抖动的话一个比较实际的最小延迟是150毫秒。如果需要在连续丢失一个到两个包的情况进行保护处理的话，可能延迟就会高达250毫秒甚至350毫秒。这种程度的延迟对于大多数游戏来说都是不能接受的量。减少这种延迟的唯一方法是增加快照发送的频率。由于许多游戏是以60fps的频率进行更新，可以尝试以每秒60次的频率发送快照而不是我们正在使用的每秒10次。但是很不幸的是，这样做的话会带来网络带宽的损耗，不仅是因为我们更加频繁的发送相同大小的数据包，而且还因为发送每个数据包都会有包头数据的负担。这个的原因听起来很明显，如果以每秒60次的频率来发送数据包，那么相比以每秒10次的频率来发送数据包，我们发送的UDP/IP数据包头的数据量很明显将是6倍。在计算数据包头的带宽消耗的时候我使用了一个经验法则，大概每个数据包头带来的带宽损耗大概是32字节。这个估计并不十分准确但是能让我们对一个典型的数据包头到底是多大有个粗略的概念。把这个大小乘以60就是每秒的带宽损耗，你会发现这样损耗的带宽其实不是一个小数目。而且这样带来带宽损耗是一个基础大小，你根本就没有办法来减少。如果是采用IPv6的话，数据包头的大小可能会更大，每秒的带宽损耗也会跟着变大。对于数据包的包头带来开销，我们基本是没有办法进行优化的，但是数据包的其他所有部分我们都可以进行优化。所以我们将在本文中要做的事情就是遍历一切可能的带宽优化方法（至少是我能想到的一切带宽优化方法）直到我们把带宽的消耗控制在我们能接受的范围内。对于这个应用程序，我们把带宽控制的目标设置为每秒256kb。从60HZ为起点开始优化吧这看起来似乎是比较小的一个流量，可能你的网络连接能够支持更大的流量，但是我们要明白的是当你对视频游戏或者物理模拟进行网络通信的时候，你的目标是减少延迟和确保对玩家来说最好的网络连接情况。为了实现这一目标，最好不要让连接一直饱和工作，做到这一点的办法是使用一个比较保守的带宽量，这样就比较不容易给你的玩家造成困扰和麻烦。让我们看下当我们以每秒60次的频率发送未压缩的快照的时候，我们到底使用了多大的带宽。这一切的带宽都是从哪里来的呢？这个数据包包含了一个有901个立方体的数组，其它的东西就没有什么了。很显然，立方体的数据是造成高带宽的原因，但是为什么发送每个立方体的数据会这么昂贵呢？每个立方体具有以下这些属性：· 用quat表示的立方体朝向：128比特。· 用vec3表示的立方体速度：96比特。· 用vec3表示的立方体位置：96比特。· 用bool表示的是否相互作用：1比特。所以一个立方体一共要占据321比特的大小（40.125字节）让我们做下数学计算并确保一切的东西都包括在内了。这个场景有901个立方体所以每次快照的立方体数据大概有901x40.125 = 36152.625字节。每秒60张快照就一共是，每秒6152.625 x 60 = 2169157.5字节。再加入报文头部大小的估计：2169157.5 + 32x60 = 2170957.5字节。将单位从每秒比特转换成每秒Mb：2170957.5 x 8 / ( 1000 x 1000 ) =17.38mbps。这与刚才的得到的结果就足够接近了！优化Orientation数据正如你看到的那样，所有的东西我们都考虑进来了。让我们先开始从立方体的方向数据进行优化，因为它是最大的一块数据。（当对带宽进行优化的时候，最有效的办法是从大数据到小数进行优化，这样才能收益最大）。在压缩四元数的时候很多人会这么想：“我知道了，我可以把四元数的每一位用8个比特的有符号整数来表示，这样大小就用一个32位的整数来表示四元数了“。当然，这是一个可行的方法，但是如果使用一些数学方面的技巧，你可以用更少的位数得到一个更加准确的表示方法，这个数学技巧被称为”最小的三个分量“。最小的三个分量这种方法是如何起作用的？因为我们知道代表旋转的四元数的长度一定是1，因此代表旋转的四元数会有这么一个性质：x^2+y^2+z^2+w^2 = 1。我们可以利用四元数的这个性质来在传输的时候丢弃一个分量并在网络的另外一端对整个四元数进行重建。举个简单的例子来说，如果你通过网络发送的是x、y、z，你就可以利用这样一个公式来重建w分量：w = sqrt( 1 – x^2 – y^2 – z^2 )。你可能觉得需要发送一个符号位来标识w的正负，以防止w是负的情况，但是事实上，你根本不需要这么做，因为总是可以保证w是正的，如果w是负的话，可以把整个四元数的四个分量取反就好了（在四元数空间中，(x,y,z,w)和(-x,-y,-z,-w)代表的是相同的旋转）。不要总是丢弃同一个分量，因为不总是丢弃同一个分量会得到更高的精度。，相反，要找到四个分量中最大的那一个（以绝对值的大小来衡量）并把这个分量的序号编码进一个2比特[0,3]的信息中（0=x,1=y, 2=z, 3=w)），然后通过网络发送最小的三个分量以及最大分量的序号（这样的话，通过最大的分量的序号，我们就知道了发送过来的三个分量的名字）。在网络的另外一端，我们会从2比特的最大分量的序号信息中解码出来我们需要重建的分量是哪一个，然后就可以利用传过来的三个分量对其进行重建。最后一个改进。如果v是四元数四个分量中最大的那个分量，可能会出现两个分量是0而另外两个分量的绝对值一样大的情况，这个四元数（v，v，0，0）的长度是1，因此v^2 + v^2= 1，2v^2 = 1，v = 1/sqrt(2)。这意味着你将会在[-0.707107，+0.707107]的大小区间里面对最小的三个分量进行编码而不是在[-1，+1]这个完整的可用空间，这将使你得到更高的精度。通过这种方法，我发现对于我的模拟情况来说保证最小的精度要求也需要用9个比特来表示一个分量。这样的话，结果就是要对每个方向需要用2 + 9 + 9 + 9= 29比特。（而原来是需要128个比特位！）。优化线性速度数据接下来我们应该优化什么？线性速度和位置数据均可（需要96个比特位）。根据我的经验来看，位置信息是非常难以压缩的，所以让我们从线性速度开始。要压缩线性速度的话，我们首先需要把线性速度的分量限制在某个范围内，这样的话我们就不需要发送完整的浮点值。我发现最大速度定为32米每秒的话就会非常好，正好是2的平方数，并且在立方体模拟的情况下不会影响玩家的体验。由于我们实际上只是使用线性速度作为一个辅助信息来提高位置采样点之间的插值，所以我们可以极大地压缩线性速度值。我发现，其实是只使用32个离散的值（0到31）也是一个可以接受的精度。线性速度已经被限定在某个范围内并进行了量化，现在三个整数会分布在【-1024，1023】内。这种分解具体如下：【-32，+ 31】（6位）用来表示分量的整数部分，然后会乘以5位的小数部分。我不喜欢用符号位乱搞，所以我只是整体都加了1024来让值的范围在【0，2047】，并把新得到的值发送出去。在接收的时候如果想要解码的话，先要减去1024，这样才会得到原始的有符号数，然后才是转换成浮点数。每个分量占据11比特，加起来就是每个线性速度要一共占据33比特的大小，只比未压缩前的数据量的1/3稍微多一点点！因为大多数立方体是固定的，我们可以处理的更好。为了利用这一点，我们可以在立方体“处于休息状态”的时候只写一个单独的比特位。如果这个比特的值是1，那么我们就知道了速度其实是0并且并不发送。否则，压缩好的速度会跟着这个比特值（压缩好的速度值一共有33比特）。“处于休息状态”的立方体现在一共占据了127比特，而正处于移动状态的立方体消耗的带宽大小比之前的方法要多一个比特：159 +1 = 160 比特。但是我们为什么要发送线性速度呢？在前一篇文章中，我们决定发送线性速度是因为它对于每秒10次快照的情况能显著的提高插值的质量。但是，现在我们每秒发送60次快照，那么是否还需要发送线性速度呢？你可以在下面看到，答案是不需要。在高发送频率的时候线性插值的效果是足够好的。优化Position数据那么现在我们只有一个位置信息需要压缩了。我们将使用用于线性速度一样的小技巧：限定在某个范围内并进行量化。大多数的游戏世界其实是相当大的所以我选择的位置限制是在水平平面上的【-256,256】米之内，因为在我的立方体模拟情形中，地板的高度是z=0，所以我选择的z的范围是【0,32】米。现在我们需要确定我们对精度的要求到底是多少。通过一些实验，我发现每米有512个值（大概精度是2mm）的情况就能够提供足够的精度。这样做的话会让x和y分量的值可以分布在区间【131072,+131071】，让z分量的值分布在区间【0, 16383】。所以这么处理的话，x分量占据18比特的大小，y分量占据18比特的大小，而z分量占据14比特的大小，加起来每个位置一共占据50比特的大小（原先是96比特的大小）。这可以把我们的每个立方体的信息减至80比特，也就是10字节。（4倍的提升，原先每个立方体的状态大概需要40字节的大小）。现在我们对位置信息和方向信息进行了压缩，我们已经通过减少我们发送的数据的精度来完成了简单的压缩。并且压缩率已经到了如果任何方向上进一步压缩都会导致精度进一步受损到不可接受的程度。我们还可以进行进一步的优化么？增量压缩答案是肯定的，但是我们需要使用一种全新的技术：增量压缩。增量压缩听起来让人感觉神秘、神奇、很艰深。实际上，这个技术根本就不难。下面是它具体如何工作的：网络连接的一侧给另外一侧发送数据包，像这样：“这是快照110相对于快照100的编码”。快照是基于某个被称为基线的东西进行编码的。具体你如何实现这种编码方式完全取决于你，这里面有很多花哨的技巧，但是基础是一样的，当你说出“在快照110里面的第n个立方体相对于基线是没有任何改变，所以它只用1个比特位表示：没有变化！”的时候，大量的数据传输就被节省下来了。为了实现增量压缩的编码，当然有一点是非常关键的就是发送方必须只编码那些相对基线发生了变化的东西，这样就要求它要知道网络连接的另外一侧已经接收了什么，否则发送方根本就没法对快照进行编码。因此，为了处理数据包丢失的情况，接收方必须持续发送“ack”（接收确认）的数据包给发送方，这个数据包是说“我已经收到的最新的快照是快照n”。发送方解析最近收到最近的接收确认包，如果这个接收确认包比之前的接收确认包记录的快照更新的话，就会把基线的值调整成最近的接收确认包里面记载的快照。下一次发送数据包的时候，快照就会根据最新更新的基线进行编码。这个过程会持续的进行，这样的话稳定状态下发送者编码快照时候与基线的差距基本就是由过去这段时间的往返时间决定的。这里面会有一个小问题：在刚开始连接的时候，因为发送方没有一次通信所需要的往返时间以及并没有从接收方收到任何的确认包，所以发送方根本就没有任何基线进行相对编码。我是通过在数据包里面添加了一个单独的标志来解决这个问题的，这个标记的意思是“这个快照是相对于模拟的初始状态进行编码的”，而这个标志位是双方都明白意思的。另外一个解决方案是如果发送方不知道发送的初始状态的话，就使用一个非增量的路径进行发送初始状态。所以有可能最初发送的是一个非常大的数据块，一旦这个数据块被接收确认的话，后续发送就会以这个大的数据块作为机箱来发送增量编码的快照，然后最终收敛到以往返时延作为基准的稳定状态。正如你可以在上面看到的那样，这是一个巨大的胜利。我们可以完善这一做法，并来获得更多的收益，但是这个收益是有限的，不会是像刚才这个做法这样带来这么大幅度的提升。从现在开始，我们将需要努力工作来获得一些比较小幅度能累积的收益来达到我们设定的256kb每秒的目标。增量的一些优化第一个小的提升。每个未发送的立方体需要花费1比特的带宽（如果立方体没有变化的话）。因为场景中一共有901个立方体，所以即使所有的数据包都没有变化的话，我们还是要在每个数据包要发送901个比特。如果是每秒发送60个数据包的频率的话，这就将增加54kb的带宽。可以看到在通常情况下会在每次快照的时候发生变化的立方体数目明显小于901个，所以我们可以通过只发送变化过的立方体来减小消耗的带宽。我们创建了一个立方体索引【0，900】来标记哪一个立方体是什么。为了做到这一点，我们需要给每个立方体增加10位的索引来标识它。这里面其实是有一个权衡点的，就是发送索引比发送一个位来表示立方体未发生变化要浪费更多的带宽。因为每个立方体的索引是10位，所以索引的消耗是10xn位。因此如果我们发送的立方体数目小于90个的话（也就是小于900位的话），使用索引是更加有效率的。我们可以依照这个数值对每个快照进行评估，并在数据包的头部发送一个单独的位来进行标示我们该使用哪种方法，我们使用如下的定义：0=索引，1=用单独的1位进行标示是否发生变化。通过这种方法我们根据快照中要发送的发生改变的立方体数目来进行最有效的编码。这种方法会在所有物体都是固定不发生变化的情况下可以减少稳定状态下的带宽到大约15kb每秒。这种情况下带宽是完全由我们自己的数据包包头（16位无符号的顺序号、16位无符号的基准线编号、用了标记是否是初始状态的布尔值）外加IP 和UDP的包头（28位）来占据的。下一个小的提升。如果我们相对于之前的立方体索引进行当前立方体索引编码怎么样？因为我们在遍历所有的立方体的时候是按照顺序进行遍历的并会按照顺序发送发生改变的立方体：比如说像立方体0、立方体10,、立方体11、立方体50、立方体52、立方体55这样，所以我们可以很容易根据前一个立方体的索引对当前立方体的编号进行相对索引，这样的话，前面的例子就会变成：+10、+1、 +39、 +2、+3。如果我们可以很聪明的利用相对index编码的话，从平均情况来说，我们可以用少于10位的数据来表示一个立方体的索引。最好的编码方法取决于和你进行交互的物体集合。如果你花了很多时间进行水平移动的同时还将很多立方体从最初的立方体位置上推开，那么就会在相对index编码的方法得到很多的+1。如果你从最初的状态开始垂直移动，那么就会在相对index编码的方法得到很多的+30(sqrt(900))。我们需要的是一种通用的编码方法能够用很少的位来表示统计学下通用的index偏移。在进行少量的实验之后，我想出了这个简单的编码方式：· [1,8] =&gt; 1 + 3 (4位)· [9,40] =&gt; 1 + 1 + 5 (7位)· [41,900] =&gt; 1 + 1 + 10 (12位)需要注意下相对偏移具体是有多大，这个大小可能超过10位的大小。这是一个统计意义的游戏。赌注是我们可能得到一个大得多的偏差，这样如果赢的话会消除大偏移带来的增加的消耗。这确实起作用了。有了这个编码方法，我的每个相对序号的大小平均下来是5.5位这么大。现在我们会有一个小问题。我们再也不能很容易地确定到底是用一位来表示是否发生了更改还是使用相对序号才是最好的编码方法。我使用的解决方案就是通过将所有发生改变的立方体通过一个模拟编码的方式写入一个数据包里面，然后计算相对序号这种编码方式所需要的位数。如果所需的位数比901大，那么我们就切换回用一位来表示是否发生了更改的方法。接下来我们要做这样的一个小的提升。利用基线时候立方体所在的位置对位置信息进行编码。这里有很多不同的选择。你可以做那些有明显效果的事情。举个简单的例子来说，用1位来表示这是相对位置的信息，然后用8-10位来表示每个分量的相对位置信息，如果所有分量的位置正好在这些位数提供的数据范围之内，否则的话就该发送绝对位置（需要使用50位）。这给出了一个还不错的编码方法，但是我们可以做的更好。如果你仔细想想的话，就会发现有如下一个情况，一个位置分量很大但是其他位置分量很小。如果我们可以利用这一点的话，就能得到更好的效果，并且用更少的位数来发送这些大小比较小的分量信息。这是一个统计游戏，到底是给每个分量选择一个比较小的数据范围还是选择一个比较大的数据范围依赖于数据集本身。我没有办法只是看带宽的大小就能告诉一些真正有用的信息，所以我捕获了位置以及基准位置数据，并把它们写入一个文件进行分析。这个格式是每一行表示一个立方体的数据信息，依次是x、y、z、base_x、base_y、base_z。目标是在每一行用基准线状态下位置的x、y、z分量来对当前状态下的x、y、z分量进行编码。如果你有兴趣的话,你可以在这里（地址是http://gafferongames.com/wp-content/uploads/2015/02/position_values.txt）下载这个数据集。我写了一个简短的ruby脚本来使用暴力搜索找到最优的编码方案。我发现最好的位打包编码的数据集是这样运作的：先对每个分量使用1位数据标识，然后如果确实是小数据的话(也就是区间在【-16.15】之内的话)，就使用5位的数据，否则的话，分量的增量的取值范围在【-256，256】之内，并用9位数据进行发送。如果任意分量的增量超出了这个范围的话就会换回使用绝对位置。通过使用这种编码方法，我可以对每个变化的位置只用平均下来26.1位来进行表示。增量编码最小的三个分量接下来，我将指出如同相对方向变化的话能几乎取得相对位置变化相同的效果。这里的问题可能会有些不一样，就是位置变化的取值范围相对于整个位置空间而言是非常非常小的，但是100毫秒内方向上的变化可能占整个四元数空间的话，是一个非常大的比例。我尝试了很多方法但并没有得到好的结果。我尝试直接对方向的增量这个四维向量进行编码并使用那个“最小的三个分量”这个技巧来隐含的表示最大的分量。我还尝试计算基线状态下的方向和当前方向之间的相对四元数。因为我知道w分量将是最大的分量（因为这个四元数表示的是旋转），我可以不用发送2位数据来确定最大的分量，但反过来将需要发送一位数据来标识w分量的正负，因为我不想对整个四元数取反。通过这种方案我能找到的最好的压缩方法只有“最小的三个分量”方法的数据量的90%。这个结果并不是太令人满意。我几乎都要放弃这个方向的优化了，但是我对“最小的三个分量”方法跑了一些分析结果。我发现按照“最小的三个分量”的格式90%的情况下方向里面最大的那个分量都和100毫秒之前基线状态下方向里面最大的分量是相同的。这意味着如果直接对最小的三个分量进行增量编码的话有可能是能获得更大收益的。更重要的是，我发现如果使用这种方法的话，在从基线数据重建整个方向信息的话不会有任何额外的精度损失。我从场景中运行一些方向信息并把这些方向信息以最小的三个分量”的格式导出来，并使用我曾经对位置信息使用过的暴力穷举方法来对每个分量的取值范围进行评估。所找到的最佳编码方法是：5-8，这意味着对于比较小的数值使用的是【-16.15】这个区间，对于比较大的数值，使用的是【-128,+127】这个区间。最后一件要做的事情：就跟位置信息的处理一样，大的取值范围可以通过单独的一位来提前知道分量值不会落在【-16.15】这个区间而进行更一步的拓展。我把如何做到这一点作为一个练习留给读者作为一个练习。要小心，不要让这个区间的首尾值最后都成为0。最后的结果是每个相对四元数平均下来只需要23.3位的数据来表示。这是“最小的三个分量”方法的数据量的80.3%。我们所做的优化大概就是这些内容了。但是还有一个小的优化还没有做。通过对传过来的位置和方向数据集进行一个了最终的分析，我注意到如果是在0.5毫米这个精确度的话大概有5%的位置信息是相比于基线状态的位置是没有任何变化的，而且有5%以“最小的三个分量”格式表示的方向信息也是相比于基线状态的方向信息是没有任何变化的。这两种概率是相互排斥的，因为这两种情况同时满足的话那么对应的立方体根本就不会发生变化进而根本就不会发送这个立方体的信息过来，这意味这在这个小规模的统计里面存在10%的立方体，它们的状态我们可以发送一个单独的位来表明是否有位置变化，再用另外一个单独的位来表明是否有方向变化。是的，如果这么做的话，就会给90%的立方体带来2位的额外负担，但是10%的立方体可以节省20多位的带宽（或者用2位信息换取了23.3位的方向信息，或者用2位信息换取了26.1位的位置信息），这种方法大概能给每个立方体在每次快照时候要发送的数据量减少大概2位。带宽优化有很多的选择方案，而且可以通过一点点工作一些看上去不可能的事情事实上也会变得可能。通过文中的种种方法，我们大概降低了20M比特下来，最后平均下来只有不到0.25m比特。这相比原来未压缩的带宽，大概只有原来的1.25％！ 总结好了, 这就是我用传统的手动位打包技术能压缩优化到的最大程度了，你可以看到做一些优化之后可以得到多么大的提升。你可以在这里（地址在这里）找到文中提到的所有压缩技术的代码实现。可以使用不同的方法得到更好的压缩比例。位打包这种方法并不是非常有效率的，是因为并不是所有的位的值取0或者1的概率都是相同的。无论根据具体的环境来如何努力的调整位打包技术，我们都可以根据数据集里面的取值的概率情况来建立一个更精确的模型来轻松的打败之前的调整结果。Fabian Giesen的实现（地址在这里）可以比我最好的位打包结果还能提升25%。Geomerics的克里斯·多兰（地址在这里）写了一篇很好的文章来探索数学上如何对四元数进行压缩，非常值得一读。如果不是直接使用”最小的三个分量“这种方法，而是利用之前的基线数据来估计下角速度和预测未来的方向应该能对增量编码的方向信息得到好的多的结果。下一章要讲的是：《状态同步》。【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权；]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟四之快照插值]]></title>
    <url>%2F2017%2F01%2F23%2Fsnapshot_interpolation%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[自我总结快照插值这种游戏同步技术的要点为 : 视觉模拟 : 每帧从网络的发送侧捕获所有相关状态的快照，并将其传输到网络的接收侧，在那里我们将试图重建一个视觉上近似合理的模拟 缓冲区 : 内插值之前会缓冲一段合适的时间来处理网络抖动 内插值Interpolation : 处理快照之间的拉扯 线性插值 Hermite插值 外插值Extrapolation (文中翻译为”预测”或”推测”) : 不可行, 因为外插值无法精准预测刚体运动以及各种物理 降低延迟 : 因为我们发送的快照的频率比较低，这样会带来的一个问题就是对快照进行插值的话会在网络延迟的基础上还要增加插值带来的延迟. 所以我们需要增加发送速率, 为了提高发送速度我们需要压缩快照数据技术的配合, 不然占用太多带宽了 减少宽带占用 : 因为所有需要在快照中包含所有实体信息, 所以数据量相当大, 得用各种方法压缩快照数据(网络物理模拟五之快照压缩) . . . 原文原文出处 原文标题 : Snapshot Interpolation (Interpolating between snapshots of visual state) Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networked Physics.In the previous article we networked a physics simulation using deterministic lockstep. Now, in this article we&rsquo;re going to network the same simulation with a completely different technique: snapshot interpolation.BackgroundWhile deterministic lockstep is very efficient in terms of bandwidth, it&rsquo;s not always possible to make your simulation deterministic. Floating point determinism across platforms is hard.Also, as the player counts increase, deterministic lockstep becomes problematic: you can&rsquo;t simulate frame n until you receive input from all players for that frame, so players end up waiting for the most lagged player. Because of this, I recommend deterministic lockstep for 2-4 players at most.So if your simulation is not deterministic or you want higher player counts then you need a different technique. Snapshot interpolation fits the bill nicely. It is in many ways the polar opposite of deterministic lockstep: instead of running two simulations, one on the left and one on the right, and using perfect determinism and synchronized inputs keep them in sync, snapshot interpolation doesn&rsquo;t run any simulation on the right side at all!SnapshotsInstead, we capture a snapshot of all relevant state from the simulation on the left and transmit it to the right, then on the right side we use those snapshots to reconstruct a visual approximation of the simulation, all without running the simulation itself.As a first pass, let&rsquo;s send across the state required to render each cube: struct CubeState { bool interacting; vec3f position; quat4f orientation; };I&rsquo;m sure you&rsquo;ve worked out by now that the cost of this technique is increased bandwidth usage. Greatly increased bandwidth usage. Hold on to your neckbeards, because a snapshot contains the visual state for the entire simulation. With a bit of math we can see that each cube serializes down to 225 bits or 28.1 bytes. Since there are 900 cubes in our simulation that means each snapshot is roughly 25 kilobytes. That&rsquo;s pretty big!At this point I would like everybody to relax, take a deep breath, and imagine we live in a world where I can actually send a packet this large 60 times per-second over the internet and not have everything explode. Imagine I have FIOS (I do), or I&rsquo;m sitting over a backbone link to another computer that is also on the backbone. Imagine I live in South Korea. Do whatever you need to do to suspend disbelief, but most of all, don&rsquo;t worry, because I&rsquo;m going to spend the entire next article showing you how to optimize snapshot bandwidth.When we send snapshot data in packets, we include at the top a 16 bit sequence number. This sequence number starts at zero and increases with each packet sent. We use this sequence number on receive to determine if the snapshot in a packet is newer or older than the most recent snapshot received. If it&rsquo;s older then it&rsquo;s thrown away.Each frame we just render the most recent snapshot received on the right:Look closely though, and even though we&rsquo;re sending the data as rapidly as possible (one packet per-frame) you can still see hitches on the right side. This is because the internet makes no guarantee that packets sent 60 times per-second arrive nicely spaced 1&frasl;60 of a second apart. Packets are jittered. Some frames you receive two snapshot packets. Other frames you receive none.Jitter and HitchesThis is actually a really common thing when you first start networking. You start out playing your game over LAN and notice you can just slam out packets really fast (60pps) and most of the time your game looks great because over the LAN those packets actually do tend to arrive at the same rate they were sent&hellip; and then you start trying to play your game over wireless or the internet and you start seeing hitches. Don&rsquo;t worry. There are ways to handle this!First, let&rsquo;s look at how much bandwidth we&rsquo;re sending with this naive approach. Each packet is 25312.5 bytes plus 28 bytes for IP + UDP header and 2 bytes for sequence number. That&rsquo;s 25342.5 bytes per-packet and at 60 packets per-second this gives a total of 1520550 bytes per-second or 11.6 megabit/sec. Now there are certainly internet connections out there that can support that amount of traffic&hellip; but since, let&rsquo;s be honest, we&rsquo;re not really getting a lot of benefit blasting packets out 60 times per-second with all the jitter, let&rsquo;s pull it back a bit and send only 10 snapshots per-second:You can see how this looks above. Not so great on the right side but at least we&rsquo;ve reduced bandwidth by a factor of six to around 2 megabit/sec. We&rsquo;re definitely headed in the right direction.Linear InterpolationNow for the trick with snapshots. What we do is instead of immediately rendering snapshot data received is that we buffer snapshots for a short amount of time in an interpolation buffer. This interpolation buffer holds on to snapshots for a period of time such that you have not only the snapshot you want to render but also, statistically speaking, you are very likely to have the next snapshot as well. Then as the right side moves forward in time we interpolate between the position and orientation for the two slightly delayed snapshots providing the illusion of smooth movement. In effect, we&rsquo;ve traded a small amount of added latency for smoothness.You may be surprised at just how good it looks with linear interpolation @ 10pps:Look closely though and you can see some artifacts on the right side. The first is a subtle position jitter when the player cube is hovering in the air. This is your brain detecting 1st order discontinuity at the sample points of position interpolation. The other artifact occurs when a bunch of cubes are in a katamari ball, you can see a sort of &ldquo;pulsing&rdquo; as the speed of rotation increases and decreases. This occurs because attached cubes interpolate linearly between two sample points rotating around the player cube, effectively interpolating through the player cube as they take the shortest linear path between two points on a circle.Hermite InterpolationI find these artifacts unacceptable but I don&rsquo;t want to increase the packet send rate to fix them. Let&rsquo;s see what we can do to make it look better at the same send rate instead. One thing we can try is upgrading to a more accurate interpolation scheme for position, one that interpolates between position samples while considering the linear velocity at each sample point.This can be done with an hermite spline (pronounced &ldquo;air-mitt&rdquo;)Unlike other splines with control points that affect the curve indirectly, the hermite spline is guaranteed to pass through the start and end points while matching the start and end velocities. This means that velocity is smooth across sample points and cubes in the katamari ball tend to rotate around the cube rather than interpolate through it at speed.Above you can see hermite interpolation for position @ 10pps. Bandwidth has increased slightly because we need to include linear velocity with each cube in the snapshot, but we&rsquo;re able to significantly increase the quality at the same send rate. I can no longer see any artifacts. Go back and compare this with the raw, non-interpolated 10pps version. It really is amazing that we&rsquo;re able to reconstruct the simulation with this level of quality at such a low send rate.As an aside, I found it was not necessary to perform higher order interpolation for orientation quaternions to get smooth interpolation. This is great because I did a lot of research into exactly interpolating between orientation quaternions with a specified angular velocity at sample points and it seemed difficult. All that was needed to achieve an acceptable result was to switch from linear interpolation + normalize (nlerp) to spherical linear interpolation (slerp) to ensure constant angular speed for orientation interpolation.I believe this is because cubes in the simulation tend to have mostly constant angular velocity while in the air and large angular velocity changes occur only discontinuously when collisions occur. It could also be because orientation tends to change slowly while in the air vs. position which changes rapidly relative to the number of pixels affected on screen. Either way, it seems that slerp is good enough and that&rsquo;s great because it means we don&rsquo;t need to send angular velocity in the snapshot.Handling Real World ConditionsNow we have to deal with packet loss. After the discussion of UDP vs. TCP in the previous article I&rsquo;m sure you can see why we would never consider sending snapshots over TCP.Snapshots are time critical but unlike inputs in deterministic lockstep snapshots don&rsquo;t need to be reliable. If a snapshot is lost we can just skip past it and interpolate towards a more recent snapshot in the interpolation buffer. We don&rsquo;t ever want to stop and wait for a lost snapshot packet to be resent. This is why you should always use UDP for sending snapshots.I&rsquo;ll let you in on a secret. Not only were the linear and hermite interpolation videos above recorded at a send rate of 10 packets per-second, they were also recorded at 5% packet loss with +/- 2 frames of jitter @ 60fps. How I handled packet loss and jitter for those videos is by simply ensuring that snapshots are held in the interpolation buffer for an appropriate amount of time before interpolation.My rule of thumb is that the interpolation buffer should have enough delay so that I can lose two packets in a row and still have something to interpolate towards. Experimentally I&rsquo;ve found that the amount of delay that works best at 2-5% packet loss is 3X the packet send rate. At 10 packets per-second this is 300ms. I also need some extra delay to handle jitter, which in my experience is typically only one or two frames @ 60fps, so the interpolation videos above were recorded with a delay of 350ms.Adding 350 milliseconds delay seems like a lot. And it is. But, if you try to skimp you end up hitching for 1/10th of a second each time a packet is lost. One technique that people often use to hide the delay added by the interpolation buffer in other areas (such as FPS, flight simulator, racing games and so on) is to use extrapolation. But in my experience, extrapolation doesn&rsquo;t work very well for rigid bodies because their motion is non-linear and unpredictable. Here you can see an extrapolation of 200ms, reducing overall delay from 350 ms to just 150ms:Problem is it&rsquo;s just not very good. The reason is that the extrapolation doesn&rsquo;t know anything about the physics simulation. Extrapolation doesn&rsquo;t know about collision with the floor so cubes extrapolate down through the floor and then spring back up to correct. Prediction doesn&rsquo;t know about the spring force holding the player cube up in the air so it the cube moves slower initially upwards than it should and has to snap to catch up. It also doesn&rsquo;t know anything about collision and how collision response works, so the cube rolling across the floor and other cubes are also mispredicted. Finally, if you watch the katamari ball you&rsquo;ll see that the extrapolation predicts the attached cubes as continuing to move along their tangent velocity when they should rotate with the player cube.ConclusionYou could conceivably spend a great deal of time to improve the quality of this extrapolation and make it aware of various movement modes for the cubes. You could take each cube and make sure that at minimum the cube doesn&rsquo;t go through the floor. You could add some approximate collision detection or response using bounding spheres between cubes. You could even take the cubes in the katamari ball and make them predict motion to rotate around with the player cube.But even if you do all this there will still be misprediction because you simply can&rsquo;t accurately match a physics simulation with an approximation. If your simulation is mostly linear motion, eg. fast moving planes, boats, space ships &ndash; you may find that a simple extrapolation works well for short time periods (50-250ms or so), but in my experience as soon as objects start colliding with other non-stationary objects, extrapolation starts to break down.How can we reduce the amount of delay added for interpolation? 350ms still seems unacceptable and we can&rsquo;t use extrapolation to reduce this delay without adding a lot of inaccuracy. The solution is simple: increase the send rate! If we send 30 snapshots per-second we can get the same amount of packet loss protection with a delay of 150ms. 60 packets per-second needs only 85ms.In order to increase the send rate we&rsquo;re going to need some pretty good bandwidth optimizations. But don&rsquo;t worry, there&rsquo;s a lot we can do to optimize bandwidth. So much so that there was too much stuff to fit in this article and I had to insert an extra unplanned article just to cover all of it! 译文 译文出处 翻译：崔国军（飞扬971） 审校：张乾光(星际迷航)介绍大家好，我是格伦·菲德勒。欢迎阅读《网络物理模拟》的系列文章，这个系列文章的主题是关于如何将一个物理模拟通过网络通信进行同步。在之前的文章中，我们通过具有确定性的帧同步将物理模拟通过网络通信进行同步。在这一篇文章中我们将使用一种完全不同的技术来将物理模拟通过网络通信进行同步，这个技术就是：快照信息插值方法。背景为什么需要一种不同的技术？这是因为虽然具有确定性的帧同步这种同步策略在节省带宽方面非常有效，但是要保证你的仿真具有完美的确定性这个事情并不总是可行的，有些时候是不实际的。此外，具有确定性的帧同步这种同步策略在玩家数目增多的情况下会遇到一些问题，因为你要收到所有玩家对应帧的输入才能对这一帧进行模拟。在实践中，这意味着每个人必须等待最滞后的那个玩家。以我的经验来说，我建议在联网环境下只在2到4个玩家的时候使用具有确定性的帧同步这种同步策略。（译者注：其实国内现在已经有20个玩家在互联网环境下使用具有确定性的帧同步这种同步策略的游戏了，就算dota也是支持5v5对战的，原作者太过于谨慎了）。如果你想要支持更多数目的玩家或者你的模拟并不具有完美的确定性，那么你就需要一种不同的技术了。快照信息插值技术在许多方面都站在具有确定性的帧同步技术的对立面。它不再需要在网络的两侧同时运行仿真，使用程序的确定性以及同步输入信息来保证网络的两侧的仿真始终保持同步。。。快照信息插值技术根本不需要在接收侧运行任何的模拟！快照我们所要做的就是每帧从网络的发送侧捕获所有相关状态的快照，并将其传输到网络的接收侧，在那里我们将试图重建一个视觉上近似合理的模拟。作为第一步，让我们把所需的状态直接发送给网络的接收侧，让它可以渲染每一个立方体：123456struct CubeState&#123; bool interacting; vec3f position; quat4f orientation;&#125;;需要注意的是，我们发送了一个布尔值用来标记这个立方体是否与玩家存在交互。为什么需要这个布尔值？这是因为在网络的接收侧并没有运行一个模拟，因此并不会有碰撞检测来告诉我们什么时候一个立方体应该被标红而什么时候不需要对一个立方体进行标红。如果我们想要一个立方体在它与玩家存在互动的情况下变红的话，我们需要在快照中包含此信息。我敢肯定这时候你已经明白这项技术的消耗在于增大了带宽的使用。其实是大大增大了对带宽的占用。这是因为快照包含了整个仿真的状态。通过一点数学计算我们可以看到每个立方体序列化下来的话大概占据225比特或者28.1字节。因为在我们的仿真中有大概900个立方体，这意味着每个快照大约需要25k的字节。这个数据量相当大了！在这一点上，我想每个人都放松、深呼吸，想象我们生活在一个世界里，在这个世界里面我可以在互联网上以60次每秒的速度来实际发送数据包，而不会有什么意外。想象一下我有光纤服务光纤服务或者我坐在骨干网的后面，与另外一台位于骨干网的电脑相连。。想象一下，我使用IPv6，而最大传输单元的大小是100k。可以想象一下，我住在韩国。做你任意想做的，不要有任何的怀疑，而且最重要的是，不用担心网络有任何的问题，这是因为我将在下一篇文章中向你展示如何快速优化快照信息插值这种方法带来的带宽负担。当我们用数据包的方式发送快照的时候，我们会在数据包的头部包括16位的序列号。这个序列号会从零开始并且随着每个快照的发送而增大。我们在接收的时候使用这个序列号来决定数据包中的快照到底比我们最近收到的快照更新还是更旧。如果比我们最近收到的快照更旧的话，我们就会丢弃这个快照。然后，在网络的接收侧我们只会渲染我们接收到的最新的快照上的信息。请注意，即使我们尽一切可能的快速发送数据（一帧一个数据包），我们仍然会在网络的接收侧看到物体发生抖动现象。这是因为在互联网上根本就不会保证数据包会按照六十分之一秒的间隔到达网络的另外一侧。数据包的到达时间会发生抖动。在某些帧你会收到两个帧的快照，而在另外一些帧则会根本收不到。抖动与拉扯这实际上是当你第一次启动网络一个非常常见的事情。你开始通过局域网来玩你的游戏并注意到你可以以一个非常高的速度来发送数据包并且你的游戏会看上表现的非常非常不错，这是因为你的数据包几乎在发送的同时就会到达网络的接收侧。。。然后你开始尝试通过无线网络或者互联网来玩你的游戏，然后你就看到各种各样的抖动。不用担心，有办法来处理这个问题！首先，让我们看看使用这种幼稚的方法进行发送数据包，我们会占据多少的带宽。每个数据包是25312.5字节加上IP 和UDP包头所占的28个字节并且还要有2个字节用来表示网络包的序号。这就是一个数据包的大小：25342.5字节，每秒60个数据包的话就一共要1520550字节，或者换算下就是11.6M每秒。现在当然也有互联网连接可以支持这种规模的流量。。。但是既然我们每秒发送60个数据包也没有什么太大的益处，还是充满了这种抖动，让我们稍微稳妥一点，只要每秒发送10次快照就可以了。你可以通过上面的效果看到修改以后的表现效果如何。这对网络的接收侧并没有什么太大的影响，起码我们将带宽减少到了六分之一，大概是2M每秒。我们正在朝着正确的方向继续前进。线性插值现在是关于处理快照的一些技巧。我们所做的不是在我们接收到快照数据立刻开始进行渲染，而是利用插值缓冲区缓冲了快照数据一小段时间。这个插值缓冲区会持有快照数据一段时间，这样你不仅会持有你准备渲染的这一帧的快照数据，而且从统计数字上看，你很有可能会持有下一帧你需要的快照数据。然后随着网络的接收侧在时间上向前移动，我们会对这两帧轻微延迟的快照数据进行物体位置和方向的插值，提供平滑运动的错觉。实际上，我们通过增加了一小段延迟来交换物体的平滑运动。你可以会很吃惊，通过每秒10帧的快照数据以及一个简单的线性插值，就可以得到表现如此只好的一个表现：但是如果你仔细观察的话，你还是可以看到在网络的接收侧里面存在大量的瑕疵。首先是当玩家立方体在空中盘旋的时候，玩家立方体的位置有一个轻微的抖动。这是因为你的大脑在位置插值的采样点那里检测到了1秒左右的中断。另外一个瑕疵出现在一大推立方体出于katamari球之中的时候，你可以看到某种“脉冲“的存在，立方体旋转的速度会出现升高或者降低的情况。这种情况的出现是是因为附加的这些立方体会在围绕玩家立方体的周围旋转的时候在两个采样点之间进行线性的插值，通过玩家立方体进行插值是有效的，这是因为这样两点之间就是最短线性距离。Hermite插值我发现这些瑕疵是不能接受的，但是不希望增加数据包的发送速率来解决这些问题。让我们看看我们能做些什么，让数据包的发送速率不变的情况让整个效果能够看起来更棒。我们可以尝试升级的一个事情是对于位置的一种更加精确的插值方案：这种方案在位置采样点之间进行插值的同时还要考虑每个采样点的线性速度。可以用于执行这种插值的一种曲线是厄米特曲线。这种曲线和其他需要控制点间接的影响曲线不同，厄米特曲线保证一定会通过起点和终点的同时还能匹配起点和终点的速度。这意味着立方体在通过采样点的速度是平滑的，而且katamari球中的立方体会倾向于围绕立方体旋转，而不是通过它的速度进行插值。上面你所看到的效果是通过每秒10次的快照数据进行的厄米特曲线插值。我们的带宽略有增加，但我们能够在显著提高质量的同时保证数据包的发送速率不变。我再也看不到任何的瑕疵。让我们重新回到厄米特曲线插值之前的效果进行下对比。我们能够在如此低的数据包发送速率下重建出来如此质量水平的模拟，这真的是令人惊讶的改变。顺便说一句，我发现没有必要对方向四元数进行更高阶的插值来得到平滑的插值。因为我做了很多的研究关于对方向四元数使用一个特定的线性速度进行准确的插值，然后我发现这其实非常的困难。为了达到一个可以接收的效果我们所有要做的事情就是从线性插值加归一化（nlerp函数）到球面线性插值插值（slerp函数）来确保方向四元数的插值得到一个固定的角速度。我相信这是因为在仿真中空中的立方体试图保持大体固定的角速度，而在发生碰撞的时候，比较高的角速度会突然变得不连续。它也可能是因为在空中的时候方向趋于变化缓慢而位置会会相对与屏幕上受影响的像素的数目而快速的变化。无论哪种方式，似乎球面插值都表现的不错，这就是很棒的方法因为这意味着我们并不需要在快照数据中发送角速度。处理真实世界的情况现在，我们必须处理的数据包丢失的问题了。在上一篇文章讨论完了UDP和TCP的优劣之后，我敢肯定你可以知道了为什么我们从来不考虑通过TCP来发送我们的快照数据。快照数据是时间敏感的，但是和有确定性的帧同步中的输入信息不一样，这个数据没有必要是可靠的。如果一个快照数据丢失了，我们可以跳过这一帧，并使用一个缓冲区中更新的快照数据进行插值。们再也不想停下来，等待丢失的快照数据包的重发。这就是为什么你总是应该使用UDP发送快照数据的原因。我要告诉你一个秘密。上面这两个视频不仅仅是在每秒10个数据包的发送频率下用线性插值或厄米特插值的效果，它们也记录了在有2帧抖动、5%丢包率下的效果。我该如何处理丢包和抖动？我的做法是确保快照数据会被保存在插值缓冲区，这样在插值之前会缓冲一段合适的时间。我的经验法则是，插值缓冲区应该有足够的延迟，这样就算我连续丢失了两个数据包，我仍然有快照数据可以进行插值。在实验中我发现在有2-5%的数据包丢包率的情况下，延迟量大概是数据包发送速度的3倍左右比较合适。如果是每秒发送10个数据包的情况，这个延迟量就是大概300毫秒。我还需要一些额外的延迟来处理抖动的情况，以我的经验来说，如果是在60fps的条件下，大概预留一到两帧就可以了，所以上面的插值效果的视频使用了一个大概350毫秒的延迟。添加350毫秒的延迟，似乎带来了很大的延迟。但是如果因为觉得不舍得而不在这里使用350毫秒的延迟的话，要么就得到一个充满拉扯的效果，要么就会遇到每秒有十分之一的数据包丢失。人们在其他领域（比如说第一人称设计游戏、飞行模拟游戏、赛车游戏以及其他的游戏）经常使用的用于隐藏由于插值缓冲区带来的延迟的方法是使用预测方法。以我的经验来看，预测方法对于缸体来说效果似乎不太好，这是因为它们的运动不是线性的并且无法预测。在这里，你可以看到使用了200毫秒的预测方法理论上可以将延迟从350毫秒减少到150毫秒：问题是，它工作起来效果似乎不是太好。当然这个原因在于预测方法根本不知道任何有关物理模拟方面的内容。预测方法不知道立方体需要与地面发生碰撞，立方体在向下遇到地板的时候会反弹回来，当然这是正确的表现。预测方法不知道有关施加在空中的玩家立方体上的反弹力，因此立方体起初会比实际的情况下移动的慢一些，然后开始加速追上自己应该在的位置。预测方法也不知道任何有关碰撞方面的内容以及如果发生碰撞了该发生何种反应，所以如果立方体在地板上或者其他立方体上滚过的时候就会发生预测上的错误。最后，如果你仔细观察katamari球的话，你会看到预测方法会让附着于其上的立方体继续沿着它们的切线速度运动，而这个时候它们应该跟玩家立方体一起转动。结论现在，你可以持续的花费大量的时间来改善推测的质量，并通过认识立方体的各种运动模式来持续提高。你可以对每个立方体进行推断，并确保最低程度是立方体不会穿越地面。你还可以添加碰撞检测的一些推断并且利用立方体的球型包围盒做出响应。你甚至可以选取katamari球体里面的立方体并预测当它们在玩家立方体周围的时候会如何运动。但是，即使你做了所有的这一切，仍然会有预测失误，因为你根本无法精确匹配物理模拟与近似估计之间的差距。如果你的仿真中大多数是直线运动，比如说，快速移动的飞机、轮船、太空飞船等等-你会发现一个简单的外推法非常适用于短时间内（50-250ms左右）的这种运动。以我的经验来看，只要物体开始与非静止的物体开始发生碰撞，这种预测就将完全的不成立。我们怎样才能减少由于插值带来的延迟？350毫秒似乎仍然是不能接受的延迟，我们不能使用预测和外推来减少这种延迟，因为这样增加了大量的不确定性。解决的办法其实很简单：增加发送速率。如果我们每秒发送30次快照的话，我们可以在相同的数据包丢失率情况将延迟降低到150ms。如果我们把发送速率增大到每秒60个数据包的程度，延迟将只有85ms。为了提高发送速度，我们会需要进行一些不错的带宽优化。不过不用担心，我们可以做很多很多的事情来优化带宽。但是我们如果把这些东西都放到这篇文章的话，这篇文章就会有太多太多的东西，我不得不插入一个计划之外的文章来涵盖带宽优化方面的东西！【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟三之具有确定性的帧同步]]></title>
    <url>%2F2017%2F01%2F22%2Fdeterministic_lockstep%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[自我总结帧同步要点如下 : 确定性 : 去除随机数 缓冲 : 因为数据包并不是均匀地到达, 所以要做一个缓冲区, 然后再均匀地取出 不用TCP : 因为我们的数据对时间非常敏感, 不接受到第n个输入包就无法继续模拟第n帧, 而TCP的确认机制以及重传机制当我们丢包时, 我们只能暂停等待它重发造成卡顿 用UDP : 发送冗余数据 : 因为帧同步只发送玩家input数据, 而input包是很小的, 所以发冗余也不会很大 增量包 : 加一个bit来标志跟上一个包的比较结果, 如果这个包跟上个包一致则只发送一个1, 如果不一致则发送0和这个包的完整数据 帧同步的缺点 : 等的人太多 : 因为你要收到所有玩家对应帧的输入才能对这一帧进行模拟.在实践中，这意味着每个人必须等待最滞后的那个玩家.人越多等得越久, 所以帧同步不适合mmo. 比较耗性能 : 因为帧同步技术的话, 在客户端中，每个对象都要执行所有的物理之类的运算; 而状态同步可以只同步当前玩家周围对象的状态, 不需要同步所有对象 . . . 原文 原文出处 原文标题 : Deterministic Lockstep (Keeping simulations in sync by sending only inputs) IntroductionHi, I&rsquo;m Glenn Fiedler and welcome to Networked Physics.In the previous article we explored the physics simulation we&rsquo;re going to network in this article series. In this article specifically, we&rsquo;re going to network this physics simulation using deterministic lockstep.Deterministic lockstep is a method of networking a system from one computer to another by sending only the inputs that control that system, rather than the state of that system. In the context of networking a physics simulation, this means we send across a small amount of input, while avoiding sending state like position, orientation, linear velocity and angular velocity per-object.The benefit is that bandwidth is proportional to the size of the input, not the number of objects in the simulation. Yes, with deterministic lockstep you can network a physics simulation of one million objects with the same bandwidth as just one.While this sounds great in theory, in practice it&rsquo;s difficult to implement deterministic lockstep because most physics simulations are not deterministic. Differences in floating point behavior between compilers, OS&rsquo;s and even instruction sets make it almost impossible to guarantee determinism for floating point calculations.DeterminismDeterminism means that given the same initial condition and the same set of inputs your simulation gives exactly the same result. And I do mean exactly the same result.Not close. Not near enough. Exactly the same. Exact down to the bit-level. So exact, you could take a checksum of your entire physics state at the end of each frame and it would be identical.Above you can see a simulation that is almost deterministic. The simulation on the left is controlled by the player. The simulation on the right has exactly the same inputs applied with a two second delay starting from the same initial condition. Both simulations step forward with the same delta time (a necessary precondition to ensure exactly the same result) and both simulations apply the same inputs. Notice how after the smallest divergence the simulation gets further and further out of sync. This simulation is non-deterministic.What&rsquo;s going on is that the physics engine I&rsquo;m using (Open Dynamics Engine) uses a random number generator inside its solver to randomize the order of constraint processing to improve stability. It&rsquo;s open source. Take a look and see! Unfortunately this breaks determinism because the simulation on the left processes constraints in a different order to the simulation on the right, leading to slightly different results.Luckily all that is required to make ODE deterministic on the same machine, with the same complied binary and on the same OS (is that enough qualifications?) is to set its internal random seed to the current frame number before running the simulation via dSetRandomSeed. Once this is done ODE gives exactly the same result and the left and right simulations stay in sync.And now a word of warning. Even though the simulation above is deterministic on the same machine, that does not necessarily mean it would also be deterministic across different compilers, a different OS or different machine architectures (eg. PowerPC vs. Intel). In fact, it&rsquo;s probably not even deterministic between debug and release builds due to floating point optimizations.Floating point determinism is a complicated subject and there&rsquo;s no silver bullet.For more information please refer to this article.Networking InputsNow let&rsquo;s get down to implementation.Our example physics simulation is driven by keyboard input: arrow keys apply forces to make the player cube move, holding space lifts the cube up and blows other cubes around, and holding &lsquo;z&rsquo; enables katamari mode.How can we network these inputs? Must we send the entire state of the keyboard? No. It&rsquo;s not necessary to send the entire keyboard state, only the state of the keys that affect the simulation. What about key press and release events then? No. This is also not a good strategy. We need to ensure that exactly the same input is applied on the right side, at exactly the same time, so we can&rsquo;t just send &lsquo;key pressed&rsquo;, and &lsquo;key released&rsquo; events over TCP.What we do instead is represent the input with a struct and at the beginning of each simulation frame on the left side, sample this struct from the keyboard: struct Input { bool left; bool right; bool up; bool down; bool space; bool z; };Next we send that input from the left simulation to the right simulation in a way that the simulation on the right side knows that the input belongs to frame n.And here&rsquo;s the key part: the simulation on the right can only simulate frame n when it has the input for that frame. If it doesn&rsquo;t have the input, it has to wait.For example, if you were sending across using TCP you could simply send the inputs and nothing else, and on the other side you could read the packets coming in, and each input received corresponds to one frame for the simulation to step forward. If no input arrives for a given render frame, the right side can&rsquo;t advance forward, it has to wait for the next input to arrive.So let&rsquo;s move forward with TCP, you&rsquo;ve disabled Nagle&rsquo;s Algorithm, and you&rsquo;re sending inputs from the left to the right simulation once per-frame (60 times per-second).Here it gets a little complicated. Since we can&rsquo;t simulate forward unless we have the input for the next frame, it&rsquo;s not enough to just take whatever inputs arrive over the network and then run the simulation on inputs as they arrive because the result would be very jittery. Data sent across the network at 60HZ doesn&rsquo;t typically arrive nicely spaced, 1/60th of a second between each packet.If you want this sort of behavior, you have to implement it yourself.Playout Delay BufferSuch a device is called a playout delay buffer.Unfortunately, the subject of playout delay buffers is a patent minefield. I would not advise searching for &ldquo;playout delay buffer&rdquo; or &ldquo;adaptive playout delay&rdquo; while at work. But in short, what you want to do is buffer packets for a short amount of time so they appear to be arriving at a steady rate even though in reality they arrive somewhat jittered.What you&rsquo;re doing here is similar to what Netflix does when you stream a video. You pause a little bit initially so you have a buffer in case some packets arrive late and then once the delay has elapsed video frames are presented spaced the correct time apart. If your buffer isn&rsquo;t large enough then the video playback will be hitchy. With deterministic lockstep your simulation behaves exactly the same way: showing hitches when the buffer isn&rsquo;t large enough to smooth out the jitter. Of course, the cost of increasing the buffer size is additional latency, so you can&rsquo;t just buffer your way out of all problems. At some point the user says enough! That&rsquo;s too much latency added. No sir, I will not play your game with 1 second of extra delay :)My playout delay buffer implementation is really simple. You add inputs to it indexed by frame, and when the very first input is received, it stores the current local time on the receiver machine and from that point on delivers packets assuming they should play at that time + 100ms. You&rsquo;ll likely need to something more complex for a real world situation, perhaps something that handles clock drift, and detecting when the simulation should slightly speed up or slow down to maintain a nice amount of buffering safety (being &ldquo;adaptive&rdquo;) while minimizing overall latency, but this is reasonably complicated and probably worth an article in itself.The goal is that under average conditions the playout delay buffer provides a steady stream of inputs for frame n, n+1, n+2 and so on, nicely spaced 1/60th of a second apart with no drama. In the worst case the time arrives for frame n and the input hasn&rsquo;t arrived yet it returns null and the simulation is forced to wait. If packets get bunched up and delivered late, it&rsquo;s possibly to have multiple inputs ready to dequeue per-frame. In this case I limit to 4 simulated frames per-render frame so the simulation has a chance to catch up, but doesn&rsquo;t simulate for so long that it falls further behind, aka. the &ldquo;spiral of death&rdquo;.Is TCP good enough?Using this playout buffer strategy and sending inputs across TCP we ensure that all inputs arrive reliably and in-order. This is convenient, and after all, TCP is designed for exactly this situation: reliable-ordered data.In fact, It&rsquo;s a common thing out there on the Internet for pundits to say stuff like:If you need reliable-ordered, you can&rsquo;t do better than TCP!Your game doesn&rsquo;t need UDP (yet)But I&rsquo;m here to tell you this kind of thinking is dead wrong.Above you can see the simulation networked using deterministic lockstep over TCP at 100ms latency and 1% packet loss. If you look closely on the right side you can see hitches every few seconds. What&rsquo;s happening here is that each time a packet is lost, TCP has to wait RTT*2 while it is resent (actually it can be much worse, but I&rsquo;m being generous&hellip;). The hitches happen because with deterministic lockstep the right simulation can&rsquo;t simulate frame n without input n, so it has to pause to wait for input n to be resent!That&rsquo;s not all. It gets significantly worse as latency and packet loss increase. Here is the same simulation networked using deterministic lockstep over TCP at 250ms latency and 5% packet loss:Now I will concede that if you have no packet loss and/or a very small amount of latency then you very well may get acceptable results with TCP. But please be aware that if you use TCP it behaves terribly under bad network conditions.Can we do better than TCP?Can we beat TCP at its own game. Reliable-ordered delivery?The answer is an emphatic YES. But only if we change the rules of the game.Here&rsquo;s the trick. We need to ensure that all inputs arrive reliably and in order. But if we send inputs in UDP packets, some of those packets will be lost. What if, instead of detecting packet loss after the fact and resending lost packets, we redundantly include all inputs in each UDP packet until we know for sure the other side has received them?Inputs are very small (6 bits). Let&rsquo;s say we&rsquo;re sending 60 inputs per-second (60fps simulation) and round trip time we know is going the be somewhere in 30-250ms range. Let&rsquo;s say just for fun that it could be up to 2 seconds worst case and at this point we&rsquo;ll time out the connection (screw that guy). This means that on average we only need to include between 2-15 frames of input and worst case we&rsquo;ll need 120 inputs. Worst case is 120 x 6 = 720 bits. That&rsquo;s only 90 bytes of input! That&rsquo;s totally reasonable.We can do even better. It&rsquo;s not common for inputs to change every frame. What if when we send our packet instead we start with the sequence number of the most recent input, and the 6 bits of the first (oldest) input, and the number of un-acked inputs. Then as we iterate across these inputs to write them to the packet we can write a single bit (1) if the next input is different to the previous, and (0) if the input is the same. So if the input is different from the previous frame we write 7 bits (rare). If the input is identical we write just one (common). Where inputs change infrequently this is a big win and in the worst case this really isn&rsquo;t that bad. 120 bits of extra data sent. Just 15 bytes overhead worst case.Of course another packet is required from the right simulation to the left so the left side knows which inputs have been received. Each frame the right simulation reads input packets from the network before adding them to the playout delay buffer and keeps track of the most recent input it has received and sends this back to the left as an &ldquo;ack&rdquo; or acknowledgment for inputs.When the left side receives this ack it discards any inputs older than the most recent received input. This way we have only a small number of inputs in flight proportional to the round trip time between the two simulations.Flawless VictoryWe have beaten TCP by changing the rules of the game.Instead of &ldquo;implementing 95% of TCP on top of UDP&rdquo; we have implemented something totally different and better suited to our requirements. A protocol that redundantly sends inputs because we know they are small, so we never have to wait for retransmission.So exactly how much better is this approach than sending inputs over TCP?Let&rsquo;s take a look&hellip;The video above shows deterministic lockstep synchronized over UDP using this technique with 2 seconds of latency and 25% packet loss. Imagine how awful TCP would look under these conditions.So in conclusion, even where TCP should have the most advantage, in the only networking model that relies on reliable-ordered data, we can still easily whip its ass with a simple protocol built on top of UDP.译文译文出处翻译：张乾光（星际迷航） 审校：陈敬凤(nunu)介绍大家好，我是格伦·菲德勒。欢迎大家阅读系列教程《网络物理仿真》，这个系列教程的目的是将物理仿真的状态通过网络进行广播。在之前的文章中，我们讨论了物理仿真需要在网络上进行广播的各种属性。在这篇文章中，我们将使用具有确定性的帧同步技术来将物理仿真通过网络进行传递和广播。具有确定性的帧同步是一种用来在一台电脑和其他电脑之间进行同步的方法，这种方法发送的是控制仿真状态变化的输入，而不是像其他方法那样发送的是仿真过程中物体的状态变化。这种方法的背后思想是给定一个初始状态，不妨设为S(n)，我们通过使用输入信息I(n)来运行仿真就能得到S(n+1)这个状态。然后我们可以通过S(n+1)这个状态和输入信息I(n+1)来运行仿真就能得到S(n+2)这个状态，我们可以一直重复这个过程得到S(n+3)、S(n+4)以及其后的各个状态。这看上去有点像是数学归纳法，我们可以只通过输入信息和之前的仿真状态就能得到后面的仿真状态-而且得到的仿真状态是高度一致，并且也不需要发送任何状态方面的同步。这个网络模型的主要优点是所需的带宽仅仅用来传递输入信息，而输入信息所占的带宽其实是与仿真中物体的数目是完全无关的。你可以通过网络来对一百万个物体进行物理仿真，它所需的带宽会跟只对一个物体进行物理仿真所需的带宽完全相同。可以很容易的看到物理物体的状态通常是包含位置、方向、线性速度和角速度（如果是未压缩的话，这些状态一共需要52字节，在这里面假设方向使用的是四元数而其他所有的变量都是用vec3来表示），所以当你有大量的物体需要进行物理仿真的时候，这是一个非常具有吸引力的方案。确定性如果要采用具有确定性的帧同步这个方案来将物理仿真网络化，首先要做的第一件事就是要确保你的仿真具有确定性。在这个上下文中，确定性其实和自由意志之类的没有关系。它只是意味着给定相同的初始条件和相同的一组输入，仿真能够给出完全相同的结果。而且我在这里要着重强调下是完全相同的结果。而不是说的什么在在浮点数容忍度内足够接近。这种精确是精确到比特位的。所以这种精确性使得你可以在每帧的末尾对整个物理状态做一个校验和，不同机器上面同一帧得到的校验和是完全一致的。从上面的图中可以看到，这里面的仿真几乎是具有确定性的，但是不完全具有确定性。左边的仿真由玩家进行控制，而右边的仿真有完全一致的初始状态，输入信息也和左边完全相同，但是要有2秒钟的延迟。这两个仿真使用相同的间隔时间进行更新（使用相同的间隔时间进行更新也是确保得到完全一致结果的一个必要前提条件），并且在每一帧前对相同的输入信息进行相应。你可以注意到随着仿真的进行，那些一开始很微小的差异是如何一点点被扩大，最后导致两个仿真完全不同步。所以说这个仿真其实不具有确定性。上面到底发生了什么?最后会导致两个仿真的结果差的这么大？这是因为我使用的物理引擎（ODE）在它的内部使用了一个随机数生成器来对约束处理的顺序进行随机化来提高稳定性。这个物理引擎是完全开源的，所以可以看看它的内部实现！不幸的是，由于左边的仿真处理约束的顺序和右边的仿真处理约束的顺序不同，这导致有一些轻微不同的结果。幸运的是我们还是能找到让ODE这个物理引擎具有确定性的条件：要在同一台机器上、使用同一个编译好的二进制文件、并且在完全相同的操作系统上运行（这是必要的限制条件么？），还有就是在运行仿真之前通过dSetRandomSeed把随机数的种子设为当前帧的帧数。一旦满足这些条件的话，ODE这个物理引擎能够给出完全相同的结果，并且左边和右边的仿真能够保持高度一致的同步。现在让我们针对上面这个情况给出一个警告。即使ODE这个物理引擎能够在相同的机器上得到确定性的结果，但是这并不一定意味着在不同编译器、不同的操作系统甚至不同的机器架构上（比如说在PowerPC架构上和在Intel架构上）它能够得到确定性的结果。事实上，由于浮点数的优化，在程序的debug版本和release版本之间可能都没有办法得到确定性的结果。浮点数的确定性是一个非常复杂的问题，而且这个问题没有银弹（意味着这个问题没有什么简单可行的解决办法）。要了解更多这方面的信息，请参考这篇文章。网络输入 Inputs让我们讨论下具有确定性的帧同步的具体实现方法。你可能想知道在我们这个示例仿真中输入信息到底是啥，以及我们该如何吧这些输入信息进行网络化。我们这个示例仿真是由键盘输入进行驱动的：方向键会给代表玩家的立方体施加一个力让他进行移动、按下空格键会把代表玩家的立方体提起来并把碰到的立方体四处滚落、按下‘z’键会启动katamari模式。但是我们该如何对这些输入信息进行网络化呢？我们需要把整个键盘的状态在网络上进行传输么？在这些键被按下和释放的时候我们要发送这些事件么？不，整个键盘的状态不需要在网络上进行传输，我们只需要传输那些会影响仿真的按键。那么被按下和释放的键的事件需要在网络上进行传输么？不，这也不是一个好的策略。我们需要确保的是在仿真第n帧的时候右边的仿真能够应用完全相同的输入信息，所以我们不能仅仅是通过TCP来发送“按键按下”和“按键释放”的事件，因为这些事件到达网络的另外一侧的时间如果早于或者晚于第n帧的时候都会给仿真造成偏差。相反我们做的事情是用一个结构来表示整个输入信息，并且在左边一侧仿真开始的时候，通过键盘的访问来填充这个结构并把填充好的结构放到一个滑动窗口中，我们在后面可以根据帧号来对这个输入进行访问。?123456789struct Input{ bool left; bool right; bool up; bool down; bool space; bool z;};现在我们就可以通过上面的方法来把左边仿真的输入信息发送到右边仿真中去，这样右边的仿真就知道属于第n帧的输入信息到底是怎么样的。举个简单的例子来说，如果你在通过TCP进行发送的话，你可以简单的只发送输入信息而不发送其他的内容，而发送的输入信息的顺序隐含着帧号N。而在网络的另外一侧，你可以读取传送过来的数据包，并且对输入信息进行处理并把输入信息应用到仿真中去。我不推荐这种方法，但我们可以从这里开始，然后再向你展示如何把这种方法变得更好。在进一步对这个方法进行优化之前，让我们先统一下使用的网络环境，让我们假设下我们是通过TCP进行数据传输，已经禁止了Nagle算法并且每帧都会从左边的仿真向右边的仿真发送一次输入信息（频率是每秒60次）。这里面有一个问题会变得比较复杂。把左边仿真发生的输入信息通过网络进行传输，然后右边仿真并没有足够的时间来从网络上收到输入信息并利用这些到达的输入信息来模拟仿真，因为这个过程需要一定的时间。你不能按照某个频率在网络上发送信息并且期望它们能够按照完全相同一致的频率到达网络的另外一侧(比如说，每六十分之一秒到达一个数据包)。互联网并不是按照这个方式工作的。根本就没有这样的保证。播放延迟缓冲区如果你想要做到这一点的话，你必须实现一个叫做播放延迟缓冲区的东西。不幸的是，播放延迟缓冲区收到了专利保护，也就是一个专利雷区。我不建议读者在实际使用确定性的帧同步模型的时候搜索“播放延迟缓冲区”或者是“自适应性延迟缓冲区”。但简而言之，你所需要做的事情是缓存收到的数据包一小段时间以便让这些数据包表现的像是以一个稳定的速度到达那样，即使实际上它们的到达时间是充满抖动的。你现在所做的事情就跟你在看一个视频流的时候，Netflix所做的事情是很类似的。你在最初开始的时候停顿了一下以便你可以拥有一个缓冲区，这样即使一些数据包的到达时间有点晚，但是这种延迟不会对视频帧按正确时间间距的表现有什么影响，视频帧仍然会按照正确的时间间隔一帧帧的播放。当然如果你的缓冲区没有足够大的话，那么这些视频帧的播放可能还是会充满一些抖动。有了确定性的帧同步机制，你的模拟仿真将会以完全相同的方式执行。我建议在播放的时候最好在一开始有100毫秒-250毫秒的延迟。在下面的例子中，我使用的是100毫秒的延迟，这是因为我让延迟最小化来增强响应性。我的播放延迟缓冲区的实现非常的简单。是将输入信息按照帧序号进行添加，当收到第一个输入信息的时候，它保存了接收方机器上的当前本地时间，并且从那一个时刻起假设所有到达的数据包都会带上100毫秒的延迟。你可能需要一些更加复杂的机制来适应真实世界的情况，比如说可能需要处理时钟漂移、检测在什么时候应该适当的加速或者减慢模拟的速度来让缓冲区的大小在能够保证整体延迟最小的情况下保持在一个适度的情形（这就是所谓的“自适应”），但是这些内容可能会相当的复杂并且可能需要一整篇文章来专门对这些情况进行专门论述。而且如前所述，这些内容还涉及到了专利保护方面的内容，所以这些内容我就不详细展开了，把如何处理这些东西全部托付给你自己实现。在平均情况下，播放延迟缓冲区给帧n、n+1、n+2以及后续的帧提供了一个稳定的输入信息流，非常完美的以六十分之一秒的间隔依次到达。在最坏的情况下，就是已经该执行第N帧的模拟仿真了，但是这一帧的输入信息还没有到达，那么它就会返回一个空指针，这样整个模拟仿真就必须在那里进行等待了。如果数据包被集中起来发送并且到达接收方的时候已经比预期时间延迟了，这可能会导致多个帧的输入信息同时准备好等待出列进行计算。如果是这种情况的话，我会限制在一个渲染帧的时间最多只能进行4次模拟仿真，这样给模拟仿真一个追上来的机会。如果你把这个值设置的更高的话，那么可能会引起更多其他的问题，比如卡顿，因为你可能需要超过六十分之一秒的时间来运行这些帧（这可能会造成一个非常不好的反馈体验）。总而言之，重要的是确保你的模拟仿真在使用确定性的帧同步这个方案的时候性能不是在中央处理器这一端受限的，否则的话，你在运行更多的模拟帧来追上正常的模拟速度的时候会遇到很多麻烦。TCP足够好了吗通过使用这种延迟缓冲区的策略以及通过TCP协议来发送输入信息，我们可以很轻松的确保所有的输入信息会有序的到达并且传输是可信赖的。这就是一开始TCP协议在设计的时候希望达到的目标。实际上，下面这些东西就是互联网的专家常说的一些东西：· 如果你需要一个可以信赖的有序的发送信息的方法，你不可能找到一种比通过TCP协议进行传输更好的方法！· 你的游戏根本就不会需要UDP协议。我在这里将告诉你上面这些想法都是大错特错的。在上面的视频中，你可以看到如果网络同步模型使用基于TCP协议的确定性的帧同步模型的话，模拟仿真的网络延迟大概是100毫秒，并且有百分之一的丢包率。如果你仔细看右边的话，你可以每隔几秒就会出现一些抖动。如果你在两边都出现这种的情况，那么很抱歉这意味着你的电脑的性能对于播放这些视频而言可能有些艰难。如果是这种情况的话，我建议下载这个视频然后离线观看。无论如何，这里所发生的事情是当一个数据包丢失的时候，TCP协议需要等待至少２个往返时延才会重新发送这个数据包（实际上这里面的等待时间可能会更糟，但是我很慷慨的设定了一个非常理想的情况。。。）。所以上面发生的抖动原因是确定性的帧同步模型要求右边的模拟仿真在没有第n帧的输入信息的时候不能执行第N帧的模拟仿真计算，所以整个模拟仿真就停下来等待对应帧的输入信息的到达!这还不是全部！随着延迟时间的增大和丢包率的增加，整个情况会变得更加的糟糕。这是在250毫秒延迟和百分之五丢包率的情况下，使用基于TCP协议的确定性的帧同步模型进行相同的仿真模拟运算导致的结果：现在我要承认一个事情，如果延迟时间设置的非常低的话同时不存在丢包的情况下，那么使用TCP协议进行输入信息的传输会是一个非常可以接受的结果。但是请注意，如果你使用TCP协议来发送时间敏感的数据的话，随着延迟时间的增大和丢包率的增加，整个结果会急剧恶化。我们能比TCP做得更好吗我们可以做得更好吗?我们能在自己的游戏里面找到一种比使用TCP协议更好的办法。同时还能实现可信赖的有序传递？答案是肯定的。但前提是我们需要改变游戏的规则。下面将具体描述下我们将使用的技巧。我们需要确保所有的输入信息能够可靠地按顺序到达。但是如果我们只发送UDP数据包输入。但是如果我们只使用UDP数据包来发送输入信息的话，这里面的一些数据包会丢失。那么如果我们不采用事后检测的方法来判断哪些数据包丢失并发送这些丢失的数据包的话，我们采用另外一种方法，只是把我们有的所有输入信息都冗余的发送直到我们知道这些输入信息成功的到达另外一侧怎么样？输入信息都非常非常的小（只有6比特这么大）。让我们假设下我们在每秒需要发送60个输入信息（因为模拟仿真的频率是60fps ），而且我们知道一个往返的时间大概是在30毫秒到250毫秒之间。纯粹为了好玩，让我们假设下载最糟糕的情况下，一个往返的时间可以高达2秒，如果出现这种情况的话，那么整个连接就会超时。这意味着在平均情况我们只需要包括大概2到15帧的输入信息，而在最坏情况下，我们大概需要120帧的输入信息。那么最坏情况下，输入信息的大小是120 x 6 = 720比特。这只是90字节的输入信息!这是安全合理的。我们还能做的更好。在每一帧中都出现输入信息的变化是非常不常见的。我们可以用最近的那个input的序列号和第一个input的6比特还有所有未被确认的输入信息的数目来做一些事。然后，当我们对这些输入信息进行遍历将它们写入数据包的时候，如果发现这一帧的输入信息如果和之前帧的输入信息不同的话，我们可以写入一个单独的比特位（1），如果发现这一帧的输入信息如果和之前帧的输入信息相同的话，我们可以写入一个单独的比特位（0）。所以这一帧的输入信息如果和之前帧的输入信息不同的话（这种情况比较少），我们需要写入7个比特位，这一帧的输入信息如果和之前帧的输入信息相同的话（这种情况其实非常常见），我们只需要写入1个比特位。在输入信息很少发生变化的情况，这是一个重大的胜利，而在最坏的情况下出现的情况也不会非常糟糕。只需要发送额外120个比特的数据，也就是说在最坏情况下，也只有15字节的额外开销.当然在这种情况下，需要从右边的模拟仿真中发送一个数据包到左边的模拟仿真中去，这样左边的模拟仿真才知道哪些输入信息被成功收到了。在每一帧，右边的模拟仿真都会从网络中读取输入的数据包，然后才会把这些数据包添加到延迟播放缓冲区，并且通过帧号记录它已经收到的最近那一帧的输入信息，或者如果你想容易一点处理这个问题的话，那么使用一个16比特的序列号就能很好的包装这个信息。在所有的输入数据包都被处理以后，如果右边的模拟仿真收到任何帧的输入信息以后都会回复一个数据包给左边的模拟仿真，告诉它最近收到的最新序列号是多少，这基本就是一个“ack”包，也就是确认包。当左边的模拟仿真收到这个“ack”包，也就是确认包以后，它会滑动输入信息窗口并且丢弃比已经确认的序列号还老的输入信息包。已经没有必要再发送这些输入信息包给右边的模拟仿真了，因为已经知道右边的模拟仿真成功的接受到了这些输入信息包。通过这种方式，我们通常只有少量的输入信息正在传输过程中，而且这个数量还是与数据包的往返时间成正比的。完美胜利我们通过改变游戏的规则成功了找到了一种比TCP协议更好的办法。我们并不是通过在UDP协议纸上构建了实现TCP协议百分之九十五的功能的新协议，而是实现了一种完全不同的方法，而且更加适合我们的要求：数据对时间非常敏感。我们开发了一个自定义的协议，可以冗余的发送input，因为我们知道这些input非常小, 所以我们不必去等待重传它们。所以这种方法到底比通过TCP协议来发送数据好多少呢？让我们通过一个例子来看一下。上面的视频是基于UDP协议来使用具有确定性的帧同步模型，延迟时间是２秒，并且有百分之二十五的丢包率。想象下如果我们是使用基于TCP协议的具有确定性的帧同步模型，我们该看到多么可怕的场景！所以最后我们能得到这么一个结论：即使这是一个TCP协议最具有优势的情况下，这是唯一一个依赖可靠性、有序性数据传输的网络模型，我们还是可以很容易的通过一个自定义的协议基于UDP来发送我们的数据包，并且得到的效果更好。【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟二之网络物理部分的视频演示]]></title>
    <url>%2F2017%2F01%2F21%2F%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F%E4%BA%8C%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E9%83%A8%E5%88%86%E7%9A%84%E8%A7%86%E9%A2%91%E6%BC%94%E7%A4%BA%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文出处 Introduction to Networked Physics Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to the first article in Networked Physics.In this article series we&rsquo;re going to network a physics simulation three different ways: deterministic lockstep, snapshot interpolation and state synchronization.But before we get to this, let&rsquo;s spend some time exploring the physics simulation we’re going to network in this article series:Your browser does not support the video tag.Here I’ve setup a simple simulation of a cube in the open source physics engine ODE. The player moves around by applying forces at its center of mass. The physics simulation takes this linear motion and calculates friction as the cube collides with the ground, inducing a rolling and tumbling motion.This is why I chose a cube instead a sphere. I want this complex, unpredictable motion because rigid bodies in general move in interesting ways according to their shape.An Interactive WorldNetworked physics get interesting when the player interacts with other physically simulated objects, especially when those objects push back and affect the motion of the player.So let&rsquo;s add some more cubes to the simulation:Your browser does not support the video tag.When the player interacts with a cube it turns red. When that cube comes to rest it turns back to grey (non-interacting).While it’s cool to roll around and interact with other cubes, what I really wanted was a way to push lots of cubes around. What I came up with is this:Your browser does not support the video tag.As you can see, interactions aren’t just direct. Red cubes pushed around by the player turn other cubes they touch red as well. This way, interactions fan out to cover all affected objects.A Complicated CaseI also wanted a very complex coupled motion between the player and non-player cubes such they become one system: a group of rigid bodies joined together by constraints.To implement this I thought it would be cool if the player could roll around and create a ball of cubes, like in one of my favorite games Katamari Damacy.Your browser does not support the video tag.Cubes within a certain distance of the player have a force applied towards the center of the cube. These cubes remain physically simulated while in the katamari ball, they are not just “stuck” to the player like in the original game.This is a very difficult situation for networked physics!]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络物理模拟一之网络物理部分的简介]]></title>
    <url>%2F2017%2F01%2F20%2F%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E6%A8%A1%E6%8B%9F%E4%B8%80%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E9%83%A8%E5%88%86%E7%9A%84%E7%AE%80%E4%BB%8B%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[译者：陈敬凤（nunu） 审校：崔国军（飞扬971）大家好，我是格伦·菲德勒，是一位来自洛杉矶的职业游戏开发者。我作为一名职业游戏开发者已经有15年了。而在这其中的十年时光里，我都是专门做网络编程的。在这期间我大部分的业余时间一直致力于研究网络物理模拟方面的问题。在这个系列文章中我的目标是分享我已经知道的一切有关网络物理模拟方面的知识。写这些文章是一个工作量很大的工作，而且我完全是在我的业余时间里面来做这些事情的。如果有了你们的支持，我可以找时间来继续写这个系列的文章。如果可以的话，请通过 patreon 支持我的工作。网络物理部分的入门是相当困难的。你可能想知道你的物理模拟是否需要确定性以便可以进行网络传输和通信？你应该通过网络来发送物体的物理状态么？或者你应该通过网络来发送一些诸如碰撞时间或者物体相互作用力这些东西么？你到底是应该通过UDP还是TCP来传送数据？你应该使用客户端/服务器通信模型还是点对点的通信模型？ 你是否需要一个专门的服务器？如何隐藏播放动作的时候的延迟？如何不让玩家作弊？人们经常问我这些问题。大多数情况下这些问题的正确答案依赖于他们所选用的网络模型。你可能甚至不知道你正在使用或计划使用的网络模型是什么，但事实是你必须选择一个网络模型并且你选择的这个网络模型会使某些事情变得容易，并使得另外一些事情变得困难。在这个系列文章中我们将使用三种不同的方式来构建一个网络物理模拟的例子，并通过这些例子来对不同的网络模型展开一个讨论。我把这些基本技术称为“同步策略”。他们是：《确定性的帧同步》、《快照插值》和《状态同步》。带宽是网络物理部分的另外一个重要方面。我该怎么做才能对所有这些对象进行同步状态？因此，我们打算花一整篇文章来进行介绍一个带宽优化的案例，向你展示如何将快照插值技术的带宽从每秒18m减少到每秒256k。在选择了合适的同步策略以及合理的优化完带宽以后，我们现在准备讨论客户机/服务器与点对点对等网络之间的优劣性了。除了讨论利弊以外，我还将分享在艰苦的发布使用客户机/服务器网络模型的游戏和点对点对等网络模型的游戏的过程中学到的那些经验和教训。这篇文章会有你在其他地方找不到的非常具体、务实的信息。最后，我们讨论构建在基本同步策略上的不同网络模型，这些网络模型的细节各有不同。在这里。你会发现你的网络物理模拟可以有许多不同的选择：《确定性帧同步网络模型》、《分布式权威网络模型》和《带有客户端预测的服务器仲裁网络模型》。所以系上安全带准备出发了。我们还有一大堆材料需要学习！接下来的这篇文章是：《物理模拟》为什么格伦·菲德勒在Patreon上寻求自助？嘿，大家好，我是格伦·菲德勒，网站gafferongames.com的作者。在过去的10年，我给大家分享了许多游戏开发方面的文章：《如何解决你的时间戳》、《整合基础》、《UDP与TCP》、《每个程序员都需要了解游戏开发网络方面的知识》以及其他一些文章。我还分享了《物理编程技巧》、《网络游戏编程的基础知识》和我的个人项目虚拟围棋。我想继续新的系列文章《网络物理模拟》以及《构建一个游戏网络协议》，但是我需要你的帮助！托管代码和文章需要费用，写这些文章需要花费大量的时间去研究和写作。如果你喜欢gafferongames.com上面的文章，请向我展现出你对我的支持并鼓励我写更多的文章并开放更多的源代码！【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。如果你觉得这篇文章有价值，请在 patreon 上支持原作者。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5分钟上手boost.asio]]></title>
    <url>%2F2017%2F01%2F12%2F5%E5%88%86%E9%92%9F%E4%B8%8A%E6%89%8Bboost.asio%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Boost.Asio入门首先，让我们先来了解一下什么是Boost.Asio？怎么编译它？ linux下直接 : sudo apt-get install libboost-all-dev 什么是Boost.Asio简单来说，Boost.Asio是一个跨平台的、主要用于网络和其他一些底层输入/输出编程的C++库。 . . . 异步VS同步 首先，异步编程和同步编程是非常不同的。 在同步编程中，所有的操作都是顺序执行的，比如从socket中读取（请求），然后写入（回应）到socket中。 每一个操作都是阻塞的。 因为操作是阻塞的，所以为了不影响主程序，当在socket上读写时，通常会创建一个或多个线程来处理socket的输入/输出。 因此，同步的服务端/客户端通常是多线程的。 相反的，异步编程是事件驱动的。 虽然启动了一个操作，但是你不知道它何时会结束；它只是提供一个回调给你，当操作结束时，它会调用这个API，并返回操作结果。 对于有着丰富经验的QT（诺基亚用来创建跨平台图形用户界面应用程序的库）程序员来说，这就是他们的第二天性。 因此，在异步编程中，你只需要一个线程。 因为中途做改变会非常困难而且容易出错，所以你在项目初期（最好是一开始）就得决定用同步还是异步的方式实现网络通信。 不仅API有极大的不同，你程序的语意也会完全改变（异步网络通信通常比同步网络通信更加难以测试和调试）。 你需要考虑是采用阻塞调用和多线程的方式（同步，通常比较简单），或者是更少的线程和事件驱动（异步，通常更复杂）。 同步例子同步客户端下面是一个基础的同步客户端例子： 12345using boost::asio;io_service service;ip::tcp::endpoint ep( ip::address::from_string("127.0.0.1"), 2001);ip::tcp::socket sock(service);sock.connect(ep); 首先，你的程序至少需要一个io_service实例。 Boost.Asio使用io_service同操作系统的输入/输出服务进行交互。 通常一个io_service的实例就足够了。 然后，创建你想要连接的地址和端口，再建立socket。 把socket连接到你创建的地址和端口。 同步服务端下面是一个简单的同步Boost.Asio的服务端：1234567891011121314151617typedef boost::shared_ptr&lt;ip::tcp::socket&gt; socket_ptr;io_service service;ip::tcp::endpoint ep( ip::tcp::v4(), 2001)); // listen on 2001ip::tcp::acceptor acc(service, ep);while ( true) &#123; socket_ptr sock(new ip::tcp::socket(service)); acc.accept(*sock); boost::thread( boost::bind(client_session, sock));&#125;void client_session(socket_ptr sock) &#123; while ( true) &#123; char data[512]; size_t len = sock-&gt;read_some(buffer(data)); if ( len &gt; 0) write(*sock, buffer("ok", 2)); &#125;&#125; 首先，同样是至少需要一个io_service实例。 然后你指定你想要监听的端口，再创建一个接收器——一个用来接收客户端连接的对象。 在接下来的循环中，你创建一个虚拟的socket来等待客户端的连接。 然后当一个连接被建立时，你创建一个线程来处理这个连接。 在client_session线程中来读取一个客户端的请求，进行解析，然后返回结果。 异步例子异步客户端而创建一个异步的客户端，你需要做如下的事情： 123456789using boost::asio;io_service service;ip::tcp::endpoint ep( ip::address::from_string("127.0.0.1"), 2001);ip::tcp::socket sock(service);sock.async_connect(ep, connect_handler);service.run();void connect_handler(const boost::system::error_code &amp; ec) &#123; // 如果ec返回成功我们就可以知道连接成功了&#125; 在程序中你需要创建至少一个io_service实例。 你需要指定连接的地址以及创建socket。 当连接完成时（其完成处理程序）你就异步地连接到了指定的地址和端口，也就是说，connect_handler被调用了。 当connect_handler被调用时，检查错误代码（ec），如果成功，你就可以向服务端进行异步的写入。 注意：只要还有待处理的异步操作，servece.run()循环就会一直运行。 在上述例子中，只执行了一个这样的操作，就是socket的async_connect。 在这之后，service.run()就退出了。 每一个异步操作都有一个完成处理程序——一个操作完成之后被调用的函数。 异步服务端 下面的代码是一个基本的异步服务端 123456789101112131415161718using boost::asio;typedef boost::shared_ptr&lt;ip::tcp::socket&gt; socket_ptr;io_service service;ip::tcp::endpoint ep( ip::tcp::v4(), 2001)); // 监听端口2001ip::tcp::acceptor acc(service, ep);socket_ptr sock(new ip::tcp::socket(service));start_accept(sock);service.run();void start_accept(socket_ptr sock) &#123; acc.async_accept(*sock, boost::bind( handle_accept, sock, _1) );&#125;void handle_accept(socket_ptr sock, const boost::system::error_code &amp;err) &#123; if ( err) return; // 从这里开始, 你可以从socket读取或者写入 socket_ptr sock(new ip::tcp::socket(service)); start_accept(sock);&#125; 在上述代码片段中，首先，你创建一个io_service实例，指定监听的端口。 然后，你创建接收器acc——一个接受客户端连接，创建虚拟的socket，异步等待客户端连接的对象。 最后，运行异步service.run()循环。 当接收到客户端连接时，handle_accept被调用（调用async_accept的完成处理程序）。 如果没有错误，这个socket就可以用来做读写操作。 在使用这个socket之后，你创建了一个新的socket，然后再次调用start_accept()，用来创建另外一个“等待客户端连接”的异步操作，从而使service.run()循环一直保持忙碌状态。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Boost</tag>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式具体实现重要组件之RPC]]></title>
    <url>%2F2017%2F01%2F10%2F%E5%88%86%E5%B8%83%E5%BC%8F%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%E9%87%8D%E8%A6%81%E7%BB%84%E4%BB%B6%E4%B9%8BRPC%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[RPC 是什么？RPC 的全称是 Remote Procedure Call 是一种进程间通信方式。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的，本质上编写的调用代码基本相同。 像腾讯的phxrpc框架是使用Protobuf作为IDL用于描述RPC接口以及通信数据结构 c++ RPC的实现 1、一套完善的序列化框架；在不同的进程间传输数据，序列化是第一步，如何可靠且方便地将对象转化为二进制（或者其他格式），在对端则是如何正确且安全地将其从二进制恢复为对象。 2、完善的底层通信协议；其需要提供合适的语义抽象：服务端支持怎样的并发，是单客户单访问，还是多访问；而客户端的并发模型由服务端决定。当然，还需要健壮且足够的接口抽象，毕竟分布式环境，“一切皆有可能”，需要应对各种问题。 3、一个可用的反射系统。是的，需要在C++环境下建立一个反射系统。这一步是最为关键的，其由C++11支持。因为，我们需要注册一个类的各种信息，以供RPC调用。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcp拥塞控制之慢启动和拥塞避免]]></title>
    <url>%2F2016%2F12%2F31%2Ftcp%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E4%B9%8B%E6%85%A2%E5%90%AF%E5%8A%A8%E5%92%8C%E6%8B%A5%E5%A1%9E%E9%81%BF%E5%85%8D%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[TCP拥塞控制概览TCP的拥塞控制算法被设计用来防止快速的发送者压垮整个网络。如果一个发送TCP发送包的速度要快于一个中间路由器转发的速度，那么该路由器就会开始丢弃包。这将会导致较高的包丢失率，其结果是如果TCP保持以相同的速度发送这些被丢弃的分段的话就会极大地降低性能。TCP的拥塞控制算法在下列两个场景中是比较重要的。 . . . 在连接建立之后：此时（或当传输在一个已经空闲了一段时间的连接上恢复时），发送者可以立即向网络中注入尽可能多的分段，只要接收者公告的窗口大小允许即可。（事实上，这就是早期的TCP实现的做法。）这里的问题在于如果网络无法处理这种分段洪泛，那么发送者会存在立即压垮整个网络的风险。 当拥塞被检测到时：如果发送TCP检测到发生了拥塞，那么它就必须要降低其传输速率。TCP是根据分段丢失来检测是否发牛了拥塞，因为传输错误率是非常低的，即如果一个包丢失了，那么就认为发生了拥塞。 TCP的拥塞控制策略组合采用了两种算法： 慢启动 拥塞避免。 慢启动算法会使发送TCP在开始的时候以低速传输分段，但同时允许它以指数级的速度提高其速率，只要这些分段都得到接收TCP的确认。慢启动能够防血一个快速的TCP发送者压垮整个网络。但如果不加限制的话，慢启动在传输速率上的指数级增长意味着发送者在短时间内就会压垮整个网络。TCP的拥塞避免算法用来防止这种情况的发生，它为速率的增长安排了一个管理实体。 有了拥塞避免之后，在连接刚建立时，发送TCP会使用一个较小的拥塞窗口，它会限制所能传输的未确认的数据数量。当发送者从对等TCP处接收到确认时，拥塞窗口在一开始时会呈现指数级增长。但一旦拥塞窗口增长到一个被认为是接近网络传输容量的阈值时，其增长速度就会变成线性，而不是指数级的。（对刚络容量的估算是根据检测到拥塞时的传输速率来计算得出的或者在一开始建立连接时设定为一个固定值。）在任何时刻，发送TCP传输的数据数量还会受到接收TCP的通告窗口和本地的TCP发送缓冲器的大小的限制。 慢启动和拥塞避免算法组合起来使得发送者可以快速地将传输速度提升至网络的可用容量，并且不会超出该容量。这些算法的作用是允许数据传输快速地到达一个平衡状态，即发送者传输包的速率与它从接收者处接收确认的速率一致。 在该图中，假定当cwnd为32个报文段时就会发生拥塞。于是设置ssthresh为16个报文段，而cwnd为1个报文段。在时刻0发送了一个报文段，并假定在时刻1接收到它的ACK，此时cwnd增加为2。接着发送了2个报文段，并假定在时刻2接收到它们的ACK，于是cwnd增加为4（对每个ACK增加1次）。这种指数增加算法一直进行到在时刻3和4之间收到8个ACK后cwnd等于ssthresh时才停止，从该时刻起，cwnd以线性方式增加，在每个往返时间内最多增加1个报文段。 慢启动当一个新的TCP连接建立或检测到由重传超时(RTO)导致的丢包时,需要执行慢启动o TCP发送端长时间处于空闲状态也可能调用慢启动算法。慢启动的目的是,使TCP在用拥塞避免探寻更多可用带宽之前得到cwnd值,以及帮助TCP建立ACK时钟。通常,TCP在建立新连接时执行慢启动,直至有丢包时,执行拥塞避免算法(参见16.2.2节)进人稳定状态。下文引自[RFC5681]: 在传输初始阶段，由于未知网络传输能力，需要缓慢探测可用传输资源，防止短时间内大量数据注入导致拥塞。慢启动算法正是针对这一问题而设计。在数据传输之初或者重传计时器检测到丢包后，需要执行慢启动。 TCP以发送一定数目的数据段开始慢启动（在SYN交换之后），称为初始窗口(Initial Window，IW)。IW的值初始设为一个SMSS（发送方的最大段大小），但在[RFC5681]中设为一个稍大的值，计算公式如下： IW= 2* (SMSS)且小于等于2个数据段（当SMSS&gt; 2190字节） IW=3+(SMSS)且小于等于3个数据段（当2190≥SMSS&gt; 1095字节） IW= 4* (SMSS)且小于等于4个数据段（其他） 述IW的计算方式可能使得初始窗口为几个数据包大小（如3个或4个），为简单起见，我们只讨论IW=1 SMSS的情况。TCP连接初始的cwnd=1 SMSS，意味着初始可用窗口矽也为1 SMSS。注意到大部分情况下，SMSS为接收方的MSS（最大段大小）和路径MTU（最大传输单元）两者中较小值。 假设没有出现丢包情况且每个数据包都有相应的ACK，第一个数据段的ACK到达，说明可发送一个新的数据段。每接收到一个好的ACK响应，慢启动算法会以min (N, SMSS)来增加cwnd值。这里的．Ⅳ是指在未经确认的传输数据中能通过这一“好的ACK”确认的字节数。所谓的“好的ACK”是指新接收的ACK号大于之前收到的ACK。 因此，在接收到一个数据段的ACK后，通常cwnd值会增加到2，接着会发送两个数据段。如果成功收到相应的新的ACK，cwnd会由2变4，由4变8，以此类推。一般情况下，假设没有丢包且每个数据包都有相应ACK，在t轮后∥的值为矽=2k，即t= log2W，需要t个RTT时间操作窗口才能达到矽大小。这种增长看似很快（以指数函数增长），但若与一开始就允许以最大可用速率（即接收方通知窗口大小）发送相比，仍显缓慢。(矽不会超过awnd。) 如果假设某个TCP连接中接收方的通知窗口非常大（比如说，无穷大），这时cwnd就是影响发送速率的主要因素（设发送方有较大发送需求）。如前所述，cwnd会随着RTT呈指数增长。因此，最终cwnd（矿也如此）会增至很大，大量数据包的发送将导致网络瘫痪(TCP吞吐量与W/RTT成正比)。当发生上述情况时，cwnd将大幅度减小（减至原值一半）。这是TCP由慢启动阶段至拥塞避免阶段的转折点，与cwnd和慢启动闽值(slow start threshold，ssthresh)相关。 拥塞避免如上所述，在连接建立之初以及由超时判定丢包发生的情况下，需要执行慢启动操作。在慢启动阶段，cwnd会快速增长，帮助确立一个慢启动阈值。一旦达到阈值，就意味着可能有更多可用的传输资源。如果立即全部占用这些资源，将会使共享路由器队列的其他连接出现严重的丢包和重传情况，从而导致整个网络性能不稳定。 为了得到更多的传输资源而不致影响其他连接传输，TCP实现了拥塞避免算法。一旦确立慢启动阈值，TCP会进入拥塞避免阶段，cwnd每次的增长值近似于成功传输的数据段大小。这种随时间线性增长方式与慢启动的指数增长相比缓慢许多。 慢启动与拥塞避免的选择在通常操作中，某个TCP连接总是选择运行慢启动和拥塞避免中的一个，不会出现两者同时进行的情况。现在我们考虑，在任一给定时刻如何决定选用哪种算法。我们已经知道，慢启动是在连接建立之初以及超时发生时执行的。那么决定使用慢启动还是拥塞避免的关键因素是什么呢？ 前面我们已经提到过慢启动阈值。这个值和cwnd的关系是决定采用慢启动还是拥塞避免的界线。当cwnd&lt; ssthresh，使用慢启动算法；当cwnd&gt; ssthresh，需要执行拥塞避免；而当两者相等时，任何一种算法都可以使用。由上面描述可以得出，慢启动和拥塞避免之间最大的区别在于，当新的ACK到达时，cwnd怎样增长。有趣的是，慢启动阈值不是固定的，而是随时间改变的。它的主要目的是，在没有丢包发生的情况下，记住上一次“最好的”操作窗口估计值。换言之，它记录TCP最优窗口估计值的下界。 慢启动阈值的初始值可任意设定（如awnd或更大），这会使得TCP总是以慢启动状态开始传输。当有重传情况发生，无论是超时重传还是快速重传，ssthresh会按下式改变： ssthresh - max(在外数据值／2，2*SMSS) (16-1) 注意微软最近的（“下一代”）TCP/IP访议栈中，上述等式变为ssthresh=max (min (cwnd, awnd) /2, 2*SMSS) 我们已经知道，如果出现重传情况，TCP会认为操作窗口超出了网络传输能力范围。这时会将慢启动阈值( ssthresh)减小至当前窗口大小的一半（但不小于2*SMSS），从而减小最优窗口估计值。这样通常会导致ssthresh减小，但也有可能会使之增大。分析TCP拥塞避免的操作流程，如果整个窗口的数据都成功传输，那么cwnd值可以近似增大1 SMSS。因此，若cwnd在一段时间范围内已经增大，将ssthresh设为整个窗口大小的一半可能使其增大。这种情况发生在当TCP探测到更多可用带宽时。在慢启动和拥塞避免结合的情况下，ssthresh和cwnd的相互作用使得TCP拥塞处理行为显现其独有特性。下面我们探讨将两者结合的完整的算法。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发五之每个游戏开发者都需要知道的游戏网络知识]]></title>
    <url>%2F2016%2F11%2F18%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E4%BA%94%E4%B9%8B%E6%AF%8F%E4%B8%AA%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91%E8%80%85%E9%83%BD%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文原文出处 Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.Have you ever wondered how multiplayer games work?From the outside it seems magical: two or more players sharing a consistent experience across the network like they actually exist together in the same virtual world.But as programmers we know the truth of what is actually going on underneath is quite different from what you see. It turns out it&rsquo;s all an illusion. A massive sleight-of-hand. What you perceive as a shared reality is only an approximation unique to your own point of view and place in time.Peer-to-Peer LockstepIn the beginning games were networked peer-to-peer, with each each computer exchanging information with each other in a fully connected mesh topology. You can still see this model alive today in RTS games, and interestingly for some reason, perhaps because it was the first way - it&rsquo;s still how most people think that game networking works.The basic idea is to abstract the game into a series of turns and a set of command messages when processed at the beginning of each turn direct the evolution of the game state. For example: move unit, attack unit, construct building. All that is needed to network this is to run exactly the same set of commands and turns on each player&rsquo;s machine starting from a common initial state.Of course this is an overly simplistic explanation and glosses over many subtle points, but it gets across the basic idea of how networking for RTS games work. You can read more about this networking model here: 1500 Archers on a 28.8: Network Programming in Age of Empires and Beyond.It seems so simple and elegant, but unfortunately there are several limitations.First, it&rsquo;s exceptionally difficult to ensure that a game is completely deterministic; that each turn plays out identically on each machine. For example, one unit could take slightly a different path on two machines, arriving sooner to a battle and saving the day on one machine, while arriving later on the other and erm. not saving the day. Like a butterfly flapping it&rsquo;s wings and causing a hurricane on the other side of the world, one tiny difference results in complete desynchronization over time.The next limitation is that in order to ensure that the game plays out identically on all machines it is necessary to wait until all player&rsquo;s commands for that turn are received before simulating that turn. This means that each player in the game has latency equal to the most lagged player. RTS games typically hide this by providing audio feedback immediately and/or playing cosmetic animation, but ultimately any truly game affecting action may occur only after this delay has passed.The final limitation occurs because of the way the game synchronizes by sending just the command messages which change the state. In order for this to work it is necessary for all players to start from the same initial state. Typically this means that each player must join up in a lobby before commencing play, although it is technically possible to support late join, this is not common due to the difficulty of capturing and transmitting a completely deterministic starting point in the middle of a live game.Despite these limitations this model naturally suits RTS games and it still lives on today in games like &ldquo;Command and Conquer&rdquo;, &ldquo;Age of Empires&rdquo; and &ldquo;Starcraft&rdquo;. The reason being that in RTS games the game state consists of many thousands of units and is simply too large to exchange between players. These games have no choice but to exchange the commands which drive the evolution of the game state.But for other genres, the state of the art has moved on. So that&rsquo;s it for the deterministic peer-to-peer lockstep networking model. Now lets look at the evolution of action games starting with Doom, Quake and Unreal.Client/ServerIn the era of action games, the limitations of peer-to-peer lockstep became apparent in Doom, which despite playing well over the LAN played terribly over the internet for typical users:Although it is possible to connect two DOOM machines together across the Internet using a modem link, the resulting game will be slow, ranging from the unplayable (e.g. a 14.4Kbps PPP connection) to the marginally playable (e.g. a 28.8Kbps modem running a Compressed SLIP driver). Since these sorts of connections are of only marginal utility, this document will focus only on direct net connections.The problem of course was that Doom was designed for networking over LAN only, and used the peer-to-peer lockstep model described previously for RTS games. Each turn player inputs (key presses etc.) were exchanged with other peers, and before any player could simulate a frame all other player&rsquo;s key presses needed to be received.In other words, before you could turn, move or shoot you had to wait for the inputs from the most lagged modem player. Just imagine the wailing and gnashing of teeth that this would have resulted in for the sort of folks with internet connections that were &ldquo;of only marginal utility&rdquo;. :)In order to move beyond the LAN and the well connected elite at university networks and large companies, it was necessary to change the model. And in 1996, that&rsquo;s exactly what John Carmack and his team did when he released Quake using client/server instead of peer-to-peer.Now instead of each player running the same game code and communicating directly with each other, each player was now a &ldquo;client&rdquo; and they all communicated with just one computer called the &ldquo;server&rdquo;. There was no longer any need for the game to be deterministic across all machines, because the game really only existed on the server. Each client effectively acted as a dumb terminal showing an approximation of the game as it played out on the server.In a pure client/server model you run no game code locally, instead sending your inputs such as key presses, mouse movement, clicks to the server. In response the server updates the state of your character in the world and replies with a packet containing the state of your character and other players near you. All the client has to do is interpolate between these updates to provide the illusion of smooth movement and BAM you have a networked game.This was a great step forward. The quality of the game experience now depended on the connection between the client and the server instead of the most lagged peer in the game. It also became possible for players to come and go in the middle of the game, and the number of players increased as client/server reduced the bandwidth required on average per-player.But there were still problems with the pure client/server model:While I can remember and justify all of my decisions about networking from DOOM through Quake, the bottom line is that I was working with the wrong basic assumptions for doing a good internet game. My original design was targeted at &lt; 200ms connection latencies. People that have a digital connection to the internet through a good provider get a pretty good game experience. Unfortunately, 99% of the world gets on with a slip or ppp connection over a modem, often through a crappy overcrowded ISP. This gives 300+ ms latencies, minimum. Client. User&#39;s modem. ISP&#39;s modem. Server. ISP&#39;s modem. User&#39;s modem. Client. God, that sucks.Ok, I made a bad call. I have a T1 to my house, so I just wasn&#39;t familliar with PPP life. I&#39;m addressing it now.The problem was of course latency.What happened next would change the industry forever.Client-Side PredictionIn the original Quake you felt the latency between your computer and the server. Press forward and you&rsquo;d wait however long it took for packets to travel to the server and back to you before you&rsquo;d actually start moving. Press fire and you wait for that same delay before shooting.If you&rsquo;ve played any modern FPS like Call of Duty: Modern Warfare, you know this is no longer what happens. So how exactly do modern FPS games remove the latency on your own actions in multiplayer?When writing about his plans for the soon to be released QuakeWorld, John Carmack said: I am now allowing the client to guess at the results of the users movement until the authoritative response from the server comes through. This is a biiiig architectural change. The client now needs to know about solidity of objects, friction, gravity, etc. I am sad to see the elegant client-as-terminal setup go away, but I am practical above idealistic.So now in order to remove the latency, the client runs more code than it previously did. It is no longer a dumb terminal sending inputs to the server and interpolating between state sent back. Instead it is able to predict the movement of your character locally and immediately in response to your input, running a subset of the game code for your player character on the client machine.Now as soon as you press forward, there is no wait for a round trip between client and server - your character start moving forward right away.The difficulty of this approach is not in the prediction, for the prediction works just as normal game code does - evolving the state of the game character forward in time according to the player&rsquo;s input. The difficulty is in applying the correction back from the server to resolve cases when the client and server disagree about where the player character should be and what it is doing.Now at this point you might wonder. Hey, if you are running code on the client - why not just make the client authoritative over their player character? The client could run the simulation code for their own character and simply tell the server where they are each time they send a packet. The problem with this is that if each player were able to simply tell the server &ldquo;here is my current position&rdquo; it would be trivially easy to hack the client such that a cheater could instantly dodge the RPG about to hit them, or teleport instantly behind you to shoot you in the back.So in FPS games it is absolutely necessary that the server is the authoritative over the state of each player character, in-spite of the fact that each player is locally predicting the motion of their own character to hide latency. As Tim Sweeney writes in The Unreal Networking Architecture: &ldquo;The Server Is The Man&rdquo;.Here is where it gets interesting. If the client and the server disagree, the client must accept the update for the position from the server, but due to latency between the client and server this correction is necessarily in the past. For example, if it takes 100ms from client to server and 100ms back, then any server correction for the player character position will appear to be 200ms in the past, relative to the time up to which the client has predicted their own movement.If the client were to simply apply this server correction update verbatim, it would yank the client back in time, completely undoing any client-side prediction. How then to solve this while still allowing the client to predict ahead?The solution is to keep a circular buffer of past character state and input for the local player on the client, then when the client receives a correction from the server, it first discards any buffered state older than the corrected state from the server, and replays the state starting from the corrected state back to the present &ldquo;predicted&rdquo; time on the client using player inputs stored in the circular buffer. In effect the client invisibly &ldquo;rewinds and replays&rdquo; the last n frames of local player character movement while holding the rest of the world fixed.This way the player appears to control their own character without any latency, and provided that the client and server character simulation code is reasonable, giving roughly exactly the same result for the same inputs on the client and server, it is rarely corrected. It is as Tim Sweeney describes:… the best of both worlds: In all cases, the server remains completely authoritative. Nearly all the time, the client movement simulation exactly mirrors the client movement carried out by the server, so the client’s position is seldom corrected. Only in the rare case, such as a player getting hit by a rocket, or bumping into an enemy, will the client’s location need to be corrected.In other words, only when the player&rsquo;s character is affected by something external to the local player&rsquo;s input, which cannot possibly be predicted on the client, will the player&rsquo;s position need to be corrected. That and of course, if the player is attempting to cheat :) 译文译文出处 翻译：黄威（横写、意气风发） 审校：艾涛（轻描一个世界）介绍作为一名程序员，你是否曾想过多人游戏是如何运作的呢？从表面来看这是非常奇妙：两个或者更多的玩家通过网络能够拥有相同的游戏体验，就像他们确实存在于同一个虚拟世界一样。但是作为程序员，我们知道底层运行的情况与你看到的完全不同。事实证明，这完全是一种错觉，是一个精妙的戏法。你能感受到游戏中的玩家都处于同一个世界中，但其实这只是在各个时间点，你自己独有的视角与位置和其他玩家的视角与位置相似。对等同步起初，网络游戏形成一个对等的网络，在这个网络中每台电脑在一个完全连接的网状拓扑结构中互相交换信息。如今在RTS游戏（即时战略游戏）中你仍能够看到这一模型，有趣的是，因为某种原因，可能因为它是第一种网络连接方式——大多数人仍认为游戏网络是这样运作的。基本思想就是在处理数据时将游戏抽象化成一系列的数据改变与一组命令消息，每一个数据改变都决定了游戏状态的演变。例如：移动单位、攻击单位、建造建筑。所有的这一切都要求网络让每一位玩家的机器都从相同的初始状态开始，并且运行完全相同的命令，数据的改变也完全相同。当然，这只是一个过于简单并且忽略掉了许多微妙细节的解释，但这个解释向我们解释了RTS游戏网络工作的基本原理。你可以点击这里了解更多关于这个网络模型的细节。这看起来是如此简单而又巧妙，但是不幸的是这个模型有几个限制因素。首先，要想保证游戏状态完全确定是非常困难的；即每台机器都进行着相同的变动。比如说，一个单位可以在两台机器上走略微不同的道路，在一台机器上玩家更早进入战斗并反败为胜，而在另一台机器上玩家到达的更晚，然后，嗯，没有取得胜利。就像一只蝴蝶扇动了翅膀，然后在世界的另一边导致了飓风的出现，随着时间的过去，一个微小的区别会导致两边完全的不同步。另一个限制因素就是为了保证游戏在所有机器上表现同步，就有必要在游戏操作在设备上模拟之前进行等待，直到设备接收到了所有玩家对于那个变动的指令。这就意味着游戏中的每一个玩家的延迟都等于延迟最高玩家的延迟。RTS游戏通常代表性地通过立即提供音频反馈与（或是）播放过渡动画来掩盖这段延迟，但是最终真正影响游戏的操作要在这段延迟过去之后才能进行。最后一个限制因素就在于游戏的同步方式是通过发送改变当前状态的命令消息。为了让其正常工作就有必要让所有的玩家由同一初始状态开始游戏。通常来说，这就意味着每个玩家都要在开始游戏之前进入房间准备游戏，尽管支持让玩家随后加入游戏从技术上来说是可行的，但是由于在一场进行中的游戏中间捕获与传输一个完全确定的起始点的难度很大，所以这种情况并不常见。尽管有这些限制，这个模型还是很适合RTS游戏的，并且在现代的游戏中它仍然存在，例如“命令与征服”、“帝国时代”与“星际争霸”等。原因就是在RTS游戏中，游戏状态包含了成千上万的单位，并且通常游戏状态太大而不能在玩家之间交换。这些游戏别无选择，只能交换这些驱动着游戏状态改变的指令。但是对于其他类别的游戏，美工的状态已经改变了。所以对于确定的对等网络同步模型就讲到这里。现在让我们从Doom（毁灭战士）、Quake（雷神之锤）以及Unreal（魔域幻境）中看看动作类游戏的演变。客户端/服务器（C/S结构）在动作游戏的时代，对等同步的限制因素在Doom中表现得更加明显，尽管它在局域网中表现很好，但是在面对互联网中的普通用户时表现得很糟糕：“虽然可以通过调制解调器将两个运行DOOM的设备在互联网上连接在一起，最终游戏将会变得缓慢，延迟的情况在完全不能进行游戏（例如一个14.4Kbps的P2P连接）到略微可玩（例如一个28.8Kbps的调制解调器运行一个压缩驱动程序）之间不等。因为这些类型的连接只有边际效用，本文将只关注于网络连接。（faqs.org）”这个问题显然就是Doom本来就是只为局域网设计的，并且使用了前面描述的为RTS游戏制作的对等同步模型。每一个玩家输入的行为（关键按键等等）与其他人进行信息交换，只有在所有其他玩家的关键按键都被接收到之后，玩家才能够进行游戏画面的模拟。换句话说，在你能够操作、移动或是射击之前，你必须等待延迟最高的玩家进行连入。想想这上述的所谓“这些连接只有边际效用”将会导致的令人咬牙切齿的情况。现在的游戏局限于局域网游戏以及拥有良好连接条件的大学网络或是大公司的精英之间的游戏，为了改变这种情况，是时候改变这个模型了。这就是John Carmack 1996年在发布雷神之锤时所做的事情——他使用客户端/服务器（C/S结构）代替了对等同步模型（P2P）。现在，玩家们不再运行相同的游戏代码，直接地互相交换数据，如今每个玩家都是一个客户端，他们都与一台叫做“服务器”的电脑进行数据交换。现在的游戏不再有任何对于所有机器都要进行确定的要求，因为游戏实际上只存在于服务器上。每个客户端实际上都是作为哑终端，用来显示出一个游戏的近似情况，因为游戏实际上只在服务器上发生。在一个纯粹的客户端/服务器模型中你没有在本地运行游戏代码，而是将你的操作例如按键、鼠标移动、点击等发送到服务器。服务器响应并更新了虚拟世界中你的角色状态，然后将一个包含着你与你周围角色状态的数据包传回。所有客户端要做的事情就是在这些数据更新之间插入自己的数据，然后给你一种流畅移动的假象，然后，boom！你就有了一个联网的客户端/服务器游戏了。这是一个伟大的进步。游戏体验的质量现在取决于客户端与服务器之间的连接，而不是取决于游戏中延迟最高的玩家。这同时让玩家在游戏进行中的加入变成了可能，并且随着客户端/服务器结构对于每位玩家需要的平均带宽减少，游戏玩家也在逐渐增长。但是对于纯粹的客户端/服务器模型仍然存在一些问题。“尽管我能记得并整理出我从DOOM到Quake做出的所有关于网络的决定，结果就是尽管我为了做出一个好的网络游戏而努力着，但这些努力都是基于一个错误的基础假设。我原先的设计目标就是使延迟低于200ms。这样的话通过一个好的供应商数字连接到网络的人，就能有一个很好的游戏体验。很不幸，世界上百分之九十就都是通过调制解调器进行SLIP连接或是PPP连接，它们通常是通过一个糟糕拥挤的ISP（网络服务提供者）进行连接的。这就导致了300ms以上的延迟，并且这只是最低值。客户端，使用者的调制解调器，ISP的调制解调器，服务器，再回到ISP的调制解调器，使用者的调制解调器，最后再回到客户端。天呐，这真是糟透了！好吧，我做了一件错误的事。我在家里都使用T1载体进行联网，所以我对使用P2P的生活并不了解，我现在就解决这个问题。”问题当然就是延迟。接下来John在他发布QuakeWorld（雷神世界）时做的事情将永久改变这个行业。客户端预测在最初的雷神之锤中，你可以明显感受到你的电脑与服务器之间的延迟。在你向前点击之后，你需要等待数据包发送至服务器然后再传回到你的电脑，然后你才能够开始移动。点击开火，然后你在射击之前同样需要等待上述延迟。如果你玩过任何像《使命召唤4：现代战争》之类的现代FPS游戏，你就会知道这种情况现在已经不会再出现了。那么现代FPS游戏到底是如何做到在多人游戏中看似消除了你自己行为的延迟呢？这个问题在历史上分两个部分来解决。第一部分就是JohnCarmack为雷神世界开发的客户端移动预测，它后来被合并作为Tim Sweeney的魔域幻境网络模型的一部分。第二部分就是维尔福公司的Yahn Bernier为反恐精英开发的延迟补偿。在本节中，我们将主要讨论第一部分——如何隐藏玩家移动的延迟。当谈到他对于即将发布的雷神世界的计划时，JohnCarmack说：“我们现在允许客户端预测使用者行动的结果，直到服务器传来命令式回复。这是一个非常非常大的架构变化。客户端现在需要知道物体的硬度、摩擦力、重力之类的数据。对于简洁的客户端作为终端计划我们已经不再采用了，我对此表示遗憾，但我是一个实用主义者而不是一个理想主义者。”所以为了消除延迟，客户端需要比之前运行更多的代码。它现在不再是一个向服务器发送输入内容并在状态发回之前进行数据插入的哑终端，它现在能够在本地预测你的角色移动，并且对你的输入迅速做出反应，在客户端设备上为你的游戏角色运行一部分游戏代码。现在只要你向前点击，不需要再等待客户端与服务器之间的信息往返——你的角色立即开始向前移动。这种方法的难点不在于预测，因为预测就像是普通游戏代码做的那样——根据玩家的操作随时间发展游戏角色状态。难点就在于，当客户端和服务器对于游戏角色所处的位置及所做的事情有分歧时，客户端如何以服务器传来的信息为基础进行修正。对于这一点，你可能会想，嘿，如果你在客户端运行游戏代码——为什么不以客户端的情况作为游戏角色的标准呢。客户端可以为自己的角色运行仿真代码，并在每次发送数据包时告诉服务器现在的情况。那么问题就是，如果每个玩家都可以简单地告诉服务器“这就是我现在的情况”，那就非常容易黑进客户端进行作弊，例如作弊者可以瞬间躲开将要射向他们的子弹，或者立即传送到你身后从后方射击你。所以在FPS游戏中，尽管每个玩家在本地预测自己角色的运动，从表面上隐藏了延迟，但以服务器状态作为每个玩家角色状态的标准是绝对有必要的。就像Tim Sweeney在UE网络架构里写到的：“服务器才是大哥！”这就是有趣的地方。如果客户端和服务器信息不一致，客户端就必须接受来自服务器的位置更新，但是由于客户端与服务器之间的延迟，这个对过去修正是必然的。举个例子，如果从客户端到服务器要消耗100ms，再经过100ms回来，那么任何服务器对于玩家角色位置的修正就会有200ms的延迟，这个时间是相对于客户端开始预测自己移动的时间。如果客户端连续接收服务器的修正更新，这就会及时拉回客户端，这就会导致客户端完全不能做任何客户端预测。怎么解决这个问题的同时仍然允许客户端进行超前预测呢？解决方法就是在客户端为过去的角色状态以及本地玩家的输入创建一个循环缓冲区，然后当客户端收到一个来自服务器的修正，（首先它丢弃比服务器的修正状态更早的缓冲状态）依据玩家储存在循环缓冲区的输入对由上一次的正确状态开始到现在预测时间的状态进行重放。实际上客户端在等待接下来的情况匹配完成之前悄悄地“倒放与重放”当地的玩家角色移动的最后几帧。这个方法可以让玩家看似无延迟地控制他们的角色，并且如果客户端与服务器的角色模拟代码一致的话——由于在客户端与服务器上相同的输入可以准确给出相同的结果——这就很少出现要修正的情况。这就像是Tim Sweeney描述的那样：“……最好的两个世界：在所有情况下，服务器都是绝对权威。在几乎任何时间内，客户端的移动模拟都与服务器计算出的客户端移动完全相同，所以客户端的情况很少需要修正。只有在极少的情况，例如玩家被一枚火箭击中，或是撞上一名敌人，客户端的情况将被修正。”换句话说，只有当玩家的角色被一些外部事情影响到了玩家的输入，并且这些不能被客户端所预测时，玩家的情况需要被修正。当然，如果玩家试图作弊时亦然。 【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发四之基于UDP的可靠性与排序和避免拥堵]]></title>
    <url>%2F2016%2F11%2F17%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E5%9B%9B%E4%B9%8B%E5%9F%BA%E4%BA%8EUDP%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%B8%8E%E6%8E%92%E5%BA%8F%E5%92%8C%E9%81%BF%E5%85%8D%E6%8B%A5%E5%A0%B5%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文原文出处 Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.In the previous article, we added our own concept of virtual connection on top of UDP. In this article we’re going to add reliability, ordering and congestion avoidance to our virtual UDP connection.The Problem with TCPThose of you familiar with TCP know that it already has its own concept of connection, reliability-ordering and congestion avoidance, so why are we rewriting our own mini version of TCP on top of UDP?The issue is that multiplayer action games rely on a steady stream of packets sent at rates of 10 to 30 packets per second, and for the most part, the data contained is these packets is so time sensitive that only the most recent data is useful. This includes data such as player inputs, the position, orientation and velocity of each player character, and the state of physics objects in the world.The problem with TCP is that it abstracts data delivery as a reliable ordered stream. Because of this, if a packet is lost, TCP has to stop and wait for that packet to be resent. This interrupts the steady stream of packets because more recent packets must wait in a queue until the resent packet arrives, so packets are received in the same order they were sent.What we need is a different type of reliability. Instead of having all data treated as a reliable ordered stream, we want to send packets at a steady rate and get notified when packets are received by the other computer. This allows time sensitive data to get through without waiting for resent packets, while letting us make our own decision about how to handle packet loss at the application level.It is not possible to implement a reliability system with these properties using TCP, so we have no choice but to roll our own reliability on top of UDP.Sequence NumbersThe goal of our reliability system is simple: we want to know which packets arrive at the other side of the connection.First we need a way to identify packets.What if we had added the concept of a &ldquo;packet id&rdquo;? Let&rsquo;s make it an integer value. We could start this at zero then with each packet we send, increase the number by one. The first packet we send would be packet 0, and the 100th packet sent is packet 99.This is actually quite a common technique. It&rsquo;s even used in TCP! These packet ids are called sequence numbers. While we’re not going to implement reliability exactly as TCP does, it makes sense to use the same terminology, so we’ll call them sequence numbers from now on.Since UDP does not guarantee the order of packets, the 100th packet received is not necessarily the 100th packet sent. It follows that we need to insert the sequence number somewhere in the packet, so that the computer at the other side of the connection knows which packet it is.We already have a simple packet header for the virtual connection from the previous article, so we&rsquo;ll just add the sequence number in the header like this: [uint protocol id] [uint sequence] (packet data…)Now when the other computer receives a packet it knows its sequence number according to the computer that sent it.AcksNow that we can identify packets using sequence numbers, the next step is to let the other side of the connection know which packets we receive.Logically this is quite simple, we just need to take note of the sequence number of each packet we receive, and send those sequence numbers back to the computer that sent them.Because we are sending packets continuously between both machines, we can just add the ack to the packet header, just like we did with the sequence number: [uint protocol id] [uint sequence] [uint ack] (packet data…)Our general approach is as follows:Each time we send a packet we increase the local sequence numberWhen we receieve a packet, we check the sequence number of the packet against the sequence number of the most recently received packet, called the remote sequence number. If the packet is more recent, we update the remote sequence to be equal to the sequence number of the packet.When we compose packet headers, the local sequence becomes the sequence number of the packet, and the remote sequence becomes the ack.This simple ack system works provided that one packet comes in for each packet we send out.But what if packets clump up such that two packets arrive before we send a packet? We only have space for one ack per-packet, so what do we do?Now consider the case where one side of the connection is sending packets at a faster rate. If the client sends 30 packets per-second, and the server only sends 10 packets per-second, we need at least 3 acks included in each packet sent from the server.Let&rsquo;s make it even more complex! What if the packet containing the ack is lost? The computer that sent the packet would think the packet got lost but it was actually received!It seems like we need to make our reliability system&hellip; more reliable!Reliable AcksHere is where we diverge significantly from TCP.What TCP does is maintain a sliding window where the ack sent is the sequence number of the next packet it expects to receive, in order. If TCP does not receive an ack for a given packet, it stops and resends a packet with that sequence number again. This is exactly the behavior we want to avoid!In our reliability system, we never resend a packet with a given sequence number. We sequence n exactly once, then we send n+1, n+2 and so on. We never stop and resend packet n if it was lost, we leave it up to the application to compose a new packet containing the data that was lost, if necessary, and this packet gets sent with a new sequence number.Because we&rsquo;re doing things differently to TCP, its now possible to have holes in the set of packets we ack, so it is no longer sufficient to just state the sequence number of the most recent packet we have received.We need to include multiple acks per-packet.How many acks do we need?As mentioned previously we have the case where one side of the connection sends packets faster than the other. Let&rsquo;s assume that the worst case is one side sending no less than 10 packets per-second, while the other sends no more than 30. In this case, the average number of acks we&rsquo;ll need per-packet is 3, but if packets clump up a bit, we would need more. Let&rsquo;s say 6-10 worst case.What about acks that don&rsquo;t get through because the packet containing the ack is lost?To solve this, we&rsquo;re going to use a classic networking strategy of using redundancy to defeat packet loss!Let&rsquo;s include 33 acks per-packet, and this isn&rsquo;t just going to be up to 33, but always 33. So for any given ack we redundantly send it up to 32 additional times, just in case one packet with the ack doesn&rsquo;t get through!But how can we possibly fit 33 acks in a packet? At 4 bytes per-ack thats 132 bytes!The trick is to represent the 32 previous acks before &ldquo;ack&rdquo; using a bitfield: [uint protocol id] [uint sequence] [uint ack] [uint ack bitfield] &lt;em&gt;(packet data…)&lt;/em&gt;We define &ldquo;ack bitfield&rdquo; such that each bit corresponds to acks of the 32 sequence numbers before &ldquo;ack&rdquo;. So let&rsquo;s say &ldquo;ack&rdquo; is 100. If the first bit of &ldquo;ack bitfield&rdquo; is set, then the packet also includes an ack for packet 99. If the second bit is set, then packet 98 is acked. This goes all the way down to the 32nd bit for packet 68.Our adjusted algorithm looks like this:Each time we send a packet we increase the local sequence numberWhen we receive a packet, we check the sequence number of the packet against the remote sequence number. If the packet sequence is more recent, we update the remote sequence number.When we compose packet headers, the local sequence becomes the sequence number of the packet, and the remote sequence becomes the ack. The ack bitfield is calculated by looking into a queue of up to 33 packets, containing sequence numbers in the range [remote sequence - 32, remote sequence]. We set bit n (in [1,32]) in ack bits to 1 if the sequence number remote sequence - n is in the received queue.Additionally, when a packet is received, ack bitfield is scanned and if bit n is set, then we acknowledge sequence number packet sequence - n, if it has not been acked already.With this improved algorithm, you would have to lose 100% of packets for more than a second to stop an ack getting through. And of course, it easily handles different send rates and clumped up packet receives.Detecting Lost PacketsNow that we know what packets are received by the other side of the connection, how do we detect packet loss?The trick here is to flip it around and say that if you don&rsquo;t get an ack for a packet within a certain amount of time, then we consider that packet lost.Given that we are sending at no more than 30 packets per second, and we are redundantly sending acks roughly 30 times, if you don&rsquo;t get an ack for a packet within one second, it is very likely that packet was lost.So we are playing a bit of a trick here, while we can know 100% for sure which packets get through, but we can only be reasonably certain of the set of packets that didn&rsquo;t arrive.The implication of this is that any data which you resend using this reliability technique needs to have its own message id so that if you receive it multiple times, you can discard it. This can be done at the application level.Handling Sequence Number Wrap-AroundNo discussion of sequence numbers and acks would be complete without coverage of sequence number wrap around!Sequence numbers and acks are 32 bit unsigned integers, so they can represent numbers in the range [0,4294967295]. Thats a very high number! So high that if you sent 30 packets per-second, it would take over four and a half years for the sequence number to wrap back around to zero.But perhaps you want to save some bandwidth so you shorten your sequence numbers and acks to 16 bit integers. You save 4 bytes per-packet, but now they wrap around in only half an hour.So how do we handle this wrap around case?The trick is to realize that if the current sequence number is already very high, and the next sequence number that comes in is very low, then you must have wrapped around. So even though the new sequence number is numerically lower than the current sequence value, it actually represents a more recent packet.For example, let&rsquo;s say we encoded sequence numbers in one byte (not recommended btw. :)), then they would wrap around after 255 like this: … 252, 253, 254, 255, 0, 1, 2, 3, …To handle this case we need a new function that is aware of the fact that sequence numbers wrap around to zero after 255, so that 0, 1, 2, 3 are considered more recent than 255. Otherwise, our reliability system stops working after you receive packet 255.Here&rsquo;s a function for 16 bit sequence numbers: inline bool sequence_greater_than( uint16_t s1, uint16_t s2 ) { return ( ( s1 &gt; s2 ) &amp;&amp; ( s1 - s2 &lt;= 32768 ) ) || ( ( s1 &lt; s2 ) &amp;&amp; ( s2 - s1 &gt; 32768 ) ); }This function works by comparing the two numbers and their difference. If their difference is less than 1&frasl;2 the maximum sequence number value, then they must be close together - so we just check if one is greater than the other, as usual. However, if they are far apart, their difference will be greater than 1&frasl;2 the max sequence, then we paradoxically consider the sequence number more recent if it is less than the current sequence number.This last bit is what handles the wrap around of sequence numbers transparently, so 0,1,2 are considered more recent than 255.Make sure you include this in any sequence number processing you do.Congestion AvoidanceWhile we have solved reliability, there is still the question of congestion avoidance. TCP provides congestion avoidance as part of the packet when you get TCP reliability, but UDP has no congestion avoidance whatsoever!If we just send packets without some sort of flow control, we risk flooding the connection and inducing severe latency (2 seconds plus!) as routers between us and the other computer become congested and buffer up packets. This happens because routers try very hard to deliver all the packets we send, and therefore tend to buffer up packets in a queue before they consider dropping them.While it would be nice if we could tell the routers that our packets are time sensitive and should be dropped instead of buffered if the router is overloaded, we can&rsquo;t really do this without rewriting the software for all routers in the world.Instead, we need to focus on what we can actually do which is to avoid flooding the connection in the first place. We try to avoid sending too much bandwidth in the first place, and then if we detect congestion, we attempt to back off and send even less.The way to do this is to implement our own basic congestion avoidance algorithm. And I stress basic! Just like reliability, we have no hope of coming up with something as general and robust as TCP&rsquo;s implementation on the first try, so let&rsquo;s keep it as simple as possible.Measuring Round Trip TimeSince the whole point of congestion avoidance is to avoid flooding the connection and increasing round trip time (RTT), it makes sense that the most important metric as to whether or not we are flooding our connection is the RTT itself.We need a way to measure the RTT of our connection.Here is the basic technique:For each packet we send, we add an entry to a queue containing the sequence number of the packet and the time it was sent.Each time we receive an ack, we look up this entry and note the difference in local time between the time we receive the ack, and the time we sent the packet. This is the RTT time for that packet.Because the arrival of packets varies with network jitter, we need to smooth this value to provide something meaningful, so each time we obtain a new RTT we move a percentage of the distance between our current RTT and the packet RTT. 10% seems to work well for me in practice. This is called an exponentially smoothed moving average, and it has the effect of smoothing out noise in the RTT with a low pass filter.To ensure that the sent queue doesn&rsquo;t grow forever, we discard packets once they have exceeded some maximum expected RTT. As discussed in the previous section on reliability, it is exceptionally likely that any packet not acked within a second was lost, so one second is a good value for this maximum RTT.Now that we have RTT, we can use it as a metric to drive our congestion avoidance. If RTT gets too large, we send data less frequently, if its within acceptable ranges, we can try sending data more frequently.Simple Binary Congestion AvoidanceAs discussed before, let&rsquo;s not get greedy, we&rsquo;ll implement a very basic congestion avoidance. This congestion avoidance has two modes. Good and bad. I call it simple binary congestion avoidance.Let&rsquo;s assume you send packets of a certain size, say 256 bytes. You would like to send these packets 30 times a second, but if conditions are bad, you can drop down to 10 times a second.So 256 byte packets 30 times a second is around 64kbits/sec, and 10 times a second is roughly 20kbit/sec. There isn&rsquo;t a broadband network connection in the world that can&rsquo;t handle at least 20kbit/sec, so we&rsquo;ll move forward with this assumption. Unlike TCP which is entirely general for any device with any amount of send/recv bandwidth, we&rsquo;re going to assume a minimum supported bandwidth for devices involved in our connections.So the basic idea is this. When network conditions are &ldquo;good&rdquo; we send 30 packets per-second, and when network conditions are &ldquo;bad&rdquo; we drop to 10 packets per-second.Of course, you can define &ldquo;good&rdquo; and &ldquo;bad&rdquo; however you like, but I&rsquo;ve gotten good results considering only RTT. For example if RTT exceeds some threshold (say 250ms) then you know you are probably flooding the connection. Of course, this assumes that nobody would normally exceed 250ms under non-flooding conditions, which is reasonable given our broadband requirement.How do you switch between good and bad? The algorithm I like to use operates as follows:If you are currently in good mode, and conditions become bad, immediately drop to bad modeIf you are in bad mode, and conditions have been good for a specific length of time &rsquo;t&rsquo;, then return to good modeTo avoid rapid toggling between good and bad mode, if you drop from good mode to bad in under 10 seconds, double the amount of time &rsquo;t&rsquo; before bad mode goes back to good. Clamp this at some maximum, say 60 seconds.To avoid punishing good connections when they have short periods of bad behavior, for each 10 seconds the connection is in good mode, halve the time &rsquo;t&rsquo; before bad mode goes back to good. Clamp this at some minimum like 1 second.With this algorithm you will rapidly respond to bad conditions and drop your send rate to 10 packets per-second, avoiding flooding of the connection. You&rsquo;ll also conservatively try out good mode, and persist sending packets at a higher rate of 30 packets per-second, while network conditions are good.Of course, you can implement much more sophisticated algorithms. Packet loss % can be taken into account as a metric, even the amount of network jitter (time variance in packet acks), not just RTT.You can also get much more greedy with congestion avoidance, and attempt to discover when you can send data at a much higher bandwidth (eg. LAN), but you have to be very careful! With increased greediness comes more risk that you&rsquo;ll flood the connection.ConclusionOur new reliability system let&rsquo;s us send a steady stream of packets and notifies us which packets are received. From this we can infer lost packets, and resend data that didn&rsquo;t get through if necessary. We also have a simple congestion avoidance system that drops from 30 packets per-second to 10 times a second so we don&rsquo;t flood the connection. 译文译文出处 翻译：艾涛（轻描一个世界） 审校：黄威（横写丶意气风发）简介嗨，我是格伦-菲德勒，欢迎来到我的游戏程序员网络设计文章系列的第四篇。在之前的文章里，我们将我们的虚拟连接的概念加入到UDP之上。现在我们将要给我们的虚拟UDP连接增加可靠性，排序和避免拥堵。这是迄今为止底层游戏网络设计中最复杂的一面，因此这将是一篇极其热情的文章，跟上我启程出发！ TCP的问题熟悉TCP的你们知道它已经有了自己关于连接、可靠性、排序和避免拥堵的概念，那么为什么我们还要重写我们自己的迷你版本的基于UDP的TCP呢？问题是多人动作游戏依靠于一个稳定的每秒发送10到30包的数据包流，而且在大多数情况下，这些数据包中包含的数据对时间是如此敏感以至于只有最新的数据才是有用的。这包括玩家的输入，位置方向和每个玩家角色的速度以及游戏世界中物理对象的状态等数据。TCP的问题是它提取的是以可靠有序的数据流发送的数据。正因为如此，如果一个数据包丢失了，TCP不得不停止以等待那个数据包重新发送，这打断了这个稳定的数据包流因为更多的最新的数据包在重新发送的数据包到达之前必须在队列中等待，所以数据包必须有序地提供。我们需要的是一种不同类型的可靠性。我们想要以一个稳定的速度发送数据包而且当数据被其他电脑接收到时我们会得到通知，而不是让所有的数据用一个可靠有效的数据流处理。这样的方法使得那些对时间敏感的数据能够不用等待重新发送的数据包就通过，而让我们自己拿主意怎么在应用层级去处理丢包。具有TCP这些特性的系统是不可能实现可靠性的，因此我们别无选择只能在UDP的基础上自行努力。不幸的是，可靠性并不是唯一一个我们必须重写的东西，这是因为TCP也提供避免拥堵功能，这样它就能够动态地衡量数据发送速率以来适应网络连接的性能。例如TCP在28.8k的调制调解器上会比在T1线路上发送更少的数据，而且它在不用事先知道这是什么类型的网络连接的情况下就能这么做！ 序列号现在回到可靠性！我们可靠性系统的目标很简单：我们想要知道哪些数据包到了网络连接的另一端。首先我们得鉴别数据包。如果我们添加一个“数据包id”的概念会怎么样？让我们先给id赋一个整数值。我们能够从零开始，然后随着我们每发送一个数据包，增加一个数值。我们发送的第一个数据包就是“包0”，发送的第100个数据包就是“包99”。这实际上是一个相当普遍的技术。甚至于在TCP中也得到了应用！这些数据包id叫做序列号，然而我们并不打算像TCP那样去做来实现可靠性，使用相同的术语是有意义的，因此从现在起我们还将称之为序列号。因为UDP并不能保证数据包的顺序，所以第100个收到的数据包并不一定是第100个发出的数据包。接下来我们需要在数据包中插入序列号这样网络连接另一端电脑便能够知道是哪个数据包。我们在前一篇文章中已经有了一个简单的关于虚拟网络连接的数据头，因此我们将只需要像这样在数据头中插入序列号： [uint protocol id] [uint sequence](packet data…)现在当其他电脑收到一个数据包时通过发送数据包的电脑它就能知道数据包的序列号啦。 应答系统既然我们已经能够使用序列号来鉴别数据包，下一步就该是让网络连接的另一端知道我们收到了哪个包了。逻辑上来说这是非常简单的，我们只需要记录我们收到的每个包的序列号，然后把那些序列号发回发送他们的电脑即可。因为我们是在两个机器间相互发送数据包，我们只能在数据包头添加上确认字符，就像我们加上序列号一样： [uint protocol id] [uint sequence] [uint ack](packet data…)我们的一般方法如下：每次我们发送一个数据包我们就增加本地序列号。当我们接收一个数据包时，我们将这个数据包的序列号与最近收到的数据包的序列号(称之为远程序列号)进行核对。如果这个包时间更近，我们就更新远程序列号使之等于这个数据包的序列号。当我们编写数据包头时，本地序列号就变成了数据包的序列号，而远程序列号则变成确认字符。这个简单的应答系统工作条件是每当我们发出一个数据包就会接收到一个数据包。但如果数据包一起发送这样在我们发送一个数据包之前有两个数据包到达该怎么办呢？我们每个数据包只留了一个确认字符的位置，那我们该怎么处理呢?现在考虑网络连接中的一端用更快的速率发送数据包这种情况。如果客户端每秒发送30个数据包，而服务器每秒只发送10个数据包，这样从服务器发出的每个数据包我们至少需要3个确认字符。让我们想得更复杂点！如果数据包留下来了而确认字符丢失了会怎么样？这样发送这个数据包的电脑会认为这个数据包已经丢失了而实际上它已经被收到了！貌似我们需要让我们的可靠性系统……更加可靠一点！ 可靠的应答系统这就是我们偏离TCP的地方。TCP的做法是在确认字符发送的地方给下一个按顺序预期该收到的数据包序列号的位置维持一个移动窗口。如果TCP对于一个已经发出的数据包没有收到确认字符，它将暂停并重新发送那个对应序列号的数据包。这正是我们想要避免的做法！因此在我们的可靠性系统里，我们从不为一个已经发出的序列号重新发送数据包，我们精确地只排序一次n，然后我们发送n+1，n+2，依次类推。如果数据包n丢失了我们也从不暂停重新发送它，而是把它留给应用程序来编写一个包含丢失数据的新的数据包，必要的话，这个包还会用一个新的序列号发送。因为我们工作的方式与TCP不同，它的做法现在可能在我们数据包的确定字符设置中有了个洞，因此现在仅仅陈述最近的数据包的序列号已经远远不够了。我们需要在每个数据包中包含多个确认字符。那我们需要多少确认字符呢?正如之前提到网络连接的一端发包速率比另一端快的情况，让我们假定最糟的情况是一端每秒钟发送不少于10个数据包，而另一端每秒钟发送不多于30个数据包。这种情况下，我们每个数据包需要的平均确认字符数是3个，但是如果数据包发送密集点，我们将需要更多。让我们说6-10个最差的情况。如果因为包含确认字符的数据包丢失而导致确认字符并没有到达怎么办?为了解决这个问题，我们将要使用一种经典的使用冗余码的网络设计策略来处理数据包丢失的情况！让我们在每个数据包中容纳33个确认字符，而且这不仅是他将要达到33个，而是一直是33个。因此对于每一个发出的确认字符我们多余地把它额外多发送了多达32次，仅仅是以防某个包含确认字符的数据包不能通过！但是我们怎么可能在一个数据包里配置33个确认字符呢？每个确认字符4字节那就是132字节了！窍门是在“相应确认字符”之前使用一段位域来代表32个之前的确认字符，就像这样： [uint protocol id] [uint sequence] [uint ack] [uint ack bitfield](packet data…)我们这样规定“位域”中每一位对应“相应确认字符”之前的32个确认字符。因此让我们说“相应确认字符”是100。如果位域的第一位设置好了，那么这个数据包也包含包99的一个确认字符。如果第二位设置好了，那么它也包含包98的一个确认字符。这样一路下来就到了包68的第32位。我们调整过的算法看起来就像这样:每次我们发送一个数据包我们就增加本地序列号。当我们接收一个数据包时，我们将这个数据包的序列号与最近收到的数据包的序列号(称之为远程序列号)进行核对。如果这个包是更新的，我们就更新远程序列号使之等于数据包的序列号。当我们编写数据包头时，本地序列号就变成了数据包的序列号，而远程序列号则变成确认字符。 计算确认字符位域是通过寻找一个多达33个数据包的队列，其中包括在[远程序列号-32，远程序列号]范围内的序列号。如果序列号“远程序列号-n”正在接收队列中那就把确认字符位域中的位n（在[1，32]范围内）设置为位1。此外，当一个数据包被接收了，确认字符位域也被扫描了，如果位n设置好了，那么即使它还没有被应答，我们也认可序列号“远程序列号-n”。利用这个改善过的算法，你将可能不得不在不止一秒内丢掉100%的数据包而不是让一个数据包停止通过。当然，它能够轻松地处理不同的发包速率和接受一起发送的数据包。 检测丢包既然我们知道网络连接另一端接受的是哪些数据包，那么我们该怎么检测数据包的丢失呢?这次的窍门是反过来想，如果你在一定时间内还没有收到某个数据包的应答，那么我们可以考虑说那个数据包已经丢失了。考虑到我们正在以每秒不超过30包的速率发送数据包，而且我们正在多余地发送数据包大概三十次。如果你在一秒内没有收到某个数据包的确认字符，那很有可能就是这个数据包已经丢失了。因此我们在这儿用了一些小窍门，尽管我们能100%确定哪个数据包通过了，但是我们只能适度地确定那些没有到达的数据包。这种情况的复杂性在于任何你重新发送的使用了这种可靠性方法的数据需要有它自己的信息id，这样的话在你多次收到它的时候你可以放弃它。这在应用层级是能够做到的。应对环绕式处理的序列号如果序列号没有环绕式处理覆盖，那么对于序列号和确认字符的讨论是不完整的！序列号和确认字符都是32比特的无符号整数，因此它们能够代表在范围[0，4294967295]内的数字。那是一个非常大的数字！那么大以至于如果你每秒发送三十个数据包也将要花费四年半来把这个序列号环绕式处理回零。但是可能你想要节省一些带宽这样你将你的序列号和确认字符缩减到到16比特整数。你每个数据包节省了4个字节，但现在他们只需要在仅仅半个小时内即可完成环绕式处理！所以我们该怎么应对这种环绕式处理的情况呢?诀窍是要认识到如果当前序列号已经非常高了，而且下一个到达的序列号很低，那么你就必须进行环绕式处理。那么即使新的序列号数值上比当前序列号值更低它也能实际代表一个更新的数据包。举个例子，让我们假设我们用一个字节编码序列号（顺便说一下并不推荐这样做）。 :))， 之后他们就会在255后面进行环绕式处理，就像这样: … 252, 253, 254, 255, 0, 1, 2, 3,…为了解决这种情况我们需要一个能够意识到在255之后需要环绕式处理回零这样一个事实的新功能，这样0，1，2，3就会被认为比255更新。否则，我们的可靠性系统就会在你收到包255后停止工作。这就是那个新功能：boolsequence_more_recent( unsigned int s1, unsigned int s2, unsigned int max ){ return ( s1 &gt; s2 ) &amp;&amp; ( s1 - s2 &lt;= max/2 ) || ( s2 &gt; s1 ) &amp;&amp; ( s2 - s1 &gt; max/2 );}这个功能通过比较两个数字和他们的不同来工作。如果它们之间的差异少于1/2的最大序列号值，那么它们必须靠在一起– 因此我们只需要照常检查某个序列号是否比另一个大。然而，如果它们相差很多，它们之间的差异将会比1/2的最大序列号值大，那么如果它比当前序列号小我们反而认为这个序列号是更新的。这最后一点是显然需要环绕式处理序列号的地方，那么0，1，2就会被认为比255更新。多么简洁而巧妙！一定要确保你在你所做的任何序列号处理当中包含了这一步！ 避免拥堵当你已经解决了可靠性的问题的时候，还有避免拥堵的问题。当你获得TCP的可靠性的时候TCP已经提供了避免拥堵的功能作为数据包的一部分，但是UDP无论怎样都不会有避免拥堵！如果我们仅仅发送数据包而没有某种流量控制，我们正在冒险占满网络连接而且会引起严重的延迟（2秒以上！），正如我们和另外一台电脑之间的路由器会超负荷而缓冲数据包。这个发生是因为路由器很努力地想要尝试传送我们发送的所有数据包，因此在它们考虑丢弃数据包之前会在队列中缓冲数据包。然而如果我们能告诉路由器我们的数据包是时间敏感的而且如果路由器超载的话这些数据包应该丢弃而不是缓冲这样会很棒的，但只有我们重写世界上所有路由器的软件才能做到这一点！那么我们反而需要把重点放在我们实际上能做的是避免占满首位网络连接。做到这个的方法是实施我们自己的基础避免拥堵算法。我强调基础！就像可靠性，我们并不寄希望于像TCP第一次尝试应用那样普通而粗暴地想出某些东西，那么让我们让它尽可能简单吧。 衡量往返时间因为所有避免拥堵的要点就是避免占满网络连接和避免增加往返时间（RTT），关于我们是不是占满网络的最重要的衡量标准是RTT它本身的观点是有道理的。我们需要一种方法来衡量我们网络连接的RTT。这是基础的技巧：对我们发送的每个数据包，我们对数据包队列中包含的序列号和他们发送的时间添加一个登记。当我们收到一个应答时，我们找到这个登记, 然后记录我们收到这个应答的时间t1与我们发送数据包的时间的t2的差值(都基于本地时间来计算)。这就是是这个数据包的RTT时间。因为数据包的到达因网络波动而不同，我们需要缓和这个值来提供某些有意义的东西，这样每次我们获得一个新的RTT我们就移动一个我们当前的RTT和数据包的RTT之间距离的百分比。10%在实践中看起来效果很好。这就叫做一个指数级平滑移动平均值，而且它在用一个低通滤波器的情况下能有效地平滑RTT中的杂音。为了确保发送队列永不增长，一旦超过某些最大预期RTT值我们就丢弃数据包。正如上一节关于可靠性讨论过的，任何在一秒内未应答的数据包都极有可能丢失了，那么对于最大RTT来说，一秒是个很棒的值。既然我们有RTT，我们能把它作为一个衡量标准来推动我们的避免拥堵功能。如果RTT变得太大了，我们更缓慢地发送数据，如果它的值低于可接受范围，我们能努力更频繁地发送数据。 简单的好坏机制避免拥堵正如之前讨论的，我们不要那么贪心，我们将要执行一个非常基础的避免拥堵机制。这个避免拥堵机制有两种模式。好和坏。我把它叫做简单的二进制避免拥堵。让我们假设你在发送一个确定大小的数据包，就假设256字节吧。你想要每秒发送这些数据包30次，但是如果网络条件差，你可以削减为每秒10次。那么30次256字节的数据包的速率大概是64kbits/sec，每秒10次的话大概20kbits/sec。世界上没有一个宽带连接不能处理至少20kbits/sec的速率，所以我们在这样的假定下继续前进。不像TCP这样对有任何数量的发送/接受带宽的任何设备都完全通用，我们将假设一个设备的最小支持带宽来参与我们的网络连接。所以基础想法就是这样了。当网络条件好的时候我们每秒发送30个数据包，当网络条件差的时候我们降至每秒10个数据包。当然，你能随你喜爱定义好和坏，但是仅考虑RTT的时候我已经得到了好的成效。举个例子，如果RTT超过某些极限值（假设250ms）那你就知道你可能已经正占满了网络连接。当然，这里假设一般没人在非占满网络条件下超过250ms，考虑到我们的宽带要求这是合理的。。好和坏之间你会怎么转换？我喜欢用下列操作的算法:如果你当前在好模式下，而网络条件突然变坏，立即降至坏模式。如果你正在坏模式下，而且网络条件已经好了一段特定时长”t”，那么回到好模式。为了避免好模式和坏模式之间的快速切换，如果你从好模式降至坏模式持续10秒钟以内，从坏模式回到好模式之前的时间是”t”的两倍。在某些最大值中固定这个时间值，假设60秒。为了避免打击良好的网络连接，当它们有一小段时期的差连接时，每过10秒连接就处于好模式，把坏模式回到好模式之前的时间“t”减半。在某些最小值中固定这个时间值，例如1秒。利用这个算法，你将对差网络连接迅速反应然后降低你的发送速率至每秒10个数据包，避免占满网络。在网络条件好时，你也将谨慎地尝试好模式，坚持以更高的每秒发送30个数据包的速率发送数据包。当然，你也能实施复杂得多的算法，丢包率百分比甚至是网络波动（数据包确认字符的时间差异）都可以考虑作为一个衡量标准，而不仅仅是RTT。对于避免拥堵你还可以更贪心点，并尝试发现什么时候你能以一个更高的带宽（例如LAN）发送数据，但是你必须非常小心！随着贪婪心的增加你占满网络连接的风险也在增大！ 结语我们全新的可靠性系统让我们稳定流畅发送数据包，而且能通知我们收到了什么数据包。从这我们能推断出丢失的数据包，必要的话重新发送没有通过的数据。基于此我们有了能够取决于网络条件在每秒10次和每秒30次发包速率间轮流切换的一个简单的避免拥堵系统，因此我们不会占满网络连接。还有很多实施细节因为太具体而不能在这篇文章一一提到，所以务必确保你检查示例源代码来看是否它都被实施了。这就是关于可靠性，排序和避免拥堵的一切了，或许是低层次网络设计中最复杂的一面了。 【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权； 源码下载因 Gaffer On Games 的源码原下载地址失效, 所以特地补上. 请点击]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发三之基于UDP的虚拟连接]]></title>
    <url>%2F2016%2F11%2F16%2Fvirtual_connection_over_udp%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文原文出处 Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.In the previous article we sent and received packets over UDP. Since UDP is connectionless, one UDP socket can be used to exchange packets with any number of different computers. In multiplayer games however, we usually only want to exchange packets between a small set of connected computers.As the first step towards a general connection system, we&rsquo;ll start with the simplest case possible: creating a virtual connection between two computers on top of UDP.But first, we&rsquo;re going to dig in a bit deeper about how the Internet really works!The Internet NOT a series of tubesIn 2006, Senator Ted Stevens made internet history with his famous speech on the net neutrality act:“The internet is not something that you just dump something on. It’s not a big truck. It’s a series of tubes”When I first started using the Internet, I was just like Ted. Sitting in the computer lab in University of Sydney in 1995, I was &ldquo;surfing the web&rdquo; with this new thing called Netscape Navigator, and I had absolutely no idea what was going on.You see, I thought each time you connected to a website there was some actual connection going on, like a telephone line. I wondered, how much does it cost each time I connect to a new website? 30 cents? A dollar? Was somebody from the university going to tap me on the shoulder and ask me to pay the long distance charges? :)Of course, this all seems silly now.There is no switchboard somewhere that directly connects you via a physical phone line to the other computer you want to talk to, let alone a series of pneumatic tubes like Sen. Stevens would have you believe.No Direct ConnectionsInstead your data is sent over Internet Protocol (IP) via packets that hop from computer to computer.A packet may pass through several computers before it reaches its destination. You cannot know the exact set of computers in advance, as it changes dynamically depending on how the network decides to route packets. You could even send two packets A and B to the same address, and they may take different routes.On unix-like systems can inspect the route that packets take by calling &ldquo;traceroute&rdquo; and passing in a destination hostname or IP address.On windows, replace &ldquo;traceroute&rdquo; with &ldquo;tracert&rdquo; to get it to work.Try it with a few websites like this: traceroute slashdot.org traceroute amazon.com traceroute google.com traceroute bbc.co.uk traceroute news.com.auTake a look and you should be able to convince yourself pretty quickly that there is no direct connection.How Packets Get DeliveredIn the first article, I presented a simple analogy for packet delivery, describing it as somewhat like a note being passed from person to person across a crowded room.While this analogy gets the basic idea across, it is much too simple. The Internet is not a flat network of computers, it is a network of networks. And of course, we don&rsquo;t just need to pass letters around a small room, we need to be able to send them anywhere in the world.It should be pretty clear then that the best analogy is the postal service!When you want to send a letter to somebody you put your letter in the mailbox and you trust that it will be delivered correctly. It&rsquo;s not really relevant to you how it gets there, as long as it does. Somebody has to physically deliver your letter to its destination of course, so how is this done?Well first off, the postman sure as hell doesn&rsquo;t take your letter and deliver it personally! It seems that the postal service is not a series of tubes either. Instead, the postman takes your letter to the local post office for processing.If the letter is addressed locally then the post office just sends it back out, and another postman delivers it directly. But, if the address is is non-local then it gets interesting! The local post office is not able to deliver the letter directly, so it passes it &ldquo;up&rdquo; to the next level of hierarchy, perhaps to a regional post office which services cities nearby, or maybe to a mail center at an airport, if the address is far away. Ideally, the actual transport of the letter would be done using a big truck.Lets be complicated and assume the letter is sent from Los Angeles to Sydney, Australia. The local post office receives the letter and given that it is addressed internationally, sends it directly to a mail center at LAX. The letter is processed again according to address, and gets routed on the next flight to Sydney.The plane lands at Sydney airport where an entirely different postal system takes over. Now the whole process starts operating in reverse. The letter travels &ldquo;down&rdquo; the hierarchy, from the general, to the specific. From the mail hub at Sydney Airport it gets sent out to a regional center, the regional center delivers it to the local post office, and eventually the letter is hand delivered by a mailman with a funny accent. Crikey! :)Just like post offices determine how to deliver letters via their address, networks deliver packets according to their IP address. The low-level details of this delivery and the actual routing of packets from network to network is actually quite complex, but the basic idea is that each router is just another computer, with a routing table describing where packets matching sets of addresses should go, as well as a default gateway address describing where to pass packets for which there is no matching entry in the table. It is routing tables, and the physical connections they represent that define the network of networks that is the Internet.The job of configuring these routing tables is up to network administrators, not programmers like us. But if you want to read more about it, then this article from ars technica provides some fascinating insight into how networks exchange packets between each other via peering and transit relationships. You can also read more details about routing tables in this linux faq, and about the border gateway protocol on wikipedia, which automatically discovers how to route packets between networks, making the internet a truly distributed system capable of dynamically routing around broken connectivity.Virtual ConnectionsNow back to connections.If you have used TCP sockets then you know that they sure look like a connection, but since TCP is implemented on top of IP, and IP is just packets hopping from computer to computer, it follows that TCP&rsquo;s concept of connection must be a virtual connection.If TCP can create a virtual connection over IP, it follows that we can do the same over UDP.Lets define our virtual connection as two computers exchanging UDP packets at some fixed rate like 10 packets per-second. As long as the packets are flowing, we consider the two computers to be virtually connected.Our connection has two sides:One computer sits there and listens for another computer to connect to it. We’ll call this computer the server.Another computer connects to a server by specifying an IP address and port. We’ll call this computer the client.In our case, we only allow one client to connect to the server at any time. We&rsquo;ll generalize our connection system to support multiple simultaneous connections in a later article. Also, we assume that the IP address of the server is on a fixed IP address that the client may directly connect to.Protocol IDSince UDP is connectionless our UDP socket can receive packets sent from any computer.We&rsquo;d like to narrow this down so that the server only receives packets sent from the client, and the client only receives packets sent from the server. We can&rsquo;t just filter out packets by address, because the server doesn&rsquo;t know the address of the client in advance. So instead, we prefix each UDP packet with small header containing a 32 bit protocol id as follows: [uint protocol id] (packet data…)The protocol id is just some unique number representing our game protocol. Any packet that arrives from our UDP socket first has its first four bytes inspected. If they don&rsquo;t match our protocol id, then the packet is ignored. If the protocol id does match, we strip out the first four bytes of the packet and deliver the rest as payload.You just choose some number that is reasonably unique, perhaps a hash of the name of your game and the protocol version number. But really you can use anything. The whole point is that from the point of view of our connection based protocol, packets with different protocol ids are ignored.Detecting ConnectionNow we need a way to detect connection.Sure we could do some complex handshaking involving multiple UDP packets sent back and forth. Perhaps a client &ldquo;request connection&rdquo; packet is sent to the server, to which the server responds with a &ldquo;connection accepted&rdquo; sent back to the client, or maybe an &ldquo;i&rsquo;m busy&rdquo; packet if a client tries to connect to server which already has a connected client.Or&hellip; we could just setup our server to take the first packet it receives with the correct protocol id, and consider a connection to be established.The client just starts sending packets to the server assuming connection, when the server receives the first packet from the client, it takes note of the IP address and port of the client, and starts sending packets back.The client already knows the address and port of the server, since it was specified on connect. So when the client receives packets, it filters out any that don&rsquo;t come from the server address. Similarly, once the server receives the first packet from the client, it gets the address and port of the client from &ldquo;recvfrom&rdquo;, so it is able to ignore any packets that don&rsquo;t come from the client address.We can get away with this shortcut because we only have two computers involved in the connection. In later articles, we&rsquo;ll extend our connection system to support more than two computers in a client/server or peer-to-peer topology, and at this point we&rsquo;ll upgrade our connection negotiation to something more robust.But for now, why make things more complicated than they need to be?Detecting DisconnectionHow do we detect disconnection?Well if a connection is defined as receiving packets, we can define disconnection as not receiving packets.To detect when we are not receiving packets, we keep track of the number of seconds since we last received a packet from the other side of the connection. We do this on both sides.Each time we receive a packet from the other side, we reset our accumulator to 0.0, each update we increase the accumulator by the amount of time that has passed.If this accumulator exceeds some value like 10 seconds, the connection &ldquo;times out&rdquo; and we disconnect.This also gracefully handles the case of a second client trying to connect to a server that has already made a connection with another client. Since the server is already connected it ignores packets coming from any address other than the connected client, so the second client receives no packets in response to the packets it sends, so the second client times out and disconnects.ConclusionAnd that&rsquo;s all it takes to setup a virtual connection: some way to establish connection, filtering for packets not involved in the connection, and timeouts to detect disconnection.Our connection is as real as any TCP connection, and the steady stream of UDP packets it provides is a suitable starting point for a multiplayer action game.Now that you have your virtual connection over UDP, you can easily setup a client/server relationship for a two player multiplayer game without TCP. 译文译文出处 译者：张华栋(wcby) 审校：崔国军（飞扬971） 序言 大家好，我是Glenn Fiedler，欢迎阅读《针对游戏程序员的网络知识》系列教程的第三篇文章。 在之前的文章中，我向你展示了如何使用UDP协议来发送和接收数据包。 由于UDP协议是无连接的传输层协议，一个UDP套接字可以用来与任意数目的不同电脑进行数据包交换。但是在多人在线网络游戏中，我们通常只需要在一小部分互相连接的计算机之间交换数据包。 作为实现通用连接系统的第一步，我们将从最简单的可能情况开始：创建两台电脑之间构建于UDP协议之上的虚拟连接。 但是首先，我们将对互联网到底是如何工作的进行一点深度挖掘！ 互联网不是一连串的管子 在2006年，参议院特德·史蒂文斯(Ted Stevens) 用他关于互联网中立（netneutrality）法案的著名演讲创造了互联网的历史： ”互联网不是那种你随便丢点什么东西进去就能运行的东西。它不是一个大卡车。它是一连串的管子“ 当我第一次开始使用互联网的时候，我也像Ted一样无知。那是1995年，我坐在悉尼大学的计算机实验室里，在用一种叫做Netscape的网络浏览器（最早最热门的网页浏览工具）“在网上冲浪（surfing the web）“，那个时候我对发生了什么根本一无所知。 你看那个时候，我觉得每次连到一个网站上就一定有某个真实存在的连接在帮我们传递信息，就像电话线一样。那时候我在想，当我每次连到一个新的网站上需要花费多少钱? 30美分吗?一美元吗? 会有大学里的某个人过来拍拍我的肩膀让我付长途通信的费用么？当然，现在回头看那时候一切的想法都非常的愚蠢。 并没有在某个地方存在一个物理交换机用物理电话线将你和你希望通话的某个电脑直接连起来。更不用说像参议院史蒂文斯想让你相信的那样存在一串气压输送管。 没有直接的连接 相反你的数据是基于IP协议(InternetProtocol)通过在电脑到电脑之间发送数据包来传递信息的。 一个数据包可能在到达它的目的地之前要经过几个电脑。你没有办法提前知道数据包会经过具体哪些电脑，因为它会依赖当前网络的情况对数据包进行路由来动态的改变路径。甚至有可能给同一个地址发送A和B两个数据包，这两个数据包都采用不同的路由。这就是为什么UDP协议不能保证数据包的到达顺序。（其实这么说稍微容易有点引起误解，TCP协议是能保证数据包的到达顺序的，但是他也是基于IP协议进行数据包的发送，并且往同一个地址发送的两个数据包也有可能采用完全不同的路由，这主要是因为TCP在自己这一层做了一些控制而UDP没有，所以导致TCP协议可以保证数据包的有序性，而UDP协议不能，当然这种保证需要付出性能方面的代价）。在类unix的系统中可以通过调用“traceroute”函数并传递一个目的地主机名或IP地址来检查数据包的路由。 在Windows系统中，可以用“tracert”代替“traceroute”，其他不变，就能检查数据包的路由了。 像下面这样用一些网址来尝试下这种方法： traceroute slashdot.org traceroute amazon.com traceroute google.com traceroute bbc.co.uk traceroute news.com.au 运行下看下输出结果，你应该很快就能说服你自己确实连接到了网站上，但是并没有一个直接的连接。 数据包是如何传递到目的地的？在第一篇文章中，我对数据包传递到目的地这个事情做了一个简单的类比，把这个过程描述的有点像在一个拥挤的房间内一个人接着一个人的把便条传递下去。 虽然这个类比的基本思想还是表达出来了，但是它有点过于简单了。互联网并不是电脑组成的一个平面的网络，实际上它是网络的网络。当然，我们不只是要在一个小房间里面传递信件，我们要做的事能够把信息传递到全世界。 这就应该很清楚了，数据包传递到目的地的最好的类比是邮政服务! 当你想给某人写信的时候，你会把你的信件放到邮箱里并且你相信它将正确的传递到目的地。这封信件具体是怎么到达目的地的和你并不是十分相关，尽管它是否正确到达会对你有影响。当然会有某个人在物理上帮你把信件传递到目的地，所以这是怎么做的呢? 首先，邮递员肯定不需要自己去把你的信件送到目的地！看起来邮政服务也不是一串管子。相反，邮递员是把你的信件带到当地的邮政部门进行处理。 如果这封信件是发送给本地的，那么邮政部门就会把这封信件发送回来，另外一个邮递员会直接投递这封信件。但是，如果这封信件不是发送给本地的，那么这个处理过程就有意思了！当地的邮政部门不能直接投递这封信件，所以这封信件会被向上传递到层次结构的上一层，这个上一层也许是地区级的邮政部门它会负责服务附近的几个城市，如果要投递的地址非常远的话，这个上一层也许是位于机场的一个邮件中心。理想情况下，信件的实际运输将通过一个大卡车来完成。 让我们通过一个例子来把上面说的过程具体的走一遍，假设有一封信件要从洛杉矶发送到澳大利亚的悉尼。当地的邮政部门收到信件以后考虑到这封信件是一封跨国投递的信件，所以会直接把它发送到位于洛杉矶机场的邮件中心。在那里，这封信件会再次根据它的地址进行处理，并被安排通过下一个到悉尼的航班投递到悉尼去。 当飞机降落到悉尼机场以后，一个完全不同的邮政系统会负责接管这封信件。现在整个过程开始逆向操作。这封信件会沿着层次结构向下传递，从大的管理部门到具体的投递区域。这封信件会从悉尼机场的邮件中心被送往一个地区级的中心，然后地区级的中心会把这封信件投递到当地的邮政部门，最终这封信件会是由一个操着有趣的本地口音的邮政人员用手投递到真正的目的地的。哎呀! ! 就像邮局是通过信件的地址来决定这些信件是该如何投递的一样，网络也是根据这些数据包的IP地址来决定它们是该如何传递的。投递机制的底层细节以及数据包从网络到网络的实际路由其实都是相当复杂的，但是基本的想法都是一样的，就是每个路由器都只是另外一台计算机，它会携带一张路由表用来描述如果数据包的IP地址匹配了这张表上的某个地址集，那么这个数据包该如何传递，这张表还会记载着默认的网关地址，如果数据包的IP地址和这张路由表上的一个地址都匹配不上，那么这个数据包该传递到默认的网关地址那里。其实是路由表以及它们代表的物理连接定义了网络的网络，也就是互联网（互联网也被称为万维网）。 因特网于1969年诞生于美国。最初名为“阿帕网”（ARPAnet）是一个军用研究系统，后来又成为连接大学及高等院校计算机的学术系统，则已发展成为一个覆盖五大洲150多个国家的开放型全球计算机网络系统，拥有许多服务商。普通电脑用户只需要一台个人计算机用电话线通过调制解调器和因特网服务商连接，便可进入因特网。但因特网并不是全球唯一的互联网络。例如在欧洲，跨国的互联网络就有“欧盟网”（Euronet），“欧洲学术与研究网”（EARN），“欧洲信息网”（EIN），在美国还有“国际学术网”（BITNET），世界范围的还有“飞多网”（全球性的BBS系统）等。但这些网络其实根本就不需要知道，感谢IP协议的帮助，只要知道他们是可以互联互通的就可以。 这些路由表的配置工作是由网络管理员完成的，而不是由像我们这样的程序员来做。但是如果你想要了解这方面的更多内容， 那么来自ars technica的这篇文章将提供网络是如何在端与端之间互联来交换数据包以及传输关系方面一些非常有趣的见解。你还可以通过linux常见问题中路由表（routing tables）方面的文章以及维基百科上面的边界网关协议（border gateway protocol ）的解释来获得更多的细节。边界网关协议是用来自动发现如何在网络之间路由数据包的协议，有了它才真正的让互联网成为一个分布式系统，能够在不稳定的连接里面进行动态的路由。 边界网关协议（BGP）是运行于 TCP 上的一种自治系统的路由协议。 BGP 是唯一一个用来处理像因特网大小的网络的协议，也是唯一能够妥善处理好不相关路由域间的多路连接的协议。 BGP 构建在 EGP 的经验之上。 BGP 系统的主要功能是和其他的 BGP 系统交换网络可达信息。网络可达信息包括列出的自治系统（AS）的信息。这些信息有效地构造了 AS 互联的拓朴图并由此清除了路由环路，同时在 AS 级别上可实施策略决策。 虚拟的连接现在让我们回到连接本身。 如果你已经使用过TCP套接字，那么你会知道它们看起来真的像是一个连接，但是由于TCP协议是在IP协议之上实现的，而IP协议是通过在计算机之间进行跳转来传递数据包的，所以TCP的连接仍然是一个虚拟连接。 如果TCP协议可以基于IP协议建立虚拟连接，那么我们在UDP协议上所做的一切都可以应用于TCP协议上。 让我们给虚拟连接下个定义：两个计算机之间以某个固定频率比如说每秒10个数据包来交换UDP的数据包。只要数据包仍然在传输，我们就认为这两台计算机之间存在一个虚拟连接。 我们的连接有两侧： 一个计算机坐在那儿侦听是否有另一台计算机连接到它。我们称负责监听的这台计算机为服务器（server）。 另一台计算机会通过一个指定的IP地址和端口连接到一个服务器。我们称主动连接的这台电脑为客户端（client）。 在我们的场景里，我们只允许一个客户端在任意的时候连接到服务器。我们将在下一篇文章里面拓展我们的连接系统以支持多个客户端的同时连接。此外，我们假定服务器的IP地址是一个固定的IP地址，客户端可以随时直接连接上来。我们将在后面的文章里面介绍匹配（matchmaking）和NAT打穿（NATpunch-through）。 协议ID由于UDP协议是无连接的传输层协议，所以我们的UDP套接字可以接受来自任何电脑的数据包。 我们想要缩小接收数据包的范围，以便我们的服务器只接收那些从我们的客户端发送出来的数据包，并且我们的客户端只接收那些从我们的服务端发送出来的数据包。我们不能只通过地址来过滤我们的数据包，因为服务器没有办法提前知道客户端的地址。所以，我们会在每一个UDP数据包前面加上一个包含32位协议id的头,如下所示: [uint protocol id] (packet data…) 协议ID只是一些独特的代表我们的游戏协议的数字。我们的UDP套接字收到的任意数据包首先都要检查数据包的首四位。如果它们和我们的协议ID不匹配的话，这个数据包就会被忽略。如果它们和我们的协议ID匹配的话，我们会剔除数据包的第一个四个字节并把剩下的部分发给我们的系统进行处理。 你只要选择一些非常独特的数字就可以了，这些数字可以是你的游戏名字和协议版本号的散列值。不过说真的，你可以使用任何东西。这种做法的重点是把我们的连接视为基于协议进行通信的连接，如果协议ID不同，那么这样的数据包将被丢弃掉。 检测连接现在我们需要一个方法来检测连接。 当然我们可以实现一些复杂的握手协议，牵扯到多个UDP数据包来回传递。比如说客户端发送一个”请求连接（request connection）“的数据包给服务器，当服务器收到这个数据包的时候会回应一个”连接接受（connection accepted）“的数据包给客户端，或者如果这个服务器已经有超过一个连接的客户端以后，会回复一个“我很忙（i’m busy）”的数据包给客户端。 或者。。我们可以设置我们的服务器，让它以它收到的第一个数据包的协议ID作为正确的协议ID，并在收到第一个数据包的时候就认为连接已经建立起来了。 客户端只是开始给服务器发送数据包，当服务器收到客户端发过来的第一个数据包的时候，它会记录下客户端的IP地址和端口号，然后开始给客户端回包。 客户端已经知道了服务器的地址和端口，因为这些信息是在连接的时候指定的。所以当客户端收到数据包的时候，它会过滤掉任何不是来自于服务器地址的数据包。同样的，一旦服务器收到客户端的第一个数据包，它就会从“recvfrom”函数里面得到客户端的地址和端口号，所以它也可以忽略任何不是发自客户端地址的数据包。 我们可以通过一个捷径来避开这个问题，因为我们的系统只有两台计算机会建立连接。在后面的文章里，我们将拓展我们的连接系统来支持超过两台计算机参与客户端/服务器或者端对端（peer-to-peer，p2p）网络模型，并且在那个时候我们会升级我们的连接协议方式来让它变得更加健壮。 但是现在，为什么我们要让事情变得超出需求的复杂度呢？（作者的意思是因为我们现在不需要解决这个问题，因为我们的场景是面对只有两台计算机的情况，所以我们可以先放过这个问题。） 检测断线的情况 我们该如何检测断线（disconnection）的情况？ 那么，如果一个连接被定义为接收数据包，我们可以定义断线为收不到数据包。 为了检测什么时候开始我们收不到数据包，我们要记录上一次我们从连接的另外一侧收到数据包到现在过去了多少秒，我们在连接的两侧都做了这个事情。 每次我们从连接的另外一端收到数据包的时候，我们都会重置我们的计数器为0.0，每一次更新的时候我们都会把这次更新到上一次更新逝去的时间量加到计数器上。 如果计数器的值超过某一个值，比如说10秒，那么我们就认定这个连接“超时”了并且我们会断开连接。 这也可以很优雅的处理当服务器已经与一个客户端建立连接以后，有第二个客户端试图与服务器建立连接的情况。因为服务器已经建立了连接，它会忽略掉不是来自连接的客户端地址发出来的数据包，所以第二个客户端在发出了数据包以后得不到任何回应，这样它就会判断连接超时并断开连接。 总结而这一切都需要设置一个虚拟连接：用某种方法建立一个连接，过滤掉那些不是来自这个连接的数据包，并且如果发现连接超时就断开连接。 我们的连接就跟任何TCP连接一样真实，并且UDP数据包构成的稳定数据流为多人在线动作网络游戏提供了一个很好的起点。 我们还获得了一些互联网是如何路由数据包的见解。举个例子来说，我们现在知道UDP数据包有时候会在到达的时候是乱序的原因是因为它们在IP层传输的时候采用不同的路由！看下互联网的地图，你会不会对你的数据包能够到达正确的目的点感到非常的神奇？如果你想对这个问题进行更加深入的了解，维基百科上的这篇文章(Internet backbone)是一个很好的起点。 现在，既然你已经有了一个基于UDP协议的虚拟连接，你可以轻松的在两个玩家的多人在线游戏里面设置一个客户端/服务器关系而不需要使用TCP协议。 你可以在这篇文章的示例源代码（examplesource code ）找到一个具体实现。 这是一个简单的客户端/服务器程序，每秒交换30个数据包。你可以在任意你喜欢的机器上运行这个服务器，只要给它提供一个公共的IP地址就可以了，需要公共IP地址的原因是我们目前还不支持NAT打穿（NAT punch-through ）。 NAT穿越（NATtraversal）涉及TCP/IP网络中的一个常见问题，即在处于使用了NAT设备的私有TCP/IP网络中的主机之间建立连接的问题。 像这样来运行客户端： ./Client 205.10.40.50 它会尝试连接到你在命令行输入的地址。如果你不输入地址的话，默认情况下它会连接到127.0.0.1。 当一个客户端已经与服务器建立连接的时候，你可以尝试用另外一个客户端来连接这个服务器，你会注意到这次连接的尝试失败了。这么设计是故意的。因为到目前为止，一次只允许一个客户端连接上服务器。 你也可以在客户端和服务器连接的状态下尝试停止客户端或者服务器，你会注意到10秒以后连接的另外一侧会判断连接超时并断开连接。当客户端超时的时候它会退到shell窗口，但是服务器会退到监听状态为下一次的连接做好准备。 预告下接下来的一篇文章的题目:《基于UDP的可靠、有序和拥塞避免的传输》，欢迎继续阅读。 如果你喜欢这篇文章的话，请考虑对我做一个小小的捐赠。捐款会鼓励我写更多的文章!（原文作者在原文的地址上提供了一个捐赠网址，有兴趣的读者可以在文章开始的地方找到原文地址进行捐赠） 【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。 源码下载因 Gaffer On Games 的源码原下载地址失效, 所以特地补上. 请点击]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发二之数据的发送与接收]]></title>
    <url>%2F2016%2F11%2F15%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E4%BA%8C%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%91%E9%80%81%E4%B8%8E%E6%8E%A5%E6%94%B6%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文原文出处 Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.In the previous article we discussed options for sending data between computers and decided to use UDP instead of TCP for time critical data.In this article I am going to show you how to send and receive UDP packets.BSD socketsFor most modern platforms you have some sort of basic socket layer available based on BSD sockets.BSD sockets are manipulated using simple functions like &ldquo;socket&rdquo;, &ldquo;bind&rdquo;, &ldquo;sendto&rdquo; and &ldquo;recvfrom&rdquo;. You can of course work directly with these functions if you wish, but it becomes difficult to keep your code platform independent because each platform is slightly different.So although I will first show you BSD socket example code to demonstrate basic socket usage, we won&rsquo;t be using BSD sockets directly for long. Once we&rsquo;ve covered all basic socket functionality we&rsquo;ll abstract everything away into a set of classes, making it easy to you to write platform independent socket code.Platform specificsFirst let&rsquo;s setup a define so we can detect what our current platform is and handle the slight differences in sockets from one platform to another: // platform detection #define PLATFORM_WINDOWS 1 #define PLATFORM_MAC 2 #define PLATFORM_UNIX 3 #if defined(_WIN32) #define PLATFORM PLATFORM_WINDOWS #elif defined(APPLE) #define PLATFORM PLATFORM_MAC #else #define PLATFORM PLATFORM_UNIX #endifNow let&rsquo;s include the appropriate headers for sockets. Since the header files are platform specific, we&rsquo;ll use the platform #define to include different sets of files depending on the platform: #if PLATFORM == PLATFORM_WINDOWS #include &lt;winsock2.h&gt; #elif PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX #include &lt;sys/socket.h&gt; #include &lt;netinet/in.h&gt; #include &lt;fcntl.h&gt; #endifSockets are built in to the standard system libraries on unix-based platforms so we don&rsquo;t have to link to any additonal libraries. However, on Windows we need to link to the winsock library to get socket functionality.Here is a simple trick to do this without having to change your project or makefile: #if PLATFORM == PLATFORM_WINDOWS #pragma comment( lib, &quot;wsock32.lib&quot; ) #endifI like this trick because I&rsquo;m super lazy. You can always link from your project or makefile if you wish.Initializing the socket layerMost unix-like platforms (including macosx) don&rsquo;t require any specific steps to initialize the sockets layer, however Windows requires that you jump through some hoops to get your socket code working.You must call &ldquo;WSAStartup&rdquo; to initialize the sockets layer before you call any socket functions, and &ldquo;WSACleanup&rdquo; to shutdown when you are done.Let&rsquo;s add two new functions: bool InitializeSockets() { #if PLATFORM == PLATFORM_WINDOWS WSADATA WsaData; return WSAStartup( MAKEWORD(2,2), &amp;WsaData ) == NO_ERROR; #else return true; #endif } void ShutdownSockets() { #if PLATFORM == PLATFORM_WINDOWS WSACleanup(); #endif }Now we have a platform independent way to initialize the socket layer.Creating a socketIt&rsquo;s time to create a UDP socket, here&rsquo;s how to do it: int handle = socket( AF_INET, SOCK_DGRAM, IPPROTO_UDP ); if ( handle &lt;= 0 ) { printf( &quot;failed to create socket\n&quot; ); return false; }Next we bind the UDP socket to a port number (eg. 30000). Each socket must be bound to a unique port, because when a packet arrives the port number determines which socket to deliver to. Don&rsquo;t use ports lower than 1024 because they are reserved for the system. Also try to avoid using ports above 50000 because they used when dynamically assigning ports.Special case: if you don&rsquo;t care what port your socket gets bound to just pass in &ldquo;0&rdquo; as your port, and the system will select a free port for you. sockaddr_in address; address.sin_family = AF_INET; address.sin_addr.s_addr = INADDR_ANY; address.sin_port = htons( (unsigned short) port ); if ( bind( handle, (const sockaddr) &amp;address, sizeof(sockaddr_in) ) &lt; 0 ) { printf( &quot;failed to bind socket\n&quot; ); return false; }Now the socket is ready to send and receive packets.But what is this mysterious call to &ldquo;htons&rdquo; in the code above? This is just a helper function that converts a 16 bit integer value from host byte order (little or big-endian) to network byte order (big-endian). This is required whenever you directly set integer members in socket structures.You&rsquo;ll see &ldquo;htons&rdquo; (host to network short) and its 32 bit integer sized cousin &ldquo;htonl&rdquo; (host to network long) used several times throughout this article, so keep an eye out, and you&rsquo;ll know what is going on.Setting the socket as non-blockingBy default sockets are set in what is called &ldquo;blocking mode&rdquo;.This means that if you try to read a packet using &ldquo;recvfrom&rdquo;, the function will not return until a packet is available to read. This is not at all suitable for our purposes. Video games are realtime programs that simulate at 30 or 60 frames per second, they can&rsquo;t just sit there waiting for a packet to arrive!The solution is to flip your sockets into &ldquo;non-blocking mode&rdquo; after you create them. Once this is done, the &ldquo;recvfrom&rdquo; function returns immediately when no packets are available to read, with a return value indicating that you should try to read packets again later.Here&rsquo;s how put a socket in non-blocking mode: #if PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX int nonBlocking = 1; if ( fcntl( handle, F_SETFL, O_NONBLOCK, nonBlocking ) == -1 ) { printf( &quot;failed to set non-blocking\n&quot; ); return false; } #elif PLATFORM == PLATFORM_WINDOWS DWORD nonBlocking = 1; if ( ioctlsocket( handle, FIONBIO, &amp;nonBlocking ) != 0 ) { printf( &quot;failed to set non-blocking\n&quot; ); return false; } #endifWindows does not provide the &ldquo;fcntl&rdquo; function, so we use the &ldquo;ioctlsocket&rdquo; function instead.Sending packetsUDP is a connectionless protocol, so each time you send a packet you must specify the destination address. This means you can use one UDP socket to send packets to any number of different IP addresses, there&rsquo;s no single computer at the other end of your UDP socket that you are connected to.Here&rsquo;s how to send a packet to a specific address: int sent_bytes = sendto( handle, (const char)packet_data, packet_size, 0, (sockaddr)&amp;address, sizeof(sockaddr_in) ); if ( sent_bytes != packet_size ) { printf( &quot;failed to send packet\n&quot; ); return false; }Important! The return value from &ldquo;sendto&rdquo; only indicates if the packet was successfully sent from the local computer. It does not tell you whether or not the packet was received by the destination computer. UDP has no way of knowing whether or not the the packet arrived at its destination!In the code above we pass a &ldquo;sockaddr_in&rdquo; structure as the destination address. How do we setup one of these structures?Let&rsquo;s say we want to send to the address 207.45.186.98:30000Starting with our address in this form: unsigned int a = 207; unsigned int b = 45; unsigned int c = 186; unsigned int d = 98; unsigned short port = 30000;We have a bit of work to do to get it in the form required by &ldquo;sendto&rdquo;: unsigned int address = ( a &lt;&lt; 24 ) | ( b &lt;&lt; 16 ) | ( c &lt;&lt; 8 ) | d; sockaddr_in addr; addr.sin_family = AF_INET; addr.sin_addr.s_addr = htonl( address ); addr.sin_port = htons( port );As you can see, we first combine the a,b,c,d values in range [0,255] into a single unsigned integer, with each byte of the integer now corresponding to the input values. We then initialize a &ldquo;sockaddr_in&rdquo; structure with the integer address and port, making sure to convert our integer address and port values from host byte order to network byte order using &ldquo;htonl&rdquo; and &ldquo;htons&rdquo;.Special case: if you want to send a packet to yourself, there&rsquo;s no need to query the IP address of your own machine, just pass in the loopback address 127.0.0.1 and the packet will be sent to your local machine.Receiving packetsOnce you have a UDP socket bound to a port, any UDP packets sent to your sockets IP address and port are placed in a queue. To receive packets just loop and call &ldquo;recvfrom&rdquo; until it fails with EWOULDBLOCK indicating there are no more packets to receive.Since UDP is connectionless, packets may arrive from any number of different computers. Each time you receive a packet &ldquo;recvfrom&rdquo; gives you the IP address and port of the sender, so you know where the packet came from.Here&rsquo;s how to loop and receive all incoming packets: while ( true ) { unsigned char packet_data[256]; unsigned int max_packet_size = sizeof( packet_data ); #if PLATFORM == PLATFORM_WINDOWS typedef int socklen_t; #endif sockaddr_in from; socklen_t fromLength = sizeof( from ); int bytes = recvfrom( socket, (char)packet_data, max_packet_size, 0, (sockaddr)&amp;from, &amp;fromLength ); if ( bytes &lt;= 0 ) break; unsigned int from_address = ntohl( from.sin_addr.s_addr ); unsigned int from_port = ntohs( from.sin_port ); // process received packet }Any packets in the queue larger than your receive buffer will be silently discarded. So if you have a 256 byte buffer to receive packets like the code above, and somebody sends you a 300 byte packet, the 300 byte packet will be dropped. You will not receive just the first 256 bytes of the 300 byte packet.Since you are writing your own game network protocol, this is no problem at all in practice, just make sure your receive buffer is big enough to receive the largest packet your code could possibly send.Destroying a socketOn most unix-like platforms, sockets are file handles so you use the standard file &ldquo;close&rdquo; function to clean up sockets once you are finished with them. However, Windows likes to be a little bit different, so we have to use &ldquo;closesocket&rdquo; instead:#if PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIXclose( socket );#elif PLATFORM == PLATFORM_WINDOWSclosesocket( socket );#endifHooray windows.Socket classSo we&rsquo;ve covered all the basic operations: creating a socket, binding it to a port, setting it to non-blocking, sending and receiving packets, and destroying the socket.But you&rsquo;ll notice most of these operations are slightly platform dependent, and it&rsquo;s pretty annoying to have to remember to #ifdef and do platform specifics each time you want to perform socket operations.We&rsquo;re going to solve this by wrapping all our socket functionality up into a &ldquo;Socket&rdquo; class. While we&rsquo;re at it, we&rsquo;ll add an &ldquo;Address&rdquo; class to make it easier to specify internet addresses. This avoids having to manually encode or decode a &ldquo;sockaddr_in&rdquo; structure each time we send or receive packets.So let&rsquo;s add a socket class: class Socket { public: Socket(); ~Socket(); bool Open( unsigned short port ); void Close(); bool IsOpen() const; bool Send( const Address &amp; destination, const void data, int size ); int Receive( Address &amp; sender, void * data, int size ); private: int handle; };and an address class: class Address { public: Address(); Address( unsigned char a, unsigned char b, unsigned char c, unsigned char d, unsigned short port ); Address( unsigned int address, unsigned short port ); unsigned int GetAddress() const; unsigned char GetA() const; unsigned char GetB() const; unsigned char GetC() const; unsigned char GetD() const; unsigned short GetPort() const; private: unsigned int address; unsigned short port; };Here&rsquo;s how to to send and receive packets with these classes: // create socket const int port = 30000; Socket socket; if ( !socket.Open( port ) ) { printf( &quot;failed to create socket!\n&quot; ); return false; } // send a packet const char data[] = &quot;hello world!&quot;; socket.Send( Address(127,0,0,1,port), data, sizeof( data ) ); // receive packets while ( true ) { Address sender; unsigned char buffer[256]; int bytes_read = socket.Receive( sender, buffer, sizeof( buffer ) ); if ( !bytes_read ) break; // process packet }As you can see it&rsquo;s much simpler than using BSD sockets directly.As an added bonus the code is the same on all platforms because everything platform specific is handled inside the socket and address classes.ConclusionYou now have a platform independent way to send and receive packets. Enjoy :) 译文译文出处 因译文很多地方均有疏漏, 本文已经对部分疏漏做了修正.翻译：杨嘉鑫（矫情到死的仓鼠君，）审校：赵菁菁（轩语轩缘）$hhd$1&gt;序言 大家好，我是Glenn Fiedler，欢迎阅读《针对游戏程序员的网络知识》系列教程的第二篇文章。 在前面的文章中我们讨论了在不同计算机之间发送数据的方法，并决定使用用户数据报协议（UDP）而非传输控制协议（TCP）。我们之所以使用用户数据报协议（UDP），是因为它能够使数据在不等待重发包而造成数据聚集的情况下按时被送达。现在我将要告诉各位如何使用用户数据报协议（UDP）发送和接收数据包。$hhd$1&gt;伯克利套接字 （BSD socket）对于大多数现代的平台来说你都可以找到建立在伯克利套接字上的sockets。伯克利套接字主要通过“socket”,“bind”, “sendto” and “recvfrom”几个简单函数进行控制。如果你愿意的话你当然可以直接对这几个函数进行调用，但是由于每个平台之间有细微差别，保持代码平台的独立性将会变得有些困难。因此，尽管我将先给各位介绍伯克利套接字的示例代码用以说明它的基本使用功能，我们也不会大量的直接使用伯克利套接字。所以当我们掌握了所有基础socket 功能后，我们将会把所有内容汇总到一个系列的课中，以便你可以轻松地编写代码。$hhd$1&gt;平台的特殊性首先 我们先建立一个“define”程序用来测试我们现有的平台是什么，这样我们就可以发现不同平台间间各个socket里的细微差别。?123456789101112131415161718192021// platform detection #define PLATFORM_WINDOWS 1 #define PLATFORM_MAC 2 #define PLATFORM_UNIX 3 #if defined(_WIN32) #define PLATFORM PLATFORM_WINDOWS #elif defined(APPLE) #define PLATFORM PLATFORM_MAC #else #define PLATFORM PLATFORM_UNIX #endif接下来我们为sockets写入适当的标头，由于头文件具有平台的特殊性所以我们将使用“#define”来根据不同的平台引用不同的文件。?123456789101112131415#if PLATFORM == PLATFORM_WINDOWS #include &lt;winsock2.h&gt; #elif PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX #include &lt;sys socket.h=””&gt; #include &lt;netinet in.h=””&gt; #include &lt;fcntl.h&gt; #endif&lt;/fcntl.h&gt;&lt;/netinet&gt;&lt;/sys&gt;&lt;/winsock2.h&gt;如果sockets是建立在unix平台上，我们就不需要任何其他多余的连接，若它是建立在windows系统里，为了确保socket正常使用我们就需要连接到“winsock”库内。以下是一个简单的技巧，它可以在不改变已有项目或生成文件的前提下完成上述工作。?12345#if PLATFORM == PLATFORM_WINDOWS #pragma comment( lib, “wsock32.lib” ) #endif我之所以非常喜欢这个小技巧是因为我太懒了~当然啦，如果你愿意每次都进行项目链接或生成文件也未尝不可。$hhd$1&gt;socket层的初始化大多数“unix-like”的平台 (包括macosx) 是不需要任何特殊的步骤去初始化socket层的。但是Windows需要进行一些特殊设置来确保你的sockets代码正常工作。在你使用其他任何sockets功能前你必须先调用 “WSAStartup” 来初始化它们，在你的程序段结束时你也必须使用 “WSACleanup”来结束。下面让我们来添加以上两个新功能：?123456789101112131415161718192021222324bool InitializeSockets(){ #if PLATFORM == PLATFORM_WINDOWS WSADATA WsaData; return WSAStartup( MAKEWORD(2,2), &amp;WsaData ) == NO_ERROR; #else return true; #endif} void ShutdownSockets(){ #if PLATFORM == PLATFORM_WINDOWS WSACleanup(); #endif}这样我们就得到了一个初始化socket层的方法。对于那些不需要socket初始化的平台来说这些功能可以忽略不计。$hhd$1&gt;建立一个socket现在是时候来建立一个基于用户数据报协议（UDP）的socket了，下面是实施的方法：?12345678int handle = socket( AF_INET, SOCK_DGRAM,IPPROTO_UDP ); if ( handle &lt;= 0 ){ printf( “failed to create socket\n” ); return false;}接下来我们把用户数据报协议（UDP）的socket对应到一个端口上（比如30000这个端口）。每一个socket都必须对应到一个独一无二的端口上。这么做的原因是端口号决定了每个数据包发送到的位置。不要使用1024以下的端口，因为这是为系统调用所预留的。有一种特殊情况，如果你不在乎socket指定到哪个端口上，你就可以输入“0”，这样系统将会自动为你选择一个闲置的端口。?123456789101112sockaddr_in address;address.sin_family = AF_INET;address.sin_addr.s_addr = INADDR_ANY;address.sin_port = htons( (unsigned short) port ); if ( bind( handle, (const sockaddr) &amp;address, sizeof(sockaddr_in) ) &lt; 0 ){ printf( “failed to bind socket\n” ); return false;}这样我们的socket已经准备就绪并可以发送和接收包了。那么上面提到的“htons”是起什么作用呢？这是一个辅助功能，它将一个16位整数的值由主机字节序列（小端或大端）转换成网络字节序列（大端）。这就要求你在任何时候都直接在socket结构里设置整数数字。你会看到“htons”（主机到网络短字节）及其32位整数大小的表兄妹”htonl”（主机到网络长字节）这在这篇文章中被多次使用，你留意了以后你在下文中再次遇到就会明白。$hhd$1&gt;将socket设置为非阻塞形式 默认情况下，socket是被设置在 “阻塞模式”的状态下。这意味着，如果你想使用“recvfrom”功能读一个包，在一个数据包被读取前该函数值将不能被返回。这与我们的目标完全不符。视频游戏是拟态在30或60帧每秒实时的程序，他们不能只是坐在那里等待数据包的到达！解决方案是你将socket转换成以“非阻塞模式”后再创建他们。一旦做到这一点，当没有包可供阅读时，“recvfrom”函数就可以立即返回，返回值显示你应该稍后再尝试读取包。 下面是如何将socket设置为非阻塞模式的方法：?1234567891011121314151617181920#if PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX int nonBlocking = 1;if ( fcntl( handle, F_SETFL, O_NONBLOCK, nonBlocking ) == -1 ){ printf( “failed to set non-blocking\n” ); return false;} #elif PLATFORM == PLATFORM_WINDOWSDWORD nonBlocking = 1;if ( ioctlsocket( handle, FIONBIO, &amp;nonBlocking ) != 0 ){ printf( “failed to set non-blocking\n” ); return false;} #endif从上面的程序我们可以发现，Windows本身并不提供“框架”的功能，所以我们使用“ioctlsocket”功能来实现。$hhd$1&gt;发送数据包用户数据报协议（UDP）是一种无连接协议，所以每次你发送一个数据包前都要指定一个目的地址。你可以使用一个用户数据报协议（UDP）发送数据包到任意数量的不同的IP地址，而在你用户数据报协议（UDP） socket的另一端并没有连接某一台计算机。下面是如何发送一个数据包到一个特定的地址方法：?1234567891011121314int sent_bytes = sendto( handle, (const char)packet_data, packet_size, 0, (sockaddr)&amp;address, sizeof(sockaddr_in) ); if ( sent_bytes != packet_size ){ printf( “failed to send packet\n” ); return false;}很重要的一点！“sendto”的返回值只是表明数据包是否被成功地从本地计算机发送，它并不能表明目标计算机是否成功接收到你的数据包！用户数据报协议（UDP）没有办法知道数据包是否能到达目的地。 上面的代码中，我们通过“sockaddr_in”结构为目的地址。那么我们如何设置这些结构呢？现在让我们以发送到207.45.186.98:30000 这个地址为例我们从以下这个程序开始：?12345678910111213141516unsigned int a = 207;unsigned int b = 45;unsigned int c = 186;unsigned int d = 98;unsigned short port = 30000; 我们还要在形式上进行设置从而符合“sendto”的要求：unsigned int address = ( a &lt;&lt; 24 ) | ( b &lt;&lt; 16 ) | ( c &lt;&lt; 8 ) | d; sockaddr_in addr;addr.sin_family = AF_INET;addr.sin_addr.s_addr = htonl( address );addr.sin_port = htons( port );正如您所看到的，我们首先将A、B、C、D值在范围[ 0, 255 ]内的值转化为一个单一的无符号整数，从而使这个整数的每个字节对应输入值。然后以整数地址和端口来初始化一个“sockaddr_in”结构，这样就确保使用“htonl” 和“htons”来将整型地址和端口值从主机字节序列转换为为网络字节序列。一种特殊情况：如果你想给自己发送一个数据包，不需要查询自己机器的IP地址，在回送地址127.0.0.1中数据包就将被发送到你的本地机器。$hhd$1&gt;接收数据包 一旦你将一个用户数据报协议（UDP）套接字绑定到一个端口，任何发送到您scoket IP地址和端口的用户数据报协议（UDP）数据包都将放在一个队列里。接收数据包的话, 只需要循环调用 “recvfrom”函数直到他失败并返回”EWOULDBLOCK”，这就意味着队列里有没有留下其他的数据包了。由于用户数据报协议（UDP）是无连接性的，数据包可以到达许多不同的计算机。每当你收到一个数据包，“recvfrom”都会给你发送者的IP地址和端口以便你知道这是来自哪里的数据包。下面是如何进行循环接收传入的数据包的方法：?12345678910111213141516171819202122232425262728while ( true ){ unsigned char packet_data[256]; unsigned int max_packet_size = sizeof( packet_data ); #if PLATFORM == PLATFORM_WINDOWS typedef int socklen_t; #endif sockaddr_in from; socklen_t fromLength = sizeof( from ); int bytes = recvfrom( socket, (char)packet_data, max_packet_size, 0, (sockaddr)&amp;from, &amp;fromLength ); if ( bytes &lt;= 0 ) break; unsigned int from_address = ntohl( from.sin_addr.s_addr ); unsigned int from_port = ntohs( from.sin_port ); // process received packet }在队列中，数据包一旦大于你接收缓冲区的范围，他们都会被系统悄悄舍弃。因此，如果你有一个256字节的缓冲区用来接收数据包，有人给你一个发送300字节的数据包，300字节的数据包都将被删除。您将不会接收到300字节数据包的前256个字节。因为您正在编写自己的游戏网络协议，以上这些操作这是没有什么影响的。在实践中您就要确保您的接收缓冲区足够大，以接收最大的数据包。$hhd$1&gt;关闭一个socket在大多数Unix平台，一旦你完成了自己所需的程序后，在socket文件中只要使用标准的文件“close”函数来清理即可。然而，在Windows系统中以上情形会有点不同，我们要用“closesocket”函数来操作：?123456789#if PLATFORM == PLATFORM_MAC || PLATFORM == PLATFORM_UNIX close( socket ); #elif PLATFORM == PLATFORM_WINDOWS closesocket( socket ); #endif $hhd$1&gt;Socket class现在，我们已经完成了所有的基本操作：创建一个socket，将他绑定到一个端口并设置为非阻塞，发送和接收数据包，清除socket。但是你会发现以上这些操作中多多少少都是依赖于平台的，在每一次你想执行socket操作时，你不得不记住“# ifdef”指令和针对不同平台的各种细节，这些繁琐的操作是很令人抓狂的。为了解决这个问题，我们可以将所有的socket功能封装成一个“socket class‘’。当我们在使用它的时候，我们将添加一个“Address class‘’，这样使它更容易指定互联网地址。这避免了我们每次发送或接收数据包时进行手动编码或解码“sockaddr_in”结构。下面是“socket class‘’的程序：?123456789101112131415161718192021class Socket{public: Socket(); ~Socket(); bool Open( unsigned short port ); void Close(); bool IsOpen() const; bool Send( const Address &amp; destination, const void data, int size ); int Receive( Address &amp; sender, void * data, int size ); private: int handle; };下面是“address class”的程序：?12345678910111213141516171819202122232425class Address{public: Address(); Address( unsigned char a, unsigned char b, unsigned char c, unsigned char d, unsigned short port ); Address( unsigned int address, unsigned short port ); unsigned int GetAddress() const; unsigned char GetA() const; unsigned char GetB() const; unsigned char GetC() const; unsigned char GetD() const; unsigned short GetPort() const; private: unsigned int address; unsigned short port; };下面是这些class如何接收和发送数据包的程序：?12345678910111213141516171819202122232425262728293031// create socketconst int port = 30000;Socket socket;if ( !socket.Open( port ) ){ printf( “failed to create socket!\n” ); return false;} // send a packetconst char data[] = “hello world!”;socket.Send( Address(127,0,0,1,port), data, sizeof( data ) ); // receive packetswhile ( true ){ Address sender; unsigned char buffer[256]; int bytes_read = socket.Receive( sender, buffer, sizeof( buffer ) ); if ( !bytes_read ) break; // process packet }$hhd$1&gt;结论我们现在有了一种不限平台的方法来发送和接收用户数据报协议（UDP）的数据包。用户数据报协议（UDP）是无连接性的，因此我编写了一个简单的示例程序，它可以从文本文件中读取IP地址，并能够每秒向这些地址发送一个数据包。每当这个程序接收到一个数据包时，它就会告诉你它们来自哪个机器，以及接收到的数据包的大小。您可以很容易地设置它，然后你就拥有了一系列在本地机器上互相发送数据包的节点。这样你就可以利用以下程序通过不同的端口，进入不同的应用程序： &gt; Node30000 &gt; Node 30001 &gt; Node 30002 etc…然后每个节点都将尝试发送数据包到每个其他节点，它的工作原理就像一个小型的“peer-to-peer”设置。我是在MacOSX系统中开发的这个程序，但我想你应该能够轻松地在任何Unix系统或Windows上对他进行编译。如果你有任何应用在其他不同机器上的兼容性补丁，也非常欢迎您与我取得联系。【版权声明】 原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权。 源码下载因 Gaffer On Games 的源码原下载地址失效, 所以特地补上. 请点击]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏网络开发一之TCPvsUDP]]></title>
    <url>%2F2016%2F11%2F14%2F%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E5%BC%80%E5%8F%91%E4%B8%80%E4%B9%8BTCPvsUDP%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文原文出处 Introduction Hi, I&rsquo;m Glenn Fiedler and welcome to Networking for Game Programmers.In this article we start with the most basic aspect of network programming: sending and receiving data over the network. This is perhaps the simplest and most basic part of what network programmers do, but still it is quite intricate and non-obvious as to what the best course of action is.You have most likely heard of sockets, and are probably aware that there are two main types: TCP and UDP. When writing a network game, we first need to choose what type of socket to use. Do we use TCP sockets, UDP sockets or a mixture of both? Take care because if you get this wrong it will have terrible effects on your multiplayer game!The choice you make depends entirely on what sort of game you want to network. So from this point on and for the rest of this article series, I assume you want to network an action game. You know, games like Halo, Battlefield 1942, Quake, Unreal, CounterStrike and Team Fortress.In light of the fact that we want to network an action game, we&rsquo;ll take a very close look at the properties of each protocol, and dig a bit into how the internet actually works. Once we have all this information, the correct choice is clear.TCP/IPTCP stands for &ldquo;transmission control protocol&rdquo;. IP stands for &ldquo;internet protocol&rdquo;. Together they form the backbone for almost everything you do online, from web browsing to IRC to email, it&rsquo;s all built on top of TCP/IP.If you have ever used a TCP socket, then you know it&rsquo;s a reliable connection based protocol. This means you create a connection between two machines, then you exchange data much like you&rsquo;re writing to a file on one side, and reading from a file on the other.TCP connections are reliable and ordered. All data you send is guaranteed to arrive at the other side and in the order you wrote it. It&rsquo;s also a stream protocol, so TCP automatically splits your data into packets and sends them over the network for you.IPThe simplicity of TCP is in stark contrast to what actually goes on underneath TCP at the IP or &ldquo;internet protocol&rdquo; level.Here there is no concept of connection, packets are simply passed from one computer to the next. You can visualize this process being somewhat like a hand-written note passed from one person to the next across a crowded room, eventually, reaching the person it&rsquo;s addressed to, but only after passing through many hands.There is also no guarantee that this note will actually reach the person it is intended for. The sender just passes the note along and hopes for the best, never knowing whether or not the note was received, unless the other person decides to write back!Of course IP is in reality a little more complicated than this, since no one computer knows the exact sequence of computers to pass the packet along to so that it reaches its destination quickly. Sometimes IP passes along multiple copies of the same packet and these packets make their way to the destination via different paths, causing packets to arrive out of order and in duplicate.This is because the internet is designed to be self-organizing and self-repairing, able to route around connectivity problems rather than relying on direct connections between computers. It&rsquo;s actually quite cool if you think about what&rsquo;s really going on at the low level. You can read all about this in the classic book TCP/IP Illustrated.UDPInstead of treating communications between computers like writing to files, what if we want to send and receive packets directly?We can do this using UDP.UDP stands for &ldquo;user datagram protocol&rdquo; and it&rsquo;s another protocol built on top of IP, but unlike TCP, instead of adding lots of features and complexity, UDP is a very thin layer over IP.With UDP we can send a packet to a destination IP address (eg. 112.140.20.10) and port (say 52423), and it gets passed from computer to computer until it arrives at the destination or is lost along the way.On the receiver side, we just sit there listening on a specific port (eg. 52423) and when a packet arrives from any computer (remember there are no connections!), we get notified of the address and port of the computer that sent the packet, the size of the packet, and can read the packet data.Like IP, UDP is an unreliable protocol. In practice however, most packets that are sent will get through, but you&rsquo;ll usually have around 1-5% packet loss, and occasionally you&rsquo;ll get periods where no packets get through at all (remember there are lots of computers between you and your destination where things can go wrong&hellip;)There is also no guarantee of ordering of packets with UDP. You could send 5 packets in order 1,2,3,4,5 and they could arrive completely out of order like 3,1,2,5,4. In practice, packets tend to arrive in order most of the time, but you cannot rely on this!UDP also provides a 16 bit checksum, which in theory is meant to protect you from receiving invalid or truncated data, but you can&rsquo;t even trust this, since 16 bits is just not enough protection when you are sending UDP packets rapidly over a long period of time. Statistically, you can&rsquo;t even rely on this checksum and must add your own.So in short, when you use UDP you&rsquo;re pretty much on your own!TCP vs. UDPWe have a decision to make here, do we use TCP sockets or UDP sockets?Lets look at the properties of each:TCP:Connection basedGuaranteed reliable and orderedAutomatically breaks up your data into packets for youMakes sure it doesn’t send data too fast for the internet connection to handle (flow control)Easy to use, you just read and write data like its a fileUDP:No concept of connection, you have to code this yourselfNo guarantee of reliability or ordering of packets, they may arrive out of order, be duplicated, or not arrive at all!You have to manually break your data up into packets and send themYou have to make sure you don’t send data too fast for your internet connection to handleIf a packet is lost, you need to devise some way to detect this, and resend that data if necessaryYou can’t even rely on the UDP checksum so you must add your ownThe decision seems pretty clear then, TCP does everything we want and its super easy to use, while UDP is a huge pain in the ass and we have to code everything ourselves from scratch.So obviously we just use TCP right?Wrong!Using TCP is the worst possible mistake you can make when developing a multiplayer game! To understand why, you need to see what TCP is actually doing above IP to make everything look so simple.How TCP really worksTCP and UDP are both built on top of IP, but they are radically different. UDP behaves very much like the IP protocol underneath it, while TCP abstracts everything so it looks like you are reading and writing to a file, hiding all complexities of packets and unreliability from you.So how does it do this?Firstly, TCP is a stream protocol, so you just write bytes to a stream, and TCP makes sure that they get across to the other side. Since IP is built on packets, and TCP is built on top of IP, TCP must therefore break your stream of data up into packets. So, some internal TCP code queues up the data you send, then when enough data is pending the queue, it sends a packet to the other machine.This can be a problem for multiplayer games if you are sending very small packets. What can happen here is that TCP may decide it&rsquo;s not going to send data until you have buffered up enough data to make a reasonably sized packet to send over the network.This is a problem because you want your client player input to get to the server as quickly as possible, if it is delayed or &ldquo;clumped up&rdquo; like TCP can do with small packets, the client&rsquo;s user experience of the multiplayer game will be very poor. Game network updates will arrive late and infrequently, instead of on-time and frequently like we want.TCP has an option to fix this behavior called TCP_NODELAY. This option instructs TCP not to wait around until enough data is queued up, but to flush any data you write to it immediately. This is referred to as disabling Nagle&rsquo;s algorithm.Unfortunately, even if you set this option TCP still has serious problems for multiplayer games and it all stems from how TCP handles lost and out of order packets to present you with the &ldquo;illusion&rdquo; of a reliable, ordered stream of data.How TCP implements reliabilityFundamentally TCP breaks down a stream of data into packets, sends these packets over unreliable IP, then takes the packets received on the other side and reconstructs the stream.But what happens when a packet is lost?What happens when packets arrive out of order or are duplicated?Without going too much into the details of how TCP works because its super-complicated (please refer to TCP/IP Illustrated) in essence TCP sends out a packet, waits a while until it detects that packet was lost because it didn&rsquo;t receive an ack (or acknowledgement), then resends the lost packet to the other machine. Duplicate packets are discarded on the receiver side, and out of order packets are resequenced so everything is reliable and in order.The problem is that if we were to send our time critical game data over TCP, whenever a packet is dropped it has to stop and wait for that data to be resent. Yes, even if more recent data arrives, that new data gets put in a queue, and you cannot access it until that lost packet has been retransmitted. How long does it take to resend the packet?Well, it&rsquo;s going to take at least round trip latency for TCP to work out that data needs to be resent, but commonly it takes 2*RTT, and another one way trip from the sender to the receiver for the resent packet to get there. So if you have a 125ms ping, you&rsquo;ll be waiting roughly 1/5th of a second for the packet data to be resent at best, and in worst case conditions you could be waiting up to half a second or more (consider what happens if the attempt to resend the packet fails to get through?). What happens if TCP decides the packet loss indicates network congestion and it backs off? Yes it actually does this. Fun times!Never use TCP for time critical dataThe problem with using TCP for realtime games like FPS is that unlike web browsers, or email or most other applications, these multiplayer games have a real time requirement on packet delivery.What this means is that for many parts of a game, for example player input and character positions, it really doesn&rsquo;t matter what happened a second ago, the game only cares about the most recent data.TCP was simply not designed with this in mind.Consider a very simple example of a multiplayer game, some sort of action game like a shooter. You want to network this in a very simple way. Every frame you send the input from the client to the server (eg. keypresses, mouse input controller input), and each frame the server processes the input from each player, updates the simulation, then sends the current position of game objects back to the client for rendering.So in our simple multiplayer game, whenever a packet is lost, everything has to stop and wait for that packet to be resent. On the client game objects stop receiving updates so they appear to be standing still, and on the server input stops getting through from the client, so the players cannot move or shoot. When the resent packet finally arrives, you receive this stale, out of date information that you don&rsquo;t even care about! Plus, there are packets backed up in queue waiting for the resend which arrive at same time, so you have to process all of these packets in one frame. Everything is clumped up!Unfortunately, there is nothing you can do to fix this behavior, it&rsquo;s just the fundamental nature of TCP. This is just what it takes to make the unreliable, packet-based internet look like a reliable-ordered stream.Thing is we don&rsquo;t want a reliable ordered stream.We want our data to get as quickly as possible from client to server without having to wait for lost data to be resent.This is why you should never use TCP when networking time-critical data!Wait? Why can&rsquo;t I use both UDP and TCP?For realtime game data like player input and state, only the most recent data is relevant, but for other types of data, say perhaps a sequence of commands sent from one machine to another, reliability and ordering can be very important.The temptation then is to use UDP for player input and state, and TCP for the reliable ordered data. If you&rsquo;re sharp you&rsquo;ve probably even worked out that you may have multiple &ldquo;streams&rdquo; of reliable ordered commands, maybe one about level loading, and another about AI. Perhaps you think to yourself, &ldquo;Well, I&rsquo;d really not want AI commands to stall out if a packet is lost containing a level loading command - they are completely unrelated!&rdquo;. You are right, so you may be tempted to create one TCP socket for each stream of commands.On the surface, this seems like a great idea. The problem is that since TCP and UDP are both built on top of IP, the underlying packets sent by each protocol will affect each other. Exactly how they affect each other is quite complicated and relates to how TCP performs reliability and flow control, but fundamentally you should remember that TCP tends to induce packet loss in UDP packets. For more information, read this paper on the subject.Also, it&rsquo;s pretty complicated to mix UDP and TCP. If you mix UDP and TCP you lose a certain amount of control. Maybe you can implement reliability in a more efficient way that TCP does, better suited to your needs? Even if you need reliable-ordered data, it&rsquo;s possible, provided that data is small relative to the available bandwidth to get that data across faster and more reliably that it would if you sent it over TCP. Plus, if you have to do NAT to enable home internet connections to talk to each other, having to do this NAT once for UDP and once for TCP (not even sure if this is possible&hellip;) is kind of painful.ConclusionMy recommendation is not only that you use UDP, but that you only use UDP for your game protocol. Don&rsquo;t mix TCP and UDP! Instead, learn how to implement the specific features of TCP that you need inside your own custom UDP based protocol.Of course, it is no problem to use HTTP to talk to some RESTful services while your game is running. I&rsquo;m not saying you can&rsquo;t do that. A few TCP connections running while your game is running isn&rsquo;t going to bring everything down. The point is, don&rsquo;t split your game protocol across UDP and TCP. Keep your game protocol running over UDP so you are fully in control of the data you send and receive and how reliability, ordering and congestion avoidance are implemented.The rest of this article series show you how to do this, from creating your own virtual connection on top of UDP, to creating your own reliability, flow control and congestion avoidance. 译文译文出处 $hhd$2 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;翻译：削微寒 审校：削微寒$hhd$2 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;介绍你一定听说过sokcet(初探socket)，它分为两种常用类型：TCP和UDP。当要写一个网络游戏，我们首先要选择使用哪种类型的socket。是用TCP、UDP还是两者都用？选择哪种类型，完全取决于你要写的游戏的类型。后面的文章，我都将假设你要写一个‘动作’网游。就像：光环系列，战地1942，雷神之锤，这些游戏。我们将非常仔细的分析这两种socket类型的优劣，并且深入到底层，弄清楚互联网是如何工作的什么。当我们弄清楚这些信息后，就很容易做出正确的选择了。 $hhd$2 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;TCP/IPTCP代表“传输控制协议”，IP代表：“互联网协议”，你在互联网上做任何事情，都是建立在这两者的基础上，比如：浏览网页、收发邮件等等。 $hhd$4 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;TCP如果你曾经用过TCP socket，你肯定知道它是可靠连接的协议，面向连接的传输协议。简单的说：两台机器先建立起连接，然后两台机器相互发送数据，就像你在一台计算机上写文件，在另外一个台读文件一样。（我是这么理解的：TCP socket就像建立起连接的计算机，之间共享的一个‘文件‘对象，两者通过读写这个‘文件‘实现数据的传输）这个连接是可靠的、有序的，代表着：发送的所有的数据，保证到达传输的另一端的时候。另一端得到的数据，和发送数据一摸一样（可靠，有序。例如：A发送数据‘abc’，通过TCPsocket传输数据到B，B得到数据一定是：‘abc’。而不是‘bca’或者‘xueweihan’之类的鬼！）。传输的数据是‘数据流’的形式(数据流：用于操作数据集合的最小的有序单位，与操作本地文件中的stream一样。所以TCP socket和文件对象很像)，也就是说：TCP把你的数据拆分后，包装成数据包，然后通过网络发送出去。注意：就像读写文件那样，这样比较好理解。 $hhd$4 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;IP“IP”协议是在TCP协议的下面（这个牵扯到七层互联网协议栈，我就简单的贴个图不做详细的介绍）“IP”协议是没有连接的概念，它做的只是把上一层(传输层)的数据包从一个计算传递到下一个计算机。你可以理解成：这个过程就像一堆人手递手传递纸条一样，传递了很多次，最终到达纸条上标记的xxx手里（纸条上写着‘xxx亲启，偷看者3cm’）。在传递的过程中，不保证这个纸条(信件)能能够准确的送到收信人的手上。发信人发送信件，但是永远不知道信件是否可以准确到达收件人的手上，除非收件人回信告诉他（发信人）：“兄弟我收到信了！”（IP层只是用于传递信息，并不做信息的校验等其它操作）当然，传递信息的这个过程还是还是很复杂的。因为，不知道具体的传递次序，也就是说，因为不知道最优的传递路线（能够让数据包快速的到达目的地的最优路径）所以，有些时候“IP”协议就传递多份一样的数据，这些数据通过不同的路线到达目的地，从而发现最优的传递路线。这就是互联网设计中的：自动优化和自动修复，解决了连接的问题。这真的是一个很酷的设计，如果你想知道更多的底层实现，可以阅读关于TCP/IP的书。（推荐上野宣的图解系列) $hhd$4 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;UDP如果我们想要直接发送和接受数据包，那么就要使用另一种socket。我们叫它UDP。UDP代表“用户数据包协议”，它是另外一种建立在IP协议之上的协议，就像TCP一样，但是没有TCP那么多功能（例如：建立连接，信息的校验，数据流的拆分合并等）使用UDP我们能够向目标IP和端口（例如80），发送数据包。数据包会达到目标计算机或者丢失。收件人（目标计算机），我们只需要监听具体的端口（例如：80），当从任意一台计算机（注意：UDP是不建立连接的）接受到数据包后，我们会得知发送数据包的计算机地址（IP地址）和端口、数据包的大小、内容。UDP是不可靠协议。现实使用的过程中，发送的大多数的数据包都会被接收到，但是通常会丢失1-5%，偶尔，有的时候还可能啥都接收不到（数据包全部丢失一个都没接收到，传递数据的计算机之间的计算机的数量越多，出错的概率越大）。UDP协议中的数据包也是没有顺序的。比如：你发送5个包，顺序是1，2，3，4，5。但是，即接收到的顺序可能是3，1，4，2，5。现实使用的过程中，大多时候，接收到的数据的顺序是正确的，但是并不是每次都是这样。最后，尽管UDP并没有比“IP”协议高级多少，而且不可靠。但是你发送的数据，要么全部到达，要么全部丢失。比如：你发送一个大小为256 byte的数据包给另外一台计算机，这台计算机不会只接收到100 byte的数据包，它只可能接收到256 byte的数据包，或者什么都没接收到。这是UDP唯一可以保证的事情，其它所有的事情都需要你来决定（我的理解，UDP协议只是个简单的传输协议，只保证数据包的完整性，注意是数据包而不是信息。其他的事情需要自己去做，完善这个协议，达到自己使用的需求。） $hhd$2 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;TCP vs. UDP我们如何选择是使用TCP socket还是UDPsocket呢？我们先看看两者的特征吧：TCP：· 面向连接· 可靠、有序· 自动把数据拆分成数据包· 确保数据的发送一直在控制中（流量控制）· 使用简单，就像读写文件一样UDP：· 没有连接的概念，你需要自己通过代码实现（这个我也没自己实现过，应该还会讲）· 不可靠，数据包无序，数据包可能无序，重复，或者丢失· 你需要手动地把数据拆分成数据包，然后发送数据包· 你需要自己做流量控制· 如果数据包太多，你需要设计重发和统计机制通过上面的描述，不难发现：TCP做了所有我们想做的事情，而且使用十分简单。反观UDP就十分难用了，我们需要自己编写设计一切。很显然，我们只要用TCP就好了！不，你想的简单了（原来，是我太年轻了！）当你开发一个像上面说过的FPS（动作网游）的时候使用TCP协议，会是一个错误的决定，这个TCP协议就不好用了！为什么这么说？那么你就需要知道TCP到底做了什么，使得一起看起来十分简单。（让我们继续往下看，这是我最好奇的地方！！！有没有兴奋起来？）$hhd$4 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;TCP内部的工作原理TCP和UDP都是建立在“IP”协议上的，但是它俩完全不同。UDP和“IP”协议很像，然而TCP隐藏了数据包的所有的复杂和不可靠的部分，抽象成了类似文件的对象。那么TCP是如何做到这一点呢？首先，TCP是一个数据流的协议，所以你只需要把输入的内容变成数据流，然后TCP协议就会确保数据会到达发送的目的地。因为“IP”协议是通过数据包传递信息，TCP是建立在“IP”协议之上，所以TCP必须把用户输入的数据流分成数据包的形式。TCP协议会对需要发送的数据进行排队，然后当有足够的排除数据的时候，就发送数据包到目标计算机。 当在多人在线的网络游戏中发送非常小的数据包的时候，这样做就有一个问题。这个时候会发生什么？如果数据没有达到缓冲区设定的数值，数据包是不会发送的。这就会出现个问题：因为客户端的用户输入请求后，需要尽快的从服务器得到响应，如果像上面TCP 等待缓冲区满后才发送的话，就会出现延时，那么客户端的用户体验就会非常差！网络游戏几乎不能出现延时，我们希望看到的是“实时”和流畅。TCP有一个选项可以修复，上面说的那种等待缓冲区满才发送的情况，就是TCP_NODELAY。这个选项使得TCP socket不需要等待缓冲区满才发送，而是输入数据后就立即发送。然而，即使你已经设置了TCP_NODELAY选项，在多人网游中还是会有一系列的问题。这一切的源头都由于TCP处理丢包和乱序包的方式。使得你产生有序和可靠的“错觉”。$hhd$4 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;TCP如何保证数据的可靠性本质上TCP做的事情，分解数据流，成为数据包，使用在不可靠的“IP”协议，发送这些数据包。然后使得数据包到达目标计算机，然后重组成数据流。但是，如何处理当丢包？如何处理重复的数据包和乱序数据包？这里不会介绍TCP处理这些事情的细节，因为这些都是非常复杂的（想弄清楚的同学可以看我上面推荐的书单），大体上：TCP发送一个数据包，等待一段时间，直到检测到数据包丢失了，因为没有接收到它的ACK（一种传输类控制符号，用于确认接收无误），接下来就重新发送丢失的数据包到目标计算机。重复的数据包将被丢弃在接收端，乱序的数据包将被重新排序。所以保证了数据包的可靠性和有序性。如果我们用TCP实现数据的实时传输，就会出现一个问题：TCP无论什么情况，只要数据包出错，就必须等待数据包的重发。也就是说，即使最新的数据已经到达，但还是不能访问这些数据包，新到的数据会被放在一个队列中，需要等待丢失的包重新发过来之后，所有数据没有丢失才可以访问。需要等待多长时间才能重新发送数据包？举个例子：如果的延时是125ms，那么需要最好的情况下重发数据包需要250ms，但是如果遇到糟糕的情况，将会等待500ms以上，比如：网络堵塞等情况。那就没救了。。。 $hhd$4 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;为什么TCP不应该用于对网络延时要求极高的条件下如果FPS（第一人称射击）这类的网络游戏使用TCP就出现问题，但是web浏览器、邮箱、大多数应用就没问题，因为多人网络游戏有实时性的要求。比如：玩家输入角色的位置，重要的不是前一秒发生了什么，而是最新的情况！TCP并没有考虑这类需求，它并不是为这种需求而设计的。这里举一个简单的多人网游的例子，比如射击的游戏。对网络的要求很简单。玩家通过客户端发送给服务器的每个场景（用鼠标和键盘输入的行走的位置），服务器处理每个用户发送过来的所有场景，处理完再返回给客户端，客户端解析响应，渲染最新的场景展示给玩家。在上面说的哪个多人游戏的例子中，如果出现一个数据包丢失，所有事情都需要停下来等待这个数据包重发。客户端会出现等待接收数据，所以玩家操作的任务就会出现站着不动的情况（卡！卡！卡！），不能射击也不能移动。当重发的数据包到达后，你接收到这个过时的数据包，然而玩家并不关心过期的数据（激战中，卡了1秒，等能动了，都已经死了）不幸的是，没有办法修复TCP的这个问题，这是它本质的东西，没办法修复。这就是TCP如何做到让不可靠，无序的数据包，看起来像有序，可靠的数据流。我并不需要可靠，有序的数据流，我们希望的是客户端和服务端之间的延时越低越好，不需要等待重发丢失的包。所以，这就是为什么在对数据的实时性要求的下，我们不用TCP。 $hhd$4 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;那为什么不UDP和TCP一起用呢？像玩家输入实时游戏数据和状态的变更，只和最新的数据有关（这些数据强调实时性）。但是另外的一些数据，例如，从一台计算机发送给另外一个台计算机的一些列指令（交易请求，聊天？），可靠、有序的传输还是非常重要的！那么，用户输入和状态用UDP，TCP用于可靠、有序的数据传输，看起来是个不错的点子。但是，问题在于TCP和UDP都是建立“IP”协议之上，所以协议之间都是发送数据包，从而相互通信。协议之间的互相影响是相当复杂的，涉及到TCP性能、可靠性和流量控制。简而言之，TCP会导致UDP丢包，请参考这篇论文此外，UDP和TCP混合使用是非常复杂的，而且实现起来是非常痛苦的。（这段我就不翻译了，总而言之：不要混用UDP和TCP，容易失去对传输数据的控制） $hhd$2 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;总结我的建议并不是就一定要使用UDP，但是UDP协议应该用于游戏。请不要混合使用TCP和UDP，你应该学习TCP中一些地方是如何实现的技巧，然后可以把这些技巧用在UDP上，从而实现适合你的需求的协议（借鉴TCP中的实现，在UDP上，完善功能，从而达到你的需求）。 这个系列，接下来会讲到：如何在UDP上创建一个虚拟的连接（因为UDP本身，是没有连接的概念的）、如何使得UDP实现可靠性，流量控制，非阻塞。 $hhd$2 style=”margin: 11.25pt 0cm 1.5pt; background-image: initial; background-attachment: initial; background-size: initial; background-origin: initial; background-clip: initial; background-position: initial; background-repeat: initial;”&gt;参考· MBA lib数据流· WiKi TCP/IP协议族· W3SchoolTCP/IP 协议· UDP和TCP的区]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>GafferOnGames</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速编译技巧]]></title>
    <url>%2F2016%2F11%2F01%2F%E5%BF%AB%E9%80%9F%E7%BC%96%E8%AF%91%E6%8A%80%E5%B7%A7%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[项目越来越大，每次需要重新编译整个项目都是一件很浪费时间的事情。Research了一下，找到以下可以帮助提高速度的方法，总结一下。 tmpfs 有人说在Windows下用了RAMDisk把一个项目编译时间从4. 5小时减少到了5分钟，也许这个数字是有点夸张了，不过粗想想，把文件放到内存上做编译应该是比在磁盘 上快多了吧，尤其如果编译器需要生成很多临时文件的话。 这个做法的实现成本最低，在Linux中，直接mount一个tmpfs就可以了。而且对所编译的工程没有任何要求，也不用改动编译环境。 mount -t tmpfs tmpfs ~/build -o size=1G 用2.6.32.2的Linux Kernel来测试一下编译速度： 用物理磁盘：40分16秒 用tmpfs：39分56秒 呃……没什么变化。看来编译慢很大程度上瓶颈并不在IO上面。但对于一个实际项目来说， 编译过程中可能还会有打包等IO密集的操作，所以只要可能，用tmpfs是有 益无害的。 当然对于大项目来说，你需要有足够的内存才能负担得起这个tmpfs的开销。 make -j 既然IO不是瓶颈，那CPU就应该是一个影响编译速度的重要因素了。 用make -j带一个参数，可以把项目在进行并行编译，比如在一台双核的机器上，完全可以用make -j4，让make最多允许4个编译命令同时执行，这样可以更有效的利用CPU资源。 还是用Kernel来测试： 用make： 40分16秒 用make -j4：23分16秒 用make -j8：22分59秒 由此看来，在多核CPU上，适当的进行并行编译还是可以明显提高编译速度的。但并行的任务不宜太多，一般是以CPU的核心数目的两倍为宜。 不过这个方案不是完全没有cost的，如果项目的Makefile不规范，没有正确的设置好依赖关系，并行编译的结果就是编译不能正常进行。如果依赖关系设置过于保守，则可能本身编译的可并行度就下降了，也不能取得最佳的效果。 ccache ccache用于把编译的中间结果进行缓存，以便在再次编译的时候可以节省时间。这对于玩Kernel来说实在是再好不过了，因为经常需要修改一些Kernel的代码，然后再重新编译，而这两次编译大部分东西可能都没有发生变化。对于平时开发项目来说，也是一样。为什么不是直接用make所支持的增量编译呢？还是因为现实中，因为Makefile的不规范，很可能这种“聪明”的方案根本不能正常工作，只有每次make clean再make才行。 安装完ccache后，可以在/usr/local/bin下建立gcc，g++，c++，cc的symboliclink，链到/usr/bin/ccache上。总之确认系统在调用gcc等命令时会调用到ccache就可以了（通常情况下/usr/local/bin会在PATH中排在/usr/bin前面）。 继续测试： 用ccache的第一次编译(make -j4)：23分38秒 用ccache的第二次编译(make -j4)：8分48秒 用ccache的第三次编译(修改若干配置，make -j4)：23分48秒 看来修改配置（我改了CPU类型…）对ccache的影响是很大的，因为基本头文件发生变化后，就导致所有缓存数据都无效了，必须重头来做。但如果只是修改一些.c文件的代码，ccache的效果还是相当明显的。而且使用ccache对项目没有特别的依赖，布署成本很低，这在日常工作中很实用。 可以用ccache -s来查看cache的使用和命中情况： cache directory /home/lifanxi/.ccache cache hit 7165 cache miss 14283 called for link 71 not a C/C++ file 120 no input file 3045 files in cache 28566 cache size 81.7 Mbytes max cache size 976.6 Mbytes 可以看到，显然只有第二编次译时cache命中了，cache miss是第一次和第三次编译带来的。两次cache占用了81.7M的磁盘，还是完全可以接受的。 distcc 一台机器的能力有限，可以联合多台电脑一起来编译。这在公司的日常开发中也是可行的，因为可能每个开发人员都有自己的开发编译环境，它们的编译器版本一般是一致的，公司的网络也通常具有较好的性能。这时就是distcc大显身手的时候了。 使用distcc，并不像想象中那样要求每台电脑都具有完全一致的环境，它只要求源代码可以用make -j并行编译，并且参与分布式编译的电脑系统中具有相同的编译器。因为它的原理只是把预处理好的源文件分发到多台计算机上，预处理、编译后的目标文件的链接和其它除编译以外的工作仍然是在发起编译的主控电脑上完成，所以只要求发起编译的那台机器具备一套完整的编译环境就可以了。 distcc安装后，可以启动一下它的服务： /usr/bin/distccd –daemon –allow 10.64.0.0/16 默认的3632端口允许来自同一个网络的distcc连接。 然后设置一下DISTCC_HOSTS环境变量，设置可以参与编译的机器列表。 通常localhost也参与编译，但如果可以参与编译的机器很多，则可以把localhost从这个列表 中去掉，这样本机就完全只是进行预处理、分发和链接了，编译都在别的机器上完成。 因为机器很多时，localhost的处理负担很重，所以它就不再“兼职”编译了。 export DISTCC_HOSTS=&quot;localhost 10.64.25.1 10.64.25.2 10.64.25.3&quot; 然后与ccache类似把g++，gcc等常用的命令链接到/usr/bin/distcc上就可以了。 在make的时候，也必须用-j参数，一般是参数可以用所有参用编译的计算机CPU内核总数的两倍做为并行的任务数。 同样测试一下： 一台双核计算机，make -j4：23分16秒 两台双核计算机，make -j4：16分40秒 两台双核计算机，make -j8：15分49秒 跟最开始用一台双核时的23分钟相比，还是快了不少的。如果有更多的计算机加入，也可以得到更好的效果。 在编译过程中可以用distccmon-text来查看编译任务的分配情况。distcc也可以与ccache同时使用，通过设置一个环境变量就可以做到，非常方便。 总结 tmpfs： 解决IO瓶颈，充分利用本机内存资源 make -j： 充分利用本机计算资源 distcc： 利用多台计算机资源 ccache： 减少重复编译相同代码的时间 这些工具的好处都在于布署的成本相对较低，综合利用这些工具，就可以轻轻松松的节省相当可观的时间。 上面介绍的都是这些工具最基本的用法，更多的用法可以参考它们各自的man page。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Compile</tag>
        <tag>Make</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个基于虚幻4群聚鱼群AI插件]]></title>
    <url>%2F2016%2F10%2F18%2Fa_fish_flock_ai_plugin_for_ue4%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[A fish flock AI Plugin for Unreal Engine 4一个基于虚幻4的鱼群 AI 插件 this Plugin version can Run 2000+ fishes at the same time这个插件版本可以同时运行 2000+ 条鱼儿 源码已放到fish Video Preview 视频演示 DownloadMyFish.exe (Win64) . . . This is packaged by an unoptimized version( check out branch old_demo)下载一个打包好的试玩看看, 这个包是没有经过优化过的版本打包出来的( 是用old_demo分支的版本打包的 ) How to play VR : (My Device is HTC Vive) Motion Controller FaceButton1 =&gt; Move forward手柄圆盘上键 =&gt; 往前移动 PC’s KeyBoard Arrow UP and Down =&gt; Move faster or slower电脑键盘的上下箭头键 =&gt; 调整移动速度 Hold Motion Controller Trigger Down =&gt; Attract fishes按住手柄扳机键 =&gt; 吸引鱼群 PC : EQ =&gt; Up &amp; DownEQ 键 =&gt; 上下 WASD =&gt; Basic movementWASD 键 =&gt; 基本的移动指令(前后左右) Hold Left Mouse Button Down =&gt; Attract fishes按住鼠标左键 =&gt; 吸引鱼群 Arrow UP and Down =&gt; Move faster or slower上下箭头键 =&gt; 调整移动速度 How to useplace Plugins folder in your project root directory, then just like把Plugins文件夹放在你项目的根目录, 接下来如图 About This Unreal Engine Version 4.15 Read Craig Reynolds’s thesis查看 Craig Reynolds的论文 This project implements a new flocking Ai algorithm, with 3 components : 算法简要 Separation : every fish will try to steer away from their neighbors分离性 ：每条鱼都会与周围的鱼保持距离 Following the leader : every fish will try to follow its leader跟随一个领头者 ： 每条鱼都会跟随一个领头者 Avoiding enemies.躲避敌人]]></content>
      <categories>
        <category>GitHub</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>UE4</tag>
        <tag>GitHub</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XXTEA的python实现]]></title>
    <url>%2F2016%2F09%2F13%2FXXTEA%E7%9A%84python%E5%AE%9E%E7%8E%B0%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[在数据的加解密领域，算法分为对称密钥与非对称密钥两种。 对称密钥与非对称密钥由于各自的特点，所应用的领域是不尽相同的。 对称密钥加密算法由于其速度快，一般用于整体数据的加密，而非对称密钥加密算法的安全性能佳，在数字签名领域得到广泛的应用。 微型加密算法（TEA）及其相关变种（XTEA，Block TEA，XXTEA） 都是分组加密算法，它们很容易被描述，实现也很简单（典型的几行代码）。 TEA是Tiny Encryption Algorithm的缩写，以加密解密速度快，实现简单著称。 TEA 算法最初是由剑桥计算机实验室的 David Wheeler 和 Roger Needham 在 1994 年设计的。 该算法使用 128 位的密钥为 64 位的信息块进行加密，它需要进行 64 轮迭代，尽管作者认为 32 轮已经足够了。 该算法使用了一个神秘常数δ作为倍数，它来源于黄金比率，以保证每一轮加密都不相同。 但δ的精确值似乎并不重要，这里 TEA 把它定义为 δ=「(√5 - 1)231」（也就是程序中的 0×9E3779B9）。 之后 TEA 算法被发现存在缺陷，作为回应，设计者提出了一个 TEA 的升级版本——XTEA（有时也被称为“tean”）。 XTEA 跟 TEA 使用了相同的简单运算，但它采用了截然不同的顺序，为了阻止密钥表攻击，四个子密钥（在加密过程中，原 128 位的密钥被拆分为 4 个 32 位的子密钥）采用了一种不太正规的方式进行混合，但速度更慢了。 在跟描述 XTEA 算法的同一份报告中，还介绍了另外一种被称为 Block TEA 算法的变种，它可以对 32 位大小任意倍数的变量块进行操作。 该算法将 XTEA 轮循函数依次应用于块中的每个字，并且将它附加于它的邻字。 该操作重复多少轮依赖于块的大小，但至少需要 6 轮。 该方法的优势在于它无需操作模式（CBC，OFB，CFB 等），密钥可直接用于信息。 对于长的信息它可能比 XTEA 更有效率。 在 1998 年，Markku-Juhani Saarinen 给出了一个可有效攻击 Block TEA 算法的代码，但之后很快 David J. Wheeler 和 Roger M. Needham 就给出了 Block TEA 算法的修订版，这个算法被称为 XXTEA。 XXTEA 使用跟 Block TEA 相似的结构，但在处理块中每个字时利用了相邻字。 它利用一个更复杂的 MX 函数代替了 XTEA 轮循函数，MX 使用 2 个输入量。 XXTEA 算法很安全，而且非常快速，非常适合应用于 Web 开发中。 TEA算法是由剑桥大学计算机实验室的David Wheeler和Roger Needham于1994年发明， TEA是Tiny Encryption Algorithm的缩写，以加密解密速度快，实现简单著称。 TEA算法每一次可以操作64bit(8byte)，采用128bit(16byte)作为key，算法采用迭代的形式，推荐的迭代轮数是64轮，最少32轮。 为解决TEA算法密钥表攻击的问题，TEA算法先后经历了几次改进，从XTEA到BLOCK TEA，直至最新的XXTEA。 XTEA也称做TEAN，它使用与TEA相同的简单运算，但四个子密钥采取不正规的方式进行混合以阻止密钥表攻击。 Block TEA算法可以对32位的任意整数倍长度的变量块进行加解密的操作，该算法将XTEA轮循函数依次应用于块中的每个字，并且将它附加于被应用字的邻字。 XXTEA使用跟Block TEA相似的结构，但在处理块中每个字时利用了相邻字，且用拥有两个输入量的MX函数代替了XTEA轮循函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import struct _DELTA = 0x9E3779B9 def _long2str(v, w): n = (len(v) - 1) &lt;&lt; 2 if w: m = v[-1] if (m &lt; n - 3) or (m &gt; n): return '' n = m s = struct.pack('&lt;%iL' % len(v), *v) return s[0:n] if w else s def _str2long(s, w): n = len(s) m = (4 - (n &amp; 3) &amp; 3) + n s = s.ljust(m, "\0") v = list(struct.unpack('&lt;%iL' % (m &gt;&gt; 2), s)) if w: v.append(n) return v def encrypt(str, key): if str == '': return str v = _str2long(str, True) k = _str2long(key.ljust(16, "\0"), False) n = len(v) - 1 z = v[n] y = v[0] sum = 0 q = 6 + 52 // (n + 1) while q &gt; 0: sum = (sum + _DELTA) &amp; 0xffffffff e = sum &gt;&gt; 2 &amp; 3 for p in xrange(n): y = v[p + 1] v[p] = (v[p] + ((z &gt;&gt; 5 ^ y &lt;&lt; 2) + (y &gt;&gt; 3 ^ z &lt;&lt; 4) ^ (sum ^ y) + (k[p &amp; 3 ^ e] ^ z))) &amp; 0xffffffff z = v[p] y = v[0] v[n] = (v[n] + ((z &gt;&gt; 5 ^ y &lt;&lt; 2) + (y &gt;&gt; 3 ^ z &lt;&lt; 4) ^ (sum ^ y) + (k[n &amp; 3 ^ e] ^ z))) &amp; 0xffffffff z = v[n] q -= 1 return _long2str(v, False) def decrypt(str, key): if str == '': return str v = _str2long(str, False) k = _str2long(key.ljust(16, "\0"), False) n = len(v) - 1 z = v[n] y = v[0] q = 6 + 52 // (n + 1) sum = (q * _DELTA) &amp; 0xffffffff while (sum != 0): e = sum &gt;&gt; 2 &amp; 3 for p in xrange(n, 0, -1): z = v[p - 1] v[p] = (v[p] - ((z &gt;&gt; 5 ^ y &lt;&lt; 2) + (y &gt;&gt; 3 ^ z &lt;&lt; 4) ^ (sum ^ y) + (k[p &amp; 3 ^ e] ^ z))) &amp; 0xffffffff y = v[p] z = v[n] v[0] = (v[0] - ((z &gt;&gt; 5 ^ y &lt;&lt; 2) + (y &gt;&gt; 3 ^ z &lt;&lt; 4) ^ (sum ^ y) + (k[0 &amp; 3 ^ e] ^ z))) &amp; 0xffffffff y = v[0] sum = (sum - _DELTA) &amp; 0xffffffff return _long2str(v, True) if __name__ == "__main__": print decrypt(encrypt('Hello XXTEA!', '16bytelongstring'), '16bytelongstring')]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>XXTEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内零头和外零头]]></title>
    <url>%2F2016%2F09%2F12%2F%E5%86%85%E9%9B%B6%E5%A4%B4%E5%92%8C%E5%A4%96%E9%9B%B6%E5%A4%B4%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[问题:在内存管理中，“内零头”和“外零头”个指的是什么？在固定式分区分配、可变式分区分配、页式虚拟存储系统、段式虚拟存储系统中，各会存在何种零头？为什么？ 解答： 在存储管理中， 内零头是指分配给作业的存储空间中未被利用的部分， 外零头是指系统中无法利用的小存储块。 在固定式分区分配中，为将一个用户作业装入内存，内存分配程序从系统分区表中找出一个能满足作业要求的空闲分区分配给作业，由于一个作业的大小并不一定与分区大小相等，因此，分区中有一部分存储空间浪费掉了。 由此可知，固定式分区分配中存在内零头。 在可变式分区分配中，为把一个作业装入内存，应按照一定的分配算法从系统中找出一个能满足作业需求的空闲分区分配给作业，如果这个空闲分区的容量比作业申 请的空间容量要大，则将该分区一分为二，一部分分配给作业，剩下的部分仍然留作系统的空闲分区。 由此可知，可变式分区分配中存在外零头。 在页式虚拟存储系统中，用户作业的地址空间被划分成若干大小相等的页面，存储空间也分成也页大小相等的物理块，但一般情况下，作业的大小不可能都是物理块大小的整数倍，因此作业的最后一页中仍有部分空间被浪费掉了。 由此可知，页式虚拟存储系统中存在内零头。 在段式虚拟存储系统中，作业的地址空间由若干个逻辑分段组成，每段分配一个连续的内存区，但各段之间不要求连续，其内存的分配方式类似于动态分区分配。 由此可知，段式虚拟存储系统中存在外零头。 详细解释 操作系统在分配内存时，有时候会产生一些空闲但是无法被正常使用的内存区域，这些就是内存碎片，或者称为内存零头，这些内存零头一共分为两类：内零头和外零头。 内零头是指进程在向操作系统请求内存分配时，系统满足了进程所需要的内存需求后，还额外还多分了一些内存给该进程，也就是说额外多出来的这部分内存归该进程所有，其他进程是无法访问的。 外零头是指内存中存在着一些空闲的内存区域，这些内存区域虽然不归任何进程所有，但是因为内存区域太小，无法满足其他进程所申请的内存大小而形成的内存零头。 页式存储管理的情况页式存储管理是以页为单位（页面的大小由系统确定，且大小是固定的）向进程分配内存的， 例如：假设内存总共有100K,分为10页，每页大小为10K。现在进程A提出申请56K内存，因为页式存储管理是以页为单位进程内存分配的，所以系统会向进程A提供6个页面，也就是60K的内存空间，那么在最后一页中进程只使用了6K，从而多出了4K的内存碎片，但是这4K的内存碎片系统已经分配给进程A了，其他进程是无法再访问这些内存区域的， 这种内存碎片就是内零头。 段式存储管理的情况段式存储管理是段（段的大小是程序逻辑确定，且大小不是固定的）为单位向进程进行内存分配的，进程申请多少内存，系统就给进程分配多少内存，这样就不会产生内零头，但是段式分配会产生外零头。 例如：假设内存总的大小为100K，现在进程A向系统申请60K的内存，系统在满足了进程A的内存申请要求后，还剩下40K的空闲内存区域；这时如果进程B向系统申请50K的内存区域，而系统只剩下了40K的内存区域，虽然这40K的内存区域不归任何进程所有，但是因为大小无法满足进程B的要求，所以也无法分配给进程B，这样就产生了外零头。 请求段式存储管理是在段式存储管理的基础上增加了请求调段功能和段置换功能。所以段式和请求段式存储管理会产生外零头 练习题下面的内存管理模式中，会产生外零头的是(正确答案B, D) A、页式B、段式C、请求页式D、请求段式]]></content>
      <categories>
        <category>Misc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PHP的魔术]]></title>
    <url>%2F2016%2F09%2F01%2FPHP%E7%9A%84%E9%AD%94%E6%9C%AF%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[PHP 将所有以 （两个下划线）开头的类方法保留为魔术方法。所以在定义类方法时，除了上述魔术方法，建议不要以 为前缀。 . . . PHP魔幻（术）变量1234567- __LINE__ 文件中的当前行号。- __FILE__ 文件的完整路径和文件名。如果用在被包含文件中，则返回被包含的文件名。- __DIR__ 文件所在的目录。如果用在被包括文件中，则返回被包括的文件所在的目录。 它等价于 dirname(__FILE__)。- __FUNCTION__ 本常量返回该函数被定义时的名字（区分大小写）- __CLASS__ 本常量返回该类被定义时的名字（区分大小写） PHP魔幻（术）方法 __construct() 实例化类时自动调用。 __destruct() 类对象使用结束时自动调用。 __set() 在给未定义的属性赋值的时候调用。 __get() 调用未定义的属性时候调用。 __isset() 使用isset()或empty()函数时候会调用。 __unset() 使用unset()时候会调用。 __sleep() 使用serialize序列化时候调用。 __wakeup() 使用unserialize反序列化的时候调用。 __call() 调用一个不存在的方法的时候调用。 __callStatic()调用一个不存在的静态方法是调用。 __toString() 把对象转换成字符串的时候会调用。比如 echo。 __invoke() 当尝试把对象当方法调用时调用。 __set_state() 当使用var_export()函数时候调用。接受一个数组参数。 __clone() 当使用clone复制一个对象时候调用。]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[boost之program_options]]></title>
    <url>%2F2016%2F08%2F15%2Fboost%E4%B9%8Bprogram_options%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[介绍命令行接口是普遍,基础的人机交互接口，从命令行提取程序的运行时选项的方法有很多。 你可以自己编写相对应的完整的解析函数，或许你有丰富的C语言编程经验，熟知getopt()函数的用法，又或许使用Python的你已经在使用optparse库来简化这一工作。 大家在平时不断地谈及到“不要重复造轮子”，那就需要掌握一些顺手的库，这里介绍一种C++方式来解析命令行选项的方法，就是使用Boost.Program_options库。 program_options提供程序员一种方便的命令行和配置文件进行程序选项设置的方法。 使用program_options库而不是你自己动手写相关的解析代码，因为它更简单，声明程序选项的语法简洁，并且库自身也非常小。 将选项值转换为适合的类型值的工作也都能自动完成。 库有着完备的错误检查机制，如果自己手写解析代码时，就可能会错过对一些出错情况的检查了。 最后，选项值不仅能从命令行获取，也能从配置文件，甚至于环境变量中提取，而这些选择不会增加明显的工作量。 示例说明以下面简单的hello程序进行说明，默认打印hello world,如果传入-p选项，就会打印出人的姓名，另外通过传入-h选项，可以打印出帮助选项。 略微看一眼代码文件和相应的屏幕输入输出，然后我们再一起来看看这些是如何发生的。 123456789101112131415161718192021222324252627//hello.cpp #include &lt;iostream&gt;#include &lt;string&gt;#include &lt;boost/program_options.hpp&gt;using namespace std;int main(int argc, char* argv[])&#123; using namespace boost::program_options; //声明需要的选项 options_description desc(&quot;Allowed options&quot;); desc.add_options() (&quot;help,h&quot;, &quot;produce help message&quot;) (&quot;person,p&quot;, value&lt;string&gt;()-&gt;default_value(&quot;world&quot;), &quot;who&quot;) ; variables_map vm; store(parse_command_line(argc, argv, desc), vm); notify(vm); if (vm.count(&quot;help&quot;)) &#123; cout &lt;&lt; desc; return 0; &#125; cout &lt;&lt; &quot;Hello &quot; &lt;&lt; vm[&quot;person&quot;].as&lt;string&gt;() &lt;&lt; endl; return 0;&#125; 下面是在Windows命令提示符窗口上的输入输出结果，其中”&gt;”表示提示符。 1234567891011&gt;hello Hello world&gt;hello -hAllowed options: -h [ --help ] produce help message -p [ --person ] arg (=world) who&gt;hello --person lenHello len 首先通过options_description类声明了需要的选项，add_options返回了定义了operator()的特殊的代理对象。 这个调用看起来有点奇怪，其参数依次为选项名，选项值，以及选项的描述。 注意到示例中的选项名为”help,h”，是因为声明了具有短选项名和长选项名的选项，这跟gnu程序的命令行具有一致性。 当然你可以省略短选项名，但是这样就不能用命令选项简写了。 第二个选项的声明，定义了选项值为string类型，其默认值为world. 接下来,声明了variables_map类的对象，它主要用来存储选项值，并且能储存任意类型的值。 然后，store,parse_command_line和notify函数使vm能存储在命令行中发现的选项。 最后我们就自由地使用这些选项了，variables_map类的使用就像使用std::map一样，除了它必须用as方法去获取值。 如果as方法调用的指定类型与实际存储的类型不同，就会有异常抛出。 具有编程的你可能有这样的经验，使用cl或gcc对源文件进行编译时，可直接将源文件名放置在命令行中，而无需什么选项字母，如gcc a.c之类的。 prgram_options也能处理这种情况，在库中被称为”positional options”(位置选项),但这需要程序员的一点儿帮助才能完成。 看下面的经过对应修改的代码，我们无需传入”-p”选项，就能可指定”person”选项值 123positional_options_description p;p.add(&quot;person&quot;, -1);store(command_line_parser(argc, argv).options(desc).positional(p).run(), vm); 12&gt;hello lenHello len 前面新增的两行是为了说明所有的位置选项都应被解释成”person”选项，这里还采用了command_line_parser类来解析命令行，而不是用parse_command_line函数。 后者只是对前者类的简单封装，但是现在我们需要传入一些额外的信息，所以要使用类本身。 选项复合来源一般来说，在命令行上指定所有选项，对用户来说是非常烦人的。 如果有些选项要应用于每次运行，那该怎么办呢。 我们当然希望能创建出带有些常用设置的选项文件，跟命令行一起应用于程序中。 当然这一切需要将命令行与配置文件中的值结合起来。 比如，在命令行中指定的某些选项值应该能覆盖配置文件中的对应值，或者将这些值组合起来。 下面的代码段将选项通过文件读取，这文件是文本格式，可用”#”表示注释，格式如命令行中的参数一样，选项=值 123ifstream ifs(&quot;config.cfg&quot;);store(parse_config_file(ifs,config),vm);notify(vm); 参考Boost.prgram_options库文档]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>Boost</tag>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于UNP与APUE与TLPI三本大部头的阅读建议(着重讲UNP)]]></title>
    <url>%2F2016%2F08%2F11%2F%E5%85%B3%E4%BA%8EUNP%E4%B8%8EAPUE%E4%B8%8ETLPI%E4%B8%89%E6%9C%AC%E5%A4%A7%E9%83%A8%E5%A4%B4%E7%9A%84%E9%98%85%E8%AF%BB%E5%BB%BA%E8%AE%AE(%E7%9D%80%E9%87%8D%E8%AE%B2UNP)%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[这本书不能一次性所有都想看完。 要有目的性的看，因为这本书类似于百科全书所有都讲， 不分轻重， 如果都看，硬啃，只会迷失了自己，反而不知道看了什么 这本书不能单独看。 这本书必须配合TCP/IP详解和UNIX环境高级编程（简称APUE）以及The Linux Programming Interface（不知道这本书的译名是什么， 简称TLPI）来看 个人看的是卷一第三版，因当前工作经验范围和阅历受限，对于IT码农，IPv6的和SCTP的章节暂且略过，所以目前大概划出的必看章节（其他可挑选）是 1 2 3 4 5 6 7 8 11 13 16 22 26 30 . . . 书中的源码在linux环境下不一定能一次性编译过。 有些地方得自己修改 建议阅读电子版。 使用可以搜索书签的pdf阅读器（比较推荐福昕）， 并且多开几个此书的副本，因为经常会源码和源码讲解对照着看，如果有双屏效率会极大的提高]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>UNP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器模型总结]]></title>
    <url>%2F2016%2F08%2F08%2Fserver_model_summary%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[关于Reactor模式讲解请转 此文 服务器模型总结 其中“互通”指的是如果开发chat服务，多个客户连接之间是否能方便地交换数据（chat也是三大TCP网络编程案例之一）。 对于echo/httpd/Sudoku这类“连接相互独立”的服务程序，这个功能无足轻重，但是对于chat类服务却至关重要。 “顺序性”指的是在httpd/Sudoku这类请求响应服务中，如果客户连接顺序发送多个请求，那么计算得到的多个响应是否按相同的顺序发还给客户（这里指的是在自然条件下，不含刻意同步）。 方案0方案0 这其实不是并发服务器，而是iterative 服务器，因为它一次只能服务一个客户。代码见[UNP]中的Figure 1.9，[UNP]以此为对比其他方案的基准点。这个方案不适合长连接，倒是很适合daytime这种write-only短连接服务。 . . . 方案1方案1 这是传统的Unix并发网络编程方案，[UNP]称之为child-per-client或fork()-per-client，另外也俗称process-per-connection。这种方案适合并发连接数不大的情况。至今仍有一些网络服务程序用这种方式实现，比如PostgreSQL和Perforce的服务端。这种方案适合“计算响应的工作量远大于fork()的开销”这种情况，比如数据库服务器。这种方案适合长连接，但不太适合短连接，因为fork()开销大于求解Sudoku的用时。 方案2方案2 这是传统的Java网络编程方案thread-per-connection，在Java 1.4引入NIO之前，Java网络服务多采用这种方案。它的初始化开销比方案1要小很多，但与求解Sudoku的用时差不多，仍然不适合短连接服务。这种方案的伸缩性受到线程数的限制，一两百个还行，几千个的话对操作系统的scheduler恐怕是个不小的负担。 方案3方案3 这是针对方案1的优化，[UNP]详细分析了几种变化，包括对accept(2)“惊群”问题（thundering herd）的考虑。 方案4方案4 这是对方案2的优化，[UNP]详细分析了它的几种变化。方案3和方案4这两个方案都是Apache httpd长期使用的方案。 方案5 - 单线程Reactor以上几种方案都是阻塞式网络编程，程序流程（thread of control）通常阻塞在read()上，等待数据到达。但是TCP是个全双工协议，同时支持read()和write()操作，当一个线程/进程阻塞在read()上，但程序又想给这个TCP连接发数据，那该怎么办？比如说echo client，既要从stdin读，又要从网络读，当程序正在阻塞地读网络的时候，如何处理键盘输入？ 又比如proxy，既要把连接a收到的数据发给连接b，又要把从b收到的数据发给a，那么到底读哪个？（proxy是附录A讲的三大TCP网络编程案例之一。） Reactor出现的原因一种方法是用两个线程/进程，一个负责读，一个负责写。[UNP]也在实现echoclient时介绍了这种方案。 另一种方法是使用IO multiplexing，也就是select/poll/epoll/kqueue这一系列的“多路选择器”，让一个thread of control 能处理多个连接。 “IO复用”其实复用的不是IO连接，而是复用线程。使用select/poll几乎肯定要配合non-blockingIO，而使用non-blocking IO肯定要使用应用层buffer。这就不是一件轻松的事儿了，如果每个程序都去搞一套自己的IO multiplexing机制（本质是event-driven事件驱动），这是一种很大的浪费。 感谢Doug Schmidt为我们总结出了Reactor模式，让event-driven网络编程有章可循。继而出现了一些通用的Reactor框架/库，比如libevent、muduo、Netty、twisted、POE等等。有了这些库，我想基本不用去编写阻塞式的网络程序了（特殊情况除外，比如proxy流量限制）。 Reactor的意义Doug Schmidt指出，其实网络编程中有很多是事务性（routine）的工作，可以提取为公用的框架或库，而用户只需要填上关键的业务逻辑代码，并将回调注册到框架中，就可以实现完整的网络服务，这正是Reactor模式的主要思想。 而Reactor的意义在于将消息（IO事件）分发到用户提供的处理函数，并保持网络部分的通用代码不变，独立于用户的业务逻辑。 Reactor的具体模型单线程Reactor的程序执行顺序下图中的左图所示。在没有事件的时候，线程等待在select/poll/epoll_wait等函数上。事件到达后由网络库处理IO，再把消息通知（回调）客户端代码。Reactor事件循环所在的线程通常叫IO线程。通常由网络库负责读写socket，用户代码负载解码、计算、编码。 注意由于只有一个线程，因此事件是顺序处理的，一个线程同时只能做一件事情。在这种协作式多任务中，事件的优先级得不到保证，因为从“poll返回之后”到“下一次调用poll进入等待之前”这段时间内，线程不会被其他连接上的数据或事件抢占（也就是说, 不会发生下图中的右图这种情况）。如果我们想要延迟计算（把compute()推迟100ms），那么也不能用sleep()之类的阻塞调用，而应该注册超时回调，以避免阻塞当前IO线程。 这种方案的优点是由网络库搞定数据收发，程序只关心业务逻辑；缺点在前面已经谈了：适合IO密集的应用，不太适合CPU密集的应用，因为较难发挥多核的威力。 另外，与方案2相比，方案5处理网络消息的延迟可能要略大一些，因为方案2直接一次read(2)系统调用就能拿到请求数据，而方案5要先poll(2)再read(2)，多了一次系统调用。 方案6方案6 这是一个过渡方案，收到Sudoku请求之后，不在Reactor线程计算，而是创建一个新线程去计算，以充分利用多核CPU。这是非常初级的多线程应用，因为它为每个请求（而不是每个连接）创建了一个新线程。这个开销可以用线程池来避免，即方案8。这个方案还有一个特点是out-of-order，即同时创建多个线程去计算同一个连接上收到的多个请求，那么算出结果的次序是不确定的，可能第2个Sudoku比较简单，比第1个先算出结果。这也是我们在一开始设计协议的时候使用了id 的原因，以便客户端区分response对应的是哪个request。 方案7方案7 为了让返回结果的顺序确定，我们可以为每个连接创建一个计算线程，每个连接上的请求固定发给同一个线程去算，先到先得。这也是一个过渡方案，因为并发连接数受限于线程数目，这个方案或许还不如直接使用阻塞IO 的thread-per-connection 方案2。 方案7与方案6的另外一个区别是单个client的最大CPU占用率。在方案6中，一个TCP连接上发来的一长串突发请求（burst requests）可以占满全部8个core；而在方案7中，由于每个连接上的请求固定由同一个线程处理，那么它最多占用12.5%的CPU资源。这两种方案各有优劣，取决于应用场景的需要（到底是公平性重要还是突发性能重要）。这个区别在方案8和方案9中同样存在，需要根据应用来取舍。 方案8 - Reactor+ThreadPool 方案8 : 为了弥补方案6中为每个请求创建线程的缺陷，我们使用固定大小线程池。全部的IO工作都在一个Reactor线程完成，而计算任务交给thread pool。如果计算任务彼此独立，而且IO的压力不大，那么这种方案是非常适用的。 线程池的另外一个作用是执行阻塞操作。比如有的数据库的客户端只提供同步访问，那么可以把数据库查询放到线程池中，可以避免阻塞IO线程，不会影响其他客户连接。另外也可以用线程池来调用一些阻塞的IO函数，例如fsync(2)/fdatasync(2)，这两个函数没有非阻塞的版本。 如果IO的压力比较大，一个Reactor处理不过来，可以试试方案9，它采用多个Reactor来分担负载。 方案9 - Reactors In Threads 方案9 : 这是muduo内置的多线程方案，也是Netty内置的多线程方案。这种方案的特点是one loop per thread，有一个main Reactor负责accept(2)连接，然后把连接挂在某个sub Reactor中（muduo采用round-robin的方式来选择sub Reactor），这样该连接的所有操作都在那个sub Reactor所处的线程中完成。 多个连接可能被分派到多个线程中，以充分利用CPU。 muduo采用的是固定大小的Reactor pool，池子的大小通常根据CPU数目确定，也就是说线程数是固定的，这样程序的总体处理能力不会随连接数增加而下降。 另外，由于一个连接完全由一个线程管理，那么请求的顺序性有保证，突发请求也不会占满全部8个核（如果需要优化突发请求，可以考虑方案11）。这种方案把IO分派给多个线程，防止出现一个Reactor的处理能力饱和。 与方案8的线程池相比，方案9减少了进出thread pool的两次上下文切换，在把多个连接分散到多个Reactor线程之后，小规模计算可以在当前IO线程完成并发回结果，从而降低响应的延迟。 这是一个适应性很强的多线程IO模型，因此陈硕把它作为muduo的默认线程模型 方案10方案10 这是Nginx的内置方案。如果连接之间无交互，这种方案也是很好的选择。工作进程之间相互独立，可以热升级。 方案11 - Reactors+Thread Pool 方案11 把方案8和方案9混合，既使用多个Reactor来处理IO，又使用线程池来处理计算。这种方案适合既有突发IO（利用多线程处理多个连接上的IO），又有突发计算的应用（利用线程池把一个连接上的计算任务分配给多个线程去做） 哪些是实用的模型 上表中的N表示并发连接数目，C1和C2是与连接数无关、与CPU数目有关的常数。 我再用银行柜台办理业务为比喻，简述各种模型的特点。银行有旋转门，办理业务的客户人员从旋转门进出（IO）；银行也有柜台，客户在柜台办理业务（计算）。要想办理业务，客户要先通过旋转门进入银行；办理完之后，客户要再次通过旋转门离开银行。一个客户可以办理多次业务，每次都必须从旋转门进出（TCP长连接）。另外，旋转门一次只允许一个客户通过（无论进出），因为read()/write()只能同时调用其中一个。 方案5这间小银行有一个旋转门、一个柜台，每次只允许一名客户办理业务。而且当有人在办理业务时，旋转门是锁住的（计算和IO在同一线程）。为了维持工作效率，银行要求客户应该尽快办理业务，最好不要在取款的时候打电话去问家里人密码，也不要在通过旋转门的时候停下来系鞋带，这都会阻塞其他堵在门外的客户。如果客户很少，这是很经济且高效的方案；但是如果场地较大（多核），则这种布局就浪费了不少资源，只能并发（concurrent）不能并行（parallel）。如果确实一次办不完，应该离开柜台，到门外等着，等银行通知再来继续办理（分阶段回调）。 方案8这间银行有一个旋转门，一个或多个柜台。银行进门之后有一个队列，客户在这里排队到柜台（线程池）办理业务。即在单线程Reactor后面接了一个线程池用于计算，可以利用多核。旋转门基本是不锁的，随时都可以进出。但是排队会消耗一点时间，相比之下，方案5中客户一进门就能立刻办理业务。另外一种做法是线程池里的每个线程有自己的任务队列，而不是整个线程池共用一个任务队列。这样的好处是避免全局队列的锁争用，坏处是计算资源有可能分配不平均，降低并行度。 方案9这间大银行相当于包含方案5中的多家小银行，每个客户进大门的时候就被固定分配到某一间小银行中，他的业务只能由这间小银行办理，他每次都要进出小银行的旋转门。但总体来看，大银行可以同时服务多个客户。这时同样要求办理业务时不能空等（阻塞），否则会影响分到同一间小银行的其他客户。而且必要的时候可以为VIP客户单独开一间或几间小银行，优先办理VIP业务。这跟方案5不同，当普通客户在办理业务的时候，VIP客户也只能在门外等着（见图6-11的右图）。这是一种适应性很强的方案，也是muduo原生的多线程IO模型。 方案11这间大银行有多个旋转门，多个柜台。旋转门和柜台之间没有一一对应关系，客户进大门的时候就被固定分配到某一旋转门中（奇怪的安排，易于实现线程安全的IO，见§4.6），进入旋转门之后，有一个队列，客户在此排队到柜台办理业务。这种方案的资源利用率可能比方案9更高，一个客户不会被同一小银行的其他客户阻塞，但延迟也比方案9略大。 应该使用几个event loop一个程序到底是使用一个event loop还是使用多个event loops呢？ ZeroMQ的手册给出的建议是，按照每千兆比特每秒的吞吐量配一个event loop的比例来设置event loop的数目，即muduo::TcpServer::setThreadNum()的参数。 依据这条经验规则，在编写运行于千兆以太网上的网络程序时，用一个event loop就足以应付网络IO。如果程序本身没有多少计算量，而主要瓶颈在网络带宽，那么可以按这条规则来办，只用一个event loop。另一方面，如果程序的IO带宽较小，计算量较大，而且对延迟不敏感，那么可以把计算放到thread pool中，也可以只用一个event loop。 值得指出的是，以上假定了TCP连接是同质的，没有优先级之分，我们看重的是服务程序的总吞吐量。但是如果TCP连接有优先级之分，那么单个event loop可能不适合，正确的做法是把高优先级的连接用单独的event loop来处理。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
        <tag>NP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reactor模式讲解]]></title>
    <url>%2F2016%2F08%2F07%2Freactor_intro%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[关于服务器模型总结请转 此文, 文中也有Reactor的讲解. 对于 IO 来说，我们听得比较多的是: BIO: 阻塞 IO NIO: 非阻塞 IO 同步 IO 异步 IO 以及其组合: 同步阻塞 IO 同步非阻塞 IO 异步阻塞 IO 异步非阻塞 IO . . . 那么什么是阻塞 IO、非阻塞 IO、同步 IO、异步 IO 呢？ 一个 IO 操作其实分成了两个步骤：发起 IO 请求和实际的 IO 操作 阻塞 IO 和非阻塞 IO 的区别在于第一步：发起 IO 请求是否会被阻塞，如果阻塞直到完成那么就是传统的阻塞 IO; 如果不阻塞，那么就是非阻塞 IO 同步 IO 和异步 IO 的区别就在于第二个步骤是否阻塞，如果实际的 IO 读写阻塞请求进程，那么就是同步 IO，因此阻塞 IO、非阻塞 IO、IO 复用、信号驱动 IO 都是同步 IO; 如果不阻塞，而是操作系统帮你做完 IO 操作再将结果返回给你，那么就是异步 IO 举个不太恰当的例子 ：比如你家网络断了，你打电话去中国电信报修！ 你拨号 — 客户端连接服务器 电话通了 — 连接建立 你说：“我家网断了, 帮我修下”— 发送消息 说完你就在那里等，那么就是阻塞 IO 如果正好你有事，你放下带电话，然后处理其他事情了，过一会你来问下，修好了没 — 那就是非阻塞 IO 如果客服说：“马上帮你处理，你稍等”— 同步 IO 如果客服说：“马上帮你处理，好了通知你”，然后挂了电话 — 异步 IO 本文只讨论 BIO 和 NIO,AIO 使用度没有前两者普及，暂不讨论！ 下面从代码层面看看 BIO 与 NIO 的流程! BIO模型图如下所示： BIO 优缺点 优点 模型简单 编码简单 缺点 性能瓶颈低 优缺点很明显。这里主要说下缺点：主要瓶颈在线程上。每个连接都会建立一个线程。虽然线程消耗比进程小，但是一台机器实际上能建立的有效线程有限，以 Java 来说，1.5 以后，一个线程大致消耗 1M 内存！且随着线程数量的增加，CPU 切换线程上下文的消耗也随之增加，在高过某个阀值后，继续增加线程，性能不增反降！而同样因为一个连接就新建一个线程，所以编码模型很简单！ 就性能瓶颈这一点，就确定了 BIO 并不适合进行高性能服务器的开发！像 Tomcat 这样的 Web 服务器，从 7 开始就从 BIO 改成了 NIO，来提高服务器性能！ NIONIO 模型示例如下： Acceptor 注册 Selector，监听 accept 事件 当客户端连接后，触发 accept 事件 服务器构建对应的 Channel，并在其上注册 Selector，监听读写事件 当发生读写事件后，进行相应的读写处理 NIO 优缺点 优点 性能瓶颈高 缺点 模型复杂 编码复杂 需处理半包问题 NIO 的优缺点和 BIO 就完全相反了! 性能高，不用一个连接就建一个线程，可以一个线程处理所有的连接！相应的，编码就复杂很多，从上面的代码就可以明显体会到了。还有一个问题，由于是非阻塞的，应用无法知道什么时候消息读完了，就存在了半包问题！ 半包问题简单看一下下面的图就能理解半包问题了！ 我们知道 TCP/IP 在发送消息的时候，可能会拆包 (如上图 1)！这就导致接收端无法知道什么时候收到的数据是一个完整的数据。例如: 发送端分别发送了 ABC,DEF,GHI 三条信息，发送时被拆成了 AB,CDRFG,H,I 这四个包进行发送，接受端如何将其进行还原呢？在 BIO 模型中，当读不到数据后会阻塞，而 NIO 中不会! 所以需要自行进行处理! 例如，以换行符作为判断依据，或者定长消息发生，或者自定义协议！ NIO 虽然性能高，但是编码复杂，且需要处理半包问题！为了方便的进行 NIO 开发，就有了 Reactor 模型! Reactor 模型 AWT Events Reactor 模型和 AWT 事件模型很像，就是将消息放到了一个队列中，通过异步线程池对其进行消费！ Reactor 中的组件 Reactor:Reactor 是 IO 事件的派发者。 Acceptor:Acceptor 接受 client 连接，建立对应 client 的 Handler，并向 Reactor 注册此 Handler。 Handler: 和一个 client 通讯的实体，按这样的过程实现业务的处理。一般在基本的 Handler 基础上还会有更进一步的层次划分， 用来抽象诸如 decode，process 和 encoder 这些过程。比如对 Web Server 而言，decode 通常是 HTTP 请求的解析， process 的过程会进一步涉及到 Listener 和 Servlet 的调用。业务逻辑的处理在 Reactor 模式里被分散的 IO 事件所打破， 所以 Handler 需要有适当的机制在所需的信息还不全（读到一半）的时候保存上下文，并在下一次 IO 事件到来的时候（另一半可读了）能继续中断的处理。为了简化设计，Handler 通常被设计成状态机，按 GoF 的 state pattern 来实现。 对应上面的 NIO 代码来看: Reactor：相当于有分发功能的 Selector Acceptor：NIO 中建立连接的那个判断分支 Handler：消息读写处理等操作类 Reactor 从线程池和 Reactor 的选择上可以细分为如下几种： Reactor 单线程模型 如果上图表达得不够明白, 还可以看看下图 如果上图还是表达得不够明白, 还可以看看下图 这个模型和上面的 NIO 流程很类似，只是将消息相关处理独立到了 Handler 中去了！ 虽然上面说到 NIO 一个线程就可以支持所有的 IO 处理。但是瓶颈也是显而易见的！我们看一个客户端的情况，如果这个客户端多次进行请求，如果在 Handler 中的处理速度较慢，那么后续的客户端请求都会被积压，导致响应变慢！所以引入了 Reactor 多线程模型! Reactor 多线程模型 如果上图表达得不够明白, 还可以看看下图 如果上图还是表达得不够明白, 还可以看看下图 Reactor 多线程模型就是将 Handler 中的 IO 操作和非 IO 操作分开，操作 IO 的线程称为 IO 线程，非 IO 操作的线程称为工作线程! 这样的话，客户端的请求会直接被丢到线程池中，客户端发送请求就不会堵塞！ 但是当用户进一步增加的时候，Reactor 会出现瓶颈！因为 Reactor 既要处理 IO 操作请求，又要响应连接请求！为了分担 Reactor 的负担，所以引入了主从 Reactor 模型! 主从 Reactor 模型 如果上图表达得不够明白, 还可以看看下图 如果上图还是表达得不够明白, 还可以看看下图 主 Reactor 用于响应连接请求，从 Reactor 用于处理 IO 操作请求！]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
        <tag>NP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新看unix网络编程的一些心得]]></title>
    <url>%2F2016%2F08%2F02%2F%E9%87%8D%E6%96%B0%E7%9C%8Bunix%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BF%83%E5%BE%97%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[老书新看, 有了许多不同的见解, 也准备拿出以前自己私人的老笔记做修正放到博客里, 加深理解. 在这个浮躁人人都能写书的时代基本要看一本书需要挑很久, 谁写的, 写得怎么样, 是否是业界经典, 都要需要一一斟酌各种查证方可, 不然看一本烂书事半功倍, 浪费生命,影响效率, 被误导跑偏, 能让人静下心来的书籍不多, 能让人看好几遍仍回味无穷的经典就更少了 陈硕说过, 学东西不要只是从网上看点大牛的博客总结就行了, 总要完整地看些相关领域的经典著作的, 系统的知识结构打下坚实的基础才能继续看有关优化效率方面的书籍, 深以为然 孟岩说, 最近不止一次听到当一个人拥有相关领域的知识基础之后就可以找一本effective*的书来研读了, 颇有道理 看书的顺序忌胶柱鼓瑟, 并不是他的目录结构怎么编排你就该怎么看的, 大多数经典书籍基本都是面面俱到, 如果直接跟着这种面向知识体系本身的编排思路去看, 容易迷失乏味而无法继续, 应该找到属于自己看书顺序 个人总结的看书顺序是先翻阅自己感兴趣的,接着仔细观察他的目录来确定他的编排思路, 然后花一到两天的时间通览全书以获得大体知识体系构架, 之后找到实战性最强的章节来实操并逐个突破实战过程中的各个知识点]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>UNP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速完成一个简易SLG游戏思路二]]></title>
    <url>%2F2016%2F07%2F30%2Fhow_to_implement_a_slg_two%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[ChatServer从登录成功就开始连接,注册一个Chat_ID,Player_ID 和 Chat_ID相互对应,会注册相应的房间频道,并为每位 Player 存了一份黑名单,在客户端做了本地黑名单,聊天服务器也做了黑名单二次验证处理. 世界频道 : 则用MsgServer的非实时推送思路 私密聊天 : 则选择 WorkerMan 的TCP, MsgServer 实时推送 : WorkerMan 的 TCP 非实时推送 : 客户端定时15秒轮询一下服务器，如果有消息就取下来，如果没消息可以逐步放长轮询时间，比如30秒；如果有消息，就缩短轮询时间到10秒，5秒， . . . DeployToolCapistrano是一个开源的部署工具, 用Ruby来写, 语法超简洁的 优点 实时则走 WorkerMan 非实时则跑 yii 访问一下，请求一下关卡数据，玩完了又提交一下，验算一下是否合法，获得什么奖励，数据库用单台 MySQL或者 MongoDB即可，后端的 Redis做缓存此类服务器用来实现一款三国类策略或者卡牌及酷跑的游戏已经绰绰有余，这类游戏因为逻辑简单，玩家之间交互不强，使用HTTP来开发的话，开发速度快，调试只需要一个浏览器就可以把逻辑调试清楚了]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>SLG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速完成一个简易SLG游戏思路一]]></title>
    <url>%2F2016%2F07%2F30%2Fhow_to_implement_a_slg_one%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[LoginServer(Gate)当完成渠道SDK回调验证之后,验证完玩家信息,用了Nginx的负载均衡给每位玩家分配一台不繁忙的游戏服务器,在Redis中存了一份玩家在线有效时间key,这个key也可以用来完成封号操作 MainServer因为弱交互/短连接的关系,大多数情况玩家和玩家之间不需要实时面对面PK，打一下对方的离线数据，计算下排行榜，排行榜是实时计算的,存在Redis里, 做了分页处理买卖下道具即可. 所以 MainServer 选择了: yii Redis MySQL Nginx . . . BattleServer而 BattleServer 则选择 WorkerMan 的TCP模式,实时交互数据,MainServer 和 BattleServer 之间通信并接同一个数据库. 其实这里可以不选择用TCP, 而自撸一个可靠UDP来提高效率]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>SLG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏四之延迟补偿实现爆头]]></title>
    <url>%2F2016%2F07%2F19%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E5%9B%9B%E4%B9%8B%E5%BB%B6%E8%BF%9F%E8%A1%A5%E5%81%BF%E5%AE%9E%E7%8E%B0%E7%88%86%E5%A4%B4%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part IV): Lag Compensation Introduction The previous three articles explained a client-server game architecture which can be summarized as follows:Server gets inputs from all the clients, with timestampsServer processes inputs and updates world statusServer sends regular world snapshots to all clientsClient sends input and simulates their effects locallyClient get world updates andSyncs predicted state to authoritative stateInterpolates known past states for other entitiesFrom a player’s point of view, this has two important consequences:Player sees himself in the presentPlayer sees other entities in the pastThis situation is generally fine, but it’s quite problematic for very time- and space-sensitive events; for example, shooting your enemy in the head!Lag CompensationSo you’re aiming perfectly at the target’s head with your sniper rifle. You shoot - it’s a shot you can’t miss.But you miss.Why does this happen?Because of the client-server architecture explained before, you were aiming at where the enemy’s head was 100ms before you shot - not when you shot!In a way, it’s like playing in an universe where the speed of light is really, really slow; you’re aiming at the past position of your enemy, but he’s long gone by the time you squeeze the trigger.Fortunately, there’s a relatively simple solution for this, which is also pleasant for most players most of the time (with the one exception discussed below).Here’s how it works:When you shoot, client sends this event to the server with full information: the exact timestamp of your shot, and the exact aim of the weapon.Here’s the crucial step. Since the server gets all the input with timestamps, it can authoritatively reconstruct the world at any instant in the past. In particular, it can reconstruct the world exactly as it looked like to any client at any point in time.This means the server can know exactly what was on your weapon’s sights the instant you shot. It was the past position of your enemy’s head, but the server knows it was the position of his head in your present.The server processes the shot at that point in time, and updates the clients.And everyone is happy!The server is happy because he’s the server. He’s always happy.You’re happy because you were aiming at your enemy’s head, shot, and got a rewarding headshot!The enemy may be the only one not entirely happy. If he was standing still when he got shot, it’s his fault, right? If he was moving… wow, you’re a really awesome sniper.But what if he was in an open position, got behind a wall, and then got shot, a fraction of a second later, when he thought he was safe?Well, that can happen. That’s the tradeoff you make. Because you shoot at him in the past, he may still be shot up to a few milliseconds after he took cover.It is somewhat unfair, but it’s the most agreeable solution for everyone involved. It would be much worse to miss an unmissable shot!ConclusionThis ends my series on Fast-paced Multiplayer. This kind of thing is clearly tricky to get right, but with a clear conceptual understanding about what’s going on, it’s not exceedingly difficult.Although the audience of these articles were game developers, it found another group of interested readers: gamers! From a gamer point of view it’s also interesting to understand why some things happen the way they happen.Further ReadingAs clever as these techniques are, I can’t claim any credit for them; these articles are just an easy to understand guide to some concepts I’ve learned from other sources, including articles and source code, and some experimentation.]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏三之实体插值]]></title>
    <url>%2F2016%2F07%2F18%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%B8%89%E4%B9%8B%E5%AE%9E%E4%BD%93%E6%8F%92%E5%80%BC%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part III): Entity Interpolation Introduction In the first article of the series, we introduced the concept of an authoritative server and its usefulness to prevent client cheats. However, using this technique naively can lead to potentially showstopper issues regarding playability and responsiveness. In the second article, we proposed client-side prediction as a way to overcome these limitations.The net result of these two articles is a set of concepts and techniques that allow a player to control an in-game character in a way that feels exactly like a single-player game, even when connected to an authoritative server through an internet connection with transmission delays.In this article, we’ll explore the consequences of having other player-controled characters connected to the same server.Server time stepIn the previous article, the behavior of the server we described was pretty simple – it read client inputs, updated the game state, and sent it back to the client. When more than one client is connected, though, the main server loop is somewhat different.In this scenario, several clients may be sending inputs simultaneously, and at a fast pace (as fast as the player can issue commands, be it pressing arrow keys, moving the mouse or clicking the screen). Updating the game world every time inputs are received from each client and then broadcasting the game state would consume too much CPU and bandwidth.A better approach is to queue the client inputs as they are received, without any processing. Instead, the game world is updated periodically at low frequency, for example 10 times per second. The delay between every update, 100ms in this case, is called thetime step. In every update loop iteration, all the unprocessed client input is applied (possibly in smaller time increments than the time step, to make physics more predictable), and the new game state is broadcast to the clients.In summary, the game world updates independent of the presence and amount of client input, at a predictable rate.Dealing with low-frequency updatesFrom the point of view of a client, this approach works as smoothly as before – client-side prediction works independently of the update delay, so it clearly also works under predictable, if relatively infrequent, state updates. However, since the game state is broadcast at a low frequency (continuing with the example, every 100ms), the client has very sparse information about the other entities that may be moving throughout the world.A first implementation would update the position of other characters when it receives a state update; this immediately leads to very choppy movement, that is, discrete jumps every 100ms instead of smooth movement.Client 1 as seen by Client 2.Depending on the type of game you’re developing there are many ways to deal with this; in general, the more predictable your game entities are, the easier it is to get it right.Dead reckoningSuppose you’re making a car racing game. A car that goes really fast is pretty predictable – for example, if it’s running at 100 meters per second, a second later it will be roughly 100 meters ahead of where it started.Why “roughly”? During that second the car could have accelerated or decelerated a bit, or turned to the right or to the left a bit – the key word here is “a bit”. The maneuverability of a car is such that at high speeds its position at any point in time is highly dependent on its previous position, speed and direction, regardless of what the player actually does. In other words, a racing car can’t do a 180º turn instantly.How does this work with a server that sends updates every 100 ms? The client receives authoritative speed and heading for every competing car; for the next 100 ms it won’t receive any new information, but it still needs to show them running. The simplest thing to do is to assume the car’s heading and acceleration will remain constant during that 100 ms, and run the car physics locally with that parameters. Then, 100 ms later, when the server update arrives, the car’s position is corrected.The correction can be big or relatively small depending on a lot of factors. If the player does keep the car on a straight line and doesn’t change the car speed, the predicted position will be exactly like the corrected position. On the other hand, if the player crashes against something, the predicted position will be extremely wrong.Note that dead reckoning can be applied to low-speed situations – battleships, for example. In fact, the term “dead reckoning” has its origins in marine navigation.Entity interpolationThere are some situations where dead reckoning can’t be applied at all – in particular, all scenarios where the player’s direction and speed can change instantly. For example, in a 3D shooter, players usually run, stop, and turn corners at very high speeds, making dead reckoning essentially useless, as positions and speeds can no longer be predicted from previous data.You can’t just update player positions when the server sends authoritative data; you’d get players who teleport short distances every 100 ms, making the game unplayable.What you do have is authoritative position data every 100 ms; the trick is how to show the player what happens inbetween. The key to the solution is to show the other players in the past relative to the user’s player.Say you receive position data at t = 1000. You already had received data at t = 900, so you know where the player was at t = 900 and t = 1000. So, from t = 1000 and t = 1100, you show what the other player did from t = 900 to t = 1000. This way you’re always showing the user actual movement data, except you’re showing it 100 ms “late”.Client 2 renders Client 1 “in the past”, interpolating last known positions.The position data you use to interpolate from t = 900 to t = 1000 depends on the game. Interpolation usually works well enough. If it doesn’t, you can have the server send more detailed movement data with each update – for example, a sequence of straight segments followed by the player, or positions sampled every 10 ms which look better when interpolated (you don’t need to send 10 times more data – since you’re sending deltas for small movements, the format on the wire can be heavily optimized for this particular case).Note that using this technique, every player sees a slightly different rendering of the game world, because each player sees itself in the present but sees the other entities in the past. Even for a fast paced game, however, seeing other entities with a 100 ms isn’t generally noticeable.There are exceptions – when you need a lot of spatial and temporal accuracy, such as when the player shoots at another player. Since the other players are seen in the past, you’re aiming with a 100 ms delay – that is, you’re shooting where your target was 100 ms ago! We’ll deal with this in the next article.SummaryIn a client-server environment with an authoritative server, infrequent updates and network delay, you must still give players the illusion of continuity and smooth movement. In part 2 of the series we explored a way to show the user controlled player’s movement in real time using client-side prediction and server reconciliation; this ensures user input has an immediate effect on the local player, removing a delay that would render the game unplayable.Other entities are still a problem, however. In this article we explored two ways of dealing with them.The first one, dead reckoning, applies to certain kinds of simulations where entity position can be acceptably estimated from previous entity data such as position, speed and acceleration. This approach fails when these conditions aren’t met.The second one, entity interpolation, doesn’t predict future positions at all – it uses only real entity data provided by the server, thus showing the other entities slightly delayed in time.The net effect is that the user’s player is seen in the present and the other entities are seen in the past. This usually creates an incredibly seamless experience.However, if nothing else is done, the illusion breaks down when an event needs high spatial and temporal accuracy, such as shooting at a moving target: the position where Client 2 renders Client 1 doesn’t match the server’s nor Client 1′s position, so headshots become impossible! Since no game is complete without headshots, we’ll deal with this issue in the next article.]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏二之客户端预测与服务器修正]]></title>
    <url>%2F2016%2F07%2F17%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%BA%8C%E4%B9%8B%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%A2%84%E6%B5%8B%E4%B8%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BF%AE%E6%AD%A3%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part II): Client-Side Prediction and Server Reconciliation Introduction In the first article of this series, we explored a client-server model with an authoritative server and dumb clients that just send inputs to the server and then render the updated game state when the server sends it.A naive implementation of this scheme leads to a delay between user commands and changes on the screen; for example, the player presses the right arrow key, and the character takes half a second before it starts moving. This is because the client input must first travel to the server, the server must process the input and calculate a new game state, and the updated game state must reach the client again.Effect of network delays.In a networked environment such as the internet, where delays can be in the orders of tenths of a second, a game may feel unresponsive at best, or in the worst case, be rendered unplayable. In this article, we’ll find ways to minimize or even eliminate that problem.Client-side predictionEven though there are some cheating players, most of the time the game server is processing valid requests (from non-cheating clients and from cheating clients who aren’t cheating at that particular time). This means most of the input received will be valid and will update the game state as expected; that is, if your character is at (10, 10) and the right arrow key is pressed, it will end up at (11, 10).We can use this to our advantage. If the game world is deterministic enough (that is, given a game state and a set of inputs, the result is completely predictable),Let’s suppose we have a 100 ms lag, and the animation of the character moving from one square to the next takes 100 ms. Using the naive implementation, the whole action would take 200 ms:Network delay + animation.Since the world is deterministic, we can assume the inputs we send to the server will be executed successfully. Under this assumption, the client can predict the state of the game world after the inputs are processed, and most of the time this will be correct.Instead of sending the inputs and waiting for the new game state to start rendering it, we can send the input and start rendering the outcome of that inputs as if they had succeded, while we wait for the server to send the “true” game state – which more often than not, will match the state calculated locally :Animation plays while the server confirms the action.Now there’s absolutely no delay between the player’s actions and the results on the screen, while the server is still authoritative (if a hacked client would send invalid inputs, it could render whatever it wanted on the screen, but it wouldn’t affect the state of the server, which is what the other players see).Synchronization issuesIn the example above, I chose the numbers carefully to make everything work fine. However, consider a slightly modified scenario: let’s say we have a 250 ms lag to the server, and moving from a square to the next takes 100 ms. Let’s also say the player presses the right key 2 times in a row, trying to move 2 squares to the right.Using the techniques so far, this is what would happen:Predicted state and authoritative state mismatch.We run into an interesting problem at t = 250 ms, when the new game state arrives. The predicted state at the client is x = 12, but the server says the new game state is x = 11. Because the server is authoritative, the client must move the character back to x = 11. But then, a new server state arrives at t = 350, which says x = 12, so the character jumps again, forward this time.From the point of view of the player, he pressed the right arrow key twice; the character moved two squares to the right, stood there for 50 ms, jumped one square to the left, stood there for 100 ms, and jumped one square to the right. This, of course, is unacceptable.Server reconciliationThe key to fix this problem is to realize that the client sees the game world in present time, but because of lag, the updates it gets from the server are actually the state of the game in the past. By the time the server sent the updated game state, it hadn’t processed all the commands sent by the client.This isn’t terribly difficult to work around, though. First, the client adds a sequence number to each request; in our example, the first key press is request #1, and the second key press is request #2. Then, when the server replies, it includes the sequence number of the last input it processed:Client-side prediction + server reconciliation.或许这幅图更加解释得清楚些Now, at t = 250, the server says “based on what I’ve seen up to your request #1, your position is x = 11”. Because the server is authoritative, it sets the character position at x = 11. Now let’s assume the client keeps a copy of the requests it sends to the server. Based on the new game state, it knows the server has already processed request #1, so it can discard that copy. But it also knows the server still has to send back the result of processing request #2. So applying client-side prediction again, the client can calculate the “present” state of the game based on the last authoritative state sent by the server, plus the inputs the server hasn’t processed yet.So, at t = 250, the client gets “x = 11, last processed request = #1”. It discards its copies of sent input up to #1 – but it retains a copy of #2, which hasn’t been acknowledged by the server. It updates it internal game state with what the server sent, x = 11, and then applies all the input still not seen by the server – in this case, input #2, “move to the right”. The end result is x = 12, which is correct.Continuing with our example, at t = 350 a new game state arrives from the server; this time it says “x = 12, last processed request = #2”. At this point, the client discards all input up to #2, and updates the state with x = 12. There’s no unprocessed input to replay, so processing ends there, with the correct result.Odds and endsThe example discussed above implies movement, but the same principle can be applied to almost anything else. For example, in a turn-based combat game, when the player attacks another character, you can show blood and a number representing the damage done, but you shouldn’t actually update the health of the character until the server says so.Because of the complexities of game state, which isn’t always easily reversible, you may want to avoid killing a character until the server says so, even if its health dropped below zero in the client’s game state (what if the other character used a first-aid kit just before receiving your deadly attack, but the server hasn’t told you yet?)This brings us to an interesting point – even if the world is completely deterministic and no clients cheat at all, it’s still possible that the state predicted by the client and the state sent by the server don’t match after a reconciliation. The scenario is impossible as described above with a single player, but it’s easy to run into when several players are connected to the server at once. This will be the topic of the next article.SummaryWhen using an authoritative server, you need to give the player the illusion of responsiveness, while you wait for the server to actually process your inputs. To do this, the client simulates the results of the inputs. When the updated server state arrives, the predicted client state is recomputed from the updated state and the inputs the client sent but the server hasn’t acknowledged yet.]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多人快节奏游戏一之C/S游戏架构]]></title>
    <url>%2F2016%2F07%2F16%2F%E5%A4%9A%E4%BA%BA%E5%BF%AB%E8%8A%82%E5%A5%8F%E6%B8%B8%E6%88%8F%E4%B8%80%E4%B9%8BCS%E6%B8%B8%E6%88%8F%E6%9E%B6%E6%9E%84%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文出处 Fast-Paced Multiplayer (Part I): Client-Server Game Architecture Introduction This is the first in a series of articles exploring the techniques and algorithms that make fast-paced multiplayer games possible. If you’re familiar with the concepts behind multiplayer games, you can safely skip to the next article – what follows is an introductory discussion.Developing any kind of game is itself challenging; multiplayer games, however, add a completely new set of problems to be dealt with. Interestingly enough, the core problems are human nature and physics!The problem of cheatingIt all starts with cheating.As a game developer, you usually don’t care whether a player cheats in your single-player game – his actions don’t affect anyone but him. A cheating player may not experience the game exactly as you planned, but since it’s their game, they have the right to play it in any way they please.Multiplayer games are different, though. In any competitive game, a cheating player isn’t just making the experience better for himself, he’s also making the experience worse for the other players. As the developer, you probably want to avoid that, since it tends to drive players away from your game.There are many things that can be done to prevent cheating, but the most important one (and probably the only really meaningful one) is simple : don’t trust the player. Always assume the worst – that players will try to cheat.Authoritative servers and dumb clientsThis leads to a seemingly simple solution – you make everything in your game happen in a central server under your control, and make the clients just privileged spectators of the game. In other words, your game client sends inputs (key presses, commands) to the server, the server runs the game, and you send the results back to the clients. This is usually called using an authoritative server, because the one and only authority regarding everything that happens in the world is the server.Of course, your server can be exploited for vulnerabilities, but that’s out of the scope of this series of articles. Using an authoritative server does prevent a wide range of hacks, though. For example, you don’t trust the client with the health of the player; a hacked client can modify its local copy of that value and tell the player it has 10000% health, but the server knows it only has 10% – when the player is attacked it will die, regardless of what a hacked client may think.You also don’t trust the player with its position in the world. If you did, a hacked client would tell the server “I’m at (10,10)” and a second later “I’m at (20,10)”, possibly going through a wall or moving faster than the other players. Instead, the server knows the player is at (10,10), the client tells the server “I want to move one square to the right”, the server updates its internal state with the new player position at (11,10), and then replies to the player “You’re at (11, 10)”:Effect of network delays.In summary: the game state is managed by the server alone. Clients send their actions to the server. The server updates the game state periodically, and then sends the new game state back to clients, who just render it on the screen.Dealing with networksThe dumb client scheme works fine for slow turn based games, for example strategy games or poker. It would also work in a LAN setting, where communications are, for all practical purposes, instantaneous. But this breaks down when used for a fast-paced game over a network such as the internet.Let’s talk physics. Suppose you’re in San Francisco, connected to a server in the NY. That’s approximately 4,000 km, or 2,500 miles (that’s roughly the distance between Lisbon and Moscow). Nothing can travel faster than light, not even bytes on the Internet (which at the lower level are pulses of light, electrons in a cable, or electromagnetic waves). Light travels at approximately 300,000 km/s, so it takes 13 ms to travel 4,000 km.This may sound quite fast, but it’s actually a very optimistic setup – it assumes data travels at the speed of light in a straight path, with is most likely not the case. In real life, data goes through a series of jumps (called hops in networking terminology) from router to router, most of which aren’t done at lightspeed; routers themselve introduce a bit of delay, since packets must be copied, inspected, and rerouted.For the sake of the argument, let’s assume data takes 50 ms from client to server. This is close to a best-case scenario – what happens if you’re in NY connected to a server in Tokyo? What if there’s network congestion for some reason? Delays of 100, 200, even 500 ms are not unheard of.Back to our example, your client sends some input to the server (“I pressed the right arrow”). The server gets it 50 ms later. Let’s say the server processes the request and sends back the updated state immediately. Your client gets the new game state (“You’re now at (1, 0)”) 50 ms later.From your point of view, what happened is that you pressed the right arrow but nothing happened for a tenth of a second; then your character finally moved one square to the right. This perceived lag between your inputs and its consequences may not sound like much, but it’s noticeable – and of course, a lag of half a second isn’t just noticeable, it actually makes the game unplayable.SummaryNetworked multiplayer games are incredibly fun, but introduce a whole new class of challenges. The authoritative server architecture is pretty good at stopping most cheats, but a straightforward implementation may make games quite unresponsive to the player.In the following articles, we’ll explore how can we build a system based on an authoritative server, while minimizing the delay experienced by the players, to the point of making it almost indistinguishable from local or single player games.]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>GabrielGambetta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏服务端常用架构三]]></title>
    <url>%2F2016%2F07%2F11%2F%E6%B8%B8%E6%88%8F%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%B8%B8%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B8%89%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[休闲游戏服务器休闲游戏同战网服务器类似，都是全区架构，不同的是有房间服务器，还有具体的游戏服务器，游戏主体不再以玩家 P2P进行，而是连接到专门的游戏服务器处理： 和战网一样的全区架构，用户数据不能象分区的 RPG那样一次性load到内存，然后在内存里面直接修改。全区架构下，为了应对一个用户同时玩几个游戏，用户数据需要区分基本数据和不同的游戏数据，而游戏数据又需要区分积分数据、和文档数据。胜平负之类的积分可以直接提交增量修改，而更为普遍的文档类数据则需要提供读写令牌，写令牌只有一块，读令牌有很多块。同帐号同一个游戏同时在两台电脑上玩时，最先开始的那个游戏获得写令牌，可以操作任意的用户数据。而后开始的那个游戏除了可以提交胜平负积分的增量改变外，对用户数据采用只读的方式，保证游戏能运行下去，但是会提示用户，游戏数据锁定。 . . . 现代动作类网游从早期的韩国动作游戏开始，传统的战网动作类游戏和 RPG游戏开始尝试融合。单纯的动作游戏玩家容易疲倦，留存也没有 RPG那么高；而单纯 RPG战斗却又慢节奏的乏味，无法满足很多玩家激烈对抗的期望，于是二者开始融合成为新一代的：动作 + 城镇模式。玩家在城镇中聚集，然后以开副本的方式几个人出去以动作游戏的玩法来完成各种 RPG任务。本质就是一套 RPG服务端+副本服务端。由于每次副本时人物可以控制在8人以内，因此可以获得更为实时的游戏体验，让玩家玩的更加爽快。 说了那么多的游戏服务器类型，其实也差不多了，剩下的类型大家拼凑一下其实也就是这个样子而已。游戏服务端经历了那么多结构上的变迁，内部开发模式是否依然不变？究竟是继续延续传统的开发方式？还是有了更多突破性的方法？经历那么多次架构变迁，后面是否有共通的逻辑？未来的发展还会存在哪些困难？游戏服务端开发如何达到最终的彼岸？]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
        <tag>GS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏服务端常用架构二]]></title>
    <url>%2F2016%2F07%2F11%2F%E6%B8%B8%E6%88%8F%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%B8%B8%E7%94%A8%E6%9E%B6%E6%9E%84%E4%BA%8C%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[第三代游戏服务器 2007从魔兽世界开始无缝世界地图已经深入人心，比较以往游戏玩家走个几步还需要切换场景，每次切换就要等待 LOADING个几十秒是一件十分破坏游戏体验的事情。于是对于 2005年以后的大型 MMORPG来说，无缝地图已成为一个标准配置。比较以往按照地图来切割游戏而言，无缝世界并不存在一块地图上面的人有且只由一台服务器处理了： 每台 Node服务器用来管理一块地图区域，由 NodeMaster（NM）来为他们提供总体管理。更高层次的 World则提供大陆级别的管理服务。这里省略若干细节服务器，比如传统数据库前端，登录服务器，日志和监控等，统统用 ADMIN概括。在这样的结构下，玩家从一块区域走向另外一块区域需要简单处理一下： 玩家1完全由节点A控制，玩家3完全由节点B控制。而处在两个节点边缘的2号玩家，则同时由A和B提供服务。玩家2从A移动到B的过程中，会同时向A请求左边的情况，并向B请求右边的情况。但是此时玩家2还是属于A管理。直到玩家2彻底离开AB边界很远，才彻底交由B管理。按照这样的逻辑将世界地图分割为一块一块的区域，交由不同的 Node去管理。 对于一个 Node所负责的区域，地理上没必要连接在一起，比如大陆的四周边缘部分和高山部分的区块人比较少，可以统一交给一个Node去管理，而这些区块在地理上并没有联系在一起的必要性。一个 Node到底管理哪些区块，可以根据游戏实时运行的负载情况，定时维护的时候进行更改 NodeMaster 上面的配置。 于是碰到第一个问题是很多 Node服务器需要和玩家进行通信，需要问管理服务器特定UID为多少的玩家到底在哪台 Gate上，以前按场景切割的服务器这个问题不大，问了一次以后就可以缓存起来了，但是现在服务器种类增加不少，玩家又会飘来飘去，按UID查找玩家比较麻烦；另外一方面 GATE需要动态根据坐标计算和哪些 Node通信，导致逻辑越来越厚，于是把：“用户对象”从负责连接管理的 GATE中切割出来势在必行于是有了下面的模型： . . . 网关服务器再次退回到精简的网络转发功能，而用户逻辑则由按照 UID划分的 OBJ服务器来承担，GATE是按照网络接入时的负载来分布，而 OBJ则是按照资源的编号（UID）来分布，这样和一个用户通信直接根据 UID计算出 OBJ服务器编号发送数据即可。而新独立出来的 OBJ则提供了更多高层次的服务： • 对象移动：管理具体玩家在不同的 Node所管辖的区域之间的移动，并同需要的 Node进行沟通。• 数据广播：Node可以给每个用户设置若干 TAG，然后通知 Object Master 按照TAG广播。• 对象消息：通用消息推送，给某个用户发送数据，直接告诉 OBJ，不需要直接和 GATE打交道。• 好友聊天：角色之间聊天直接走 OBJ/OBJ MASTER。整个服务器主体分为三层以后，NODE专注场景，OBJ专注玩家对象，GATE专注网络。这样的模型在无缝场景服务器中得到广泛的应用。但是随着时间的推移，负载问题也越来越明显，做个活动，远来不活跃的区域变得十分活跃，靠每周维护来调整还是比较笨重的，于是有了动态负载均衡。 动态负载均衡有两种方法，第一种是按照负载，由 Node Master 定时动态移动修改一下各个 Node的边界，而不同的玩家对象按照先前的方法从一台 Node上迁移到另外一台 Node上： 图11 动态负载均衡 这样 Node Master定时查找地图上的热点区域，计算新的场景切割方式，然后告诉其他服务器开始调整，具体处理方式还是和上面对象跨越边界移动的方法一样。 但是上面这种方式实现相对复杂一些，于是人们设计出了更为简单直接的一种新方法： 图12 基于网格的动态负载均衡 还是将地图按照标准尺寸均匀切割成静态的网格，每个格子由一个具体的Node负责，但是根据负载情况，能够实时的迁移到其他 Node上。在迁移分为三个阶段：准备，切换，完成。三个状态由Node Master负责维护。准备阶段新的 Node开始同步老 Node上面该网格的数据，完成后告诉NM；NM确认OK后同时通知新旧 Node完成切换。完成切换后，如果 Obj服务器还在和老的 Node进行通信，老的 Node将会对它进行纠正，得到纠正的 OBJ将修正自己的状态，和新的 Node进行通信。 很多无缝动态负载均衡的服务端宣称自己支持无限的人数，但不意味着 MMORPG游戏的人数上限真的可以无限扩充，因为这样的体系会受制于网络带宽和客户端性能。带宽决定了同一个区域最大广播上限，而客户端性能决定了同一个屏幕到底可以绘制多少个角色。 从无缝地图引入了分布式对象模型开始，已经完全脱离 MUDOS体系，成为一种新的服务端模型。又由于动态负载均衡的引入，让无缝服务器如虎添翼，容纳着超过上一代游戏服务器数倍的人数上限，并提供了更好的游戏体验，我们称其为第三代游戏服务端架构。网游以大型多人角色扮演为开端，RPG网游在相当长的时间里一度占据90%以上，使得基于 MMORPG的服务端架构得到了蓬勃的发展，然而随着玩家对RPG的疲惫，各种非MMORPG游戏如雨后春笋般的出现在人们眼前，受到市场的欢迎。 战网游戏服务器经典战网服务端和 RPG游戏有两个区别：RPG是分区分服的，北京区的用户和广州区的用户老死不相往来。而战网，虽然每局游戏一般都是 8人以内，但全国只有一套服务器，所有的玩家都可以在一起游戏，而玩家和玩家之使用 P2P的方式连接在一起，组成一局游戏： 玩家通过 Match Making 服务器使用：创建、加入、自动匹配、邀请等方式组成一局游戏。服务器会选择一个人做 Host，其他人 P2P连接到做主的玩家上来。STUN是帮助玩家之间建立 P2P的牵引服务器，而由于 P2P联通情况大概只有 75%，实在联不通的玩家会通过 Forward进行转发。 大量的连接对战，体育竞技游戏采用类似的结构。P2P有网状模型（所有玩家互相连接），和星状模型（所有玩家连接一个主玩家）。复杂的游戏状态在网状模型下难以形成一致，因此星状P2P模型经受住了历史的考验。除去游戏数据，支持语音的战网系统也会将所有人的语音数据发送到做主的那个玩家机器上，通过混音去重再编码的方式返回给所有用户。 战网类游戏，以竞技、体育、动作等类型的游戏为主，较慢节奏的 RPG（包括ARPG）有本质上的区别，而激烈的游戏过程必然带来到较 RPG复杂的多的同步策略，这样的同步机制往往带来的是很多游戏结果由客户端直接计算得出，那在到处都是破解的今天，如何保证游戏结果的公正呢？ 主要方法就是投票法，所有客户端都会独立计算，然后传递给服务器。如果结果相同就更新记录，如果结果不一致，会采取类似投票的方式确定最终结果。同时记录本剧游戏的所有输入，在可能的情况下，找另外闲散的游戏客户端验算整局游戏是否为该结果。并且记录经常有作弊嫌疑的用户，供运营人员封号时参考。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
        <tag>GS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏服务端常用架构一]]></title>
    <url>%2F2016%2F07%2F11%2F%E6%B8%B8%E6%88%8F%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%B8%B8%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B8%80%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[卡牌、跑酷等弱交互服务端卡牌跑酷类因为交互弱，玩家和玩家之间不需要实时面对面PK，打一下对方的离线数据，计算下排行榜，买卖下道具即可，所以实现往往使用简单的 HTTP服务器： 登录时可以使用非对称加密（RSA, DH），服务器根据客户端uid，当前时间戳还有服务端私钥，计算哈希得到的加密 key 并发送给客户端。之后双方都用 HTTP通信，并用那个key进行RC4加密。客户端收到key和时间戳后保存在内存，用于之后通信，服务端不需要保存 key，因为每次都可以根据客户端传上来的 uid 和时间戳以及服务端自己的私钥计算得到。用模仿 TLS的行为，来保证多次 HTTP请求间的客户端身份，并通过时间戳保证同一人两次登录密钥不同。 每局开始时，访问一下，请求一下关卡数据，玩完了又提交一下，验算一下是否合法，获得什么奖励，数据库用单台 MySQL或者 MongoDB即可，后端的 Redis做缓存（可选）。如果要实现通知，那么让客户端定时15秒轮询一下服务器，如果有消息就取下来，如果没消息可以逐步放长轮询时间，比如30秒；如果有消息，就缩短轮询时间到10秒，5秒，即便两人聊天，延迟也能自适应。 此类服务器用来实现一款三国类策略或者卡牌及酷跑的游戏已经绰绰有余，这类游戏因为逻辑简单，玩家之间交互不强，使用 HTTP来开发的话，开发速度快，调试只需要一个浏览器就可以把逻辑调试清楚了。 第一代游戏服务器 19781978年，英国著名的财经学校University of Essex的学生 Roy Trubshaw编写了世界上第一个MUD程序《MUD1》，在University of Essex于1980年接入 ARPANET之后加入了不少外部的玩家，甚至包括国外的玩家。《MUD1》程序的源代码在 ARPANET共享之后出现了众多的改编版本，至此MUD才在全世界广泛流行起来。不断完善的 MUD1的基础上产生了开源的 MudOS（1991），成为众多网游的鼻祖： MUDOS采用 C语言开发，因为玩家和玩家之间有比较强的交互（聊天，交易，PK），MUDOS使用单线程无阻塞套接字来服务所有玩家，所有玩家的请求都发到同一个线程去处理，主线程每隔1秒钟更新一次所有对象（网络收发，更新对象状态机，处理超时，刷新地图，刷新NPC）。 . . . 游戏世界采用房间的形式组织起来，每个房间有东南西北四个方向可以移动到下一个房间，由于欧美最早的网游都是地牢迷宫形式的，因此场景的基本单位被成为 “房间”。MUDOS使用一门称为LPC的脚本语言来描述整个世界（包括房间拓扑，配置，NPC，以及各种剧情）。游戏里面的高级玩家（巫师），可以不断的通过修改脚本来为游戏添加房间以及增加剧情。早年 MUD1上线时只有17个房间，Roy Trubshaw毕业以后交给他的师弟 Richard Battle，在 Richard Battle手上，不断的添加各种玩法到一百多个房间，终于让 MUD发扬光大。 用户使用 Telnet之类的客户端用 Tcp协议连接到 MUDOS上，使用纯文字进行游戏，每条指令用回车进行分割。比如 1995年国内第一款 MUD游戏《侠客行》，你敲入：”go east”，游戏就会提示你：“后花园 - 这里是归云庄的后花园，种满了花草，几个庄丁正在浇花。此地乃是含羞草生长之地。这里唯一的出口是 north。这里有：花待阿牧（A mu），还有二位庄丁（Zhuang Ding）”，然后你继续用文字操作，查看阿牧的信息：“look a mu”，系统提示：“花待阿牧（A mu）他是陆乘风的弟子，受命在此看管含羞草。他看起来三十多岁，生得眉清目秀，端正大方，一表人才。他的武艺看上去【不是很高】，出手似乎【极轻】”。然后你可以选择击败他获得含羞草，但是你吃了含羞草却又可能会中毒死亡。在早期网上资源贫乏的时候，这样的游戏有很强的代入感。 用户数据保存在文件中，每个用户登录时，从文本文件里把用户的数据全部加载进来，操作全部在内存里面进行，无需马上刷回磁盘。用户退出了，或者每隔5分钟检查到数据改动了，都会保存会磁盘。这样的系统在当时每台服务器承载个4000人同时游戏，不是特别大的问题。从1991年的 MUDOS发布后，全球各地都在为他改进，扩充，退出新版本，随着 Windows图形机能的增强。1997游戏《UO》在 MUDOS的基础上为角色增加的x,y坐标，为每个房间增加了地图，并且为每个角色增加了动画，形成了第一代的图形网络游戏。 因为游戏内容基本可以通过 LPC脚本进行定制，所以MUDOS也成为名副其实的第一款服务端引擎，引擎一次性开发出来，然后制作不同游戏内容。后续国内的《万王之王》等游戏，很多都是跟《UO》一样，直接在 MUDOS上进行二次开发，加入房间的地图还有角色的坐标等要素，该架构一直为国内的第一代 MMORPG提供了稳固的支持，直到 2003年，还有游戏基于 MUDOS开发。 虽然后面图形化增加了很多东西，但是这些MMORPG后端的本质还是 MUDOS。随着游戏内容的越来越复杂，架构变得越来越吃不消了，各种负载问题慢慢浮上水面，于是有了我们的第二代游戏服务器。 类型3：第二代游戏服务器 2003 2000年后，网游已经脱离最初的文字MUD，进入全面图形化年代。最先承受不住的其实是很多小文件，用户上下线，频繁的读取写入用户数据，导致负载越来越大。随着在线人数的增加和游戏数据的增加，服务器变得不抗重负。同时早期 EXT磁盘分区比较脆弱，稍微停电，容易发生大面积数据丢失。因此第一步就是拆分文件存储到数据库去。 此时游戏服务端已经脱离陈旧的 MUDOS体系，各个公司在参考 MUDOS结构的情况下，开始自己用 C在重新开发自己的游戏服务端。并且脚本也抛弃了 LPC，采用扩展性更好的 Python或者 Lua来代替。由于主逻辑使用单线程模型，随着游戏内容的增加，传统单服务器的结构进一步成为瓶颈。于是有人开始拆分游戏世界，变为下面的模型： 游戏服务器压力拆分后得意缓解，但是两台游戏服务器同时访问数据库，大量重复访问，大量数据交换，使得数据库成为下一个瓶颈。于是形成了数据库前端代理（DB Proxy），游戏服务器不直接访问数据库而是访问代理，再有代理访问数据库，同时提供内存级别的cache。早年 MySQL4之前没有提供存储过程，这个前端代理一般和 MySQL跑在同一台上，它转化游戏服务器发过来的高级数据操作指令，拆分成具体的数据库操作，一定程度上代替了存储过程： 但是这样的结构并没有持续太长时间，因为玩家切换场景经常要切换连接，中间的状态容易错乱。而且游戏服务器多了以后，相互之间数据交互又会变得比较麻烦，于是人们拆分了网络功能，独立出一个网关服务 Gate（有的地方叫 Session，有的地方叫 LinkSvr之类的，名字不同而已）： 把网络功能单独提取出来，让用户统一去连接一个网关服务器，再有网关服务器转发数据到后端游戏服务器。而游戏服务器之间数据交换也统一连接到网管进行交换。这样类型的服务器基本能稳定的为玩家提供游戏服务，一台网关服务1-2万人，后面的游戏服务器每台服务5k-1w，依游戏类型和复杂度不同而已，图中隐藏了很多不重要的服务器，如登录和管理。这是目前应用最广的一个模型，到今天任然很多新项目会才用这样的结构来搭建。 人都是有惯性的，按照先前的经验，似乎把 MUDOS拆分的越开性能越好。于是大家继续想，网关可以拆分呀，基础服务如聊天交易，可以拆分呀，还可以提供web接口，数据库可以拆分呀，于是有了下面的模型： 这样的模型好用么？确实有成功游戏使用类似这样的架构，并且发挥了它的性能优势，比如一些大型 MMORPG。但是有两个挑战：每增加一级服务器，状态机复杂度可能会翻倍，导致研发和找bug的成本上升；并且对开发组挑战比较大，一旦项目时间吃紧，开发人员经验不足，很容易弄挂。 比如我见过某上海一线游戏公司的一个 RPG上来就要上这样的架构，我看了下他们团队成员的经验，问了下他们的上线日期，劝他们用前面稍微简单一点的模型。人家自信得很，认为有成功项目是这么做的，他们也要这么做，自己很想实现一套。于是他们义无反顾的开始编码，项目做了一年多，然后，就没有然后了。 现今在游戏成功率不高的情况下，一开始上一套比较复杂的架构需要考虑投资回报率，比如你的游戏上线半年内 PCU会去到多少？如果一个 APRG游戏，每组服务器5千人都到不了的话，那么选择一套更为贴近实际情况的结构更为经济。即使后面你的项目真的超过5千人朝着1万人目标奔的话，相信那个时候你的项目已经挣大钱了，你数着钱加着班去逐步迭代，一次次拆分它，相信心里也是乐开花的。 上面这些类型基本都是从拆分 MUDOS开始，将 MUDOS中的各个部件从单机一步步拆成分布式。虽然今天任然很多新项目在用上面某一种类似的结构，或者自己又做了其他热点模块的拆分。因为他们本质上都是对 MUDOS的分解，故将他们归纳为第二代游戏服务器。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
        <tag>GS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅读开源服务器源码基础]]></title>
    <url>%2F2016%2F07%2F04%2F%E9%98%85%E8%AF%BB%E5%BC%80%E6%BA%90%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%BA%90%E7%A0%81%E5%9F%BA%E7%A1%80%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[当阅读一些开源服务器源码的时候, 如果不知道以下知识, 就会有知识盲点, 导致不知所云.这篇博客会讲述一些相关的编程知识点, 把之前的笔记总结一下.还是那句老话, 带着问题阅读是最容易让人类迅速进入状态的. 进程的内存布局是什么样的?记忆口诀 : 文初堆栈 每个进程所分配的内存由很多部分组成，通常称之为“段( segment)”。如下所示。 文本段 包含了进程运行的程序机器语言指令。文本段具有只读属性，以防止进程通过错 误指针意外修改自身指令。因为多个进程可同时运行同一程序，所以又将文本段设为可 共享，这样，一份程序代码的拷贝可以映射到所有这些进程的虚拟地址空间中。 初始化数据段 包含显式初始化的全局变量和静态变量。当程序加载到内存时，从可执 行文件中读取这些变量的值。 未初始化数据段 包含了未进行显式初始化的全局变量和静态变量。程序启动之前，系统 将本段内所有内存初始化为0。出于历史原因，此段常被称为BSS段，这源于老版本的 汇编语言助记符“block started by symbol”o将经过初始化的全局变量和静态变量与未经 初始化的全局变量和静态变量分开存放，其主要原因在于程序在磁盘上存储时，没有必 要为未经初始化的变量分配存储空间。相反，可执行文件只需记录未初始化数据段的位 置及所需大小，直到运行时再由程序加载器来分配这一空间。 堆(heap) 是可在运行时（为变量）动态进行内存分配的一块区域盛顶端称作program break。 栈( stack) 是一个动态增长和收缩的段，由栈帧（stack frames）组成。系统会为每个 当前调用的函数分配一个栈帧。栈帧中存倍了函数的局部变量（所谓自动变量）、实 参和返回值。 线程的同步机制有哪些? 互斥量 条件变量 自旋锁 自旋锤与互斥量类似，但它不是通过休眠使进程阻塞，而是在获取镪之前一直处于忙等（自 旋）阻塞状态．自旋锁可用于“下情况锁被持有的时间短，而且线程并不希望在重新调度上花 费太多的成本。 读写锁(也叫做共享互斥锁) 读写锁也叫做共享互斥锁( shared-exclusive lock)。当读写锁是读模式锁住时，就可以说成是 以共享模式锁住的。当它是写模式锁住的时候，就可以说成是以互斥模式锁住的。 . . . 如何避免死锁总结 顺序加锁 可以先释放占有的锁，然后过一段时间再试 如果线程试图对同一个互斥量加锁两次，那么它自身就会陷入死锁状态，但是使用互斥量时，还有其他不太明显的方式也能产牛死锁。例如，程序中使用一个以上的互斥量时，如果允许一个线程一直占有第一个互斥量，并且在试图锁住第二个互斥量时处于阻塞状态，但是拥有第二个互斥量的线程也在试图锁住第一个互斥量。因为两个线程都在相互请求对方拥有的资源，所以这两个线程都无法向前运行，于是就产生死锁。 可以通过仔细控制互斥量加锁的顺序来避免死锁的发生。例如，假设需要对两个互斥量A和B同时加锁。如果所有线程总是在对互斥量B加锁之前锁住互斥量A即可，那么使用这两个瓦斥量就不会产生死锁（当然在其他的资源上仍可能出现死锁）。 类似地，如果所有的线程总是在锁住互斥量A之前锁住互斥量B，那么也不会发生死锁。可能出现的死锁只会发生在一个线程试图锁住另一个线程以相反的顺序锁住的互斥量。 有时候，应用程序的结构使得对互斥量进行排序是很困难的。如果涉及了太多的锁和数据结构，可用的函数并不能把它转换成简单的层次，那么就需要采用另外的方法。 在这种情况下，可以先释放占有的锁，然后过一段时间再试。这种情况可以使用pthread_mutex_trylock接口避免死锁。如果已经占有某些锁而且pthread_mutextrylock接口返回成功，那么就可以前进。但是，如果不能获取锁，可以先释放已经占有的锁，做好浦理工作，然后过一段时间再重新试。 进程间的同步机制(也就是进程间通信, 能通信就能同步了嘛)有哪些?博客中 进程间的通信与同步有详细说明 管道 匿名管道(父子进程间使用) 命名管道(无亲缘关系进程间使用) FIFO 消息队列 信号量 信号 共享内存 套接字(本地进程如果用套接字通信, 一般用unix套接字) linux的任务调度机制是什么？Linux 分实时进程和普通进程，实时进程应该先于普通进程而运行。而实时进程的调度机制为： FIFO(先入先出服务调度) RR（时间片轮转调度）。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>UNP</tag>
        <tag>TLPI</tag>
        <tag>APUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB能取代MySQL或者Redis能取代memcached么]]></title>
    <url>%2F2016%2F06%2F30%2FMongoDB%E8%83%BD%E5%8F%96%E4%BB%A3MySQL%E6%88%96%E8%80%85Redis%E8%83%BD%E5%8F%96%E4%BB%A3memcached%E4%B9%88%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[mongodb和memcached不是一个范畴内的东西。 mongodb是文档型的非关系型数据库，其优势在于查询功能比较强大，能存储海量数据。 mongodb和memcached不存在谁替换谁的问题。和memcached更为接近的是redis。 它们都是内存型数据库，数据保存在内存中，通过tcp直接存取，优势是速度快，并发高，缺点是数据&gt; 类型有限，查询功能不强，一般用作缓存。 一般现在的项目中，用redis来替代memcached。 . . . Redis相比memcached： redis具有持久化机制，可以定期将内存中的数据持久化到硬盘上。 redis具备binlog功能，可以将所有操作写入日志，当redis出现故障，可依照binlog进行数据恢复。 redis支持virtual memory，可以限定内存使用大小，当数据超过阈值，则通过类似LRU的算法把内存中的最不常用数据保存到硬盘的页面文件中。 redis原生支持的数据类型更多，使用的想象空间更大。 而monggodb: mongodb 是文档数据库，用于方便懒人替代mysql等关系数据库的。MongoDB 是一个基于分布式文件存储的数据库。由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。 MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。 不过mongodb在内存足够的情况下读写性能不错，大部分应用可以省去cache这一层了。 根据业务场景, 懒人可以使用MongoDB来取代MySQL+memcached,.]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
        <tag>Redis</tag>
        <tag>MySQL</tag>
        <tag>Memcached</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NAT穿越基础]]></title>
    <url>%2F2016%2F06%2F21%2FNAT%E7%A9%BF%E8%B6%8A%E5%9F%BA%E7%A1%80%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[NAT类型 锥NAT 对称NAT NAT作用 穿透锥NAT 网络拓扑结构 使用UDP穿透NAT 使用TCP穿透NAT 穿透对称NAT 同时开放TCP（ Simultaneous TCP open ）策略 UDP端口猜测策略 问题总结 参考 NAT类型注 : 我们本文主要讨论穿越锥NAT 锥NAT 全锥NAT ：全锥NAT 把所有来自相同内部IP 地址和端口的请求映射到相同的外部IP 地址和端口。任何一个外部主机均可通过该映射发送数据包到该内部主机。 限制性锥NAT ：限制性锥NAT 把所有来自相同内部IP 地址和端口的请求映射到相同的外部IP 地址和端口。但是, 和全锥NAT 不同的是：只有当内部主机先给外部主机发送数据包, 该外部主机才能向该内部主机发送数据包。 端口限制性锥NAT ：端口限制性锥NAT 与限制性锥NAT 类似, 只是多了端口号的限制, 即只有内部主机先向外部地址：端口号对发送数据包, 该外部主机才能使用特定的端口号向内部主机发送数据包。 对称NAT对称NAT 与上述3 种类型都不同, 不管是全锥NAT ，限制性锥NAT 还是端口限制性锥NAT ，它们都属于锥NAT （Cone NAT ）。当同一内部主机使用相同的端口与不同地址的外部主机进行通信时, 对称NAT 会重新建立一个Session ，为这个Session 分配不同的端口号，或许还会改变IP 地址。 . . . NAT作用NAT 不仅实现地址转换，同时还起到防火墙的作用，隐藏内部网络的拓扑结构，保护内部主机。 NAT 不仅完美地解决了 lP 地址不足的问题，而且还能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。 这样对于外部主机来说，内部主机是不可见的。 但是，对于P2P 应用来说，却要求能够建立端到端的连接，所以如何穿透NAT 也是P2P 技术中的一个关键。 穿透锥NAT要让处于NAT 设备之后的拥有私有IP 地址的主机之间建立P2P 连接，就必须想办法穿透NAT ，现在常用的传输层协议主要有TCP 和UDP ，下面就是用这两种协议来介绍穿透NAT 的策略。 网络拓扑结构下面假设有如图1 所示网络拓扑结构图。 Server （129.208.12.38 ）是公网上的服务器，NAT-A 和NAT-B 是两个NAT 设备（可能是集成NAT 功能的路由器，防火墙等），它们具有若干个合法公网IP ，在NAT-A 阻隔的私有网络中有若干台主机【ClientA-1 ，ClientA-N 】，在NAT-B 阻隔的私有网络中也有若干台主机【ClientB-1 ，ClientB-N 】。 为了以后说明问题方便，只讨论主机ClientA-1 和ClientB-1 。 假设主机ClientA-1 和主机ClientB-1 都和服务器Server 建立了“连接”，如图2 所示。 由于NAT 的透明性，所以ClientA-1 和ClientB-1 不用关心和Server 通信的过程，它们只需要知道Server 开放服务的地址和端口号即可。 根据图1 ，假设在ClientA-1 中有进程使用socket （192.168.0.2 ：7000 ）和Server 通信，在ClientB-1 中有进程使用socket （192.168.1.12:8000 ）和Server 通信。 它们通过各自的NAT 转换后分别变成了socket （202.103.142.29 ：5000 ）和socket （221.10.145.84 ：6000 ）。 使用UDP穿透NAT通常情况下，当进程使用UDP 和外部主机通信时，NAT 会建立一个Session ，这个Session 能够保留多久并没有标准，或许几秒，几分钟，几个小时。 假设ClientA-1 在应用程序中看到了ClientB-1 在线，并且想和ClientB-1 通信，一种办法是Server 作为中间人，负责转发ClientA-1 和ClientB-1 之间的消息，但是这样服务器太累，会吃不消。 另一种方法就是让ClientA-1 何ClientB-1 建立端到端的连接，然后他们自己通信。 这也就是P2P 连接。 根据不同类型的NAT ，下面分别讲解。 全锥NAT ，穿透全锥型NAT 很容易，根本称不上穿透，因为全锥型NAT 将内部主机的映射到确定的地址，不会阻止从外部发送的连接请求，所以可以不用任何辅助手段就可以建立连接。 限制性锥NAT 和端口限制性锥NAT （简称限制性NAT ），穿透限制性锥NAT 会丢弃它未知的源地址发向内部主机的数据包。 所以如果现在ClientA-1 直接发送UDP 数据包到ClientB-1 ，那么数据包将会被NAT-B 无情的丢弃。 所以采用下面的方法来建立ClientA-1 和ClientB-1 之间的通信。 ClientA-1 （202.103.142.29:5000 ）发送数据包给Server ，请求和ClientB-1 （221.10.145.84:6000 ）通信。 Server 将ClientA-1 的地址和端口（202.103.142.29:5000 ）发送给ClientB-1 ，告诉ClientB-1 ，ClientA-1 想和它通信。 ClientB-1 向ClientA-1 （202.103.142.29:5000 ）发送UDP 数据包，当然这个包在到达NAT-A 的时候，还是会被丢弃，这并不是关键的，因为发送这个UDP 包只是为了让NAT-B 记住这次通信的目的地址：端口号，当下次以这个地址和端口为源的数据到达的时候就不会被NAT-B 丢弃，这样就在NAT-B 上打了一个从ClientB-1 到ClientA-1 的孔。 为了让ClientA-1 知道什么时候才可以向ClientB-1 发送数据，所以ClientB-1 在向ClientA-1 （202.103.142.29:5000 ）打孔之后还要向Server 发送一个消息，告诉Server 它已经准备好了。 Server 发送一个消息给ClientA-1 ，内容为：ClientB-1 已经准备好了，你可以向ClientB-1 发送消息了。 ClientA-1 向ClientB-1 发送UDP 数据包。 这个数据包不会被NAT-B 丢弃，以后ClientB-1 向ClientA-1 发送的数据包也不会被ClientA-1 丢弃，因为NAT-A 已经知道是ClientA-1 首先发起的通信。 至此，ClientA-1 和ClientB-1 就可以进行通信了。 使用TCP穿透NAT使用TCP 协议穿透NAT 的方式和使用UDP 协议穿透NAT 的方式几乎一样，没有什么本质上的区别，只是将无连接的UDP 变成了面向连接的TCP 。 值得注意是： ClientB-1 在向ClientA-1 打孔时，发送的SYN 数据包，而且同样会被NAT-A 丢弃。同时，ClientB-1 需要在原来的socket 上监听，由于重用socket ，所以需要将socket 属性设置为SO_REUSEADDR 。 ClientA-1 向ClientB-1 发送连接请求。同样，由于ClientB-1 到ClientA-1 方向的孔已经打好，所以连接会成功，经过3 次握手后，ClientA-1 到ClientB-1 之间的连接就建立起来了。 穿透对称NAT上面讨论的都是怎样穿透锥（Cone ）NAT ，对称NAT 和锥NAT 很不一样。 对于 对称NAT ，当一个私网内主机和外部多个不同主机通信时，对称NAT 并不会像锥（Cone ，全锥，限制性锥，端口限制性锥）NAT 那样分配同一个端口。 而是会新建立一个Session ，重新分配一个端口。 参考上面穿透限制性锥NAT 的过程，在步骤3 时：ClientB-1 （221.10.145.84: ？）向ClientA-1 打孔的时候，对称NAT 将给ClientB-1 重新分配一个端口号，而这个端口号对于Server 、ClientB-1 、ClientA-1 来说都是未知的。 同样， ClientA-1 根本不会收到这个消息，同时在步骤4 ，ClientB-1 发送给Server 的通知消息中，ClientB-1 的socket 依旧是（221.10.145.84:6000 ）。 而且，在步骤6 时：ClientA-1 向它所知道但错误的ClientB-1 发送数据包时，NAT-1 也会重新给ClientA-1 分配端口号。 所以，穿透对称NAT 的机会很小。 下面是两种有可能穿透对称NAT 的策略。 同时开放TCP（ Simultaneous TCP open ）策略如果一个 对称 NAT 接收到一个来自 本地 私有网 络 外面的 TCP SYN 包， 这 个包想 发 起一个 “ 引入” 的 TCP 连 接，一般来 说 ， NAT 会拒 绝这 个 连 接 请 求并扔掉 这 个 SYN 包，或者回送一个TCP RST （connection reset ，重建 连 接）包 给请 求方。 但是，有一 种 情况 却会接受这个“引入”连接。 RFC 规定：对于对称NAT ， 当 这 个接收到的 SYN 包中的源IP 地址 ： 端口、目 标 IP 地址 ： 端口都与NAT 登 记 的一个已 经 激活的 TCP 会 话 中的地址信息相符 时 ， NAT 将会放行 这 个 SYN 包。 需要 特 别 指出 的是：怎样才是一个已经激活的TCP 连接？除了真正已经建立完成的TCP 连接外，RFC 规范指出： 如果 NAT 恰好看到一个 刚刚发 送出去的一个 SYN 包和 随之 接收到的SYN 包中的地址 ：端口 信息相符合的 话 ，那 么 NAT 将会 认为这 个 TCP 连 接已 经 被激活，并将允 许这 个方向的 SYN 包 进 入 NAT 内部。 同时开放TCP 策略就是利用这个时机来建立连接的。 如果 Client A -1 和 Client B -1 能 够 彼此正确的 预 知 对 方的 NAT 将会 给 下一个 TCP 连 接分配的公网 TCP 端口，并且两个客 户 端能 够 同 时 地 发 起一 个面向对方的 “ 外出 ” 的 TCP 连 接 请求 ，并在 对 方的 SYN 包到达之前，自己 刚发 送出去的 SYN 包都能 顺 利的穿 过 自己的 NAT 的 话 ，一条端 对 端的 TCP 连 接就 能 成功地建立了 。 UDP端口猜测策略同时开放TCP 策略非常依赖于猜测对方的下一个端口，而且强烈依赖于发送连接请求的时机，而且还有网络的不确定性，所以能够建立的机会很小，即使Server 充当同步时钟的角色。 下面是一种通过UDP 穿透的方法，由于UDP 不需要建立连接，所以也就不需要考虑“同时开放”的问题。 为了介绍ClientB-1 的诡计，先介绍一下STUN 协议。 STUN （Simple Traversal of UDP Through NATs ）协议是一个轻量级协议，用来探测被NAT 映射后的地址：端口。 STUN 采用C/S 结构，需要探测自己被NAT 转换后的地址：端口的Client 向Server 发送请求，Server 返回Client 转换后的地址：端口。 参考4.2 节中穿透NAT 的步骤2 ，当ClientB-1 收到Server 发送给它的消息后，ClientB-1 即打开3 个socket 。 socket-0 向STUN Server 发送请求，收到回复后，假设得知它被转换后的地址：端口（ 221.10.145.84:600 5 ），socket-1 向ClientA-1 发送一个UDP 包，socket-2 再次向另一个STUN Server 发送请求，假设得到它被转换后的地址：端口（ 221.10.145.84:60 20 ）。 通常，对称NAT 分配端口有两种策略，一种是按顺序增加，一种是随机分配。 如果这里对称NAT 使用顺序增加策略，那么，ClientB-1 将两次收到的地址：端口发送给Server 后，Server 就可以通知ClientA-1 在这个端口范围内猜测刚才ClientB-1 发送给它的socket-1 中被NAT 映射后的地址：端口，ClientA-1 很有可能在孔有效期内成功猜测到端口号，从而和ClientB-1 成功通信。 问题总结从上面两种穿透对称NAT 的方法来看，都建立在了严格的假设条件下。 但是现实中多数的NAT 都是锥NAT ，因为资源毕竟很重要，反观对称NAT ，由于太不节约端口号所以相对来说成本较高。 所以，不管是穿透锥NAT ，还是对称NAT ，现实中都是可以办到的。 除非对称NAT 真的使用随机算法来分配可用的端口。 参考这篇博客]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>NAT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL进阶二]]></title>
    <url>%2F2016%2F06%2F16%2FMySQL%E8%BF%9B%E9%98%B6%E4%BA%8C%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[事务MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你即需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！ 在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 事务处理可以用来维护数据库的完整性，保证成批的 SQL 语句要么全部执行，要么全部不执行。 事务用来管理 insert,update,delete 语句 . . . ACID一般来说，事务是必须满足4个条件（ACID）： Atomicity（原子性）、Consistency（一致性或稳定性）、Isolation（隔离性）、Durability（持久性） A 事务的原子性：一组事务，要么成功, 要么撤回。 C 稳定性 ：有非法数据（外键约束之类），事务撤回。 I 隔离性：事务独立运行。一个事务处理后的结果，影响了其他事务，那么其他事务会撤回。事务的100%隔离，需要牺牲速度。 D 持久性：软、硬件崩溃后，InnoDB数据表驱动会利用日志文件重构修改。可靠性和高速度不可兼得. 在 MySQL 命令行的默认设置下，事务都是自动提交的，即执行 SQL 语句后就会马上执行 COMMIT 操作。因此要显式地开启一个事务务须使用命令 BEGIN 或 START TRANSACTION，或者执行命令 SET AUTOCOMMIT=0，用来禁止使用当前会话的自动提交。 事务控制语句： BEGIN或START TRANSACTION；显式地开启一个事务； COMMIT；也可以使用COMMIT WORK，不过二者是等价的。COMMIT会提交事务，并使已对数据库进行的所有修改称为永久性的； ROLLBACK；有可以使用ROLLBACK WORK，不过二者是等价的。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改； SAVEPOINT identifier；SAVEPOINT允许在事务中创建一个保存点，一个事务中可以有多个SAVEPOINT； RELEASE SAVEPOINT identifier；删除一个事务的保存点，当没有指定的保存点时，执行该语句会抛出一个异常； ROLLBACK TO identifier；把事务回滚到标记点； SET TRANSACTION；用来设置事务的隔离级别。InnoDB存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ和SERIALIZABLE。 MYSQL 事务处理主要有两种方法： 用 BEGIN, ROLLBACK, COMMIT来实现 BEGIN 开始一个事务 ROLLBACK 事务回滚 COMMIT 事务确认 直接用 SET 来改变 MySQL 的自动提交模式: SET AUTOCOMMIT=0 禁止自动提交 SET AUTOCOMMIT=1 开启自动提交 事务的4个隔离级别具体例子参考这篇博客 读未提交(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 读已提交(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 相关术语 脏读 :脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。 不可重复读 :是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。 幻读 :第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。 临时表临时表在我们需要保存一些临时数据时是非常有用的。临时表只在当前连接可见，当关闭连接时，Mysql会自动删除表并释放所有空间。 防SQL注入防止SQL注入，我们需要注意以下几个要点： 永远不要信任用户的输入。对用户的输入进行校验，可以通过正则表达式，或限制长度；对单引号和 双-“进行转换等。 永远不要使用动态拼装SQL，可以使用参数化的SQL或者直接使用存储过程进行数据查询存取。 永远不要使用管理员权限的数据库连接，为每个应用使用单独的权限有限的数据库连接。 不要把机密信息直接存放，加密或者hash掉密码和敏感的信息。 应用的异常信息应该给出尽可能少的提示，最好使用自定义的错误信息对原始错误信息进行包装 SQL注入的检测方法一般采取辅助软件或网站平台来检测，软件一般采用SQL注入检测工具jsky，网站平台就有亿思网站安全平台检测工具。MDCSOFT SCAN等。采用MDCSOFT-IPS可以有效的防御SQL注入，XSS攻击等。 可以总结为 : 包装提示 (白) 连接权限 (莲) 校验输入 (教)]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL进阶一]]></title>
    <url>%2F2016%2F06%2F14%2FMySQL%E8%BF%9B%E9%98%B6%E4%B8%80%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[引擎MySQL是有多个引擎的, 不同的场景情况用不同的引擎以提升性能和灵活性.三大最常用的引擎 : InnoDB : 可靠的事务处理引擎 ,不支持全文搜索 MyISAM : 支持全文搜索, 不支持事务处理 MEMORY : 功能等同于MyISAM, 但数据存储在内存而不是磁盘, 所以速度非常快, 特别适用于临时表(temporary table) 索引索引是用来改善搜索性能的, 不要滥用索引, 比如如对表进行INSERT、UPDATE和DELETE的操作索引反而会降低更新表的速度。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。 . . . 索引有三种 : 普通索引normal 唯一索引unique : 它与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。 全文索引fulltext : 表示 全文搜索的索引。 FULLTEXT 用于搜索很长一篇文章的时候，效果最好。用在比较短的文本，如果就一两行字的，普通的 INDEX 也可以。 建立索引需要遵守的规则 : 选择唯一性索引 为经常需要排序、分组和联合操作的字段建立索引 为常作为查询条件的字段建立索引 限制索引的数目 尽量使用数据量少的索引 尽量使用前缀来索引 删除不再使用或者很少使用的索引 一些优化建议 like和fulltext like没有fulltext快, 但需要大内容的全文本搜索的时候来一发fulltext吧 选择正确数据类型 比如 : 定长的数据类型比可变长度的数据类型性能要高 正确使用索引 勿滥用select *语句 关闭自动提交]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UE4旋转笔记]]></title>
    <url>%2F2016%2F05%2F31%2FUE4%E6%97%8B%E8%BD%AC%E7%AC%94%E8%AE%B0%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[最近想将一个vector转化为rotator，转而需要考虑UE4到底是怎么旋转的。下面我们做个实验： 我们先将两个staticMesh放入场景，并将它们的rotation调成一样，如上图。上面那个为renti_a_gear，下面那个为renti_a_gear2. . . . 第一种情况： 绕自身坐标系来旋转 如上图，两个staticMesh旋转之后rotation是一样的，可以证明，绕自身坐标系旋转的顺序是Z-&gt;Y-&gt;X 第二种情况： 绕世界坐标系来旋转 如上图，两个staticMesh旋转之后rotation是一样的，可以证明，绕世界坐标系旋转的顺序跟第一种情况刚好反过来， 是X-&gt;Y-&gt;Z]]></content>
      <categories>
        <category>UE4</category>
      </categories>
      <tags>
        <tag>UE4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http报文笔记整理]]></title>
    <url>%2F2016%2F05%2F24%2Fhttp%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%E4%B9%8B%E6%8A%A5%E6%96%87%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[看了书和各种网上资料, 学东西嘛, 要做总结, 这些老笔记整理一下, 供以后方便查阅也加强印象和理解. 报文的组成 起始行(start line) 首部(header) 主体(body) 可细分为 : 方法 :如GET, HEAD, POST . . . 关于HTTP请求GET和POST的区别 : 1.提交方式的区别: GET提交，请求的数据会附在URL之后（就是把数据放置在http起始行中），以?分割URL和传输数据，多个参数用&amp;连接;例如：login.action?name=hyddd&amp;password=idontknow&amp;verify=%E4%BD%A0 %E5%A5%BD。如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64加密，得出如： %E4%BD%A0%E5%A5%BD，其中％XX中的XX为该符号以16进制表示的ASCII。 POST提交：把提交的数据放置在是HTTP主体中。 因此，GET提交的数据会在地址栏中显示出来，而POST提交，地址栏不会改变 2.传输数据的大小： 首先声明,HTTP协议没有对传输的数据大小进行限制，HTTP协议规范也没有对URL长度进行限制。 而在实际开发中存在的限制主要有： GET:特定浏览器和服务器对URL长度有限制，例如IE对URL长度的限制是2083字节(2K+35)。对于其他浏览器，如Netscape、FireFox等，理论上没有长度限制，其限制取决于操作系统的支持。 因此对于GET提交时，传输数据就会受到URL长度的限制。 POST:由于不是通过URL传值，理论上数据不受限。但实际各个WEB服务器会规定对post提交数据大小进行限制，Apache、IIS6都有各自的配置。 3.安全性： POST的安全性要比GET的安全性高。注意：这里所说的安全性和上面GET提到的“安全”不是同个概念。上面“安全”的含义仅仅是不作数据修改，而这里安全的含义是真正的Security的含义，比如：通过GET提交数据，用户名和密码将明文出现在URL上，因为 (1)登录页面有可能被浏览器缓存， (2)其他人查看浏览器的历史纪录，那么别人就可以拿到你的账号和密码了， 请求URL URL是浏览器寻找信息时所需的资源位置 .URL分为三个部分 : URL文案 服务器位置 资源路径 版本号上图中的HTTP/1.0 200 OK, HTTP/1.0就是版本号 状态码 : 如最著名的404, 302, 如上图中的HTTP/1.0 200 OK中, 状态码就是200 原因短语 如上图中的HTTP/1.0 200 OK中, OK就是原因短语 首部 主体主体部分是可选的, 主体是http报文要传输的内容, 可以承载很多类型的数字数据 : 图片, 视频, 软件应用程序, 电子邮件等]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[php与cgi]]></title>
    <url>%2F2016%2F05%2F22%2Fphp_cgi%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[首先，CGI是干嘛的？CGI是为了保证web server传递过来的数据是标准格式的，方便CGI程序的编写者。 web server（比如说nginx）只是内容的分发者。比如，如果请求/index.html，那么web server会去文件系统中找到这个文件，发送给浏览器，这里分发的是静态数据。好了，如果现在请求的是/index.php，根据配置文件，nginx知道这个不是静态文件，需要去找PHP解析器来处理，那么他会把这个请求简单处理后交给PHP解析器。Nginx会传哪些数据给PHP解析器呢？url要有吧，查询字符串也得有吧，POST数据也要有，HTTP header不能少吧，好的，CGI就是规定要传哪些数据、以什么样的格式传递给后方处理这个请求的协议。仔细想想，你在PHP代码中使用的用户从哪里来的。 当web server收到/index.php这个请求后，会启动对应的CGI程序，这里就是PHP的解析器。接下来PHP解析器会解析php.ini文件，初始化执行环境，然后处理请求，再以规定CGI规定的格式返回处理后的结果，退出进程。web server再把结果返回给浏览器。 好了，CGI是个协议，跟进程什么的没关系。那fastcgi又是什么呢？Fastcgi是用来提高CGI程序性能的。 . . . 提高性能，那么CGI程序的性能问题在哪呢？”PHP解析器会解析php.ini文件，初始化执行环境”，就是这里了。标准的CGI对每个请求都会执行这些步骤（不闲累啊！启动进程很累的说！），所以处理每个时间的时间会比较长。这明显不合理嘛！那么Fastcgi是怎么做的呢？首先，Fastcgi会先启一个master，解析配置文件，初始化执行环境，然后再启动多个worker。当请求过来时，master会传递给一个worker，然后立即可以接受下一个请求。这样就避免了重复的劳动，效率自然是高。而且当worker不够用时，master可以根据配置预先启动几个worker等着；当然空闲worker太多时，也会停掉一些，这样就提高了性能，也节约了资源。这就是fastcgi的对进程的管理。 那PHP-FPM又是什么呢？是一个实现了Fastcgi的程序，被PHP官方收了。 大家都知道，PHP的解释器是php-cgi。php-cgi只是个CGI程序，他自己本身只能解析请求，返回结果，不会进程管理（皇上，臣妾真的做不到啊！）所以就出现了一些能够调度php-cgi进程的程序，比如说由lighthttpd分离出来的spawn-fcgi。好了PHP-FPM也是这么个东东，在长时间的发展后，逐渐得到了大家的认可（要知道，前几年大家可是抱怨PHP-FPM稳定性太差的），也越来越流行。好了，最后来回来你的问题。 网上有的说，fastcgi是一个协议，php-fpm实现了这个协议 对。 有的说，php-fpm是fastcgi进程的管理器，用来管理fastcgi进程的 对。php-fpm的管理对象是php-cgi。但不能说php-fpm是fastcgi进程的管理器，因为前面说了fastcgi是个协议，似乎没有这么个进程存在，就算存在php-fpm也管理不了他（至少目前是）。 有的说，php-fpm是php内核的一个补丁 以前是对的。因为最开始的时候php-fpm没有包含在PHP内核里面，要使用这个功能，需要找到与源码版本相同的php-fpm对内核打补丁，然后再编译。后来PHP内核集成了PHP-FPM之后就方便多了，使用–enalbe-fpm这个编译参数即可。 有的说，修改了php.ini配置文件后，没办法平滑重启，所以就诞生了php-fpm 是的，修改php.ini之后，php-cgi进程的确是没办法平滑重启的。php-fpm对此的处理机制是新的worker用新的配置，已经存在的worker处理完手上的活就可以歇着了，通过这种机制来平滑过度。 还有的说PHP-CGI是PHP自带的FastCGI管理器，那这样的话干吗又弄个php-fpm出 不对。php-cgi只是解释PHP脚本的程序而已。 参考]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>CGI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[也想做一个这样的博客吗?]]></title>
    <url>%2F2016%2F05%2F22%2Fhow_to_make_blog_like_this%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[这是我的博客源码 ,我修改了很多NexT的代码来对原版 NexT 做了优化, 如下 : 改了NexT的很多地方来优化移动端的表现, header的布局 移动端和PC端的侧边栏更加统一 移动端的文章目录列表现在可以滑动了 重做了本地搜索引擎 现在移动端不会经常无故弹不出键盘了 也不会列出加密文章的内容了 更优雅的过渡动画 添加了headroom支持, 现在有一个可以会自动隐藏的header了, 往下滚一下鼠标则隐藏, 往上则出现 升级到了fancybox3并完成适配, 3更流畅且拥有更多效果 添加了文章加密的支持 CSDN sucks 实在是被CSDN的广告恶心到了 最近CSDN的Markdown的无序列表每一列前面的小黑点都没有了 手机网页版本的CSDN排版全无 所有博客瞬间排版全部变成一坨, 实在是不能忍. 所以才做了这个私人博客.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[vector和string的内存分配与使用注意点]]></title>
    <url>%2F2016%2F05%2F17%2Fstl_vector_string%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[增长方式为了支持快速随机访问 ， vector 将元素连续存储一一每个元素紧挨着前一个元素存储 。 问题假定容器中元素是连续存储 的， 且容器的大小是可变的 ， 考虑 向 vector 或 string中添加元素会发生什么 :如果没有空间容纳新元素，容器不可能简单地将它添加到内存中其他位置一一因为元素必须连续存储。 容器必须分配新的内存空间来保存己有元素和新元素 ， 将已有元素从 旧位置移动到新空 间中， 然后添加新元素，释放旧存储空间 。 如果我们每添加一个新元素， vector 就执行一次这样的内存分配和释放操作 ，性能会慢到不可接受 。 . . . 解决方案为了避免这种代价，标准库实现者采用了可以减少容器空间重新分配次数的策略。当不得不在取新的内 存空间 时， vector 和 string 的实现通常会分配比新的空间需求更大的内存空间 。 容器预留这些空间作为备用 ， 可用来保存更多的新元素 。这样，就不需要每次添加新元素都重新分配容器的内存空间了 。这种分配策略比每次添加新元素时都重新分配容器内存空间的策略要高效得多 。 其实际性能也表现得足够好一一虽然 vector 在每次重新分配内存空间时都要移动所有元素，但使用 此策略后，其扩张操作通常比 list 和 deque 还要快 。 增长方式的具体实现STL提供了很多泛型容器，如vector，list和map。程序员在使用这些容器时只需关心何时往容器内塞对象，而不用关心如何管理内存，需要用多少内存，这些STL容器极大地方便了C++程序的编写。 例如可以通过以下语句创建一个vector，它实际上是一个按需增长的动态数组，其每个元素的类型为int整型： stl::vector&lt;int&gt; testVector; 拥有这样一个动态数组后，用户只需要调用push_back方法往里面添加对象，而不需要考虑需要多少内存： 12testVector.push_back(10); testVector.push_back(2); vector会根据需要自动增长内存，在testVector退出其作用域时也会自动销毁占有的内存，这些对于用户来说是透明的，stl容器巧妙的避开了繁琐且易出错的内存管理工作。 隐藏在这些容器后的内存管理工作是通过STL提供的一个默认的allocator实现的。当然，用户也可以定制自己的allocator，只要实现allocator模板所定义的接口方法即可，然后通过将自定义的allocator作为模板参数传递给STL容器，创建一个使用自定义allocator的STL容器对象，如： stl::vector&lt;int, UserDefinedAllocator&gt; testVector; 大多数情况下，STL默认的allocator就已经足够了。这个allocator是一个由两级分配器构成的内存管理器， 当申请的内存大小大于128byte时，就启动第一级分配器通过malloc直接向系统的堆空间分配， 如果申请的内存大小小于128byte时，就启动第二级分配器，从一个预先分配好的内存池中取一块内存交付给用户，这个内存池由16个不同大小（8的倍数，8~128byte）的空闲列表组成，allocator会根据申请内存的大小（将这个大小round up成8的倍数）从对应的空闲块列表取表头块给用户。 这种做法有两个优点： 小对象的快速分配。小对象是从内存池分配的，这个内存池是系统调用一次malloc分配一块足够大的区域给程序备用，当内存池耗尽时再向系统申请一块新的区域，整个过程类似于批发和零售，起先是由allocator向总经商批发一定量的货物，然后零售给用户，与每次都总经商要一个货物再零售给用户的过程相比，显然是快捷了。当然，这里的一个问题时，内存池会带来一些内存的浪费，比如当只需分配一个小对象时，为了这个小对象可能要申请一大块的内存池，但这个浪费还是值得的，况且这种情况在实际应用中也并不多见。 避免了内存碎片的生成。程序中的小对象的分配极易造成内存碎片，给操作系统的内存管理带来了很大压力，系统中碎片的增多不但会影响内存分配的速度，而且会极大地降低内存的利用率。以内存池组织小对象的内存，从系统的角度看，只是一大块内存池，看不到小对象内存的分配和释放。 vector的内存释放由于vector的内存占用空间只增不减，比如你首先分配了10,000个字节， 然后erase掉后面9,999个，留下一个有效元素，但是内存占用仍为10,000个。 所有内存空间是在vector析构时候才能被系统回收。empty()用来检测容器是否为空的， clear()可以清空所有元素。但是即使clear()， vector所占用的内存空间依然如故，无法保证内存的回收。 如果需要空间动态缩小，可以考虑使用 deque 。如果vector，可以用swap()来帮助你释放内存。具体方法如下： vector&lt;int&gt;().swap(tempVector); //或者 tempVector.swap(vector&lt;int&gt;()) vector的内存释放代码实例1123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;#include &lt;vector&gt;int main()&#123; std::vector&lt;int&gt; foo; foo.push_back(1); foo.push_back(2); foo.push_back(3); foo.push_back(4); foo.push_back(5); std::vector&lt;int&gt; bar; bar.push_back(1); bar.push_back(2); std::cout &lt;&lt; "foo size:" &lt;&lt; foo.size() &lt;&lt; std::endl; std::cout &lt;&lt; "foo capacity:" &lt;&lt; foo.capacity() &lt;&lt; std::endl; std::cout &lt;&lt; "bar size:" &lt;&lt; bar.size() &lt;&lt; std::endl; std::cout &lt;&lt; "bar capacity:" &lt;&lt; bar.capacity() &lt;&lt; std::endl; foo.swap(bar); std::cout &lt;&lt; "after swap foo size:" &lt;&lt; foo.size() &lt;&lt; std::endl; std::cout &lt;&lt; "after swap foo capacity:" &lt;&lt; foo.capacity() &lt;&lt; std::endl; std::cout &lt;&lt; "after swap bar size:" &lt;&lt; bar.size() &lt;&lt; std::endl; std::cout &lt;&lt; "after swap bar capacity:" &lt;&lt; bar.capacity() &lt;&lt; std::endl; return 0;&#125; 输出：12345678foo size:5foo capacity:6bar size:2bar capacity:2after swap foo size:2after swap foo capacity:2after swap bar size:5after swap bar capacity:6 看到了吗，swap之后，不仅仅是size变化了，capacity也是变化了。那么于是就把swap替代clear了： vector的内存释放代码实例21234567891011121314151617181920#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;int main()&#123; vector&lt;int&gt; v; v.push_back(1); v.push_back(2); v.push_back(3); v.push_back(4); v.push_back(5); cout &lt;&lt; "size:" &lt;&lt; v.size() &lt;&lt; endl; cout &lt;&lt; "capacity:" &lt;&lt; v.capacity() &lt;&lt; endl; vector&lt;int&gt;().swap(v); cout &lt;&lt; "after swap size:" &lt;&lt; v.size() &lt;&lt; endl; cout &lt;&lt; "after swap capacity:" &lt;&lt; v.capacity() &lt;&lt; endl; return 0;&#125; 输出：1234size:5capacity:6after swap size:0after swap capacity:0 迭代器失效的问题看了上方的实现方式, 相信很容易能理解迭代器失效问题了啊 向容器中添加元素和从容器中删除元素 的操作可能(下面将会讨论为什么是可能)会使指向容器元素的指针、引用或法代器失效。 一个失效的指针、引用或法代器将不再表示任何元素 。 使用失效的指针、引用或迭代器是一种严重的程序设计错误，很可能引起与使用未初始化指针一样的问题 所以, 向容器中添加元素和从容器中删除元素的操作都需要用以下方法重置一下迭代器 : i = q.insert(i,22); i = q.erase(i); (以下三种情况都是在ubuntu g++环境测试的, 不同平台不同编译器会有不同表现, 比如同样的代码有可能会在vs下崩溃) 删除元素123456789101112131415161718192021//erase操作#include&lt;vector&gt;#include&lt;iostream&gt;using namespace std;int main()&#123; vector&lt;int&gt;q&#123;1,2,3,4,5,6,7,8,9,10&#125;; int cnt = 0; int flag = 0; for(vector&lt;int&gt;::iterator i = q.begin(); i != q.end(); ++i)&#123; ++cnt; if(cnt &gt; 15)&#123; cout&lt;&lt;"gg"&lt;&lt;endl; break; &#125; if(*i == 3) //删除第三个 i = q.erase(i); cout &lt;&lt; *i &lt;&lt; endl; cout &lt;&lt; &amp;(*i) &lt;&lt; endl; &#125; return 0;&#125; output:12345678910111213141516171810xc7215820xc7215c40xc7216050xc7216460xc7216870xc7216c80xc7217090xc72174100xc72178 输出结果分析: 当删除第3个元素以后我们发现第四个元素是紧邻第二个元素的（刚好差一个int的内存）, 也就是说vector执行erase（i）后会将迭代器i之后的元素逐个向前移动一个type单位,所以其实这种c++实现迭代器没失效, 但是其他的c++实现, 有可能所有元素全部移到另外一块内存, 比如这段代码放到vs是会崩溃的 添加元素时若预分配的内存足够迭代器就不会失效123456789101112131415161718192021222324252627282930//insert操作//内存充足情况#include&lt;vector&gt;#include&lt;iostream&gt;using namespace std;int main()&#123; vector&lt;int&gt;q&#123;1,2,3,4,5,6,7,8,9,10&#125;; q.push_back(11); cout&lt;&lt;"初始vector分配的容量:"&lt;&lt;q.capacity()&lt;&lt;endl; int cnt = 0; int flag = 0; //flag保证只插入一次 for(vector&lt;int&gt;::iterator i = q.begin(); i != q.end(); ++i)&#123; ++cnt; if(cnt &gt; 15)&#123; cout&lt;&lt; "gg" &lt;&lt;endl; break; &#125; if(*i == 3&amp;&amp;!flag)&#123; flag = 1; i = q.insert(i,22); cout&lt;&lt;"插入元素后vector分配的容量:" &lt;&lt;q.capacity() &lt;&lt;endl; &#125; cout &lt;&lt; *i &lt;&lt; endl; cout &lt;&lt; &amp;(*i) &lt;&lt; endl; &#125; return 0;&#125; 输出为:1234567891011121314151617181920212223242526初始vector分配的容量:20 1 0x1f2188 2 0x1f218c 插入元素后vector分配的容量:20 22 0x1f2190 3 0x1f2194 4 0x1f2198 5 0x1f219c 6 0x1f21a0 7 0x1f21a4 8 0x1f21a8 9 0x1f21ac 10 0x1f21b0 11 0x1f21b4 输出结果分析: 很显然当内存充足的情况下, 执行insert操作只会将迭代器i及i之后的的所有元素向后移动一个type单位.所以这种情况下即使没有使用返回值也不会发生迭代器失效 添加元素时若预分配的内存不足迭代器就会失效1234567891011121314151617181920212223242526272829303132333435//insert操作//内存不够情况#include&lt;vector&gt;#include&lt;iostream&gt;using namespace std;int main()&#123; vector&lt;int&gt;q&#123;1,2,3,4,5,6,7,8,9,10&#125;; // c++11列表初始化 vector&lt;int&gt;::iterator j = q.begin(); j++; cout&lt;&lt;"第二个元素:"&lt;&lt;*j&lt;&lt;endl; cout&lt;&lt;"第二个元素地址:"&lt;&lt;&amp;(*j)&lt;&lt;endl; cout&lt;&lt;"初始vector分配的容量:"&lt;&lt;q.capacity()&lt;&lt;endl; // 有多少元素即分配多少内存 int cnt = 0; int flag = 0; //flag保证只插入一次 for(vector&lt;int&gt;::iterator i = q.begin(); i != q.end(); ++i)&#123; ++cnt; if(cnt &gt; 15)&#123; cout&lt;&lt; "gg" &lt;&lt;endl; break; &#125; if(*i == 3&amp;&amp;!flag)&#123; flag = 1; i = q.insert(i,22); cout&lt;&lt;"\n插入后原第二个元素:"&lt;&lt;*j&lt;&lt;endl; cout&lt;&lt;"插入后原第二个元素地址:"&lt;&lt;&amp;(*j)&lt;&lt;endl; cout&lt;&lt;"插入元素后vector分配的容量:" &lt;&lt;q.capacity() &lt;&lt;endl; &#125; cout &lt;&lt; *i &lt;&lt; endl; cout &lt;&lt; &amp;(*i) &lt;&lt; endl; &#125; return 0;&#125; output:1234567891011121314151617181920212223242526272829第二个元素:2 第二个元素地址:0xe5215c 初始vector分配的容量:10 1 0xe52158 2 0xe5215c 插入后第二个元素:15007936 插入后第二个元素地址:0xe5215c 插入元素后vector分配的容量:20 22 0xe52190 3 0xe52194 4 0xe52198 5 0xe5219c 6 0xe521a0 7 0xe521a4 8 0xe521a8 9 0xe521ac 10 0xe521b0 输出结果分析: vector内存分配策略为 二倍扩容 , 每次当内存不够的情况下vector会将容量扩展为当前的两倍. 那这些新分配的会在原内存的后面吗？ 根据输出结果显然不是的。 上例代码在插入元素22 后, 新的3号元素内存位置距离上一个元素不是4byte(1个int单位), 也就是说, 当vector扩容时, 会在另一个内存分配一段新的内存(原内存的二倍). 并把原内存中的元素全部拷贝到新内存中… 指向二号元素的迭代器在插入操作之后指向的值由2变成了15007936,也验证了上述结论. capacity &amp; size 理解 capacity 和 size 的区别非常重要。 容器的 size 是指它已经保存的元素的数目 ; 而 capacity 则是在不分配新的内存空间的前提下它最多可以保存多少元素。 函数 函数 表述 c.begin() 传回指向第一个元素的迭代器。 c.capacity() 返不分配新的内存空间的前提下它最多可以保存多少元素。 c.clear() 移除容器中所有数据。 c.empty() 判断容器是否为空。 c.end() 指向迭代器中的最后一个数据地址。 c.insert(iter,elem) 在pos位置插入一个elem拷贝，传回新数据位置。 c.pop_back() 删除最后一个数据。 c.push_back(elem) 在尾部加入一个数据。 c.size() 返回容器中实际数据的个数。 c1.swap(c2) 将c1和c2这两个容器中的元素互换。 swap(c1,c2) 同上操作。 c.empty() 判断容器是否为空。 c.erase(iter) 删除pos位置的数据，传回下一个数据的位置。 c.reserve() 保留(预先分配)适当的容量。 c.assign(beg,end) 将[beg; end)区间中的数据赋值给c。 c.assign(n,elem) 将n个elem的拷贝赋值给c。 c.at(idx) 传回索引idx所指的数据，如果idx越界，抛出out_of_range。 c.back() 传回指向最后一个元素的迭代器 c.erase(beg,end) 删除[beg,end)区间的数据，传回下一个数据的位置。 c.front() 传回第一个数据。 get_allocator 使用构造函数返回一个拷贝。 c.insert(pos,n,elem) 在pos位置插入n个elem数据。无返回值。 c.insert(pos,beg,end) 在pos位置插入在[beg,end)区间的数据。无返回值。 c.max_size() 返回容器中最大数据的数量。 c.rbegin() 传回一个逆向队列的第一个数据。 c.rend() 传回一个逆向队列的最后一个数据的下一个位置。 c.resize(num) 重新指定队列的长度。 构造 &amp; 销毁 写法 表述 vector c 创建一个空的vector。 vector c1(c2) 复制一个vector。 vector c(n) 创建一个vector，含有n个数据，数据均已缺省构造产生。 vector c(n, elem) 创建一个含有n个elem拷贝的vector。 vector c(beg,end) 创建一个以[beg;end)区间的vector。 c.~ vector () 销毁所有数据，释放内存。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[stl关联容器的特性]]></title>
    <url>%2F2016%2F04%2F26%2Fstll_set_map_tutorial%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[概绍关联容器和顺序容器有着根本的不同 : 关联容器中的元素是按关键字来保存和访问的 。与之相对，顺序容器中的元素是按它们在容器中的位置来顺序保存和访问的 。 关联容器支持高效的关键字查找和访问 。两个主要的关联容器类型是 : map set map概绍map 中 的元素是一些关键字一值 ( key-value )对 : 关键字起到索 引 的作用，值则表示与索引相关联的数据 。字典则是一个很好的使用 map 的例子, 可以将单词作为关键字，将单词释义作为值 。 set概绍set 中每个元素只包含一个关键字 : set 支持高效的关键字检查一个给定关键字是否在 set 中 。例如，在某些文本处理过程中，可以用一个 set 来保存想要忽略的单词。 . . . map &amp; set 的实现因为需要快速定位到键值的关系, 以红黑树的结构实现，其自平衡特性可以让插入删除等操作都可以在O(log n)时间内完成 map的基本操作函数 函数 含义 begin() 返回指向map头部的迭代器 clear() 删除所有元素 insert() 插入元素 empty() 如果map为空则返回true end() 返回指向map末尾的迭代器 erase() 删除一个元素 find() 查找一个元素 lower_bound() 返回键值&gt;=给定元素的第一个元素的迭代器 upper_bound() 返回键值&gt;给定元素的第一个元素的迭代器 size() 返回map中元素的个数 count() 返回指定元素出现的次数 equal_range() 返回特殊条目的迭代器对 get_allocator() 返回map的配置器 key_comp() 返回比较元素key的函数 max_size() 返回可以容纳的最大元素个数 rbegin() 返回一个指向map尾部的逆向迭代器 rend() 返回一个指向map头部的逆向迭代器 swap() 交换两个map value_comp() 返回比较元素value的函数 迭代器失效123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;map&gt;#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;int main()&#123; map&lt;int, string&gt; map_student; map_student.insert(pair&lt;int, string&gt;(1, "stu1")); map_student.insert(pair&lt;int, string&gt;(2, "stu2")); map_student.insert(pair&lt;int, string&gt;(3, "stu3")); map_student.insert(pair&lt;int, string&gt;(4, "stu4")); map&lt;int, string&gt;::iterator iter; if (map_student.find(2) != map_student.end()) &#123; cout &lt;&lt; "found" &lt;&lt; endl; &#125; for (iter = map_student.begin(); iter != map_student.end(); ++iter) &#123; if (iter-&gt;first == 2) &#123; map_student.erase(iter); // 移除元素会让迭代器失效, 所以上面这5行应改为: // for (iter = map_student.begin(); iter != map_student.end();) // 注意, 这里没有 `++iter` 了 // &#123; // if (iter-&gt;first == 2) // &#123; // iter = map_student.erase(iter); // 这里也可以用 `map_student.erase(iter++);`代替 // map_student.insert(pair&lt;int, string&gt;(5, "stu5")); // map增加元素并不会使迭代器失效, 因为map增加元素跟vector不一样, // vector要重新找一块内存把当前所有元素复制过去并释放原有元素所以会导致vector的迭代器失效, // 但是map只是直接在红黑树上增加一个结点而已, 并不会移动原有元素, 内存没动, // 自然map的迭代器不会失效了 &#125; cout &lt;&lt; iter-&gt;first &lt;&lt; " : " &lt;&lt; iter-&gt;second &lt;&lt; endl; &#125; // test lower_bound &amp; upper_bound set&lt;int&gt; s; s.insert(1); s.insert(3); s.insert(4); cout&lt;&lt;*s.lower_bound(2)&lt;&lt;endl; // output : 3 cout&lt;&lt;*s.lower_bound(3)&lt;&lt;endl; // output : 3 cout&lt;&lt;*s.upper_bound(3)&lt;&lt;endl; // output : 4 cout&lt;&lt;*s.upper_bound(1)&lt;&lt;endl; // output : 3 return 0;&#125;]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程局部存储]]></title>
    <url>%2F2016%2F04%2F22%2Fthread_local_storage%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[使用全局变量或者静态变量是导致多线程编程中非线程安全的常见原因。在多线程程序中，保障非线程安全的常用手段之一是使用互斥锁来做保护，这种方法带来了并发性能下降，同时也只能有一个线程对数据进行读写。 如果程序中能避免使用全局变量或静态变量，那么这些程序就是线程安全的，性能也可以得到很大的提升。 如果有些数据只能有一个线程可以访问，那么这一类数据就可以使用线程局部存储机制来处理，虽然使用这种机制会给程序执行效率上带来一定的影响，但对于使用锁机制来说，这些性能影响将可以忽略。 还有一种大致相当的编程技术就是使用 线程特有数据(没 线程局部存储 易用, 也没 线程局部存储 高效) ，这将在 线程特有数据 中讨论。 . . . 线程局部存储介绍__thread 是GCC内置的线程局部存储设施，存取效率可以和全局变量相比。 __thread 变量每一个线程有一份独立实体，各个线程的值互不干扰。可以用来修饰那些带有全局性且值可能变，但是又不值得用全局变量保护的变量。 __thread 使用规则：只能修饰POD类型(类似整型指针的标量，不带自定义的构造、拷贝、赋值、析构的类型， 二进制内容可以任意复制memset, memcpy, 且内容可以复原， 不能修饰class类型，因为无法自动调用构造函数和析构函数，可以用于修饰全局变量，函数内的静态变量，不能修饰函数的局部变量或者class的普通成员变量，且 __thread 变量值只能初始化为编译器常量( 例如 : 值在编译器就可以确定const int i=5,运行期常量是运行初始化后不再改变const int i=rand() ). 一个简单例子123456789101112131415161718192021222324252627#include&lt;iostream&gt;#include&lt;pthread.h&gt;#include&lt;unistd.h&gt;using namespace std;const int i=5;__thread int var=i;//两种方式效果一样//__thread int var=5;//void* worker1(void* arg);void* worker2(void* arg);int main()&#123; pthread_t pid1,pid2; //__thread int temp=5; static __thread int temp=10;//修饰函数内的static变量 pthread_create(&amp;pid1,NULL,worker1,NULL); pthread_create(&amp;pid2,NULL,worker2,NULL); pthread_join(pid1,NULL); pthread_join(pid2,NULL); cout&lt;&lt;temp&lt;&lt;endl;//输出10 return 0;&#125;void* worker1(void* arg)&#123; cout&lt;&lt;++var&lt;&lt;endl;//输出 6&#125;void* worker2(void* arg)&#123; sleep(1);//等待线程1改变var值，验证是否影响线程2 cout&lt;&lt;++var&lt;&lt;endl;//输出6&#125; 程序输出 : 6 6 //可见__thread值线程间互不干扰 10 如何使用线程局部存储技术来实现函数的线程安全我们先讨论一下非线程安全的 stderror() 的实现, 接着说明如何使用线程局部存储技术来实现该函数的线程安全. 非线程安全的stderror()An implementation of strerror() that is not thread-safe. 1234567891011121314151617181920212223242526272829303132333435363738/*************************************************************************\* Copyright (C) Michael Kerrisk, 2017. ** ** This program is free software. You may use, modify, and redistribute it ** under the terms of the GNU General Public License as published by the ** Free Software Foundation, either version 3 or (at your option) any ** later version. This program is distributed without any warranty. See ** the file COPYING.gpl-v3 for details. *\*************************************************************************//* Listing 31-1 *//* strerror.c An implementation of strerror() that is not thread-safe.*/#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist' declarations from &lt;stdio.h&gt; */#include &lt;stdio.h&gt;#include &lt;string.h&gt; /* Get declaration of strerror() */#define MAX_ERROR_LEN 256 /* Maximum length of string returned by strerror() */static char buf[MAX_ERROR_LEN]; /* Statically allocated return buffer */char *strerror(int err)&#123; if (err &lt; 0 || err &gt;= _sys_nerr || _sys_errlist[err] == NULL) &#123; snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err); &#125; else &#123; strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */ &#125; return buf;&#125; 线程安全的stderror()这是使用线程局部存储技术实现的线程安全的stderror(). 如果对使用线程特有数据技术实现的线程安全的stderror()感兴趣,请转 线程特有数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/*************************************************************************\* Copyright (C) Michael Kerrisk, 2017. ** ** This program is free software. You may use, modify, and redistribute it ** under the terms of the GNU General Public License as published by the ** Free Software Foundation, either version 3 or (at your option) any ** later version. This program is distributed without any warranty. See ** the file COPYING.gpl-v3 for details. *\*************************************************************************//* Listing 31-4 *//* strerror_tls.c An implementation of strerror() that is made thread-safe through the use of thread-local storage. See also strerror_tsd.c. Thread-local storage requires: Linux 2.6 or later, NPTL, and gcc 3.3 or later.*/#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist' declarations from &lt;stdio.h&gt; */#include &lt;stdio.h&gt;#include &lt;string.h&gt; /* Get declaration of strerror() */#include &lt;pthread.h&gt;#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread buffer returned by strerror() */static __thread char buf[MAX_ERROR_LEN]; /* Thread-local return buffer */char *strerror(int err)&#123; if (err &lt; 0 || err &gt;= _sys_nerr || _sys_errlist[err] == NULL) &#123; snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err); &#125; else &#123; strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */ &#125; return buf;&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Linux</tag>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程特有数据]]></title>
    <url>%2F2016%2F04%2F17%2Fthread_specific_data%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[在 Linux 系统中使用 C/C++ 进行多线程编程时，我们遇到最多的就是对同一变量的多线程读写问题，大多情况下遇到这类问题都是通过锁机制来处理，但这对程序的性能带来了很大的影响， 当然对于那些系统原生支持原子操作的数据类型来说，我们可以使用原子操作来处理，这能对程序的性能会得到一定的提高。那么对于那些系统不支持原子操作的自定义数据类型， 在不使用锁的情况下如何做到线程安全呢？本文将从线程特有数据方面，简单讲解处理这一类线程安全问题的方法。 如果有些数据只能有一个线程可以访问，那么这一类数据就可以使用线程特有数据机制来处理，虽然使用这种机制会给程序执行效率上带来一定的影响，但对于使用锁机制来说，这些性能影响将可以忽略。 还有一种大致相当的编程技术就是使用 __thread (比 线程特有数据 易用, 也比 线程特有数据 高效), 它是 GCC 内置的线程局部存储设施 ，这将在 线程局部存储 中讨论。 数据类型在 C/C++ 程序中常存在全局变量、函数内定义的静态变量以及局部变量，对于局部变量来说，其不存在线程安全问题，因此不在本文讨论的范围之内。 全局变量和函数内定义的静态变量，是同一进程中各个线程都可以访问的共享变量，因此它们存在多线程读写问题。 在一个线程中修改了变量中的内容，其他线程都能感知并且能读取已更改过的内容，这对数据交换来说是非常快捷的，但是由于多线程的存在，对于同一个变量可能存在两个或两个以上的线程同时修改变量所在的内存内容，同时又存在多个线程在变量在修改的时去读取该内存值，如果没有使用相应的同步机制来保护该内存的话，那么所读取到的数据将是不可预知的，甚至可能导致程序崩溃。 如果需要在一个线程内部的各个函数调用都能访问、但其它线程不能访问的变量，这就需要新的机制来实现，我们称之为 Static memory local to a thread (线程局部静态变量)，同时也可称之为线程特有数据（TSD: Thread-Specific Data） 这一类型的数据，在程序中每个线程都会分别维护一份变量的副本 (copy)，并且长期存在于该线程中，对此类变量的操作不影响其他线程。如下图： 一次性初始化在讲解线程特有数据之前，先让我们来了解一下一次性初始化。 多线程程序有时有这样的需求：不管创建多少个线程，有些数据的初始化只能发生一次。 列如：在 C++ 程序中某个类在整个进程的生命周期内只能存在一个实例对象，在多线程的情况下，为了能让该对象能够安全的初始化，一次性初始化机制就显得尤为重要了。 ——在设计模式中这种实现常常被称之为单例模式（Singleton）。Linux 中提供了如下函数来实现一次性初始化： 123456789101112#include &lt;pthread.h&gt;// Returns 0 on success, or a positive error number on errorint pthread_once (pthread_once_t *once_control, void (*init) (void));// 利用参数once_control的状态，函数pthread_once()可以确保无论有多少个线程调用多少次该函数，// 也只会执行一次由init所指向的由调用者定义的函数。// init所指向的函数没有任何参数，形式如下：void init (void)&#123; // some variables initializtion in here&#125; 另外，参数 once_control 必须是 pthread_once_t 类型变量的指针，指向初始化为 PTHRAD_ONCE_INIT 的静态变量。在 C++0x 以后提供了类似功能的函数 std::call_once ()，用法与该函数类似。使用实例请参考 :https://github.com/ApusApp/Swift/blob/master/swift/base/singleton.hpp实现。 线程特有数据API介绍在 Linux 中提供了如下函数来对线程特有数据进行操作 12345678910111213#include &lt;pthread.h&gt;// Returns 0 on success, or a positive error number on errorint pthread_key_create (pthread_key_t *key, void (*destructor)(void *));// Returns 0 on success, or a positive error number on errorint pthread_key_delete (pthread_key_t key);// Returns 0 on success, or a positive error number on errorint pthread_setspecific (pthread_key_t key, const void *value);// Returns pointer, or NULL if no thread-specific data is associated with keyvoid *pthread_getspecific (pthread_key_t key); pthread_key_create// Returns 0 on success, or a positive error number on error int pthread_key_create (pthread_key_t *key, void (*destructor)(void *)); 函数 pthread_key_create() 为线程特有数据创建一个新键，并通过 key 指向新创建的键缓冲区。 因为所有线程都可以使用返回的新键，所以参数 key 可以是一个全局变量（在 C++ 多线程编程中一般不使用全局变量，而是使用单独的类对线程特有数据进行封装，每个变量使用一个独立的 pthread_key_t）。 destructor 所指向的是一个自定义的函数，其格式如下： void Dest (void *value) { // Release storage pointed to by &apos;value&apos; } 只要线程终止时与 key 关联的值不为 NULL，则 destructor 所指的函数将会自动被调用。 如果一个线程中有多个线程特有数据变量，那么对各个变量所对应的 destructor 函数的调用顺序是不确定的，因此，每个变量的 destructor 函数的设计应该相互独立。 pthread_key_delete// Returns 0 on success, or a positive error number on error int pthread_key_delete (pthread_key_t key); 函数 pthread_key_delete() 并不检查当前是否有线程正在使用该线程特有数据变量，也不会调用清理函数 destructor，而只是将其释放以供下一次调用 pthread_key_create() 使用。 在 Linux 线程中，它还会将与之相关的线程数据项设置为 NULL。 由于系统对每个进程中 pthread_key_t 类型的个数是有限制的，所以进程中并不能创建无限个的 pthread_key_t 变量。Linux 中可以通过 PTHREAD_KEY_MAX（定义于 limits.h 文件中）或者系统调用 sysconf(_SC_THREAD_KEYS_MAX) 来确定当前系统最多支持多少个键。Linux 中默认是 1024 个键，这对于大多数程序来说已经足够了。如果一个线程中有多个线程特有数据变量，通常可以将这些变量封装到一个数据结构中，然后使封装后的数据结构与一个线程局部变量相关联，这样就能减少对键值的使用。 pthread_setspecific// Returns 0 on success, or a positive error number on error int pthread_setspecific (pthread_key_t key, const void *value); 函数 pthread_setspecific() 用于将 value 的副本存储于一数据结构中，并将其与调用线程以及 key 相关联。 参数 value 通常指向由调用者分配的一块内存，当线程终止时，会将该指针作为参数传递给与 key 相关联的 destructor 函数。 pthread_getspecific// Returns pointer, or NULL if no thread-specific data is associated with key void *pthread_getspecific (pthread_key_t key); 当线程被创建时，会将所有的线程特有数据变量初始化为 NULL，因此第一次使用此类变量前必须先调用 pthread_getspecific() 函数来确认是否已经于对应的 key 相关联，如果没有，那么 pthread_getspecific() 会分配一块内存并通过 pthread_setspecific() 函数保存指向该内存块的指针。 参数 value 的值也可以不是一个指向调用者分配的内存区域，而是任何可以强制转换为 void * 的变量值，在这种情况下，先前的 pthread_keycreate() 函数应将参数 _destructor 设置为 NULL 函数 pthread_getspecific() 正好与 pthread_setspecific() 相反，其是将 pthread_setspecific() 设置的 value 取出。在使用取出的值前最好是将 void * 转换成原始数据类型的指针。 使用线程特有数据API我们先讨论一下非线程安全的 stderror() 的实现, 接着说明如何使用线程特有数据来实现该函数的线程安全. 非线程安全的stderror()An implementation of strerror() that is not thread-safe. 1234567891011121314151617181920212223242526272829303132333435363738/*************************************************************************\* Copyright (C) Michael Kerrisk, 2017. ** ** This program is free software. You may use, modify, and redistribute it ** under the terms of the GNU General Public License as published by the ** Free Software Foundation, either version 3 or (at your option) any ** later version. This program is distributed without any warranty. See ** the file COPYING.gpl-v3 for details. *\*************************************************************************//* Listing 31-1 *//* strerror.c An implementation of strerror() that is not thread-safe.*/#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist' declarations from &lt;stdio.h&gt; */#include &lt;stdio.h&gt;#include &lt;string.h&gt; /* Get declaration of strerror() */#define MAX_ERROR_LEN 256 /* Maximum length of string returned by strerror() */static char buf[MAX_ERROR_LEN]; /* Statically allocated return buffer */char *strerror(int err)&#123; if (err &lt; 0 || err &gt;= _sys_nerr || _sys_errlist[err] == NULL) &#123; snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err); &#125; else &#123; strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */ &#125; return buf;&#125; 线程安全的stderror()这是使用线程特有数据技术实现的线程安全的stderror(). 如果对使用线程局部存储技术实现的线程安全的stderror()感兴趣,请转 线程局部存储 An implementation of strerror() that is made thread-safe through the use of thread-specific data. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/*************************************************************************\* Copyright (C) Michael Kerrisk, 2017. ** ** This program is free software. You may use, modify, and redistribute it ** under the terms of the GNU General Public License as published by the ** Free Software Foundation, either version 3 or (at your option) any ** later version. This program is distributed without any warranty. See ** the file COPYING.gpl-v3 for details. *\*************************************************************************//* Listing 31-3 *//* strerror_tsd.c An implementation of strerror() that is made thread-safe through the use of thread-specific data. See also strerror_tls.c.*/#define _GNU_SOURCE /* Get '_sys_nerr' and '_sys_errlist' declarations from &lt;stdio.h&gt; */#include &lt;stdio.h&gt;#include &lt;string.h&gt; /* Get declaration of strerror() */#include &lt;pthread.h&gt;#include "tlpi_hdr.h"static pthread_once_t once = PTHREAD_ONCE_INIT;static pthread_key_t strerrorKey;#define MAX_ERROR_LEN 256 /* Maximum length of string in per-thread buffer returned by strerror() */static void /* Free thread-specific data buffer */destructor(void *buf)&#123; free(buf);&#125;static void /* One-time key creation function */createKey(void)&#123; int s; /* Allocate a unique thread-specific data key and save the address of the destructor for thread-specific data buffers */ s = pthread_key_create(&amp;strerrorKey, destructor); if (s != 0) errExitEN(s, "pthread_key_create");&#125;char *strerror(int err)&#123; int s; char *buf; /* Make first caller allocate key for thread-specific data */ s = pthread_once(&amp;once, createKey); if (s != 0) errExitEN(s, "pthread_once"); buf = pthread_getspecific(strerrorKey); if (buf == NULL) &#123; /* If first call from this thread, allocate buffer for thread, and save its location */ buf = malloc(MAX_ERROR_LEN); if (buf == NULL) errExit("malloc"); s = pthread_setspecific(strerrorKey, buf); if (s != 0) errExitEN(s, "pthread_setspecific"); &#125; if (err &lt; 0 || err &gt;= _sys_nerr || _sys_errlist[err] == NULL) &#123; snprintf(buf, MAX_ERROR_LEN, "Unknown error %d", err); &#125; else &#123; strncpy(buf, _sys_errlist[err], MAX_ERROR_LEN - 1); buf[MAX_ERROR_LEN - 1] = '\0'; /* Ensure null termination */ &#125; return buf;&#125; 深入理解线程特有数据机制深入理解线程特有数据的实现有助于对其 API 的使用。 在典型的实现中包含以下数组： 一个全局（进程级别）的数组，用于存放线程特有数据的键值信息pthread_key_create() 返回的 pthread_key_t 类型值只是对全局数组的索引，该全局数组标记为 pthread_keys，其格式大概如下： 数组的每个元素都是一个包含两个字段的结构，第一个字段标记该数组元素是否在用，第二个字段用于存放针对此键、线程特有数据变的解构函数的一个副本，即 destructor 函数。 每个线程还包含一个数组，存有为每个线程分配的线程特有数据块的指针（通过调用 pthread_setspecific() 函数来存储的指针，即参数中的 value） 在常见的存储 pthread_setspecific()函数参数 value 的实现中，大多数都类似于下图的实现。 图中假设 pthread_keys[1]分配给 func1()函数，pthread API 为每个函数维护指向线程特有数据数据块的一个指针数组，其中每个数组元素都与图线程特有数据键的实现 (上图) 中的全局 pthread_keys 中元素一一对应。 参考 [1] Linux/UNIX 系统编程手册（上） [2] http://www.groad.net/bbs/thread-2182-1-1.html [3] http://baike.baidu.com/view/598128.htm]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Linux</tag>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git教程]]></title>
    <url>%2F2016%2F04%2F12%2Fgit_tutorial%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[之前有一份私人git笔记老长老长了, 今天得空, 把它浓缩成5分钟版本.感觉纯基础性的东西整理成博客差也差不多了, 还有很多凌乱的工作笔记慢慢在一点一点整理放上来吧,估计下面几篇博客就开始游戏服务器的开发心得之类的了. . . . 安装sudo apt-get install git 通过SSH的key来push到GithubCreate a repo. Make sure there is at least one file in it (even just the README) Generate ssh key: ssh-keygen -t rsa -C &quot;your_email@example.com&quot; Copy the contents of the file ~/.ssh/id_rsa.pub to your SSH keys in your GitHub account settings. Test SSH key: ssh -T git@github.com clone the repo: git clone git://github.com/username/your-repository Now cd to your git clone folder and do: git remote set-url origin git@github.com:username/your-repository.git Now try editing a file (try the README) and then do: git add -A git commit -am &quot;my update msg&quot; git push Git概念图 集中式版本控制只有中心服务器拥有一份代码，而分布式版本控制每个人的电脑上就有一份完整的代码。 集中式版本控制有安全性问题，当中心服务器挂了所有人都没办法工作了。 集中式版本控制需要连网才能工作，如果网速过慢，那么提交一个文件会慢的无法让人忍受。而分布式版本控制不需要连网就能工作。 分布式版本控制新建分支、合并分支操作速度非常快，而集中式版本控制新建一个分支相当于复制一份完整代码。 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 新建一个仓库之后，当前目录就成为了工作区，工作区下有一个隐藏目录 .git，它属于 Git 的版本库。Git 的版本库有一个称为 Stage 的暂存区以及最后的 History 版本库，History 存储所有分支信息，使用一个 HEAD 指针指向当前分支。 查看状态 比如查看当前分支的状态 : git status, 这条命令也会给很多其他的git命令提示的喔 查看当前在哪个分支 : git branch12345b@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git branch master new_test_branch* old_demo plugin 标记为*的那个就是当前分支, 也就是old_demo分支 克隆比如从我的一个远端github项目克隆一份到本地 : git clone git@github.com:no5ix/Flock-AI-Fish-Unreal-VR.git这个地址是这样得来的, 如图 : 分支 比如创建一个新的分支test_branch :git branch test_branch 比如切换到分支test_branch :git checkout test_branch 比如把test_branch合并到主分支master上来 : 先执行git checkout master切换到master上来, 然后 git merge test_branch git rebase test_branch(一般不这么用) : rebase 跟 merge 的区别你们可以理解成有两个书架，你需要把两个书架的书整理到一起去，第一种做法是 merge ，比较粗鲁暴力，就直接腾出一块地方把另一个书架的书全部放进去，虽然暴力，但是这种做法你可以知道哪些书是来自另一个书架的；第二种做法就是 rebase ，他会把两个书架的书先进行比较，按照购书的时间来给他重新排序，然后重新放置好，这样做的好处就是合并之后的书架看起来很有逻辑，但是你很难清晰的知道哪些书来自哪个书架的。各有好处的，不同的团队根据不同的需要以及不同的习惯来选择就好。 比如删除本地分支test_branch :有些时候可能会删除失败，比如如果a分支的代码还没有合并到master，你执行 git branch -d a 是删除不了的，它会智能的提示你a分支还有未合并的代码，但是如果你非要删除，那就执行 git branch -D a 就可以强制删除a分支。 git branch -d test_branch git branch -D test_branch 删除远程分支 : git push origin --delete 分支名 加到暂存区和提交 git add files 把文件的修改添加到暂存区 git commit 比如将暂存区里的提交到当前分支并加入提交信息”update test_file” ，提交之后暂存区就被清空了 git reset -- files 使用当前分支上的修改覆盖暂存区，用来撤销最后一次 git add files git checkout -- files 使用暂存区的修改覆盖工作目录，用来撤销本地修改 可以跳过暂存区域直接从分支中取出修改，或者直接提交修改到分支中。 git commit -a 直接把所有文件的修改添加到暂存区然后执行提交 git checkout HEAD -- files 取出最后一次修改，可以用来进行回滚操作 提交以后，发现提交信息写错了，这时可以使用git commit命令的--amend参数，可以修改上一次的提交信息。 $ git commit --amend -m &quot;Fixes bug #42&quot; 它的原理是产生一个新的提交对象，替换掉上一次提交产生的提交对象.这时如果暂存区有发生变化的文件，会一起提交到仓库。所以，--amend不仅可以修改提交信息，还可以整个把上一次提交替换掉。 远程同步 下载远程仓库的所有变动git fetch [remote] 取回远程仓库的变化，并与本地分支合并git pull [remote] [branch] 上传本地指定分支到远程仓库git push [remote] [branch]将代码推到远端仓库 : 这之前的所有这些add, commit都是本地的操作, 比如我们把本地的master分支推到github的那个项目的master分支 :git push origin master 撤销与回退 比如只是撤销某个文件test_file的修改或者某个目录一下的所有文件的修改(都是指还未被add的) : git checkout -- test_file git checkout -- . git checkout -- temp_folder/ 撤销刚刚的git add : git reset HEAD : 把add了的都从暂存区中移出 git reset HEAD test_file : 只把test_file从暂存区中移出 回退到某个版本(在公司很少用, 因为把之前的commit都弄没了) git reset :git reset 是回到某次提交A，提交A及之前的commit都会被保留，A之后的commit都没有了, A之后的修改都会被退回到暂存区 git reset的作用是修改HEAD的位置，即将HEAD指向的位置改变为之前存在的某个版本 只是把commit回退并且把文件从暂存区中移出, 但保留已有的文件更改 :通用命令为 git reset commit_id, 这个commit_id用git log命令来查看, 比如要恢复到刚刚提交的上一次提交的版本, 就用git reset HEAD^(这句命令的意思是说: 恢复到commit id 为HEAD^的版本, HEAD是指向最新的提交，上一次提交是HEAD^,上上次是HEAD^^,也可以写成HEAD～2 ,依次类推. ) 把commit回退且不保留已有的文件更改(慎用) :git reset --hard commit_id 把git commit撤销(抹除并覆盖, 此命令还算常用) git revert :git revert 是生成一个新的提交来撤销(或者说是抹除并覆盖某次提交)，此次提交之前的commit都会被保留, git revert命令有两个参数: --no-edit(默认参数)：执行时不打开默认编辑器，直接使用 Git 自动生成的提交信息。 --no-commit(一般使用这个)：只抵消暂存区和工作区的文件变化，不产生新的提交。 git revert和git reset的区别总结: git reset 是把HEAD向后移动了一下，而git revert是HEAD继续前进，只是新的commit的内容和要revert的内容正好相反，能够抵消要被revert的内容。 git reset适用场景: 如果想恢复到之前某个提交的版本，且那个版本之后提交的版本我们都不要了，就可以用这种方法 git revert适用场景: 如果我们想撤销之前的某一版本，但是又想保留该目标版本后面的版本，记录下这整个版本变动流程，就可以用这种方法。 git revert 和 git reset 的区别看一个例子 具体一个例子，假设有三个commit: commit3: add test3.c commit2: add test2.c commit1: add test1.c 我们看看以下三种情况: 当执行git revert HEAD~1时， commit2被撤销了git log可以看到： revert “commit2”:this reverts commit 5fe21s2… commit3: add test3.c commit2: add test2.c commit1: add test1.c而git status 没有任何变化 如果换做执行git reset HEAD~1后，运行git log commit2: add test2.c commit1: add test1.c运行git status， 则test3.c处于暂存区，准备提交。 如果换做执行git reset --hard HEAD~1后，显示：HEAD is now at commit2，运行git log commit2: add test2.c commit1: add test1.c运行git status， 没有任何变化 git reset回退之后不能push的问题假设一开始你的本地和远程都是：a -&gt; b -&gt; c你想把HEAD回退git reset到b，那么在本地就变成了：a -&gt; b这个时候，如果没有远程库，你就接着怎么操作都行，比如：a -&gt; b -&gt; d但是在有远程库的情况下，你push会失败，因为远程库是 a-&gt;b-&gt;c，你的是 a-&gt;b-&gt;d此时, push的时候用--force，强制把远程库变成a -&gt; b -&gt; d，不过大部分公司严禁这么干，会被别人揍一顿 综上所述, 一个是撤销某个版本(抹除并覆盖), 一个是回退到某个版本 GitIgnore在git中如果想忽略掉某个文件，不让这个文件提交到版本库中，可以使用修改根目录中 .gitignore 文件的方法（如无，则需自己手工建立此文件）。这个文件每一行保存了一个匹配的规则例如： # 符号为注释 – #开头的那行的内容将被 Git 忽略 *.a # 忽略所有 .a 结尾的文件 !lib.a # 但 lib.a 除外 /TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO build/ # 忽略 build/ 目录下的所有文件 doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt 忽略本地改动且删除已经存在于远端的文件的方式规则很简单，不做过多解释，但是有时候在项目开发过程中，突然心血来潮想把某些目录或文件加入忽略规则，按照上述方法定义后发现并未生效，原因是.gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理中，则修改.gitignore是无效的。那么解决方法就是先把本地缓存删除（改变成未track状态），然后再提交 git rm -r --cached . &amp;&amp; git add . &amp;&amp; git commit -m &apos;update .gitignore&apos; 忽略本地改动但不删除已经存在于远端的文件的方式这种情况 gitignore 搞不定, 需要执行指令 : git update-index --assume-unchanged &lt;file&gt; In this case a file is being tracked in the remote origin repo.You can revert it with : git update-index --no-assume-unchanged &lt;file&gt; If you want to list them : git ls-files -v | grep &apos;^h&apos;. rebase参考： https://www.codercto.com/a/45325.html rebase的原理?假设我们先从 master 分支切出一个 feature1 分支，进行开发, 当在 feature1 分支上执行 git rebase master时: 首先， git 会把 feature1 分支里面的每个 commit 取消掉； 其次，把上面的操作临时保存成 patch 文件，存在 .git/rebase 目录下； 然后，把 feature1 分支更新到最新的 master 分支, 这也是为什么叫做rebase(变基)的原因; 最后，把上面保存的 patch 文件应用到 feature1 分支上； 以上就是rebase的原理 rebase的使用git rebase相对来说是比较复杂的一个命令了,但只要掌握了使用方式,你会深深地喜欢上他,如果有时间我也许会细细地讲一下,现将git rebase的正确使用步骤总结如下, 假设Git目前只有一个分支master。那么开发人员的工作流程是: git clone master branch 在自己本地checkout -b local创建一个本地开发分支 在本地的开发分支上开发和测试 阶段性开发完成后（包含功能代码和单元测试），可以在local分支上commit代码, 然后 首先切换到master分支，git pull拉取最新的分支状态 然后切回local分支 通过git rebase -i 将本地的多次提交合并为一个，以简化提交历史。本地有多个提交时,如果不进行这一步,在git rebase master时会多次解决冲突(最坏情况下,每一个提交都会相应解决一个冲突) git rebase master 将master最新的分支同步到本地，这个过程可能需要手动解决冲突(如果进行了上一步的话,只用解决一次冲突) .在 rebase 的过程中，也许会出现冲突 conflict 。在这种情况， git 会停止 rebase 并会让你去解决冲突。在解决完冲突后，用 git add 命令去更新这些内容。 注意，你无需执行 git commit，只要执行 git rebase --continue, 这样 git 会继续应用余下的 patch 补丁文件。 然后切换到master分支，git merge将本地的local分支内容合并到master分支 git push将master分支的提交上传 如何合并多次提交纪录每一次功能开发， 对多个 commit 进行合并处理。这时候就需要用到 git rebase -i 了。这个命令没有太难，不常用可能源于不熟悉，所以我们来通过示例学习一下: 执行git rebase -i, 我们设置第二个”pick 657a291 add 2.txt” 为” s 657a291 add 2.txt”这里的s就是squash命令的简写。squash的意思是说, 让657a291 add 2.txt这个提交压缩入前一个提交里面去(即a7b18c4 add 1.txt这个提交), 所以git rebase -i之后就把add 2.txt和add 1.txt这两个提交变成一个提交了.跳出来了一个临时文件，最上面是两行commit message。我们修改下这个总体的commit message。 删除之前的两条message(ESC dd)，设置一总的message 然后保存退出。(ESC wq)我们查看下log, git log是不是没有了之前的两个commit。 贮藏stash设想一个场景，假设我们正在一个新的分支做新的功能，这个时候突然有一个紧急的bug需要修复，而且修复完之后需要立即发布。当然你说我先把刚写的一点代码进行提交不就行了么？这样理论上当然是ok的，但是这会产品垃圾commit，原则上我们每次的commit都要有实际的意义，你的代码只是刚写了一半，还没有什么实际的意义是不建议就这样commit的，那么有没有一种比较好的办法，可以让我暂时切到别的分支，修复完bug再切回来，而且代码也能保留的呢？ 试试git stash吧 常用git stash命令： git stash save &quot;save message&quot; : 执行存储时，添加备注，方便查找，只有git stash 也要可以的，但查找时不方便识别。 git stash list ：查看stash了哪些存储 git stash show ：显示做了哪些改动，默认show第一个存储,如果要显示其他存贮，后面加stash@{$num}，比如第二个 git stash show stash@{1} git stash show -p : 显示第一个存储的改动，如果想显示其他存存储，命令：git stash show stash@{$num} -p ，比如第二个：git stash show stash@{1} -p git stash apply :应用某个存储,但不会把存储从存储列表中删除，默认使用第一个存储,即stash@{0}，如果要使用其他个，git stash apply stash@{$num} ， 比如第二个：git stash apply stash@{1} git stash pop ：命令恢复之前缓存的工作目录，将缓存堆栈中的对应stash删除，并将对应修改应用到当前的工作目录下,默认为第一个stash,即stash@{0}，如果要应用并删除其他stash，命令：git stash pop stash@{$num} ，比如应用并删除第二个：git stash pop stash@{1} git stash drop stash@{$num} ：丢弃stash@{$num}存储，从列表中删除这个存储 git stash clear ：删除所有缓存的stash 12345678910111213141516171819202122232425b@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git checkout old_demo error: Your local changes to the following files would be overwritten by checkout: README.mdPlease, commit your changes or stash them before you can switch branches.Abortingb@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git stash Saved working directory and index state WIP on new_test_branch: b3ad5d2 modify mdHEAD is now at b3ad5d2 modify mdb@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git checkout old_demo Switched to branch &apos;old_demo&apos;Your branch is up-to-date with &apos;origin/old_demo&apos;.b@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git checkout new_test_branch Switched to branch &apos;new_test_branch&apos;b@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git stash liststash@&#123;0&#125;: WIP on new_test_branch: b3ad5d2 modify mdb@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git stash pop stash@&#123;0&#125;On branch new_test_branchChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: README.mdno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)Dropped stash@&#123;0&#125; (88cd440c10c80bb6eaef9f4d86ab4a0be3d6dc00) . . . 处理冲突假设这样一个场景，A和B两位同学各自开了两个分支来开发不同的功能，大部分情况下都会尽量互不干扰的，但是有一个需求A需要改动一个基础库中的一个类的方法，不巧B这个时候由于业务需要也改动了基础库的这个方法，因为这种情况比较特殊，A和B都认为不会对地方造成影响，等两人各自把功能做完了，需要合并的到主分支 master 的时候，我们假设先合并A的分支，这个时候没问题的，之后再继续合并B的分支，这个时候想想也知道就有冲突了，因为A和B两个人同时更改了同一个地方，Git 本身他没法判断你们两个谁更改的对，但是这个时候他会智能的提示有 conflicts 就像下面这种情况 :1234b@b-VirtualBox:~/git_test_link/Flock-AI-Fish-Unreal-VR$ git merge plugin Auto-merging README.mdCONFLICT (content): Merge conflict in README.mdAutomatic merge failed; fix conflicts and then commit the result. 打开READ.md文件一看发现冲突的地方如下:12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADmmmp=======nani&gt;&gt;&gt;&gt;&gt;&gt;plugin 冲突的地方由 ==== 分出了上下两个部分，上部分一个叫 HEAD 的字样代表是我当前所在分支的代码，下半部分是一个叫 plugin 分支的代码，可以看到 HEAD 是那里加了一行mmmp，而plugin分支则加了一句nani, 所以我们得跟团队的其他人商量一下看看要改成什么样，而且同时也要把那些 «&lt; HEAD、==== 以及 »»»plugin 这些标记符号也一并删除，最后进行一次 commit 就ok了。 处理完之后, git add那个文件即可 标签tag主要介绍附注标签( annotated tag) 创建附注标签git tag -a v0.1.2 -m &quot;0.1.2版本&quot; 创建轻量标签不需要传递参数，直接指定标签名称即可。创建附注标签时，参数a即annotated的缩写，指定标签类型，后附标签名。参数m指定标签说明，说明信息会保存在标签对象中。 切换到标签与切换分支命令相同，用git checkout [tagname] 查看标签信息用git show命令可以查看标签的版本信息：git show v0.1.2 删除标签误打或需要修改标签时，需要先将标签删除，再打新标签。 删除本地标签git tag -d v0.1.2参数d即delete的缩写，意为删除其后指定的标签。 删除远程标签用法 : git push origin :refs/tags/标签名 或 git push origin --delete tag 标签名 比如 : git push origin :refs/tags/protobuf-2.5.0rc1 或 git push origin --delete tag protobuf-2.5.0rc1 给指定的commit打标签打标签不必要在head之上，也可在之前的版本上打，这需要你知道某个提交对象的校验和（通过git log获取）。git tag -a v0.1.1 9fbc3d0 标签发布通常的git push不会将标签对象提交到git服务器，我们需要进行显式的操作： git push origin v0.1.2 将v0.1.2标签提交到git服务器 git push origin -–tags 将本地所有标签一次性提交到git服务器 对比diff 比如查看当前还未git add的文件的不同 : git diff 比如查看当前已经add 没有commit 的改动 : git diff --cached 比如查看当前所有改动和HEAD的区别(当前还未git add的文件的改动和当前当前已经add 但没有commit 的改动), 也就是上面两条命令的合并 : git diff HEAD 比如查看 commit_id为a和commit_id为b的temp文件夹的差异 : git diff a b temp]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/S游戏架构中延迟补偿的设计和优化方法]]></title>
    <url>%2F2016%2F01%2F06%2Flatency_compensating_methods_in_client_server_in_game_protocol_design_and_optimization%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[自我总结注 : V社这篇文章相当有价值, 所以会有尽可能详细的注解以及对原译文各种翻译纰漏的修正. 广义的延迟补偿主要包括两个方面 : A 如何显示目标 a. 对于本玩家自己 客户端预表现(本文翻译为”客户端预测”) : 比如对于玩家移动的预测, 可以把服务器确认过的movement信息作为开始, 然后使用自己本地的movement input来进行预表现, 服务器跟客户端共享同一套move代码, 当服务器的确认信息过来之后就直接使用服务器发过来的from state进行修正并以from state为基础执行之后的预测. b. 对于其他玩家 i. 外推法 : 即航位推测法, 外推法把其它玩家/物体看作一个点，这个点开始的位置、方向、速度已知，沿着自己的弹道向前移动。因此，假设延时是100ms，最新的协议通知客户端这个玩家奔跑速度是500单位每秒，方向垂直于玩家视线，客户端就可以假设事实上这个玩家当前实际的位置已经向前移动了50个单位。客户端可以在这个外推的位置渲染这个玩家. 这个方法不适用于FPS游戏, 因为大部分FPS游戏采用非现实的玩家系统，玩家可以随时转弯，可以在任意角度作用不现实的加速度，因此外推法得到的结果经常是错误地 ii. 内推法 : 即影子跟随法, 这种方法是用延时来换取平滑, 客户端物体实际移动位置总是滞后一段时间。举个例子，如果服务器每秒同步10次世界信息，客户端渲染的时候会有100ms滞后。这样，每一帧渲染的时候，我们通过最新收到的位置信息和前100ms的位置信息（或者上一帧渲染位置）进行差值得到结果. 如果一个更新包没有收到，有2种处理方法 : 用上面介绍的外推法（有可能产生较大误差）； 保持玩家位于当前位置直到收到下一个更新包(会导致玩家移动顿挫) B. 延迟补偿, 步骤如下 : 为玩家计算一个相当精确的延迟时间 对每个玩家，从服务器历史信息中找一个已发送给这个玩家并且这个玩家已收到的的world update, 这个world update是在这个玩家将要执行这个movement command之前的world update 对于每一个玩家，将其从上述的world update处拉回到这个玩家生成此user command的更新时间中执行用户命令。这个回退时间需要考虑到命令执行的时候的网络延时和插值量 执行玩家命令（包括武器开火等。） 将所有移动的、错位的玩家移动到他们当前正确位置 原文原文出处 原文标题 : Latency Compensating Methods in Client/Server In-game Protocol Design and Optimization Contents 1 Overview 2 Basic Architecture of a Client / Server Game 3 Contents of the User Input messages 4 Client Side Prediction 5 Client-Side Prediction of Weapon Firing 6 Umm, This is a Lot of Work 7 Display of Targets 8 Lag Compensation 9 Game Design Implications of Lag Compensation 10 Conclusion 11 Footnotes Overview Dsigning first-person action games for Internet play is a challenging process. Having robust on-line gameplay in your action title, however, is becoming essential to the success and longevity of the title. In addition, the PC space is well known for requiring developers to support a wide variety of customer setups. Often, customers are running on less than state-of-the-art hardware. The same holds true for their network connections.While broadband has been held out as a panacea for all of the current woes of on-line gaming, broadband is not a simple solution allowing developers to ignore the implications of latency and other network factors in game designs. It will be some time before broadband truly becomes adopted in the United States, and much longer before it can be assumed to exist for your clients in the rest of the world. In addition, there are a lot of poor broadband solutions, where users may occasionally have high bandwidth, but more often than not also have significant latency and packet loss in their connections.Your game must behave well in this world. This discussion will give you a sense of some of the tradeoffs required to deliver a cutting-edge action experience on the Internet. The discussion will provide some background on how client / server architectures work in many on-line action games. In addition, the discussion will show how predictive modeling can be used to mask the effects of latency. Finally, the discussion will describe a specific mechanism, lag compensation, for allowing the game to compensate for connection quality. . . . Basic Architecture of a Client / Server Game Most action games played on the net today are modified client / server games. Games such as Half-Life, including its mods such as Counter-Strike and Team Fortress Classic, operate on such a system, as do games based on the Quake3 engine and the Unreal Tournament engine. In these games, there is a single, authoritative server that is responsible for running the main game logic. To this are connected one or more “dumb” clients. These clients, initially, were nothing more than a way for the user input to be sampled and forwarded to the server for execution. The server would execute the input commands, move around other objects, and then send back to the client a list of objects to render. Of course, the real world system has more components to it, but the simplified breakdown is useful for thinking about prediction and lag compensation.With this in mind, the typical client / server game engine architecture generally looks like this: For this discussion, all of the messaging and coordination needed to start up the connection between client and server is omitted. The client’s frame loop looks something like the following:Sample clock to find start timeSample user input (mouse, keyboard, joystick)Package up and send movement command using simulation timeRead any packets from the server from the network systemUse packets to determine visible objects and their stateRender SceneSample clock to find end timeEnd time minus start time is the simulation time for the next frameEach time the client makes a full pass through this loop, the “frametime” is used for determining how much simulation is needed on the next frame. If your framerate is totally constant then frametime will be a correct measure. Otherwise, the frametimes will be incorrect, but there isn’t really a solution to this (unless you could deterministically figure out exactly how long it was going to take to run the next frame loop iteration before running it…).The server has a somewhat similar loop:Sample clock to find start timeRead client user input messages from networkExecute client user input messagesSimulate server-controlled objects using simulation time from last full passFor each connected client, package up visible objects/world state and send to clientSample clock to find end timeEnd time minus start time is the simulation time for the next frameIn this model, non-player objects run purely on the server, while player objects drive their movements based on incoming packets. Of course, this is not the only possible way to accomplish this task, but it does make sense.Contents of the User Input messagesIn Half-Life engine games, the user input message format is quite simple and is encapsulated in a data structure containing just a few essential fields:typedef struct usercmd_s&#123; // Interpolation time on client short lerp_msec; // Duration in ms of command byte msec; // Command view angles. vec3_t viewangles; // intended velocities // Forward velocity. float forwardmove; // Sideways velocity. float sidemove; // Upward velocity. float upmove; // Attack buttons unsigned short buttons; // // Additional fields omitted… //&#125; usercmd_t;The critical fields here are the msec, viewangles, forward, side, and upmove, and buttons fields. The msec field corresponds to the number of milliseconds of simulation that the command corresponds to (it’s the frametime). The viewangles field is a vector representing the direction the player was looking during the frame. The forward, side, and upmove fields are the impulses determined by examining the keyboard, mouse, and joystick to see if any movement keys were held down. Finally, the buttons field is just a bit field with one or more bits set for each button that is being held down.Using the above data structures and client / server architecture, the core of the simulation is as follows. First, the client creates and sends a user command to the server. The server then executes the user command and sends updated positions of everything back to client. Finally, the client renders the scene with all of these objects. This core, though quite simple, does not react well under real world situations, where users can experience significant amounts of latency in their Internet connections. The main problem is that the client truly is “dumb” and all it does is the simple task of sampling movement inputs and waiting for the server to tell it the results. If the client has 500 milliseconds of latency in its connection to the server, then it will take 500 milliseconds for any client actions to be acknowledged by the server and for the results to be perceptible on the client. While this round trip delay may be acceptable on a Local Area Network (LAN), it is not acceptable on the Internet.Client Side PredictionOne method for ameliorating this problem is to perform the client’s movement locally and just assume, temporarily, that the server will accept and acknowledge the client commands directly. This method is labeled as client-side prediction.Client-side prediction of movements requires us to let go of the “dumb” or minimal client principle. That’s not to say that the client is fully in control of its simulation, as in a peer-to-peer game with no central server. There still is an authoritative server running the simulation just as noted above. Having an authoritative server means that even if the client simulates different results than the server, the server’s results will eventually correct the client’s incorrect simulation. Because of the latency in the connection, the correction might not occur until a full round trip’s worth of time has passed. The downside is that this can cause a very perceptible shift in the player’s position due to the fixing up of the prediction error that occurred in the past.To implement client-side prediction of movement, the following general procedure is used. As before, client inputs are sampled and a user command is generated. Also as before, this user command is sent off to the server. However, each user command (and the exact time it was generated) is stored on the client. The prediction algorithm uses these stored commands.For prediction, the last acknowledged movement from the server is used as a starting point. The acknowledgement indicates which user command was last acted upon by the server and also tells us the exact position (and other state data) of the player after that movement command was simulated on the server. The last acknowledged command will be somewhere in the past if there is any lag in the connection. For instance, if the client is running at 50 frames per second (fps) and has 100 milliseconds of latency (roundtrip), then the client will have stored up five user commands ahead of the last one acknowledged by the server. These five user commands are simulated on the client as a part of client-side prediction. Assuming full prediction1, the client will want to start with the latest data from the server, and then run the five user commands through “similar logic” to what the server uses for simulation of client movement. Running these commands should produce an accurate final state on the client (final player position is most important) that can be used to determine from what position to render the scene during the current frame.In Half-Life, minimizing discrepancies between client and server in the prediction logic is accomplished by sharing the identical movement code for players in both the server-side game code and the client-side game code. These are the routines in the pm_shared/ (which stands for “player movement shared”) folder of the HL SDK. The input to the shared routines is encapsulated by the user command and a “from” player state. The output is the new player state after issuing the user command. The general algorithm on the client is as follows:“from state” &lt;- state after last user command acknowledged by the server;“command” &lt;- first command after last user command acknowledged by server;while (true){ run “command” on “from state” to generate “to state”; if (this was the most up to date “command”) break; “from state” = “to state”; “command” = next “command”;};The origin and other state info in the final “to state” is the prediction result and is used for rendering the scene that frame. The portion where the command is run is simply the portion where all of the player state data is copied into the shared data structure, the user command is processed (by executing the common code in the pm_shared routines in Half-Life’s case), and the resulting data is copied back out to the “to state”.There are a few important caveats to this system. First, you’ll notice that, depending upon the client’s latency and how fast the client is generating user commands (i.e., the client’s framerate), the client will most often end up running the same commands over and over again until they are finally acknowledged by the server and dropped from the list (a sliding window in Half-Life’s case) of commands yet to be acknowledged. The first consideration is how to handle any sound effects and visual effects that are created in the shared code. Because commands can be run over and over again, it’s important not to create footstep sounds, etc. multiple times as the old commands are re-run to update the predicted position. In addition, it’s important for the server not to send the client effects that are already being predicted on the client. However, the client still must re-run the old commands or else there will be no way for the server to correct any erroneous prediction by the client. The solution to this problem is easy: the client just marks those commands which have not been predicted yet on the client and only plays effects if the user command is being run for the first time on the client.The other caveat is with respect to state data that exists solely on the client and is not part of the authoritative update data from the server. If you don’t have any of this type of data, then you can simply use the last acknowledged state from the server as a starting point, and run the prediction user commands “in-place” on that data to arrive at a final state (which includes your position for rendering). In this case, you don’t need to keep all of the intermediate results along the route for predicting from the last acknowledged state to the current time. However, if you are doing any logic totally client side (this logic could include functionality such as determining where the eye position is when you are in the process of crouching—and it’s not really totally client side since the server still simulates this data also) that affects fields that are not replicated from the server to the client by the networking layer handling the player’s state info, then you will need to store the intermediate results of prediction. This can be done with a sliding window, where the “from state” is at the start and then each time you run a user command through prediction, you fill in the next state in the window. When the server finally acknowledges receiving one or more commands that had been predicted, it is a simple matter of looking up which state the server is acknowledging and copying over the data that is totally client side to the new starting or “from state”.So far, the above procedure describes how to accomplish client side prediction of movements. This system is similar to the system used in QuakeWorld2.Client-Side Prediction of Weapon FiringLayering prediction of the firing effects of weapons onto the above system is straightforward. Additional state information is needed for the local player on the client, of course, including which weapons are being held, which one is active, and how much ammo each of these weapons has remaining. With this information, the firing logic can be layered on top of the movement logic because, once again, the state of the firing buttons is included in the user command data structure that is shared between the client and the server. Of course, this can get complicated if the actual weapon logic is different between client and server. In Half-Life, we chose to avoid this complication by moving the implementation of a weapon’s firing logic into “shared code” just like the player movement code. All of the variables that contribute to determining weapon state (e.g., ammo, when the next firing of the weapon can occur, what weapon animation is playing, etc.), are then part of the authoritative server state and are replicated to the client-side so there, they can be used for prediction of weapon state.Predicting weapon firing on the client will likely lead to the decision also to predict weapon switching, deployment, and holstering. In this fashion, the user feels that the game is 100% responsive to his or her movement and weapon activation activities. This goes a long way toward reducing the feeling of latency that many players have come to endure with today’s Internet-enabled action experiences.Umm, This is a Lot of WorkReplicating the necessary fields to the client and handling all of the intermediate state is a fair amount of work. At this point, you may be asking, why not eliminate all of the server stuff and just have the client report where s/he is after each movement? In other words, why not ditch the server stuff and just run the movement and weapons purely on the client-side? Then, the client would just send results to the server along the lines of, “I’m now at position x and, by the way, I just shot player 2 in the head.” This is fine if you can trust the client. This is how a lot of the military simulation systems work (i.e., they are a closed system and they trust all of the clients). This is how peer-to-peer games generally work. For Half-Life, this mechanism is unworkable because of realistic concerns about cheating. If we encapsulated absolute state data in this fashion, we’d raise the motivation to hack the client even higher than it already is3. For our games, this risk is too high and we fall back to requiring an authoritative server.A system where movements and weapon effects are predicted client-side is a very workable system. For instance, this is the system that the Quake3 engine supports. One of the problems with this system is that you still have to have a feel for your latency to determine how to lead your targets (for instant hit weapons). In other words, although you get to hear the weapons firing immediately, and your position is totally up-to-date, the results of your shots are still subject to latency. For example, if you are aiming at a player running perpendicular to your view and you have 100 milliseconds of latency and the player is running at 500 units per second, then you’ll need to aim 50 units in front of the target to hit the target with an instant hit weapon. The greater the latency, the greater the lead targeting needed. Getting a “feel” for your latency is difficult. Quake3 attempted to mitigate this by playing a brief tone whenever you received confirmation of your hits. That way, you could figure out how far to lead by firing your weapons in rapid succession and adjusting your leading amount until you started to hear a steady stream of tones. Obviously, with sufficient latency and an opponent who is actively dodging, it is quite difficult to get enough feedback to focus in on the opponent in a consistent fashion. If your latency is fluctuating, it can be even harder.Display of TargetsAnother important aspect influencing how a user perceives the responsiveness of the world is the mechanism for determining, on the client, where to render the other players. The two most basic mechanisms for determining where to display objects are extrapolation and interpolation4.For extrapolation, the other player/object is simulated forward in time from the last known spot, direction, and velocity in more or less a ballistic manner. Thus, if you are 100 milliseconds lagged, and the last update you received was that (as above) the other player was running 500 units per second perpendicular to your view, then the client could assume that in “real time” the player has moved 50 units straight ahead from that last known position. The client could then just draw the player at that extrapolated position and the local player could still more or less aim right at the other player.The biggest drawback of using extrapolation is that player’s movements are not very ballistic, but instead are very non-deterministic and subject to high jerk5. Layer on top of this the unrealistic player physics models that most FPS games use, where player’s can turn instantaneously and apply unrealistic forces to create huge accelerations at arbitrary angles and you’ll see that the extrapolation is quite often incorrect. The developer can mitigate the error by limiting the extrapolation time to a reasonable value (QuakeWorld, for instance, limited extrapolation to 100 milliseconds). This limitation helps because, once the true player position is finally received, there will be a limited amount of corrective warping. In a world where most players still have greater than 150 milliseconds of latency, the player must still lead other players in order to hit them. If those players are “warping” to new spots because of extrapolation errors, then the gameplay suffers nonetheless.The other method for determining where to display objects and players is interpolation. Interpolation can be viewed as always moving objects somewhat in the past with respect to the last valid position received for the object. For instance, if the server is sending 10 updates per second (exactly) of the world state, then we might impose 100 milliseconds of interpolation delay in our rendering. Then, as we render frames, we interpolate the position of the object between the last updated position and the position one update before that (alternatively, the last render position) over that 100 milliseconds. As the object just gets to the last updated position, we receive a new update from the server (since 10 updates per second means that the updates come in every 100 milliseconds) we can start moving toward this new position over the next 100 milliseconds.If one of the update packets fails to arrive, then there are two choices: We can start extrapolating the player position as noted above (with the large potential errors noted) or we can simply have the player rest at the position in the last update until a new update arrives (causing the player’s movement to stutter).The general algorithm for this type of interpolation is as follows:Each update contains the server time stamp for when it was generated6From the current client time, the client computes a target time by subtracting the interpolation time delta (100 ms)If the target time is in between the timestamp of the last update and the one before that, then those timestamps determine what fraction of the time gap has passed.This fraction is used to interpolate any values (e.g., position and angles).In essence, you can think of interpolation, in the above example, as buffering an additional 100 milliseconds of data on the client. The other players, therefore, are drawn where they were at a point in the past that is equal to your exact latency plus the amount of time over which you are interpolating. To deal with the occasional dropped packet, we could set the interpolation time as 200 milliseconds instead of 100 milliseconds. This would (again assuming 10 updates per second from the server) allow us to entirely miss one update and still have the player interpolating toward a valid position, often moving through this interpolation without a hitch. Of course, interpolating for more time is a tradeoff, because it is trading additional latency (making the interpolated player harder to hit) for visual smoothness.In addition, the above type of interpolation (where the client tracks only the last two updates and is always moving directly toward the most recent update) requires a fixed time interval between server updates. The method also suffers from visual quality issues that are difficult to resolve. The visual quality issue is as follows. Imagine that the object being interpolated is a bouncing ball (which actually accurately describes some of our players). At the extremes, the ball is either high in the air or hitting the pavement. However, on average, the ball is somewhere in between. If we only interpolate to the last position, it is very likely that this position is not on the ground or at the high point. The bounciness of the ball is “flattened” out and it never seems to hit the ground. This is a classical sampling problem and can be alleviated by sampling the world state more frequently. However, we are still quite likely never actually to have an interpolation target state be at the ground or at the high point and this will still flatten out the positions.In addition, because different users have different connections, forcing updates to occur at a lockstep like 10 updates per second is forcing a lowest common denominator on users unnecessarily. In Half-Life, we allow the user to ask for as many updates per second as he or she wants (within limit). Thus, a user with a fast connection could receive 50 updates per second if the user wanted. By default, Half-Life sends 20 updates per second to each player the Half-Life client interpolates players (and many other objects) over a period of 100 milliseconds.7To avoid the flattening of the bouncing ball problem, we employ a different algorithm for interpolation. In this method, we keep a more complete “position history” for each object that might be interpolated.The position history is the timestamp and origin and angles (and could include any other data we want to interpolate) for the object. Each update we receive from the server creates a new position history entry, including timestamp and origin/angles for that timestamp. To interpolate, we compute the target time as above, but then we search backward through the history of positions looking for a pair of updates that straddle the target time. We then use these to interpolate and compute the final position for that frame. This allows us to smoothly follow the curve that completely includes all of our sample points. If we are running at a higher framerate than the incoming update rate, we are almost assured of smoothly moving through the sample points, thereby minimizing (but not eliminating, of course, since the pure sampling rate of the world updates is the limiting factor) the flattening problem described above.The only consideration we have to layer on top of either interpolation scheme is some way to determine that an object has been forcibly teleported, rather than just moving really quickly. Otherwise we might “smoothly” move the object over great distances, causing the object to look like it’s traveling way too fast. We can either set a flag in the update that says, “don’t interpolate” or “clear out the position history,” or we can determine if the distance between the origin and one update and another is too big, and thereby presumed to be a teleportation/warp. In that case, the solution is probably to just move the object to the latest know position and start interpolating from there.Lag CompensationUnderstanding interpolation is important in designing for lag compensation because interpolation is another type of latency in a user’s experience. To the extent that a player is looking at other objects that have been interpolated, then the amount of interpolation must be taken into consideration in computing, on the server, whether the player’s aim was true.Lag compensation is a method of normalizing server-side the state of the world for each player as that player’s user commands are executed. You can think of lag compensation as taking a step back in time, on the server, and looking at the state of the world at the exact instant that the user performed some action. The algorithm works as follows:Before executing a player’s current user command, the server:Computes a fairly accurate latency for the playerSearches the server history (for the current player) for the world update that was sent to the player and received by the player just before the player would have issued the movement commandFrom that update (and the one following it based on the exact target time being used), for each player in the update, move the other players backwards in time to exactly where they were when the current player’s user command was created. This moving backwards must account for both connection latency and the interpolation amount8 the client was using that frame.Allow the user command to execute (including any weapon firing commands, etc., that will run ray casts against all of the other players in their “old” positions).Move all of the moved/time-warped players back to their correct/current positionsNote that in the step where we move the player backwards in time, this might actually require forcing additional state info backwards, too (for instance, whether the player was alive or dead or whether the player was ducking). The end result of lag compensation is that each local client is able to directly aim at other players without having to worry about leading his or her target in order to score a hit. Of course, this behavior is a game design tradeoff.Game Design Implications of Lag CompensationThe introduction of lag compensation allows for each player to run on his or her own clock with no apparent latency. In this respect, it is important to understand that certain paradoxes or inconsistencies can occur. Of course, the old system with the authoritative server and “dumb” or simple clients had it’s own paradoxes. In the end, making this tradeoff is a game design decision. For Half-Life, we believe deciding in favor of lag compensation was a justified game design decision.The first problem of the old system was that you had to lead your target by some amount that was related to your latency to the server. Aiming directly at another player and pressing the fire button was almost assured to miss that player. The inconsistency here is that aiming is just not realistic and that the player controls have non-predictable responsiveness.With lag compensation, the inconsistencies are different. For most players, all they have to do is acquire some aiming skill and they can become proficient (you still have to be able to aim). Lag compensation allows the player to aim directly at his or her target and press the fire button (for instant hit weapons9). The inconsistencies that sometimes occur, however, are from the points of view of the players being fired upon.For instance, if a highly lagged player shoots at a less lagged player and scores a hit, it can appear to the less lagged player that the lagged player has somehow “shot around a corner”10. In this case, the lower lag player may have darted around a corner. But the lagged player is seeing everything in the past. To the lagged player, s/he has a direct line of sight to the other player. The player lines up the crosshairs and presses the fire button. In the meantime, the low lag player has run around a corner and maybe even crouched behind a crate. If the high lag player is sufficiently lagged, say 500 milliseconds or so, this scenario is quite possible. Then, when the lagged player’s user command arrives at the server, the hiding player is transported backward in time and is hit. This is the extreme case, and in this case, the low ping player says that s/he was shot from around the corner. However, from the lagged player’s point of view, they lined up their crosshairs on the other player and fired a direct hit. From a game design point of view, the decision for us was easy: let each individual player have completely responsive interaction with the world and his or her weapons.In addition, the inconsistency described above is much less pronounced in normal combat situations. For first-person shooters, there are two more typical cases. First, consider two players running straight at each other pressing the fire button. In this case, it’s quite likely that lag compensation will just move the other player backwards along the same line as his or her movement. The person being shot will be looking straight at his attacker and no “bullets bending around corners” feeling will be present.The next example is two players, one aiming at the other while the other dashes in front perpendicular to the first player. In this case, the paradox is minimized for a wholly different reason. The player who is dashing across the line of sight of the shooter probably has (in first-person shooters at least) a field of view of 90 degrees or less. In essence, the runner can’t see where the other player is aiming. Therefore, getting shot isn’t going to be surprising or feel wrong (you get what you deserve for running around in the open like a maniac). Of course, if you have a tank game, or a game where the player can run one direction, and look another, then this scenario is less clear-cut, since you might see the other player aiming in a slightly incorrect direction.ConclusionLag compensation is a tool to ameliorate the effects of latency on today’s action games. The decision of whether to implement such a system rests with the game designer since the decision directly changes the feel of the game. For Half-Life, Team Fortress and Counter Strike, the benefits of lag compensation easily outweighed the inconsistencies noted above.Footnotes In the Half-Life engine, it is possible to ask the client-side prediction algorithm to account for some, but not all, of the latency in performing prediction. The user could control the amount of prediction by changing the value of the “pushlatency” console variable to the engine. This variable is a negative number indicating the maximum number of milliseconds of prediction to perform. If the number is greater (in the negative) than the user’s current latency, then full prediction up to the current time occurs. In this case, the user feels zero latency in his or her movements. Based upon some erroneous superstition in the community, many users insisted that setting pushlatency to minus one-half of the current average latency was the proper setting. Of course, this would still leave the player’s movements lagged (often described as if you are moving around on ice skates) by half of the user’s latency. All of this confusion has brought us to the conclusion that full prediction should occur all of the time and that the pushlatency variable should be removed from the Half-Life engine. (Return) http://www.quakeforge.net/files/q1source.zip (Return) A discussion of cheating and what developers can do to deter it is beyond the scope of this paper. (Return) Though hybrids and corrective methods are also possible. (Return) “Jerk” is a measure of how fast accelerative forces are changing. (Return) It is assumed in this paper that the client clock is directly synchronized to the server clock modulo the latency of the connection. In other words, the server sends the client, in each update, the value of the server’s clock and the client adopts that value as its clock. Thus, the server and client clocks will always be matched, with the client running the same timing somewhat in the past (the amount in the past is equal to the client’s current latency). Smoothing out discrepancies in the client clock can be solved in various ways. (Return) The time spacing of these updates is not necessarily fixed. The reason why is that during high activity periods of the game (especially for users with lower bandwidth connections), it’s quite possible that the game will want to send you more data than your connection can accommodate. If we were on a fixed update interval, then you might have to wait an entire additional interval before the next packet would be sent to the client. However, this doesn’t match available bandwidth effectively. Instead, the server, after sending every packet to a player, determines when the next packet can be sent. This is a function of the user’s bandwidth or “rate” setting and the number of updates requested per second. If the user asks for 20 updates per second, then it will be at least 50 milliseconds before the next update packet can be sent. If the bandwidth choke is active (and the server is sufficiently high framerate), it could be 61, etc., milliseconds before the next packet gets sent. Thus, Half-Life packets can be somewhat arbitrarily spaced. The simple move to latest goal interpolation schemes don’t behave as well (think of the old anchor point for movement as being variable) under these conditions as the position history interpolation method (described below). (Return) Which Half-Life encodes in the lerp_msec field of the usercmd_t structure described previously. (Return) For weapons that fire projectiles, lag compensation is more problematic. For instance, if the projectile lives autonomously on the server, then what time space should the projectile live in? Does every other player need to be “moved backward” every time the projectile is ready to be simulated and moved by the server? If so, how far backward in time should the other players be moved? These are interesting questions to consider. In Half-Life, we avoided them; we simply don’t lag compensate projectile objects (that’s not to say that we don’t predict the sound of you firing the projectile on the client, just that the actual projectile is not lag compensated in any way). (Return) This is the phrase our user community has adopted to describe this inconsistency. (Return)中英对照译文出处Overview综述Designing first-person action games for Internet play is a challenging process. Having robust on-line gameplay in your action title, however, is becoming essential to the success and longevity of the title. In addition, the PC space is well known for requiring developers to support a wide variety of customer setups. Often, customers are running on less than state-of-the-art hardware. The same holds true for their network connections.第一人称角色网络游戏的设计是一项很有挑战性的工作。网络环境下的健壮性，是动作游戏能否成功的一个重要因素。另外，PC上面的开发者需要考虑到玩家层次不齐的机器配置以及网络状况，很多用户的硬件配置跟网络跟当前最好的配置跟网络有一定差距。While broadband has been held out as a panacea for all of the current woes of on-line gaming, broadband is not a simple solution allowing developers to ignore the implications of latency and other network factors in game designs. It will be some time before broadband truly becomes adopted the United States, and much longer before it can be assumed to exist for your clients in the rest of the world. In addition, there are a lot of poor broadband solutions, where users may occasionally have high bandwidth, but more often than not also have significant latency and packet loss in their connections.宽带网络的出现有利于在线游戏开发，但是开发者还是需要考虑网络延迟和其它网络特性。而且宽带网络在美国被广泛采用还需要一段时间，在世界上其它国家可能需要更长的一段时间。另外，很多宽带网络质量很差，用户虽然偶尔能够享受到高带宽，但更多的时候他们不得不面对高延迟和高丢包率。Your game must behave well in this world. This discussion will give you a sense of some of the tradeoffs required to deliver a cutting-edge action experience on the Internet. The discussion will provide some background on how client / server architectures work in many on-line action games. In addition, the discussion will show how predictive modeling can be used to mask the effects of latency. Finally, the discussion will describe a specific mechanism, lag compensation, for allowing the game to compensate for connection quality.我们应该提供给玩家良好的游戏。本篇文章讨论了如何提供给玩家顶尖的操作体验；介绍了很多在线动作游戏中采用的C/S架构背景。此外，我们还讨论了如何通过一个预测模型来掩饰延迟带来的影响。文章的最后描述了一个叫做延迟补偿的机制，弥补了因为网络质量不好带来的负面影响Basic Architecture of a Client / Server GameC/S游戏的基本架构Most action games played on the net today are modified client / server games. Games such as Half-Life, including its mods such as Counter-Strike and Team Fortress Classic, operate on such a system, as do games based on the Quake3 engine and the Unreal Tournament engine. In these games, there is a single, authoritative server that is responsible for running the main game logic. To this are connected one or more &quot;dumb&quot; clients. These clients, initially, were nothing more than a way for the user input to be sampled and forwarded to the server for execution. The server would execute the input commands, move around other objects, and then send back to the client a list of objects to render. Of course, the real world system has more components to it, but the simplified breakdown is useful for thinking about prediction and lag compensation.网络上可玩的大部分动作游戏都是C/S结构游戏基础上修改完成的，比如半条命以及其修改版反恐精英、军团要塞，以及一些基于quake3引擎和虚幻引擎的游戏。这类游戏都有一个用来执行游戏逻辑的服务器以及连接到这个服务器的多个客户端。客户端仅仅是用来接收玩家的操作并发给服务器，服务器对这些操作作出响应，移动玩家周围物体，并将游戏世界的信息发给客户端显示出来。当然世界的游戏系统有更多组件，我们这样简化有利于分析预测和延迟补偿。With this in mind, the typical client / server game engine architecture generally looks like this:基于这种考虑，典型的C/S游戏引擎通常看起来是这样的For this discussion, all of the messaging and coordination needed to start up the connection between client and server is omitted. The client’s frame loop looks something like the following:为了便于讨论，我们假定客户端跟服务器之间已经建立连接；客户端的每一帧循环如下：1.Sample clock to find start time1.获取帧开始时间2.Sample user input (mouse, keyboard, joystick)2.采集用户输入3.Package up and send movement command using simulation time3.根据模拟时间将移动命令打包发送给服务器4.Read any packets from the server from the network system4.获取处理服务器传过来的数据包5.Use packets to determine visible objects and their state5.根据服务器数据包的内容决定可见物体及其状态6.Render Scene6.渲染场景7.Sample clock to find end time7.获取帧结束时间8.End time minus start time is the simulation time for the next frame8.结束时间减去开始时间就是下一帧的模拟时间Each time the client makes a full pass through this loop, the &quot;frametime&quot; is used for determining how much simulation is needed on the next frame. If your framerate is totally constant then frametime will be a correct measure. Otherwise, the frametimes will be incorrect, but there isn’t really a solution to this (unless you could deterministically figure out exactly how long it was going to take to run the next frame loop iteration before running it…).客户端每完成一个帧循环，就用“frametime”来决定下一帧需要多少时间，如果帧率恒定，“frametime”就是准确的，否则就没办法获得准确的“frametime”（因为在每一帧开始之前你不可能知道这一帧需要多长时间）The server has a somewhat similar loop:服务器的循环大同小异：1.Sample clock to find start time1.获取帧开始时间2.Read client user input messages from network2.读取客户端发过来的操作信息3.Execute client user input messages3.根据客户端操作执行逻辑运算4.Simulate server-controlled objects using simulation time from last full pass4.采用上一个循环得到的模拟时间来模拟服务器控制的物体移动状态5.For each connected client, package up visible objects/world state and send to client5.对每一个连接的客户端，发送打包相应的物体/世界状态6.Sample clock to find end time6.获取帧结束时间7.End time minus start time is the simulation time for the next frame7.结束时间减去开始时间就是下一帧的模拟时间In this model, non-player objects run purely on the server, while player objects drive their movements based on incoming packets. Of course, this is not the only possible way to accomplish this task, but it does make sense.在这个模型中，非玩家物体完全由服务器控制其状态，每个玩家根据服务器发过来的数据包控制自己的移动。这是一种很自然的方法，当然还有其它的方法也可以完成这个功能。Contents of the User Input messages用户消息的内容In Half-Life engine games, the user input message format is quite simple and is encapsulated in a data structure containing just a few essential fields:基于half-life引擎的游戏用户消息都很简单，只需要封装在一个包含几个关键成员的结构中：typedef struct usercmd_s{// Interpolation time on clientshortlerp_msec; &nbsp;&nbsp;// Duration in ms of commandbytemsec; &nbsp; &nbsp; &nbsp;// Command view angles.vec3_tviewangles; &nbsp;&nbsp;// intended velocities// Forward velocity.floatforwardmove; &nbsp;// Sideways velocity.floatsidemove; &nbsp; &nbsp;// Upward velocity.floatupmove; &nbsp;&nbsp;// Attack buttonsunsigned short buttons;&nbsp;//// Additional fields omitted…//} usercmd_t;The critical fields here are the msec, viewangles, forward, side, and upmove, and buttons fields. The msec field corresponds to the number of milliseconds of simulation that the command corresponds to (it’s the frametime). The viewangles field is a vector representing the direction the player was looking during the frame. The forward, side, and upmove fields are the impulses determined by examining the keyboard, mouse, and joystick to see if any movement keys were held down. Finally, the buttons field is just a bit field with one or more bits set for each button that is being held down.结构中最关键的变量时msec,viewangles,forward,side,upmove和buttons。msec表示这个命令执行对应的毫秒数（也就是上面提到的“frametime”）。viewangles是一个三维向量，表示玩家的朝向。forward,side和upmove表示玩家是否通过键盘、鼠标或控制杆控制移动。最后，buttons这个字段包含一个或多个比特，标志玩家是否按着某些按键。Using the above data structures and client / server architecture, the core of the simulation is as follows. First, the client creates and sends a user command to the server. The server then executes the user command and sends updated positions of everything back to client. Finally, the client renders the scene with all of these objects. This core, though quite simple, does not react well under real world situations, where users can experience significant amounts of latency in their Internet connections. The main problem is that the client truly is &quot;dumb&quot; and all it does is the simple task of sampling movement inputs and waiting for the server to tell it the results. If the client has 500 milliseconds of latency in its connection to the server, then it will take 500 milliseconds for any client actions to be acknowledged by the server and for the results to be perceptible on the client. While this round trip delay may be acceptable on a Local Area Network (LAN), it is not acceptable on the Internet.基于C/S架构的游戏采用以上数据结构运行如下：客户端创建命令并发送到服务器，服务器响应这些命令并把更新了的世界和物体位置信息发回客户端，客户端收到以后进行渲染。这种方式非常简单，但是在实际应用中效果差强人意，用户会感觉到网络连接带来的明显延迟。这主要是由于客户端完全没有逻辑操作，发出消息以后就等待服务器响应。如果客户端跟服务器有500ms的延迟，客户端执行了操作到看到操作的结果就需要500ms，这种延迟在局域网通常可以接受（因为通常延迟比较小），但在因特网上是没法接受的Client Side Prediction客户端预测One method for ameliorating this problem is to perform the client’s movement locally and just assume, temporarily, that the server will accept and acknowledge the client commands directly. This method is labeled as client-side prediction.有一种方法可以改善这种情况：客户端本地即时执行移动操作，假定服务器即时通知客户端可以执行操作，这种方法可以称为客户端预测。Client-side prediction of movements requires us to let go of the &quot;dumb&quot; or minimal client principle. That’s not to say that the client is fully in control of its simulation, as in a peer-to-peer game with no central server. There still is an authoritative server running the simulation just as noted above. Having an authoritative server means that even if the client simulates different results than the server, the server’s results will eventually correct the client’s incorrect simulation. Because of the latency in the connection, the correction might not occur until a full round trip’s worth of time has passed. The downside is that this can cause a very perceptible shift in the player’s position due to the fixing up of the prediction error that occurred in the past.采用客户端运动预测以后，客户端就不再是一个“小型客户端”，不再单单响应服务器命令；但也不是说客户端可以像没有中央服务器的p2p游戏完全自治。服务器仍然在运行并保证在客户端跟服务器运行结果不一致的情况下纠正客户端错误的模拟。由于网络延迟，修正在一个网络传输周期以后才会执行，这个时候纠正信息通常已经过期，这样会导致明显的位置漂移，因为客户端收到的修正信息是过去某个时间的。To implement client-side prediction of movement, the following general procedure is used. As before, client inputs are sampled and a user command is generated. Also as before, this user command is sent off to the server. However, each user command (and the exact time it was generated) is stored on the client. The prediction algorithm uses these stored commands.为了使客户端运动预测有效，我们采用以下方法：还是客户端采样并生成命令发送到服务器，但是每个包含生成时间的命令在客户端本地存起来并在预测算法中使用。For prediction, the last acknowledged movement from the server is used as a starting point. The acknowledgement indicates which user command was last acted upon by the server and also tells us the exact position (and other state data) of the player after that movement command was simulated on the server. The last acknowledged command will be somewhere in the past if there is any lag in the connection. For instance, if the client is running at 50 frames per second (fps) and has 100 milliseconds of latency (roundtrip), then the client will have stored up five user commands ahead of the last one acknowledged by the server. These five user commands are simulated on the client as a part of client-side prediction. Assuming full prediction[1], the client will want to start with the latest data from the server, and then run the five user commands through &quot;similar logic&quot; to what the server uses for simulation of client movement. Running these commands should produce an accurate final state on the client (final player position is most important) that can be used to determine from what position to render the scene during the current frame.预测的过程中，我们把服务器确认的移动信息作为开始，这样客户端就可以确定服务器执行上次命令以后游戏中玩家的准确信息（比如位置）。如果网络有延迟，这个确认命令也会有一定延迟。假设客户端运行帧率为50fps，网络延时为100ms，这样在客户端收到服务器的确认命令的时候，本地命令队列中已经有5条信息，这5条信息被用来执行客户端预测。假设执行完全预测【1】客户端在收到来自服务器的最新信息后，就开始按照与服务器相同的逻辑执行本地消息队列中的5个命令。这些命令执行以后得到当前状态（最重要的是玩家最后的位置），然后根据玩家的状态信息渲染当前帧。In Half-Life, minimizing discrepancies between client and server in the prediction logic is accomplished by sharing the identical movement code for players in both the server-side game code and the client-side game code. These are the routines in the pm_shared/ (which stands for &quot;player movement shared&quot;) folder of the HL SDK. The input to the shared routines is encapsulated by the user command and a &quot;from&quot; player state. The output is the new player state after issuing the user command. The general algorithm on the client is as follows:在半条命这个游戏中，客户端跟服务器采用相同的代码来计算移动，这样可以减小客户端预测跟服务器之间的误差。这些代码位于HLSDK中的pm_shared/（意思是“player movement shared”）。这段代码的输入是玩家操作和客户端的初始状态，输出是玩家操作以后的状态。客户端算法大致如下： 12345678910111213“from state” &lt;- state after last user command acknowledged by the server;“command” &lt;- first command after last user command acknowledged by server;while (true)&#123; run “command” on “from state” to generate “to state”; if (this was the most up to date “command”) break; “from state” = “to state”; “command” = next “command”;&#125;; 123456789101112“初始状态” &lt;- 上一条已被服务器确认过的玩家command之后的state"命令" &lt;- 上一条已被服务器确认过的玩家command之后的commandwhile (true)&#123; 以"from state" 为基础执行"command"得到 "to state"; if (这是最新的 "command") break; "from state" = "to state"; "command" = next "command";&#125; The origin and other state info in the final &quot;to state&quot; is the prediction result and is used for rendering the scene that frame. The portion where the command is run is simply the portion where all of the player state data is copied into the shared data structure, the user command is processed (by executing the common code in the pm_shared routines in Half-Life’s case), and the resulting data is copied back out to the &quot;to state&quot;.玩家的初始状态和预测结果用来渲染场景。命令的执行过程就是：将玩家状态复制到共享数据结构中，执行玩家操作（执行hlsdk中pm_shared中的共用代码），然后将结果复制到目标状态（to state）There are a few important caveats to this system. First, you’ll notice that, depending upon the client’s latency and how fast the client is generating user commands (i.e., the client’s framerate), the client will most often end up running the same commands over and over again until they are finally acknowledged by the server and dropped from the list (a sliding window in Half-Life’s case) of commands yet to be acknowledged. The first consideration is how to handle any sound effects and visual effects that are created in the shared code. Because commands can be run over and over again, it’s important not to create footstep sounds, etc. multiple times as the old commands are re-run to update the predicted position. In addition, it’s important for the server not to send the client effects that are already being predicted on the client. However, the client still must re-run the old commands or else there will be no way for the server to correct any erroneous prediction by the client. The solution to this problem is easy: the client just marks those commands which have not been predicted yet on the client and only plays effects if the user command is being run for the first time on the client.这个系统中有几个需要注意的地方，首先，由于网络延迟，客户端又在不停地以一定速度（客户端帧率）生成命令，一个命令通常会被客户端反复执行，直到得到服务器的确认以后才将其从命令列表中删除（这就是半条命中的滑动窗口）。首先要考虑的是如何处理共享代码中生成的声效和动画效果。因为命令可能会被多次执行，预测位置的过程被多次执行的时候要注意避免重声等不正确的效果。另外，服务器也要避免客户端意见预测的效果。然而，客户端必须重新运行旧的命令，否则就没法根据服务器来纠正客户端的预测错误。解决方法很简单：客户端将没有执行的客户端命令进行标记，如果这些命令在客户端第一次执行，则播放相应的效果。The other caveat is with respect to state data that exists solely on the client and is not part of the authoritative update data from the server. If you don’t have any of this type of data, then you can simply use the last acknowledged state from the server as a starting point, and run the prediction user commands &quot;quot;in-place&quot; on that data to arrive at a final state (which includes your position for rendering). In this case, you don’t need to keep all of the intermediate results along the route for predicting from the last acknowledged state to the current time. However, if you are doing any logic totally client side (this logic could include functionality such as determining where the eye position is when you are in the process of crouching—and it’s not really totally client side since the server still simulates this data also) that affects fields that are not replicated from the server to the client by the networking layer handling the player’s state info, then you will need to store the intermediate results of prediction. This can be done with a sliding window, where the &quot;from state&quot; is at the start and then each time you run a user command through prediction, you fill in the next state in the window. When the server finally acknowledges receiving one or more commands that had been predicted, it is a simple matter of looking up which state the server is acknowledging and copying over the data that is totally client side to the new starting or &quot;from state&quot;.另外需要注意的是服务器不处理，只有客户端才有的一些数据；如果没有这种类型的数据，我们可以如上面所述，以服务器第一条消息作为起点进行预测得到下一帧状态（包括用来渲染的位置信息）。然而，如果有些逻辑是纯客户端的，服务器不会处理（比如玩家蹲下来&#30524;睛的位置-然而这也不是纯客户端信息，因为服务器也会处理这个数据），这种情况下我们需要将预测的中间结果存起来。可以用一个滑动窗口完成这项工作，其中“开始状态”是开始，以后每次执行一个玩家命令预测完成后，填写窗口中的下一个状态；当服务器通知某个命令被接受并执行以后，从窗口中查找服务器处理的是哪条命令并将相应的数据传到下一个帧的“起始状态”So far, the above procedure describes how to accomplish client side prediction of movements. This system is similar to the system used in QuakeWorld2.到此为止，我们描述了客户端的运动预测。quakeworld2中采用了这种类型的预测Client-Side Prediction of Weapon Firing开火过程中的客户端预测Layering prediction of the firing effects of weapons onto the above system is straightforward. Additional state information is needed for the local player on the client, of course, including which weapons are being held, which one is active, and how much ammo each of these weapons has remaining. With this information, the firing logic can be layered on top of the movement logic because, once again, the state of the firing buttons is included in the user command data structure that is shared between the client and the server. Of course, this can get complicated if the actual weapon logic is different between client and server. In Half-Life, we chose to avoid this complication by moving the implementation of a weapon’s firing logic into &quot;shared code&quot; just like the player movement code. All of the variables that contribute to determining weapon state (e.g., ammo, when the next firing of the weapon can occur, what weapon animation is playing, etc.), are then part of the authoritative server state and are replicated to the client so that they can be used on the client for prediction of weapon state there.上面描述的系统可以很自然地用于武器开火效果预测。客户端玩家需要记录一些状态，比如身上有哪些武器，正在使用的是哪一个，每把武器都还剩多少弹药。有了这些信息，开火逻辑可以建立在运动逻辑上面，只需要在客户端和服务器使用的命令里面加上玩家开火的按键信息。在半条命中，为了简单，武器开火逻辑代码也跟运动代码一样也作为“共享代码”。所有会影响到武器状态的变量，比如弹药、下次可开火时间、正在播放那个武器动画，都作为服务器的状态，这些状态会通知给客户端用来预测武器状态。Predicting weapon firing on the client will likely lead to the decision also to predict weapon switching, deployment, and holstering. In this fashion, the user feels that the game is 100% responsive to his or her movement and weapon activation activities. This goes a long way toward reducing the feeling of latency that many players have come to endure with today’s Internet-enabled action experiences.客户端武器开火预测包括预测武器切换、部署、手枪皮套。这样，玩家会感觉游戏中的移动和武器状态100%受他控制。这在减小网络延迟给玩家带来的不爽上面迈出了一大步。Umm, This is a Lot of Work一些工作Replicating the necessary fields to the client and handling all of the intermediate state is a fair amount of work. At this point, you may be asking, why not eliminate all of the server stuff and just have the client report where s/he is after each movement? In other words, why not ditch the server stuff and just run the movement and weapons purely on the client-side? Then, the client would just send results to the server along the lines of, &quot;I’m now at position x and, by the way, I just shot player 2 in the head.&quot; This is fine if you can trust the client. This is how a lot of the military simulation systems work (i.e., they are a closed system and they trust all of the clients). This is how peer-to-peer games generally work. For Half-Life, this mechanism is unworkable because of realistic concerns about cheating. If we encapsulated absolute state data in this fashion, we’d raise the motivation to hack the client even higher than it already is3. For our games, this risk is too high and we fall back to requiring an authoritative server.服务器需要将必要的字段发给客户端，并且处理很多中间状态，有人可能有这样的疑问，为什么不把服务器逻辑取消，让客户端广播自己的位置，也就是将所有的移动、开火逻辑放在客户端。这样，客户端就会给服务器发送类&#20284;这样的结果报告：“我在X位置，我爆了玩家2的脑袋”。如果客户端可信的话，这样做是可以的，很多军方仿真系统就是这样做的（他们是一个封闭系统，所有客户端都可信）。点对点的游戏也是这么做的。对于半条命来说不可以这样做，因为客户端可能“欺骗”服务器。如果我们以这种方法封装状态数据，就会诱导玩家破解客户端【3】。对于我们的游戏来说这样做风险太大，我们还是选择采用服务器模式来做校验。A system where movements and weapon effects are predicted client-side is a very workable system. For instance, this is the system that the Quake3 engine supports. One of the problems with this system is that you still have to have a feel for your latency to determine how to lead your targets (for instant hit weapons). In other words, although you get to hear the weapons firing immediately, and your position is totally up-to-date, the results of your shots are still subject to latency. For example, if you are aiming at a player running perpendicular to your view and you have 100 milliseconds of latency and the player is running at 500 units per second, then you’ll need to aim 50 units in front of the target to hit the target with an instant hit weapon. The greater the latency, the greater the lead targeting needed. Getting a &quot;feel&quot; for your latency is difficult. Quake3 attempted to mitigate this by playing a brief tone whenever you received confirmation of your hits. That way, you could figure out how far to lead by firing your weapons in rapid succession and adjusting your leading amount until you started to hear a steady stream of tones. Obviously, with sufficient latency and an opponent who is actively dodging, it is quite difficult to get enough feedback to focus in on the opponent in a consistent fashion. If your latency is fluctuating, it can be even harder.客户端进行运动和武器效果预测是非常可行的。例如quake3就支持这样的预测。这个系统需要注意一点，在判断目标的时候需要考虑到延迟（比如即时射击武器）。换句话说，虽然你看到自己用即时武器进行了射击，你自己的位置也是最新的，射击结果仍然跟延迟有关。例如，如果你射击一个玩家，这个玩家沿与你实现垂直的方向奔跑，假设你客户端延迟为100ms，玩家奔跑速度是500单位每秒，这样你需要瞄准玩家前方50单位才能准确击中。延迟越大，就需要更大的提前量。靠感觉弥补延迟太困难了。为了减轻这种效果，quake3对你的射击播放一个短音来进行确定。这样，玩家可以算出快速发射武器的时候需要多大的提前量，同时调整提前量直到听到稳定的音调串。如果延迟比较大，而你的对手又在不断躲避，就很难获得足够的反馈判断。如果延迟也不断变化，就更难了。Display of Targets目标的显示Another important aspect influencing how a user perceives the responsiveness of the world is the mechanism for determining, on the client, where to render the other players. The two most basic mechanisms for determining where to display objects are extrapolation and interpolation[4].影响玩家游戏体验的另一个重要方面是客户端如何渲染其它玩家。两种基本的判断机制是：外推法和内插法【4】For extrapolation, the other player/object is simulated forward in time from the last known spot, direction, and velocity in more or less a ballistic manner. Thus, if you are 100 milliseconds lagged, and the last update you received was that (as above) the other player was running 500 units per second perpendicular to your view, then the client could assume that in &quot;real time&quot; the player has moved 50 units straight ahead from that last known position. The client could then just draw the player at that extrapolated position and the local player could still more or less aim right at the other player.外推法把其它玩家/物体看作一个点，这个点开始的位置、方向、速度已知，沿着自己的弹道向前移动。因此，假设延时是100ms，最新的协议通知客户端这个玩家奔跑速度是500单位每秒，方向垂直于玩家视线，客户端就可以假设事实上这个玩家当前实际的位置已经向前移动了50个单位。客户端可以在这个外推的位置渲染这个玩家，这样本地玩家就差不多可以正确瞄准。The biggest drawback of using extrapolation is that player’s movements are not very ballistic, but instead are very non-deterministic and subject to high jerk[5]. Layer on top of this the unrealistic player physics models that most FPS games use, where player’s can turn instantaneously and apply unrealistic forces to create huge accelerations at arbitrary angles and you’ll see that the extrapolation is quite often incorrect. The developer can mitigate the error by limiting the extrapolation time to a reasonable value (QuakeWorld, for instance, limited extrapolation to 100 milliseconds). This limitation helps because, once the true player position is finally received, there will be a limited amount of corrective warping. In a world where most players still have greater than 150 milliseconds of latency, the player must still lead other players in order to hit them. If those players are &quot;warping&quot; to new spots because of extrapolation errors, then the gameplay suffers nonetheless.外推法的最大缺点是玩家的移动并不是完全弹道的，而是不确定的并且高&quot;jerk&quot;【5】。大部分FPS游戏采用非现实的玩家系统，玩家可以随时转弯，可以在任意角度作用不现实的加速度，因此外推法得到的结果经常是错误地。开发者可以通过限制外推时间来减轻外推误差（比如quake限制不能超过100ms）。这种限制使得在客户端收到玩家正确位置以后，纠错不至于太大。当前大部分玩家的网络延迟高于150ms，玩家必须对游戏中的其他玩家进行外推以便正确击中。如果别的玩家因为外推错误，被服务器拉回，游戏体验将非常差。The other method for determining where to display objects and players is interpolation. Interpolation can be viewed as always moving objects somewhat in the past with respect to the last valid position received for the object. For instance, if the server is sending 10 updates per second (exactly) of the world state, then we might impose 100 milliseconds of interpolation delay in our rendering. Then, as we render frames, we interpolate the position of the object between the last updated position and the position one update before that (alternatively, the last render position) over that 100 milliseconds. As the object just gets to the last updated position, we receive a new update from the server (since 10 updates per second means that the updates come in every 100 milliseconds) we can start moving toward this new position over the next 100 milliseconds.另一种方法叫插&#20540;法。插&#20540;法可以这样理解：客户端物体实际移动位置总是滞后一段时间。举个例子，如果服务器每秒同步10次世界信息，客户端渲染的时候会有100ms滞后。这样，每一帧渲染的时候，我们通过最新收到的位置信息和前100ms的位置信息（或者上一帧渲染位置）进行差&#20540;得到结果。我们每收到一个物体位置的更新信息，（每秒10个更新意味着每100ms收到一个更新）接下来的100ms我们就可以朝这个新的位置移动。If one of the update packets fails to arrive, then there are two choices: We can start extrapolating the player position as noted above (with the large potential errors noted) or we can simply have the player rest at the position in the last update until a new update arrives (causing the player’s movement to stutter).如果一个更新包没有收到，有2种处理方法：第一、用上面介绍的外推法（有可能产生较大误差）；第二、保持玩家位于当前位置直到收到下一个更新包（会导致玩家移动顿挫）The general algorithm for this type of interpolation is as follows:内插法的大致过程如下：1.Each update contains the server time stamp for when it was generated[6]1.每个更新包包含生成的服务器时间戳【6】2.From the current client time, the client computes a target time by subtracting the interpolation time delta (100 ms)2.根据客户端当前时间，客户端通过减去时间差（100ms）计算 一个目标时间3.If the target time is in between the timestamp of the last update and the one before that, then those timestamps determine what fraction of the time gap has passed.3.如果计算得到的目标时间在上一个更新时间和上上个更新时间之间，这些时间戳可以决定目标时间在过去的时间间隙中的情况4.This fraction is used to interpolate any values (e.g., position and angles).4.目标时间情况用来通过插&#20540;计算结果（如位置、角度）In essence, you can think of interpolation, in the above example, as buffering an additional 100 milliseconds of data on the client. The other players, therefore, are drawn where they were at a point in the past that is equal to your exact latency plus the amount of time over which you are interpolating. To deal with the occasional dropped packet, we could set the interpolation time as 200 milliseconds instead of 100 milliseconds. This would (again assuming 10 updates per second from the server) allow us to entirely miss one update and still have the player interpolating toward a valid position, often moving through this interpolation without a hitch. Of course, interpolating for more time is a tradeoff, because it is trading additional latency (making the interpolated player harder to hit) for visual smoothness.上面提到的插&#20540;法，本质上是客户端缓存了接下来100ms的数据。对于每一个周围的玩家，他们都位于过去某个时间的位置，根据每一个具体的时间点进行插&#20540;。如果偶尔发生丢包，我们就将插&#20540;时间延长到200ms。这样我们就可以忽略一次更新（假设同步频率还是10次每秒），玩家还可以移动到合理的目标位置，这样进行插&#20540;通常不会有什么问题。当然，插&#20540;多少时间需要权衡，因为这种方法是用延时（玩家更难击中）来换取平滑。In addition, the above type of interpolation (where the client tracks only the last two updates and is always moving directly toward the most recent update) requires a fixed time interval between server updates. The method also suffers from visual quality issues that are difficult to resolve. The visual quality issue is as follows. Imagine that the object being interpolated is a bouncing ball (which actually accurately describes some of our players). At the extremes, the ball is either high in the air or hitting the pavement. However, on average, the ball is somewhere in between. If we only interpolate to the last position, it is very likely that this position is not on the ground or at the high point. The bounciness of the ball is &quot;flattened&quot; out and it never seems to hit the ground. This is a classical sampling problem and can be alleviated by sampling the world state more frequently. However, we are still quite likely never actually to have an interpolation target state be at the ground or at the high point and this will still flatten out the positions.另外，上述插&#20540;方法（客户端通过2个更新信息插&#20540;并且朝最新更新位置移动）需要服务器更新信息间隔固定。对于所谓的“视觉效果因素”，这种方式很难处理，“视觉效果因素”是这样的：假设我们插&#20540;的物体是弹球（这种模型可以准确描述某些玩家）。极端情况下，球或者在空中，或者正在碰地板。然而，通常情况下球在这两种状态之间。如果我们只插&#20540;上一个位置，这个位置可能既不在地面上，也不是最高点，这样，弹球弹的效果就被平滑掉了，好像永远没有弹到地面一样。这是一个经典问题，增加采样率可以减轻这种影响，但是仍然有可能我们采样不到球在地面的点跟最高点，这些点会给平滑掉。In addition, because different users have different connections, forcing updates to occur at a lockstep like 10 updates per second is forcing a lowest common denominator on users unnecessarily. In Half-Life, we allow the user to ask for as many updates per second as he or she wants (within limit). Thus, a user with a fast connection could receive 50 updates per second if the user wanted. By default, Half-Life sends 20 updates per second to each player the Half-Life client interpolates players (and many other objects) over a period of 100 milliseconds.[7]另外，不同用户网络状况不同，强迫每个用户都以固定速度更新（比如每秒10次）效果不是很好，在半条命中，用户每秒可以请求任意数量的更新包（没有限制）。这样，高速网络用户可以每秒更新50次，只要用户愿意。半条命的默认设置是每秒每个用户（以及游戏中其它物体）发送20次更新，以100ms为时间窗口进行插&#20540;。【7】To avoid the flattening of the bouncing ball problem, we employ a different algorithm for interpolation. In this method, we keep a more complete &quot;position history&quot; for each object that might be interpolated.为了避免“反弹球&quot;平滑问题，我们在插&#20540;的过程中采用了一个不同的算法，这种算法中我们对每一个可能插&#20540;的物体记录了一个完整的“历史位置”信息。The position history is the timestamp and origin and angles (and could include any other data we want to interpolate) for the object. Each update we receive from the server creates a new position history entry, including timestamp and origin/angles for that timestamp. To interpolate, we compute the target time as above, but then we search backward through the history of positions looking for a pair of updates that straddle the target time. We then use these to interpolate and compute the final position for that frame. This allows us to smoothly follow the curve that completely includes all of our sample points. If we are running at a higher framerate than the incoming update rate, we are almost assured of smoothly moving through the sample points, thereby minimizing (but not eliminating, of course, since the pure sampling rate of the world updates is the limiting factor) the flattening problem described above.历史位置信息记录了物体的时间戳、远点、角度（以及其它我们需要插&#20540;计算的数据）。我们每收到一个服务器的更新，我们就创建一条包含时间戳的记录，其中包含原始位置、角度信息。在插&#20540;过程中，我们用上面的方法计算目标时间，然后搜索位置历史信息，找到包含目标时间的记录区间。然后用找到的信息插&#20540;计算当前帧的位置。这样我们就可以平滑跟踪到包含所有采样点的曲线。如果客户端帧率比服务器更新频率大，我们就可以将采样点平滑处理，减小上面提到的平滑处理带来的问题（当然没法避免，因为采用频率限制，而世界本身是连续的）。The only consideration we have to layer on top of either interpolation scheme is some way to determine that an object has been forcibly teleported, rather than just moving really quickly. Otherwise we might &quot;smoothly&quot; move the object over great distances, causing the object to look like it’s traveling way too fast. We can either set a flag in the update that says, &quot;don’t interpolate&quot; or &quot;clear out the position history,&quot; or we can determine if the distance between the origin and one update and another is too big, and thereby presumed to be a teleportation/warp. In that case, the solution is probably to just move the object to the latest know position and start interpolating from there.需要注意的是，上面提到的插&#20540;方法使用的时候，物体有时候会被服务器拉回，而不是快速移动。当然我们也可以平滑地将物体移动一段较长的距离，这样看起来物体移动很快。更新的过程中我们可以设一个标志表示不插&#20540;或清除历史记录，或者如果起始点与目标点距离过长，我们就认为数据不正常。这种情况我们就将物体直接拉过去。并以这个位置为起始点进行插&#20540;。Lag Compensation延迟补偿Understanding interpolation is important in designing for lag compensation because interpolation is another type of latency in a user’s experience. To the extent that a player is looking at other objects that have been interpolated, then the amount of interpolation must be taken into consideration in computing, on the server, whether the player’s aim was true.插&#20540;也会带来延迟，所以考虑延迟补偿的过程中需要理解插&#20540;过程。玩家看到的别的物体是经过插&#20540;计算出来的，所以插&#20540;过程中需要考虑在服务器上玩家的目标是否正确。Lag compensation is a method of normalizing server-side the state of the world for each player as that player’s user commands are executed. You can think of lag compensation as taking a step back in time, on the server, and looking at the state of the world at the exact instant that the user performed some action. The algorithm works as follows:延迟补偿是服务器执行的一种策略，当服务器收到客户端命令并执行的过程中，根据客户端的具体情况进行归一。延迟补偿可以看做服务器处理用户命令的时候回退一段时间，退到客户端发送命令时候的准确时间。算法流程如下：1.Before executing a player’s current user command, the server:1.服务器执行客户端命令之前执行以下操作：&nbsp; &nbsp; 1.Computes a fairly accurate latency for the player&nbsp; &nbsp; 1.为玩家计算一个相当精确的延迟时间&nbsp; &nbsp; 2.Searches the server history (for the current player) for the world update that was sent to the player and received by the player just before the player would have issued the movement command&nbsp; &nbsp;&nbsp;2.对每个玩家，从服务器历史信息中找一个已发送给这个玩家并且这个玩家已收到的的world update, 这个world update是在这个玩家将要执行这个movement command之前的world update&nbsp; &nbsp; 3.From that update (and the one following it based on the exact target time being used), for each player in the update, move the other players backwards in time to &nbsp;exactly &nbsp;where they were when the current player’s user command was created. This moving backwards must account for both connection latency andthe&nbsp;interpolation amount[8] the client was using that frame.&nbsp; &nbsp; 3. 对于每一个玩家，将其从上述的world update处拉回到这个玩家生成此user command的更新时间中执行用户命令。这个回退时间需要考虑到命令执行的时候的网络延时和插&#20540;量【8】2.Allow the user command to execute (including any weapon firing commands, etc., that will run ray casts against all of the other players in their &quot;old&quot; positions).2.执行玩家命令（包括武器开火等。）3.Move all of the moved/time-warped players back to their correct/current positions3.将所有移动的、错位的玩家移动到他们当前正确位置。Note that in the step where we move the player backwards in time, this might actually require forcing additional state info backwards, too (for instance, whether the player was alive or dead or whether the player was ducking). The end result of lag compensation is that each local client is able to directly aim at other players without having to worry about leading his or her target in order to score a hit. Of course, this behavior is a game design tradeoff.注意：我们把时间往后推算的时候，需要考虑那个时候玩家的状态，比如玩家是或者还是已经已经死掉，玩家是否处于躲避状态。执行运动补偿以后，玩家就可以直接瞄准目标进行设计，而不需要计算一个提前量。当然，这种方案是游戏中的权衡设计。Game Design Implications of Lag Compensation游戏涉及中延迟补偿的使用The introduction of lag compensation allows for each player to run on his or her own clock with no apparent latency. In this respect, it is important to understand that certain paradoxes or inconsistencies can occur. Of course, the old system with the authoritative server and &quot;dumb&quot; or simple clients had it’s own paradoxes. In the end, making this tradeoff is a game design decision. For Half-Life, we believe deciding in favor of lag compensation was a justified game design decision.采用延迟补偿以后，每个玩家游戏的过程中感觉不到明显延迟。在这里需要理解可能会产生一些矛盾和不一致。当然，验证服务器和无逻辑的客户端老系统也会有自相矛盾的情况。最后，这个这种事游戏设计决定的。对于半条命，我们相信采用延迟补偿是正确的游戏决定。The first problem of the old system was that you had to lead your target by some amount that was related to your latency to the server. Aiming directly at another player and pressing the fire button was almost assured to miss that player. The inconsistency here is that aiming is just not realistic and that the player controls have non-predictable responsiveness.老系统的一个问题是，由于网络延迟，目标需要有一个提前量。瞄准敌人进行射击几乎总是不能击中。这种不一致导致射击很不真实，响应也不可控制。With lag compensation, the inconsistencies are different. For most players, all they have to do is acquire some aiming skill and they can become proficient (you still have to be able to aim). Lag compensation allows the player to aim directly at his or her target and press the fire button (for instant hit weapons[9]). The inconsistencies that sometimes occur, however, are from the points of view of the players being fired upon.采用延迟补偿以后带来的是另一种形式的不一致。对于大部分玩家，他们只需要专注于得到更多的射击技能来武装他们（当然他们也是需要瞄准的）。延时补偿使得玩家只需要直接瞄准他的目标并按下开火按钮即可（对于即时击中武器【9】）。不一致也时有发生，但是是在击中以后。For instance, if a highly lagged player shoots at a less lagged player and scores a hit, it can appear to the less lagged player that the lagged player has somehow &quot;shot around a corner&quot;10. In this case, the lower lag player may have darted around a corner. But the lagged player is seeing everything in the past. To the lagged player, s/he has a direct line of sight to the other player. The player lines up the crosshairs and presses the fire button. In the meantime, the low lag player has run around a corner and maybe even crouched behind a crate. If the high lag player is sufficiently lagged, say 500 milliseconds or so, this scenario is quite possible. Then, when the lagged player’s user command arrives at the server, the hiding player is transported backward in time and is hit. This is the extreme case, and in this case, the low ping player says that s/he was shot from around the corner. However, from the lagged player’s point of view, they lined up their crosshairs on the other player and fired a direct hit. From a game design point of view, the decision for us was easy: let each individual player have completely responsive interaction with the world and his or her weapons.例如，如果一个延时比较大的玩家击中一个延时比较小的玩家并且得到一分，低延时的玩家会感觉高延时玩家“在角落里被击中”【10】。这种情况下，低延迟玩家可能已经从角落里冲出，而高延时玩家看到的是过去的信息。每一个有延迟的玩家都有一个朝向别的玩家的直的视线，直的视线指向一个瞄准点然后开火。这个时候，低延时的玩家可能已经跑到角落里并且蹲在一个箱子后面，如果高延迟玩家延迟比较大，比如500ms，这是经常发生的；这样当高延时玩家的命令传到服务器的时候，已经隐藏起来的玩家需要取一个历史位置并计算是否击中，在这种极端情况下，低延时玩家会觉得他再角落里被击中了。然而，对于高延时玩家来说，他是正对着别的玩家开火的。从游戏设计的角度来讲，我们需要这样决定：让每个玩家即时与世界交互并开火。In addition, the inconsistency described above is much less pronounced in normal combat situations. For first-person shooters, there are two more typical cases. First, consider two players running straight at each other pressing the fire button. In this case, it’s quite likely that lag compensation will just move the other player backwards along the same line as his or her movement. The person being shot will be looking straight at his attacker and no &quot;bullets bending around corners&quot; feeling will be present.此外，在正常战斗中，上面提到的不一致并不明显。对于第一人称射击游戏，有两种典型情况。第一、考虑两个玩家直线跑向对方并且开火；这种情况下，延时补偿只会把玩家在移动直线上往后拉。被击中的玩家看他的射击者在前方，这样就不会有“子弹拐到角落里”的情况发生。The next example is two players, one aiming at the other while the other dashes in front perpendicular to the first player. In this case, the paradox is minimized for a wholly different reason. The player who is dashing across the line of sight of the shooter probably has (in first-person shooters at least) a field of view of 90 degrees or less. In essence, the runner can’t see where the other player is aiming. Therefore, getting shot isn’t going to be surprising or feel wrong (you get what you deserve for running around in the open like a maniac). Of course, if you have a tank game, or a game where the player can run one direction, and look another, then this scenario is less clear-cut, since you might see the other player aiming in a slightly incorrect direction.第二种情况是两个玩家中的一个射击，另外一个玩家在垂直于第一个玩家视线的方向冲锋。这种情况下的解决问题的原理与刚才不同。刚才提到的冲锋的玩家视野差不多是90°（至少第一人称射击游戏是这样），因此，这个玩家看不到正在射击他的那个人。因此他被击中也不会感觉奇怪或者错误（谁让你在空旷区域狂奔呢，活该）。当然，如果你开发的是一个坦克游戏，或者在你的游戏中玩家朝一个方向跑的时候可以看到别的方向，错误可能就会比较明显，你可能发现玩家设计方向不对。Conclusion总结Lag compensation is a tool to ameliorate the effects of latency on today’s action games. The decision of whether to implement such a system rests with the game designer since the decision directly changes the feel of the game. For Half-Life, Team Fortress and Counter Strike, the benefits of lag compensation easily outweighed the inconsistencies noted above.延迟补偿是当前动作游戏改善延迟影响的一种方法。是否采用这种方法取决于游戏设计者，因为如何设计直接影响到游戏的体验。对于把那条命、军团要塞、cs这样的游戏，延迟补偿所带来的效果提升显著大于其带来的错误。Footnotes脚注[1]In the Half-Life engine, it is possible to ask the client-side prediction algorithm to account for some, but not all, of the latency in performing prediction. The user could control the amount of prediction by changing the value of the &quot;pushlatency&quot; console variable to the engine. This variable is a negative number indicating the maximum number of milliseconds of prediction to perform. If the number is greater (in the negative) than the user’s current latency, then full prediction up to the current time occurs. In this case, the user feels zero latency in his or her movements. Based upon some erroneous superstition in the community, many users insisted that setting pushlatency to minus one-half of the current average latency was the proper setting. Of course, this would still leave the player’s movements lagged (often described as if you are moving around on ice skates) by half of the user’s latency. All of this confusion has brought us to the conclusion that full prediction should occur all of the time and that the pushlatency variable should be removed from the Half-Life engine.&nbsp;【1】在半条命引擎中，预测的过程中允许一定的延迟，但不能容忍实际网络延迟这么大的延迟。通过调整参数，我们可以控制预测过程中的延迟，这个参数pushlatency是一个负数，以毫秒为单位表示预测过程中的延迟。如果这个&#20540;大于（绝对&#20540;）实际网络延迟，这时预测就是完全的预测（译注：客户端服务器完全同步）。这种情况下玩家感觉不到任何延迟。实际应用中，一些人错误地认为参数pushlatency应该设为实际网络延迟的一半，这种情况下玩家移动仍然有网络延迟一半的延迟（感觉类&#20284;于冰面移动）。基于这个原因，实际应用总应该总是采用完全预测，pushlatency这个变量应该从半条命引擎中移除[2]http://www.quakeforge.net/files/q1source.zip (Return)[3]A discussion of cheating and what developers can do to deter it is beyond the scope of this paper. (Return)【3】关于作弊和反作弊的问题超出了本篇文章讨论的范围[4]Though hybrids and corrective methods are also possible. (Return)【4】虽然混合纠正方法也可以使用[5]&quot;Jerk&quot; is a measure of how fast accelerative forces are changing. (Return)【5】“jerk”用来度量使玩家改变加速度的作用的快慢[6]It is assumed in this paper that the client clock is directly synchronized to the server clock modulo the latency of the connection. In other words, the server sends the client, in each update, the value of the server’s clock and the client adopts that value as its clock. Thus, the server and client clocks will always be matched, with the client running the same timing somewhat in the past (the amount in the past is equal to the client’s current latency). Smoothing out discrepancies in the client clock can be solved in various ways. (Return)【6】本文假设计算连接延时的时候客户端与服务器完全同步，也就是说，每次更新的时候客户端收到服务器发过来的时间被直接当做客户端的时间使用。这样，客户端跟服务器完全匹配，只是客户端稍微晚一点（晚多少取决于延时多少）。平滑客户端时钟差&#20540;可以有很多方法。[7]The time spacing of these updates is not necessarily fixed. The reason why is that during high activity periods of the game (especially for users with lower bandwidth connections), it’s quite possible that the game will want to send you more data than your connection can accommodate. If we were on a fixed update interval, then you might have to wait an entire additional interval before the next packet would be sent to the client. However, this doesn’t match available bandwidth effectively. Instead, the server, after sending every packet to a player, determines when the next packet can be sent. This is a function of the user’s bandwidth or &quot;rate&quot; setting and the number of updates requested per second. If the user asks for 20 updates per second, then it will be at least 50 milliseconds before the next update packet can be sent. If the bandwidth choke is active (and the server is sufficiently high framerate), it could be 61, etc., milliseconds before the next packet gets sent. Thus, Half-Life packets can be somewhat arbitrarily spaced. The simple move to latest goal interpolation schemes don’t behave as well (think of the old anchor point for movement as being variable) under these conditions as the position history interpolation method (described below). (Return)【7】更新时间间隔没必要是固定的。因为对于剧烈运动的游戏，如果带宽不够，很有可能客户端发过来的数据超过了处理能力。如果采用固定更新间隔，在发完一个更新包以后就需要等待一个固定更新周期时间以后再发下一个包。这种逻辑不能很好地使用带宽。因此，服务器发给每个客户端数据包以后，应该自己决定下一个包什么时候发，决定的依据是用户的带宽、用户设置的每秒更新频率。如果用户要求更新20次每秒，那么需要等待50ms以后下个更新包才能发送。如果激活了带宽限制（而服务器帧率又足够高），我们可能就需要等待比如61ms（或其他&#20540;）以后发送下一个更新包。因此，半条命游戏数据包发送间隔是随机的。基于服务器的这种情况，将启动点作为一个变量，移动到最新目标点进行插&#20540;这种方法效果欠佳。[8]Which Half-Life encodes in the lerp_msec field of the usercmd_t structure described previously. (Return)【8】半条命代码中usercmd_t结构中变量lerp_msec前面描述过。[9]For weapons that fire projectiles, lag compensation is more problematic. For instance, if the projectile lives autonomously on the server, then what time space should the projectile live in? Does every other player need to be &quot;moved backward&quot; every time the projectile is ready to be simulated and moved by the server? If so, how far backward in time should the other players be moved? These are interesting questions to consider. In Half-Life, we avoided them; we simply don’t lag compensate projectile objects (that’s not to say that we don’t predict the sound of you firing the projectile on the client, just that the actual projectile is not lag compensated in any way). (Return)【9】对于发射导弹的武器，延迟补偿有更多需要解决的问题。假如导弹是由服务器处理的，那么导弹应该位于哪个时间区间？每次导弹准备发射的时候，是否需要把每个玩家往后拉一段时间的？如果是这样，那么需要往后拉多少？这些问题是需要考虑的。在半条命中，为了避免这种问题，我们对导弹不进行延迟补偿（这并不意味着客户端不进行声音预测，只是实际的导弹不进行延迟补偿）。[10]This is the phrase our user community has adopted to describe this inconsistency. (Return)【10】用户社区通常采用这种情况来描述不一致性。]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>Valve</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Source引擎多人游戏网络设计]]></title>
    <url>%2F2016%2F01%2F06%2FSource%E5%BC%95%E6%93%8E%E5%A4%9A%E4%BA%BA%E6%B8%B8%E6%88%8F%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[原文原文出处 原文标题 : Source Multiplayer Networking Multiplayer games based on the Source Engine use a Client-Server networking architecture. Usually a server is a dedicated host that runs the game and is authoritative about world simulation, game rules, and player input processing. A client is a player’s computer connected to a game server. The client and server communicate with each other by sending small data packets at a high frequency (usually 20 to 30 packets per second). A client receives the current world state from the server and generates video and audio output based on these updates. The client also samples data from input devices (keyboard, mouse, microphone, etc.) and sends these input samples back to the server for further processing. Clients only communicate with the game server and not between each other (like in a peer-to-peer application). In contrast with a single player game, a multiplayer game has to deal with a variety of new problems caused by packet-based communication.Network bandwidth is limited, so the server can’t send a new update packet to all clients for every single world change. Instead, the server takes snapshots of the current world state at a constant rate and broadcasts these snapshots to the clients. Network packets take a certain amount of time to travel between the client and the server (i.e. half the ping time). This means that the client time is always a little bit behind the server time. Furthermore, client input packets are also delayed on their way back, so the server is processing temporally delayed user commands. In addition, each client has a different network delay which varies over time due to other background traffic and the client’s framerate. These time differences between server and client causes logical problems, becoming worse with increasing network latencies. In fast-paced action games, even a delay of a few milliseconds can cause a laggy gameplay feeling and make it hard to hit other players or interact with moving objects. Besides bandwidth limitations and network latencies, information can get lost due to network packet loss. To cope with the issues introduced by network communication, the Source engine server employs techniques such as data compression and lag compensation which are invisible to the client. The client then performs prediction and interpolation to further improve the experience.Basic networkingThe server simulates the game in discrete time steps called ticks. By default, the timestep is 15ms, so 66.666… ticks per second are simulated, but mods can specify their own tickrate. During each tick, the server processes incoming user commands, runs a physical simulation step, checks the game rules, and updates all object states. After simulating a tick, the server decides if any client needs a world update and takes a snapshot of the current world state if necessary. A higher tickrate increases the simulation precision, but also requires more CPU power and available bandwidth on both server and client. The server admin may override the default tickrate with the -tickrate command line parameter, though tickrate changes done this way are not recommended because the mod may not work as designed if its tickrate is changed.Note:The -tickrate command line parameter is not available on CSS, DoD S, TF2, L4D and L4D2 because changing tickrate causes server timing issues. The tickrate is set to 66 in CSS, DoD S and TF2, and 30 in L4D and L4D2.Clients usually have only a limited amount of available bandwidth. In the worst case, players with a modem connection can’t receive more than 5 to 7 KB/sec. If the server tried to send them updates with a higher data rate, packet loss would be unavoidable. Therefore, the client has to tell the server its incoming bandwidth capacity by setting the console variable rate (in bytes/second). This is the most important network variable for clients and it has to be set correctly for an optimal gameplay experience. The client can request a certain snapshot rate by changing cl_updaterate (default 20), but the server will never send more updates than simulated ticks or exceed the requested client rate limit. Server admins can limit data rate values requested by clients with sv_minrate and sv_maxrate (both in bytes/second). Also the snapshot rate can be restricted with sv_minupdaterate and sv_maxupdaterate (both in snapshots/second).The client creates user commands from sampling input devices with the same tick rate that the server is running with. A user command is basically a snapshot of the current keyboard and mouse state. But instead of sending a new packet to the server for each user command, the client sends command packets at a certain rate of packets per second (usually 30). This means two or more user commands are transmitted within the same packet. Clients can increase the command rate with cl_cmdrate. This will increase responsiveness but requires more outgoing bandwidth, too.Game data is compressed using delta compression to reduce network load. That means the server doesn’t send a full world snapshot each time, but rather only changes (a delta snapshot) that happened since the last acknowledged update. With each packet sent between the client and server, acknowledge numbers are attached to keep track of their data flow. Usually full (non-delta) snapshots are only sent when a game starts or a client suffers from heavy packet loss for a couple of seconds. Clients can request a full snapshot manually with the cl_fullupdate command.Responsiveness, or the time between user input and its visible feedback in the game world, are determined by lots of factors, including the server/client CPU load, simulation tickrate, data rate and snapshot update settings, but mostly by the network packet traveling time. The time between the client sending a user command, the server responding to it, and the client receiving the server’s response is called the latency or ping (or round trip time). Low latency is a significant advantage when playing a multiplayer online game. Techniques like prediction and lag compensation try to minimize that advantage and allow a fair game for players with slower connections. Tweaking networking setting can help to gain a better experience if the necessary bandwidth and CPU power is available. We recommend keeping the default settings, since improper changes may cause more negative side effects than actual benefits.Servers that Support TickrateThe tickrate can be altered by using the -tickrate parameterCounter Strike: Global OffensiveHalf-Life 2: DeathmatchThe following servers tickrate cannot be altered as changing this causes server timing issues.Tickrate 66Counter Strike: SourceDay of Defeat: SourceTeam Fortress 2Tickrate 30Left 4 DeadLeft 4 Dead 2Entity interpolationBy default, the client receives about 20 snapshot per second. If the objects (entities) in the world were only rendered at the positions received by the server, moving objects and animation would look choppy and jittery. Dropped packets would also cause noticeable glitches. The trick to solve this problem is to go back in time for rendering, so positions and animations can be continuously interpolated between two recently received snapshots. With 20 snapshots per second, a new update arrives about every 50 milliseconds. If the client render time is shifted back by 50 milliseconds, entities can be always interpolated between the last received snapshot and the snapshot before that.Source defaults to an interpolation period (‘lerp’) of 100-milliseconds (cl_interp 0.1); this way, even if one snapshot is lost, there are always two valid snapshots to interpolate between. Take a look at the following figure showing the arrival times of incoming world snapshots: The last snapshot received on the client was at tick 344 or 10.30 seconds. The client time continues to increase based on this snapshot and the client frame rate. If a new video frame is rendered, the rendering time is the current client time 10.32 minus the view interpolation delay of 0.1 seconds. This would be 10.22 in our example and all entities and their animations are interpolated using the correct fraction between snapshot 340 and 342.Since we have an interpolation delay of 100 milliseconds, the interpolation would even work if snapshot 342 were missing due to packet loss. Then the interpolation could use snapshots 340 and 344. If more than one snapshot in a row is dropped, interpolation can’t work perfectly because it runs out of snapshots in the history buffer. In that case the renderer uses extrapolation (cl_extrapolate 1) and tries a simple linear extrapolation of entities based on their known history so far. The extrapolation is done only for 0.25 seconds of packet loss (cl_extrapolate_amount), since the prediction errors would become too big after that.Entity interpolation causes a constant view “lag” of 100 milliseconds by default (cl_interp 0.1), even if you’re playing on a listenserver (server and client on the same machine). This doesn’t mean you have to lead your aiming when shooting at other players since the server-side lag compensation knows about client entity interpolation and corrects this error.Tip:More recent Source games have the cl_interp_ratio cvar. With this you can easily and safely decrease the interpolation period by setting cl_interp to 0, then increasing the value of cl_updaterate (the useful limit of which depends on server tickrate). You can check your final lerp with net_graph 1.Note:If you turn on sv_showhitboxes (not available in Source 2009) you will see player hitboxes drawn in server time, meaning they are ahead of the rendered player model by the lerp period. This is perfectly normal!Input predictionLets assume a player has a network latency of 150 milliseconds and starts to move forward. The information that the +FORWARD key is pressed is stored in a user command and send to the server. There the user command is processed by the movement code and the player’s character is moved forward in the game world. This world state change is transmitted to all clients with the next snapshot update. So the player would see his own change of movement with a 150 milliseconds delay after he started walking. This delay applies to all players actions like movement, shooting weapons, etc. and becomes worse with higher latencies.A delay between player input and corresponding visual feedback creates a strange, unnatural feeling and makes it hard to move or aim precisely. Client-side input prediction (cl_predict 1) is a way to remove this delay and let the player’s actions feel more instant. Instead of waiting for the server to update your own position, the local client just predicts the results of its own user commands. Therefore, the client runs exactly the same code and rules the server will use to process the user commands. After the prediction is finished, the local player will move instantly to the new location while the server still sees him at the old place.After 150 milliseconds, the client will receive the server snapshot that contains the changes based on the user command he predicted earlier. Then the client compares the server position with his predicted position. If they are different, a prediction error has occurred. This indicates that the client didn’t have the correct information about other entities and the environment when it processed the user command. Then the client has to correct its own position, since the server has final authority over client-side prediction. If cl_showerror 1 is turned on, clients can see when prediction errors happen. Prediction error correction can be quite noticeable and may cause the client’s view to jump erratically. By gradually correcting this error over a short amount of time (cl_smoothtime), errors can be smoothly corrected. Prediction error smoothing can be turned off with cl_smooth 0.Prediction is only possible for the local player and entities affected only by him, since prediction works by using the client’s keypresses to make a “best guess” of where the player will end up. Predicting other players would require literally predicting the future with no data, since there’s no way to instantaneously get keypresses from them.Lag compensationAll source code for lag compensation and view interpolation is available in the Source SDK. See for implementation details.Let’s say a player shoots at a target at client time 10.5. The firing information is packed into a user command and sent to the server. While the packet is on its way through the network, the server continues to simulate the world, and the target might have moved to a different position. The user command arrives at server time 10.6 and the server wouldn’t detect the hit, even though the player has aimed exactly at the target. This error is corrected by the server-side lag compensation.The lag compensation system keeps a history of all recent player positions for one second. If a user command is executed, the server estimates at what time the command was created as follows: Command Execution Time = Current Server Time - Packet Latency - Client View InterpolationThen the server moves all other players - only players - back to where they were at the command execution time. The user command is executed and the hit is detected correctly. After the user command has been processed, the players revert to their original positions.Note:Since entity interpolation is included in the equation, failing to have it on can cause undesired results.On a listen server you can enable sv_showimpacts 1 to see the different server and client hitboxes: This screenshot was taken on a listen server with 200 milliseconds of lag (using net_fakelag), right after the server confirmed the hit. The red hitbox shows the target position on the client where it was 100ms + interp period ago. Since then, the target continued to move to the left while the user command was travelling to the server. After the user command arrived, the server restored the target position (blue hitbox) based on the estimated command execution time. The server traces the shot and confirms the hit (the client sees blood effects).Client and server hitboxes don’t exactly match because of small precision errors in time measurement. Even a small difference of a few milliseconds can cause an error of several inches for fast-moving objects. Multiplayer hit detection is not pixel perfect and has known precision limitations based on the tickrate and the speed of moving objects.The question arises, why is hit detection so complicated on the server? Doing the back tracking of player positions and dealing with precision errors while hit detection could be done client-side way easier and with pixel precision. The client would just tell the server with a “hit” message what player has been hit and where. We can’t allow that simply because a game server can’t trust the clients on such important decisions. Even if the client is “clean” and protected by Valve Anti-Cheat, the packets could be still modified on a 3rd machine while routed to the game server. These “cheat proxies” could inject “hit” messages into the network packet without being detected by VAC (a “man-in-the-middle” attack).Network latencies and lag compensation can create paradoxes that seem illogical compared to the real world. For example, you can be hit by an attacker you can’t even see anymore because you already took cover. What happened is that the server moved your player hitboxes back in time, where you were still exposed to your attacker. This inconsistency problem can’t be solved in general because of the relatively slow packet speeds. In the real world, you don’t notice this problem because light (the packets) travels so fast and you and everybody around you sees the same world as it is right now.Net graphThe Source engine offers a couple of tools to check your client connection speed and quality. The most popular one is the net graph, which can be enabled with net_graph 2 (or +graph). Incoming packets are represented by small lines moving from right to left. The height of each line reflects size of a packet. If a gap appears between lines, a packet was lost or arrived out of order. The lines are color-coded depending on what kind of data they contain.Under the net graph, the first line shows your current rendered frames per second, your average latency, and the current value of cl_updaterate. The second line shows the size in bytes of the last incoming packet (snapshots), the average incoming bandwidth, and received packets per second. The third line shows the same data just for outgoing packets (user commands). Optimizations The default networking settings are designed for playing on dedicated server on the Internet. The settings are balanced to work well for most client/server hardware and network configurations. For Internet games the only console variable that should be adjusted on the client is “rate”, which defines your available bytes/second bandwidth of your network connection. Good values for “rate” is 4500 for modems, 6000 for ISDN, 10000 DSL and above.In an high-performance network environment, where the server and all clients have the necessary hardware resources available, it’s possible to tweak bandwidth and tickrate settings to gain more gameplay precision. Increasing the server tickrate generally improves movement and shooting precision but comes with a higher CPU cost. A Source server running with tickrate 100 generates about 1.5x more CPU load than a default tickrate 66. That can cause serious calculation lags, especially when lots of people are shooting at the same time. It’s not suggested to run a game server with a higher tickrate than 66 to reserve necessary CPU resources for critical situations. Note:It is not possible to change tickrate on CSS, DoD S TF2, L4D and L4D2 because changing tickrate causes server timing issues. The tickrate is set to 66 in CSS, DoD S and TF2, and 30 in L4D and L4D2.If the game server is running with a higher tickrate, clients can increase their snapshot update rate (cl_updaterate) and user command rate (cl_cmdrate), if the necessary bandwidth (rate) is available. The snapshot update rate is limited by the server tickrate, a server can’t send more then one update per tick. So for a tickrate 66 server, the highest client value for cl_updaterate would be 66. If you increase the snapshot rate and encounter packet loss or choke, you have to turn it down again. With an increased cl_updaterate you can also lower the view interpolation delay (cl_interp). The default interpolation delay is 0.1 seconds, which derives from the default cl_updaterate 20. View interpolation delay gives a moving player a small advantage over a stationary player since the moving player can see his target a split second earlier. This effect is unavoidable, but it can be reduced by decreasing the view interpolation delay. If both players are moving, the view lag delay is affecting both players and nobody has an advantage.This is the relation between snapshot rate and view interpolation delay is the following:interpolation period = max( cl_interp, cl_interp_ratio / cl_updaterate )“Max(x,y)” means “whichever of these is higher”. You can set cl_interp to 0 and still have a safe amount of interp. You can then increase cl_updaterate to decrease your interp period further, but don’t exceed tickrate (66) or flood your connection with more data than it can handle.Tips Don’t change console settings unless you are 100% sure what you are doing Most &quot;high-performance&quot; setting cause exactly the opposite effect, if the server or network can&#39;t handle the load. Don’t turn off view interpolation and/or lag compensation It will not improve movement or shooting precision. Optimized setting for one client may not work for other clients Do not just use settings from other clients without verifing them for your system. If you follow a player in “First-Person” as a spectator in a game or SourceTV, you don’t exactly see what the player sees Spectators see the game world without lag compensation.译文译文出处 翻译：pluswu，审校：pluswuSource引擎的多人游戏使用基于UDP通信的C/S架构。游戏以服务器逻辑作为世界权威，客户端和服务器通过UDP协议(20~30packet/s）通信。客户端从服务器接收信息并基于当前世界状态渲染画面和输出音频。客户端以固定频率发送操作输入到服务器。客户端仅与游戏服务器，而不是彼此之间通信。多人游戏必须处理基于网络消息同步所带来的一系列问题。网络的带宽是有限的，所以服务器不能为每一个世界的变化发送新的更新数据包发送到所有客户端。相反，服务器以固定的频率取当前世界状态的快照并广播这些快照到客户端。网络数据包需要一定的时间量的客户端和服务器（RTT的一半）来往。这意味着客户端时间相对服务器时间总是稍有滞后。此外，客户端输入数据包同步到服务器也有一定网络传输时间，所以服务器处理客户端输入也存在延迟的。不同的客户端因为网络带宽和通信线路不同也会存在不同的网络延时。随着服务器和客户端之间的这些网络延迟增大, 网络延迟可能会导致逻辑问题。比如在快节奏的动作游戏中，在几毫秒的延迟甚至就会导致游戏卡顿的感觉，玩家会觉得很难打到对方玩家或运动的物体。此外除了带宽限制和网络延迟还要考虑网络传输中会有消息丢失的情况。为了解决网络通信引入的一系列问题，Source引擎在服务器同步时采用了数据压缩和延迟补偿的逻辑，客户端采用了预测运行和插值平滑处理等技术来获得更好的游戏体验。基本网络模型服务器以一个固定的时间间隔更新模拟游戏世界。默认情况下，时间步长为15ms，以66.66次每秒的频率更新模拟游戏世界，但不同游戏可以指定更新频率。在每个更新周期内服务器处理传入的用户命令，运行物理模拟步，检查游戏规则，并更新所有的对象状态。每一次模拟更新tick之后服务器会决定是否更新当前时间快照以及每个客户端当前是否需更新。较高的tickrate增加了模拟精度，需要服务器和客户端都有更多可用的CPU和带宽资源。客户通常只能提供有限的带宽。在最坏的情况下，玩家的调制解调器连接不能获得超过5-7KB /秒的流量。如果服务器的数据更新发送频率超过了客户端的带宽处理限制，丢包是不可避免的。因此客户端可以通过在控制台设置接受带宽限制，以告诉服务器其收到的带宽容量。这是客户最重要的网络参数，想要获得最佳的游戏体验的话必须正确的设置此参数。客户端可以通过设置cl_updaterate（默认20）来改变获得快照平的频率，但服务器永远不会发送比tickerate更多的更新或超过请求的客户端带宽限制。服务器管理员可以通过sv_minrate和sv_maxrate(byte/s)限制客户端的上行请求频率。当然快照更新同步频率都受到sv_minupdaterate和sv_maxupdaterate（快照/秒）的限制。客户端使用与服务端tickrate一样的频率采样操作输入创建用户命令。用户命令基本上是当前的键盘和鼠标状态的快照。客户端不会把每个用户命令都立即发送到服务器而是以每秒（通常是30）的速率发送命令包。这意味着两个或更多个用户的命令在同一包内传输。客户可以增加与的cl_cmdrate命令速率。这可以提高响应速度，但需要更多的出口带宽。游戏数据使用增量更新压缩来减少网络传输。服务器不会每次都发送一个完整的世界快照，而只会更新自上次确认更新(通过ACK确认)之后所发生的变化（增量快照)。客户端和服务器之间发送的每个包都会带有ACK序列号来跟踪网络数据流。当游戏开始时或客户端在发生非常严重的数据包丢失时, 客户可以要求全额快照同步。用户操作的响应速度(操作到游戏世界中的可视反馈之间的时间)是由很多因素决定的，包括服务器/客户端的CPU负载，更新频率，网络速率和快照更新设置，但主要是由网络包的传输时间确定。从客户端发送命令到服务器响应, 再到客户端接收此命令对应的服务器响应被称为延迟或ping（或RTT）。低延迟在玩多人在线游戏时有显著的优势。客户端本地预测和服务器的延迟补偿技术可以尽量为网络较差的游戏玩家提供相对公平的体验。如果有良好的带宽和CPU可用，可以通过调整网络设置以获得更好的体验, 反之我们建议保持默认设置，因为不正确的更改可能导致负面影响大于实际效益。Enitiy插值平滑通常情况下客户端接收每秒约20个快照更新。如果世界中的对象（实体）直接由服务器同步的位置呈现，物体移动和动画会看起来很诡异。网络通信的丢包也将导致明显的毛刺。解决这个问题的关键是要延迟渲染，玩家位置和动画可以在两个最近收到快照之间的连续插值。以每秒20快照为例，一个新的快照更新到达时大约每50毫秒。如果客户端渲染延迟50毫秒，客户端收到一个快照，并在此之前的快照之间内插(Source默认为100毫秒的插补周期)；这样一来，即使一个快照丢失，总是可以在两个有效快照之间进行平滑插值。如下图显示传入世界快照的到达时间： 在客户端接收到的最后一个快照是在tick 344或10.30秒。客户的时间将继续在此快照的基础上基于客户端的帧率增加。下一个视图帧渲染时间是当前客户端的时间10.32减去0.1秒的画面插值延迟10.20。在我们的例子下一个渲染帧的时间是10.22和所有实体及其动画都可以基于快照340和342做正确的插值处理。既然我们有一个100毫秒的延迟插值，如果快照342由于丢包缺失，插值可以使用快照340和344来进行平滑处理。如果连续多个快照丢失，插值处理可能表现不会很好，因为插值是基于缓冲区的历史快照进行的。在这种情况下，渲染器会使用外推法（cl_extrapolate 1），并尝试基于其已知的历史，为实体做一个基于目前为止的一个简单线性外推。外推只会快照更新包连续丢失（cl_extrapolate_amount）0.25秒才会触发，因为该预测之后误差将变得太大。实体内会插导致100毫秒默认（cl_interp 0.1）的恒定视图“滞后”，就算你在listenserver（服务器和客户端在同一台机器上）上玩游戏。这并不是说你必须提前预判动画去瞄准射击，因为服务器端的滞后补偿知道客户端实体插值并纠正这个误差。最近Source引擎的游戏有cl_interp_ratioCVaR的。有了这个，你可以轻松，安全地通过设置cl_interp为0，那么增加的cl_updaterate的值（这同时也会受限于服务器tickrate）来减少插补周期。你可以用net_graph 1检查您的最终线性插值。如果打开sv_showhitboxes，你会看到在服务器时间绘制的玩家包围盒，这意味着他们在前进的线性插值时期所呈现的播放器模式。输入预测让我们假设一个玩家有150毫秒的网络延迟，并开始前进。前进键被按下的信息被存储在用户命令，并发送至服务器。用户命令是由移动代码逻辑处理，玩家的角色将在游戏世界中向前行走。这个世界状态的变化传送到所有客户端的下一个快照的更新。因此玩家看到自己开始行动的响应会有150毫秒延迟，这种延迟对于高频动作游戏(体育，设计类游戏)会有明显的延迟感。玩家输入和相应的视觉反馈之间的延迟会产生一种奇怪的，不自然的感觉，使得玩家很难移动或精确瞄准。客户端的输入预测（cl_predict 1）执行是一种消除这种延迟的方法，让玩家的行动感到更即时。与其等待服务器来更新自己的位置，在本地客户端只是预测自己的用户命令的结果。因此，客户端准确运行相同的代码和规则服务器将使用来处理用户命令。预测完成后，当地的玩家会移动到新位置，而服务器仍然可以看到他在老地方。150毫秒后，客户会收到包含基于他早期预测用户命令更改服务器的快照。客户端会将预测位置同服务器的位置对比。如果它们是不同的，则发生了预测误差。这表明，在客户端没有关于其他实体的正确信息和环境时，它处理用户命令。然后，客户端必须纠正自己的位置，因为服务器拥有客户端预测最终决定权。如果cl_showerror 1开启，客户端可以看到，当预测误差发生。预测误差校正可以是相当明显的，并且可能导致客户端的视图不规则跳动。通过在一定时间（cl_smoothtime）逐渐纠正这个错误，错误可以顺利解决。预测误差平滑处理可以通过设置cl_smooth 0来关闭。预测只对本地玩家以及那些只收它影响的实体有效，因为预测的工作原理是使用客户端的操作来预测的。对于其他玩家没法做有效预测, 因为没有办法立即从他们身上得到操作信息。延迟补偿 比方说，一个玩家在10.5s的时刻射击了一个目标。射击信息被打包到用户命令，该命令通过网络的方式发送至服务器。服务器持续模拟游戏世界，目标可能已经移动到一个不同的位置。用户命令到达服务器时间10.6时服务器就无法检测到射击命中，即使玩家已经在目标准确瞄准。这个错误需要由服务器侧进行延迟补偿校正。延迟补偿系统使所有玩家最近位置的历史一秒。如果在执行用户的命令，服务器预计在命令创建什么时间如下：命令执行时间=当前服务器时间 - 数据包延迟 - 客户端查看插值然后服务器会将所有其他玩家回溯到命令执行时的位置，他们在命令执行时间。用户指令被执行，并正确地检测命中。用户命令处理完成后，玩家将会恢复到原来的位置。由于实体插值包含在公式中，可能会导致意外的结果。服务器端可以启用sv_showimpacts 1，显示服务器和客户端射击包围盒位置差异：该画面在主机上设置延迟200毫秒(net_fakelag设置)时获取的，射击真实命中玩家。红色命中包围盒显示了客户端那里是100毫秒+插补周期前的目标位置。此后，目标继续向左移动，而用户命令被行进到服务器。用户命令到达后，服务器恢复基于所述估计的命令执行时间目标位置（蓝色击中盒）。服务器回溯演绎，并确认命中（客户端看到流血效果）。因为在时间测量精度的误差客户端和服务器命中包围盒不完全匹配。对于快速移动的物体甚至几毫秒的误差也会导致几英寸的误差。多人游戏击中检测不是基于像素的完美匹配，此外基于tickrate模拟的运动物体的速度也有精度的限制。既然击中检测服务器上的逻辑如此复杂为什么不把命中检查放在客户端呢？如果在客户端进行命中检查, 玩家位置和像素命中处理检测都可以精准的进行。客户端将只告诉服务器用“打”的消息一直打到什么样的玩家。因为游戏服务器不能信任客户端这种重要决定。因为即使客户端是“干净”的，并通过了Valve反作弊保护，但是报文可以被截获修改然后发送到游戏服务器。这些“作弊代理”可以注入“打”的消息到网络数据包而不被VAC被检测。网络延迟和滞后补偿可能会引起真实的世界不可能的逻辑。例如，您可能被你看不到的目标所击中。服务器移到你的命中包围盒时光倒流，你仍然暴露给了攻击者。这种不一致问题不能通过一般化的防范解决，因为相对网络包传输的速度。在现实世界中，因为光传播如此之快，你，每个人都在你身边看到同一个世界，所以你才你没有注意到这个问题。网络视图Source引擎提供了一些工具来检查您的客户端连接速度和质量。使用net_graph 2可以启用相关的视图。下面的曲线图中，第一行显示每秒当前的渲染的帧，您的平均延迟时间，以及的cl_updaterate的当前值。第二行显示在最后进来的数据包（快照），平均传入带宽和每秒接收的数据包的字节大小。第三行显示刚刚传出的数据包（用户命令）相同的数据。优化默认的网络设置是专门为通过互联网连接的游戏服务器设计的。可以适用大多数客户机/服务器的硬件和网络配置工作。对于网络游戏，应该在客户端上进行调整，唯一的控制台变量是“rate”，它定义客户端可用的字节/网络连接带宽。 在一个良好的网络环境中，服务器和所有客户端都具有必要的硬件资源可用，可以调整带宽和更新频率设置，来获得更多的游戏精度。增加tickrate通常可以提高运动和射击精度，但会消耗更多的服务器CPU资源。tickrate 100运行的服务器的负载大概是tickrate 66运行时的约1.5倍, 因此如果CPU性能不足可能会导致严重的计算滞后，尤其是在玩家数量比较多的时候。建议对具有更高tickrate超的游戏服务器预留必要的CPU资源。 如果游戏服务器使用较高tickrate运行时，客户端可以在带宽可用的情况下增加他们的快照更新率（的cl_updaterate）和用户命令速率（的cl_cmdrate）。快照更新速率由服务器tickrate限制，一台服务器无法发送每个时钟周期的一个以上的更新。因此，对于一个tickrate66服务器，为的cl_updaterate最高的客户价值，将是66。如果你增加快照率遇到，你必须再次打开它。与增加的cl_updaterate你也可以降低画面插值延迟（cl_interp）。默认的插值延迟为0.1秒(默认的cl_updaterate为20) 视图内插延迟会导致移动的玩家会比静止不动的玩家更早发现对方。这种效果是不可避免的，但可以通过减小视图内插值延迟来减小。如果双方玩家正在移动，画面滞后会延迟影响双方玩家,双方玩家都不能获利。快照速率和视图延迟插值之间的关系如下：插补周期= MAX(cl_interp，cl_interp_ratio /cl_updaterate)可以设置cl_interp为0，仍然有插值的安全量。也可以把cl_updaterate增加，进一步降低你的插补周期，但不会超过更新tickrate(66)或客户端的网络处理能力。小贴士不要瞎改终端配置除非你完全确定你在干嘛 如果客户端和服务器没有足够CPU和网络资源，绝大多数所所谓高性能优化都是起负面作用不要关闭画面插值和延迟补偿 这样并不能代理移动和设计精准度提升优化设置可能不会对每个客户端都有效如果是你是在游戏里或者SourceTv里第一视角观看你看到的画面和玩家可能不一样观战者的画面没有延迟补偿【版权声明】原文作者未做权利声明，视为共享知识产权进入公共领域，自动获得授权；]]></content>
      <categories>
        <category>GS</category>
      </categories>
      <tags>
        <tag>Valve</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++可变参数]]></title>
    <url>%2F2015%2F12%2F09%2Fcpp_vargs%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[可变参数的宏一般在调试打印Debug信息的时候, 需要可变参数的宏. 从C99开始可以使编译器标准支持可变参数宏(variadic macros), 另外GCC也支持可变参数宏, 但是两种在细节上可能存在区别. __VA_ARGS__ 将 “…” 传递给宏 . 如 #define debug(format, ...) fprintf(stderr, format, __VA_ARGS__) GCC使用一种不同的语法,从而可以给可变参数一个名字,如同其它参数一样. #define debug(format, args...) fprintf (stderr, format, args) 这和第一条的宏例子是完全一样的,但是这么写可读性更强并且更容易进行描述. 上面两个定义的宏,如果出现 debug(“A Message”)的时候,由于宏展开后有个多余的逗号,所以在GCC中将导致编译错误, 而VS则不会. 所以移植性更好的写法是使用一个特殊的”##”操作,格式如下: #define debug(format, ...) fprintf (stderr, format, ## __VA_ARGS__) 这里,如果可变参数被忽略或为空,”##”操作将使预处理器(preprocessor)去除掉它前面的那个逗号. 什么是可变形参函数 在 c++ 编程中，有时我们需要编写一些在源代码编写阶段无法确定参数个数，有时甚至无法确定参数类型的函数。 例如，一个求和函数。可以通过重载实现若干个数的和。 1234567int sum(int i1, int i2);int sum(int i1, int i2, int i3);...//还可以重载更多类似函数double sum(double d1, double d2);double sum(double d1, double d2, double d3);...//还可以重载更多类似函数 以上代码通过重载机制来解决变参问题。但很快我们就会发现这种方法存在的问题：必须确保所有可能的实参列表都有对应的重载声明和定义，如果上述方法如果参与运算的参数个数可能从 2——20 个不等，那么我们就需要重载 19 次同一个函数。 我们需要的是这样一类函数：它们可以在运行时取任意的实参个数并根据实参的个数自动处理不同实参的情形，或者至少可以在运行时指定任意的实参个数。 实现变参函数的三种方法 在 C++ 中实现一个变参函数的方法有三种：第一种方法，将函数形参声明为 C++11 新标准中的 initializer_list 标准库类型；第二种方法继承自 C 语言，形参声明为省略符，函数实现时用参数列表宏访问参数；最后一种方法利用 C++ 泛型特性，声明一个可变参数模板来实现。 1. 可变参数宏 实现步骤如下： １. 函数原型中使用省略号； ２. 函数定义中创建一个 va_list 变量； 3. 初始化 va_list 变量； 4. 访问参数列表； 5. 完成清理工作； 上述步骤的实现需要使用到四个宏：_va_list、va_start(va_list, arg)、va_arg(va_list, type)、va_end(valist) 这些宏在头文件 stdarg.h 中声明定义。因此使用时需要包含该头文件。 以下代码使用可变参数宏实现一个函数 sum，该函数接受任意个数的整形实参，返回这些实参的和。（忽略可能存在的整形溢出） 12345678910111213141516/* --sum.cpp-- 可变参数宏实现求任意个整形值得和 */#include &lt;stdarg.h&gt;int sum(int count, ...); //原型中使用省略号int sum(int count, ...)&#123; //count 表示可变参数个数 va_list ap; //声明一个va_list变量 va_start(ap, count); //初始化，第二个参数为最后一个确定的形参 int sum = 0; for(int i = 0; i &lt; count; i++) sum += va_arg(ap, int); //读取可变参数，的二个参数为可变参数的类型 va_end(ap); //清理工作 return sum;&#125; 使用这种方法需要注意一下几点： 1. 函数原型中，省略号必须在参数列表的末尾：也就是说，在函数原型中参数列表省略号的右边不能再出现确定参数； 2. 运行时，函数必须能够根据已有信息（既有约定，或确定实参）确定可变参数的具体个数与类型：函数定义需要知道可变参数的具体类型、个数，这些信息是在运行时确定的，那么显然应该由实参来确定。在上面的例子中 count 传递了可变参数的个数，而参数类型则是既有约定（整形）； 3. 使用完成时需要用 va_end() 做清理工作，可变参数宏可能使用了动态分配的内存，忘记执行清理操作有可能导致内存泄漏等问题； 4. 可变参数宏只能实现顺序访问可变参数，无法后退访问，但是可以在清理操作完成后重新使用 va_start 初始化 va_list 变量，重新遍历形参表； 5. 该方法是极不安全的，宏本身无法提供任何安全性保证，他总是按照既定代码 “自作多情” 的认为实参就应该是那么多，即使实参并不是那么多。这就要求所有安全性必须由程序员来保证。例如，在以上的示例代码中，如果调用时指定 count 为 10，但实际上只给出 9 个可变形参，那么函数还是会读取 10 个参数，显然第十次读取是多余的，多余的操作一般不会有什么好结果，当然如果实参过多，多余的实参也不会被读取而是被忽略。 使用这种方法的一个实例是 printf() 函数。printf() 函数通过分析第一个字符串参数中的占位符个数来确定形参的个数；通过占位符的不同来确定参数类型（%d 表示 int 类型、%s 表示 char *）；它也有上述提到的安全问题，如果不小心少提供了个实参，那么越界访问就会发生。 2. initializer_list 标准库类型 实现步骤如下： １. 函数原型中使用实例化 initializer_list 模板代表可变参数列表； ２. 使用迭代器访问 initializer_list 中的参数； 3. 传入实参写在 {} 之内。 以上步骤中使用到 initializer_list。这是 C++11 新标准中引入的一个标准库类型，与 vector 等容器一样 initializer_list 也支持 begin() 和 end() 操作，返回指向首元素的迭代器和尾后迭代器。initializer_list 在同名头文件中声明，其实现由编译器支持。 以下代码使用 initializer_list 实现函数 sum。（忽略可能存在的整形溢出） 1234567891011/* --sum.cpp-- 利用initializer_list模板实现求人一个整形值得和 */#include &lt;initializer_list&gt;int sum(initializer_list&lt;int&gt; il); //函数原型用int实例化initializer_list作为形参int sum(inttializer_list&lt;int&gt; il)&#123; int sum = 0; for(auto p = il.begin(); p != il.end(); p++) //使用迭代器访问参数 sum += *p; return sum;&#125; 使用这种方法需要注意一下几点： １. initializer_list 在 C++11 中才被引入，这意味着在编译时可能需要加上这个选项 -std=c++11 才能成功编译。上述代码中的 auto 关键字也是 C++11 的一部分； 2. 参数必须放在一组‘{}’（大括号）内，编译器通过大括号来将这组参数转化为 initializer_list. 大括号的的一组实参与 initializer_list 形参对应； 3. 函数原型 initializer_list 与普通形参无异。这表明形参列表中可以包含其他类型参数且位置不限，以下函数原型是正确的： 1void func(char c, initializer_list&lt;int&gt; il, double d); ４. 同一个 initializer_list 中的参数具有相同的类型。本质上来说 initializer_list 是一个编译器支持的容器类模板，同其他容器一样，容器中的元素具有相同的类型。 使用这种方法的一个实例是 C++11 中 vector 的列表初始化构造函数。 3. 可变参数模板 在介绍这种方法之前需要先介绍两个并不常用的概念：模板参数包和函数参数包。 模板参数包是零个或多个类型参数的集合。模板参数列表中，class… 或 typename… 表明其后的类型参数表示一个模板参数包； 函数参数包是零个或多个非类型参数的集合。函数形参列表中类型名加省略号表明其后的参数表示一个函数参数包；另外，类型为模板参数包的函数形参是一个函数参数包。 以下引用参考书目 2 中的示例代码来直观展现这两个概念： 12345//args是一个模板参数包；rest是一个函数参数包//args表示零个或多个模板类型参数//rest表示零个或多个函数参数template&lt;typename T, typename... args&gt;void foo(const T &amp;t, const args&amp;... rest); 与 sizeof() 运算符类似，sizeof…() 运算符用于参数包。sizeof…() 将返回参数包中参数个数。 利用可变参数模板实现可变参数函数的步骤如下： １. 编写含有模板参数包和函数参数包的模板函数； 2. 函数定义递归调用自己，每一步递归参数包中参数减一； 3. 编写处理边界情况（参数包含有零个参数）的模板。 以下引用参考书目２中示例代码： 123456789101112//用来终止递归并答应最后一个元素的函数//此函数必须在可变参数版本的print定义之前声明template &lt;typename T&gt;std::ostream &amp;print(std::ostream &amp;os, const T &amp;t)&#123; return os &lt;&lt; t; //包中最后一个元素&#125;//包中除最后一个元素之外的其他元素都会调用这个版本的pirnttemplate &lt;typename T, typename... Args&gt;std::ostream &amp;print(std::ostream &amp;os, const T &amp;t, cosnt Args &amp;... rest)&#123; os &lt;&lt; t &lt;&lt; &quot;,&quot;; //打印第一个实参，包中元素减一 return print(os, rest...); //递归调用，打印剩余实参&#125; 使用这种方法需要注意的是： 1. 必须处理边界情况。且如代码注释所示：应当首先定义处理边界情况的模板。 2. 参数包在参数列表最右侧，参数包只能从左至右展开？ 3. 参数包能够实现更加复杂的模板，更多内容参考 C++ Primer(第五版) 第 16 章相关内容。 这种实现方式的根本原理实际上与最初提到的重载是一致的。通过定义模板，让编译器根据实参类型自动生成对应的重载函数。 三种实现方法的比较 以上提到的三种方法都可以实现变参函数。但三种方法都有其各自的有点和局限性，在选择时可以从以下几个方面考虑： 1. 若非必要，不要使用可变参数函数。应该首先考虑函数重载等其他方法。 2. 除非需要兼容 C 语言编译器，否则不要使用可变参数宏。应为这种方法最不安全；尤其是当参数为对象时这种方法易产生各种问题。毕竟这些宏是为 C 语言设计的，C 语言中没有对象。 3. 如果参数类型相同且 C++11 可用，则通过声明形参为 initializer_list 往往是最简单、最有效的办法。 4. 变参模板看似最为强大。参数的类型可以不同、比可变参数宏更加安全并且可以自动推断参数类型和参数个数。但考虑到模板会为每一个不同的实例生成代码，如果函数的实例过多可能会使代码体积增大。另外，依靠递归使得功能具有局限性，并且效率也会受到影响。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++很基础的易混淆点二]]></title>
    <url>%2F2015%2F12%2F09%2Fcplusplus_confused_points_two%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[位操作运算符的问题二进制的100 的第0位是 01(第2位) 0(第1位) 0(第0位)，所以把一个数的第三位进行清零、置位(把某个bit置为1)、取反的操作如下：123456789101112131415161718192021#include &lt;stdio.h&gt;#define Bit3 (0X01&lt;&lt;3)/*对一个数的第三位进行清零、置位、取反*/int main()&#123; int a=15 ; // 0000 1111 printf("原大小：%d\n", a); a &amp;= ~Bit3; //清零, 0000 0111 printf("清零后：%d\n", a); a |= Bit3; //置位, 0000 1111 printf("置位后：%d\n", a); a ^= Bit3; //取反, 0000 0111 printf("取反后：%d\n", a); return 0;&#125; . . . 字符串分配的位置问题程序的存储区域分为：代码段、只读数据段、已初始化的读写数据段、未初始化的数据段、堆、栈。1、代码段、只读数据段、已初始化的读写数据段、未初始化的数据段都属于静态区域。2、堆内存只在程序运行时出现，一般有程序员分配和释放。3、栈内存只在程序运行时出现，在函数内部使用的变量，函数参数以及返回值将使用栈空间。1234567891011121314151617char* get_str()&#123; char *str = "hello"; //第一种情况：分配在静态存储区上 //char str[] = "hello"; //第二种情况分配在栈上 return str;&#125;int main()&#123; char* p = get_str(); // 如果是第一种情况，下述打印可以打印出正确的值；但是第二种情况打印结果是错的。 printf("%s/n", p); *++p = 'a'; // 如果是第一种情况，运行时将会段错误，因为不能修改它； printf("%s/n", p); return 0;&#125; 复杂类型的声明和typedef定义 用变量a给出下面的定义:一个有10个指针的数组，该指针指向一个函数，该函数有一个整型参数并返回一个整型数 int (*a[10])(int); typedef表示一个长度为4的int数组;typedef int ARR[4]; typedef表示一个函数指针有一个整型参数并返回一个整型数：typedef int（*FUNC）（int）；]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tolua++安装]]></title>
    <url>%2F2015%2F11%2F11%2Flua_cpp_toluapp_tutorial%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[我们用一个例子来说明. 本文环境为 : ubuntu1404 g++ 4.8.4 python git lua5.1( 因为tolua++只支持到5.1, 安装5.1教程看 Lua的win和linux环境简单安装 ) . . . 安装tolua++1. git clone git@github.com:LuaDist/toluapp.git2. sudo apt-get install scons3. cd toluapp/ 4. vi custom.py 然后添加内容 :custom.py12345678910# 自己通过命令 sudo find / -name "*lua.h*" 来查头文件.h在哪, 然后把路径填到下面CCFLAGS = ['-I/usr/include/lua5.1', '-O2', '-ansi']# 自己通过命令 sudo find / -name "*liblua*" 来查静态库.a文件在哪, 然后把路径填到下面LIBPATH = ['/usr/lib/x86_64-linux-gnu']LIBS = ['lua5.1', 'dl', 'm']#prefix = '/mingw'#build_dev=1tolua_bin = 'tolua++5.1'tolua_lib = 'tolua++5.1'TOLUAPP = 'tolua++5.1' 5. scons all6. scons install 测试用以下五个文件测试, 输入命令 : 1. tolua++5.1 -o lua_Student.cpp Student.pkg2. g++ *.cpp -I/usr/include/lua5.1 -llua5.1 -lm -ltolua++5.13. 如果执行 ./a.out 之后, 打印结果如下则为环境全部安装成功 : 1 2 3 4 5 6 7 8 9 10 Student Run Student Run10 1 2 3 4 5 6 7 8 9 10 Student Run Student Run10 1 2 3 4 5 6 7 8 9 10 Student Run Student Run10 ... 五个测试文件Student.h123456789101112131415#pragma once #include&lt;iostream&gt;using namespace std; class Student&#123;public: Student(); ~Student(); void Run(); void Run(int a);&#125;; Student.cpp123456789101112131415161718192021#include "Student.h" Student::Student()&#123;&#125; void Student::Run()&#123; cout &lt;&lt; "Student Run" &lt;&lt; endl;&#125; void Student::Run(int a)&#123; cout &lt;&lt; "Student Run" &lt;&lt;a&lt;&lt; endl;&#125; Student::~Student()&#123;&#125; 1234567891011$#include&quot;Student.h&quot; class Student&#123;public: Student(); ~Student(); void Run(); void Run @ Run2(int a);&#125;; test.lua12345678910111213141516171819202122232425studentB=Student:new() --实例化Student全局对象 function Run() studentB:Run();end function Run2(a) studentB:Run2(a);end function show() local b = &#123;&#125; local index for index = 1,10,1 do print(index) end end show() Run() Run2(10) main.cpp1234567891011121314151617181920212223242526272829303132#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;extern "C"&#123; #include "lua.h" #include "lualib.h" #include "lauxlib.h" #include "luaconf.h"&#125;#include "tolua++.h"#include"Student.h"extern int tolua_Student_open(lua_State* tolua_S);int main(int argc, char* argv[])&#123; while(1) &#123; sleep(2); lua_State* L = luaL_newstate(); luaL_openlibs(L); tolua_Student_open(L); luaL_dofile(L, "./test.lua"); lua_close(L); &#125; return 0;&#125; 在运行的时候把test.lua文件的Run2(10) 改为 Run2(99) 之后,打印也会跟着变为 : 1 2 3 4 5 6 7 8 9 10 Student Run Student Run10 1 2 3 4 5 6 7 8 9 10 Student Run Student Run99 1 2 3 4 5 6 7 8 9 10 Student Run Student Run99 ...]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua中ipairs和pairs的区别与使用]]></title>
    <url>%2F2015%2F11%2F11%2Flua_pairs_ipairs%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[关于ipairs()和pairs(),Lua官方手册是这样说明的： pairs (t) If t has a metamethod __pairs, calls it with t as argument and returns the first three results from the call. Otherwise, returns three values: the next function, the table t, and nil, so that the construction ` for k,v in pairs(t) do body end` will iterate over all key–value pairs of table t. See function next for the caveats of modifying the table during its traversal. ipairs (t) If t has a metamethod __ipairs, calls it with t as argument and returns the first three results from the call. Otherwise, returns three values: an iterator function, the table t, and 0, so that the construction ` for i,v in ipairs(t) do body end` will iterate over the pairs (1,t[1]), (2,t[2]), …, up to the first integer key absent from the table. 根据官方手册的描述，pairs会遍历表中所有的key-value值，而pairs会根据key的数值从1开始加1递增遍历对应的table[i]值，直到出现第一个不是按1递增的数值时候退出。 . . . 例子下面我们以例子说明一下吧1234stars = &#123;[1] = "Sun", [2] = "Moon", [5] = 'Earth'&#125;for i, v in pairs(stars) do print(i, v)end 使用pairs()将会遍历表中所有的数据，输出结果是： 1 Sun 2 Moon 5 Earth 如果使用ipairs（）的话， 12345for i, v in ipairs(stars) do print(i, v)end 当i的值遍历到第三个元素时，i的值为5，此时i并不是上一个次i值（2）的+1递增，所以遍历结束，结果则会是： 1 Sun 2 Moon ipairs()和pairs()的区别就是这么简单。 还有一个要注意的是pairs()的一个问题，用pairs()遍历是[key]-[value]形式的表是随机的，跟key的哈希值有关系。看以下这个例子： 1234567stars = &#123;[1] = "Sun", [2] = "Moon", [3] = "Earth", [4] = "Mars", [5] = "Venus"&#125;for i, v in pairs(stars) do print(i, v)end 结果是： 2 Moon 3 Earth 1 Sun 4 Mars 5 Venus 并没有按照其在表中的顺序输出。 但是如果是这样定义表stars的话 stars = {&quot;Sun&quot;, &quot;Moon&quot;, “Earth”, &quot;Mars&quot;, &quot;Venus&quot;} 结果则会是 1 Sun 2 Moon 3 Earth 4 Mars 5 Venus 你清楚了吗？:)]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++与Lua本质原始交互API]]></title>
    <url>%2F2015%2F11%2F11%2Flua_cpp_bind%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[我们用一个例子来说明. . . . 创建c++主程序首先, 我们需要创建一个 C++ 的主程序，以便同 Lua 进行通信. 如下 : lua_test.cpp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475extern "C" &#123; #include "lua.h" #include "lualib.h" #include "lauxlib.h"&#125;; // 注意 : 这个extern "C" &#123;&#125; 非常重要, 不然会找不到相关库函数#include &lt;iostream&gt;#include &lt;lua.hpp&gt;extern "C" &#123; static int l_cppfunction(lua_State *L) &#123; double arg = luaL_checknumber(L,1); lua_pushnumber(L, arg * 0.5); return 1; &#125;&#125;using namespace std;int main(int argc, const char * argv[])&#123; lua_State *L; L = luaL_newstate(); cout &lt;&lt; "&gt;&gt; 载入（可选）标准库，以便使用打印功能" &lt;&lt; endl; luaL_openlibs(L); cout &lt;&lt; "&gt;&gt; 载入文件，暂不执行" &lt;&lt; endl; if (luaL_loadfile(L, "luascript.lua")) &#123; cerr &lt;&lt; "载入文件出现错误" &lt;&lt; endl; cerr &lt;&lt; lua_tostring(L, -1) &lt;&lt; endl; lua_pop(L,1); &#125; cout &lt;&lt; "&gt;&gt; 从 C++ 写入数据 cppvar" &lt;&lt; endl; lua_pushnumber(L, 1.1); lua_setglobal(L, "cppvar"); cout &lt;&lt; "&gt;&gt; 执行 lua 文件" &lt;&lt; endl &lt;&lt; endl; if (lua_pcall(L,0, LUA_MULTRET, 0)) &#123; cerr &lt;&lt; "执行过程中出现错误" &lt;&lt; endl; cerr &lt;&lt; lua_tostring(L, -1) &lt;&lt; endl; lua_pop(L,1); &#125; cout &lt;&lt; "&gt;&gt; 从 Lua 读取全局变量 luavar 到 C++" &lt;&lt; endl; lua_getglobal(L, "luavar"); double luavar = lua_tonumber(L,-1); lua_pop(L,1); cout &lt;&lt; "C++ 从 Lua 读取到的 luavar = " &lt;&lt; luavar &lt;&lt; endl &lt;&lt; endl; cout &lt;&lt; "&gt;&gt; 从 C++ 执行 Lua 的方法 myfunction" &lt;&lt; endl; lua_getglobal(L, "myluafunction"); lua_pushnumber(L, 5); lua_pcall(L, 1, 1, 0); cout &lt;&lt; "函数返回值是：" &lt;&lt; lua_tostring(L, -1) &lt;&lt; endl &lt;&lt; endl; lua_pop(L,1); cout &lt;&lt; "&gt;&gt; 从 Lua 执行 C++ 的方法" &lt;&lt; endl; cout &lt;&lt; "&gt;&gt;&gt;&gt; 首先在 Lua 中注册 C++ 方法" &lt;&lt; endl; lua_pushcfunction(L,l_cppfunction); lua_setglobal(L, "cppfunction"); cout &lt;&lt; "&gt;&gt;&gt;&gt; 调用 Lua 函数以执行 C++ 函数" &lt;&lt; endl; lua_getglobal(L, "myfunction"); lua_pushnumber(L, 5); lua_pcall(L, 1, 1, 0); cout &lt;&lt; "函数返回值是：" &lt;&lt; lua_tonumber(L, -1) &lt;&lt; endl &lt;&lt; endl; lua_pop(L,1); cout &lt;&lt; "&gt;&gt; 释放 Lua 资源" &lt;&lt; endl; lua_close(L); return 0;&#125; 编译命令 : g++ lua_test.cpp -o ltest -llua -ldl 创建Lua文件其次，是 lua 文件，我们将它命名为 luascript.lua luascript.lua1234567891011print("Hello from Lua")print("Lua code is capable of reading the value set from C++", cppvar)luavar = cppvar * 3function myluafunction(times) return string.rep("(-)", times)endfunction myfunction(arg) return cppfunction(arg)end 打印结果运行 cpp 文件，结果如下： &gt;&gt; 载入（可选）标准库，以便使用打印功能 &gt;&gt; 载入文件，暂不执行 &gt;&gt; 从 C++ 写入数据 cppvar &gt;&gt; 执行 lua 文件 Hello from Lua Lua code is capable of reading the value set from C++ 1.1 &gt;&gt; 从 Lua 读取全局变量 luavar 到 C++ C++ 从 Lua 读取到的 luavar = 3.3 &gt;&gt; 从 C++ 执行 Lua 的方法 myfunction 函数返回值是：(-)(-)(-)(-)(-) &gt;&gt; 从 Lua 执行 C++ 的方法 &gt;&gt;&gt;&gt; 首先在 Lua 中注册 C++ 方法 &gt;&gt;&gt;&gt; 调用 Lua 函数以执行 C++ 函数 函数返回值是：2.5 &gt;&gt; 释放 Lua 资源 参考参考]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KMP查找子字符串]]></title>
    <url>%2F2015%2F11%2F10%2Fkmp%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[KMP查找子字符串前言 KMP 算法是一种改进的字符串匹配算法，由 D.E.Knuth，J.H.Morris 和 V.R.Pratt 同时发现，因此人们称它为克努特——莫里斯——普拉特操作（简称 KMP 算法）。KMP 算法的关键是利用匹配失败后的信息（已经匹配的部分中的对称信息），尽量减少模式串（待搜索词）与文本串的匹配次数以达到快速匹配的目的。具体实现就是实现一个 next() 函数，该函数本身包含了模式串的局部匹配信息。 一，kmp 算法的原理：字符串匹配是计算机的基本任务之一。它所执行的任务就是在一个文本（较长的字符串）中查找是否包含着当前指定的模式字符串（较短的字符串），并找到其位置，这就是字符串匹配问题。注：在算法导论里面待搜索的词叫 “模式” 字符串，并用 P 来表示，本文有的地方叫他“待搜索词”。如无特殊说明数组下标以 0 为起始。举例来说，有一个长字符串 “BBC ABCDAB ABCDABCDABDE”（即文本串），我想知道里面是否包含另一个模式字符串 “ABCDABD”，它的位置在哪呢？ 1，朴素匹配算法过程：1）. 首先，长字符串 “BBC ABCDAB ABCDABCDABDE” 的第一个字符与搜索词 “ABCDABD” 的第一个字符，进行比较。因为 B 与 A 不匹配，所以搜索词后移一位。 2）. 因为 B 与 A 不匹配，搜索词再往后移。 3）. 就这样，直到长字符串有一个字符，与搜索词的第一个字符相同为止（准备开始位置不变搜索下一个字符）。 4）. 接着比较字符串和搜索词的下一个字符，还是相同。 5）. 但是后面的有可能是不完全匹配的，这时在长字符串中遇到一个字符与搜索词对应的字符不相同。 6）. 最自然的反应是，将搜索词整个后移一位，再从头逐个比较。这样做虽然可行，但是效率很差，因为你要把搜索位置 BCD 这一段又要重比一次，然而它的比较在直到了后面有 “AB” 与前面的 “AB” 对称的情况下是可以避免的，直到遇到 “AB” 才萌发新的可能使整个字符匹配的可能性。 2，KMP 匹配算法过程：1). 接着前面的匹配过程，现在来看：一个基本事实是，当空格与 D 不匹配时，你其实知道前面六个符 “ABCDAB” 已经匹配的部分。KMP 算法的想法是，设法利用这个搜索词的子串（就是搜索词的某个前缀）的已知对称值，注意这里的对称非中心对称，而是基于搜索词的某一个前缀的对称信息，在不匹配的时候跳过一些不必要的位置来加速匹配速度，这样就提高了效率。 2). 怎么做到这一点呢（如何跳过不必要的位置在不匹配时）？可以针对搜索词，算出一张模式字符串的前缀函数表。这张表是如何产生的，后面再介绍，这里只要知道就可以了。 3). 已知空格与 D 不匹配时，前面六个字符 “ABCDAB” 是匹配的。查模式串的《前缀函数表》可知，其前缀（即 ABCDAB）中的最后一个匹配字符 B 的位置对应的 “部分匹配值” 为 2，因此按照下面的公式算出向后移动的位数（直接将前面的对称信息调到后面的对称位置，跳过一些不必要的位置）： 移动位数 = 已匹配的字符数 - 当前已匹配字符串的部分匹配值 因为 6 - 2 等于 4，所以将搜索词向后移动 4 位（之所以移动 4，是因为搜索词的已匹配的前缀串 “ABCDAB” 的部分匹配值为 2，即有 “AB” 对称，因为当前已匹配字符串的 “AB” 始终要找到下一个 “AB” 的开始位置，而这个开始位置恰好就在本字符串中的后缀中，所以直接移动 4，我们可以最大减少匹配次数）。 4). 因为空格与Ｃ不匹配，搜索词还要继续往后移。这时，已匹配的字符数为 2（”AB”），对应的 “部分匹配值” 为 0。所以，移动位数 = 2 - 0，结果为 2，于是将搜索词向后移 2 位。 5). 因为空格与 A 不匹配，继续后移一位。 6). 逐位比较，直到发现 C 与 D 不匹配。于是，移动位数 = 6 - 2，继续将搜索词向后移动 4 位。 7). 逐位比较，直到搜索词的最后一位，发现完全匹配，于是搜索完成。如果还要继续搜索（即找出全部匹配），移动位数 = 7 - 0，再将搜索词向后移动 7 位，这里就不再重复了。 3, 关于前缀函数表1）前缀和后缀 首先，要了解两个概念：”前缀” 和 “后缀”。 “前缀” 指除了最后一个字符以外，一个字符串的全部头部组合；”后缀” 指除了第一个字符以外，一个字符串的全部尾部组合。 2）. 前缀函数表的产生 “前缀函数” 的对称值（也叫部分匹配值）就是模式串的对应前缀中的 “前缀” 和 “后缀” 的最长的共有元素的长度（实质是最大对称程度）。以 “ABCDABD” 为例， “A” 的前缀和后缀都为空集，共有元素的长度为 0； “AB” 的前缀为 [A]，后缀为 [B]，共有元素的长度为 0； “ABC” 的前缀为 [A, AB]，后缀为 [BC, C]，共有元素的长度 0； “ABCD” 的前缀为 [A, AB, ABC]，后缀为 [BCD, CD, D]，共有元素的长度为 0； “ABCDA” 的前缀为 [A, AB, ABC, ABCD]，后缀为 [BCDA, CDA, DA, A]，共有元素为 “A”，长度为 1； “ABCDAB” 的前缀为 [A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为 “AB”，长度为 2（“AB” 是其最大对称串，长度为 2）； “ABCDABD” 的前缀为 [A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为 [BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为 0。 3）. 前缀函数表的意义 A, 首相要搞清楚的是它是模式字符串的所有前缀产生的一张表，保存的值表征了最大对称度，我们一般用 next 数组来保存其某个前缀的对称值，例如 next[6]=2，代表的就是模式串的某个有 6 个字符的前缀 “ABCDAB”，这个前缀的对称度就是 2，即“AB” 是对称的。 B, 模式串的某前缀的对称值的实质是，有时候，搜索词已经部分匹配了（一定是某个前缀），其某个前缀（已经匹配部分）的头部和尾部会有重复。比如，”ABCDAB” 之中前缀有 “AB”，后缀也有 “AB”，那么它的 “前缀函数对称值” 就是 2（”AB” 的长度, 且 “AB” 是 “ABCDAB” 所有前缀和后缀中的最长公有元素）。搜索词移动的时候，第一个 “AB” 向后移动 4 位（字符串长度 - 部分匹配值），就可以来到第二个 “AB” 的位置，在这个新的位置我们跳过了不必要的比较位置，并且直接就有 “AB” 匹配。 二，前缀函数表实现原理 通过上文完全可以对 kmp 算法的原理有个清晰的了解，那么下一步就是编程实现了，其中最重要的就是如何根据待匹配的模式字符串求出其所有前缀函数表中的最大对称值，在下文中我们将其存储在 next 数组中。 1，编程策略： 1）、当前字符的前面所有字符的对称程度为 0 的时候，只要将当前字符与前面这个子串的第一个字符进行比较。这个很好理解啊，前面所有字符串的对称值都是 0，说明都不对称了，如果多加了一个字符，要对称的话只能是当前字符和前面字符串的第一个字符对称。比如 “ABCDA” 这个里面 “ABCD” 的最大对称值是 0，那么后面的 A 的对称程度只需要看它是不是等于前面字符串的第一个字符 A 相等，如果相等就增加 1，如果不相等那就保持不变，显然还是为 0。 2）、按照这个推理，我们就可以总结一个规律，不仅前面是 0 呀，如果前面字符串的的最大对称值是 1（k），那么我们就把当前字符与前面字符串的第二（k）个字符即 P[1]（P[k]）进行比较，因为前面的是 1（k），说明前面的字符已经和第一（k）个字符相等了，如果这个又与第二（k+1）个相等了，说明对称程度就是 2（k+1）了。有两（k+1）个字符对称了。比如上面 “ABCDA” 的最大对称值是 1，说明它只和第一个 A 对称了，接着我们就把下一个字符 “B” 与 P[1]（即第二个字符）比较，又相等，自然对称程度就累加了，就是 2 了。 但是如果不相等呢？那么这个对称值显然要减少，并且我们只能到前面去寻找对称值，而在找的过程我们同时也利用前缀函数表快速搜索找到与当前字符匹配的位置。比如假设是 “(AGCTAGC)(AGCTAGC)T”（请无视字符串中的括号，只为方便看出对称）， 模式字符串：AGCTAGCAGCTAGCT 模式字符串的前缀函数表： 000012312345674 显然最后一个 T 的前一个位置的对称度是 7, 说明 T 的前一个位置的 7 个字符的后缀必与 7 个字符的前缀相等，然而 T!=P[7]，说明 T 位置的对称度只能是比 7 小的长度的前缀，所以递减 k 值，递减为多少呢？那么我们应该利用已经得到的 next[0]···next[k-1] 来求 P[0]···P[k-1] 这个前缀中最大相同前后缀, 当前字符前一个位置的对称度为 k=next[13]=7，显然必须以 7 为基准减少，即在前缀长度为 7 以内的范围重新寻找以 T 结尾的前缀，所以 k=next[6] (即下面参考代码中的k = next[k-1];)，再接着判断是否相等 3）、按照上面的推理，我们总是在找当前字符 P[q]（q 为遍历到的位置下标，见下面程序）通过其前一个位置的对称值判断是否与 P[k] 相等，如果相等，那么加，如果不相等，那么就减少 k 值，重新寻找与与 P[q] 相等的元素位置 2，参考代码：1234567891011121314void MakeNext(const char P[],int next[]) &#123; int k;//k:最大对称长度 int m = strlen(P);//模版字符串长度 next[0] = 0;//模版字符串的第一个字符的最大对称值必为0 for (int q = 1,k = 0; q &lt; m; ++q)//for循环，从第二个字符开始，依次计算每一个字符对应的next值 &#123;//在前一个位置的k不为0，但是却不相等，那么减少k，重新寻找与P[q]相等的位置，让下面的if来增加k while(k &gt; 0 &amp;&amp; P[q] != P[k])// k = next[k-1]; //while循环是整段代码的精髓所在， if (P[q] == P[k])//如果相等，那么最大相同前后缀长度加1 k++;//增加k的唯一方式 next[q] = k; &#125; &#125; 附完整代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include "vector"#include "string"#include &lt;iostream&gt;#include "algorithm"using namespace std;//计算模式P的部分匹配值，保存在next数组中 void MakeNext(const string &amp;P, vector&lt;int&gt; &amp;next)&#123; int q, k;//k记录所有前缀的对称值 int m = P.size();//模式字符串的长度 next[0] = 0;//首字符的对称值肯定为0 for (q = 1, k = 0; q &lt; m; ++q)//计算每一个位置的对称值 &#123; //k总是用来记录上一个前缀的最大对称值 while (k &gt; 0 &amp;&amp; P[q] != P[k]) //k = next[k - 1];//k将循环递减，值得注意的是next[k]&lt;k总是成立 --k; if (P[q] == P[k]) k++;//增加k的唯一方法 next[q] = k;//获取最终值 &#125;&#125;void KmpMatch(const string &amp;T, const string &amp;P, vector&lt;int&gt; &amp;next)&#123; int n, m; n = T.size(); m = P.size(); MakeNext(P, next); for (int i = 0, q = 0; i &lt; n; ++i) &#123; while (q &gt; 0 &amp;&amp; P[q] != T[i]) q = next[q - 1]; if (P[q] == T[i]) q++; if (q == m) &#123; cout &lt;&lt; "模式文本的偏移为：" &lt;&lt; (i - m + 1) &lt;&lt; endl; q = next[q - 1];//寻找下一个匹配 &#125; &#125;&#125;int main()&#123; system("color 0A"); vector&lt;int&gt; next(20, 0);//保存待搜索字符串的部分匹配表（所有前缀函数的对称值） string T = "BBC ABCDAB ABCDABCDABDE";//文本 string P = "ABCDABD";//待搜索字符串 cout &lt;&lt; "文本字符串：" &lt;&lt; T &lt;&lt; endl; cout &lt;&lt; "模式字符串：" &lt;&lt; P &lt;&lt; endl; KmpMatch(T, P, next); cout &lt;&lt; "模式字符串的前缀函数表：" &lt;&lt; endl; for (int i = 0; i &lt; P.size(); i++) cout &lt;&lt; next[i]; cout &lt;&lt; endl; system("pause"); return 0;&#125; 参考资源：【1】网友，c_cloud，《KMP，深入讲解 next 数组的求解》，博客地址，http://www.cnblogs.com/c-cloud/p/3224788.html【2】网友，yearn520，《KMP 算法的前缀 next 数组最通俗的解释》，博客地址，http://blog.csdn.net/yearn520/article/details/6729426【3】《算法导论》，第三十二章，字符串匹配【4】网友，jBoxer，《The Knuth-Morris-Pratt Algorithm in my own words》，博客地址，http://jakeboxer.com/blog/2009/12/13/the-knuth-morris-pratt-algorithm-in-my-own-words/【5】九度 OJ，http://ac.jobdu.com/problemset.php?page=2【6】 https://blog.csdn.net/EbowTang/article/details/49129363]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>noodle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua的win和linux环境搭建]]></title>
    <url>%2F2015%2F11%2F08%2Flua_install_tutorial%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[ubuntu环境. . . 测试文件a.cpp123456789101112131415extern "C" &#123; #include "lua.h" #include "lualib.h" #include "lauxlib.h"&#125;; // 注意 : 这个extern "C" &#123;&#125; 非常重要, 不然会找不到相关库函数//#include "lua.h"//#include "lauxlib.h"int main(int argc, char **argv)&#123; lua_State *L = luaL_newstate(); luaL_openlibs(L); luaL_dostring(L, "print('hello, '.._VERSION)"); return 0;&#125; lua5.1sudo apt-get install lua5.1 sudo apt-get install liblua5.1-0-dev 编译命令 : gcc a.cpp -I/usr/include/lua5.1 -llua5.1 -lm 生成 a.out 之后, 运行 a.out, 若打印 hello, Lua 5.1 即为安装成功. lua5.3sudo apt-get install libreadline-dev sudo curl -R -O http://www.lua.org/ftp/lua-5.3.0.tar.gz sudo tar zxf lua-5.3.0.tar.gz sudo cd lua-5.3.0 sudo make linux test sudo make install 编译命令 : g++ a.cpp -llua -ldl 生成 a.out 之后, 运行 a.out, 若打印 hello, Lua 5.3 即为安装成功. luajit //下载git clone http://luajit.org/git/luajit-2.0.gittar zxf LuaJIT-2.0.4.tar.gzcd LuaJIT-2.0.4//linux下编译make//安装sudo make installluajit -v出现版权信息即为安装成功。 luarocks到luarocks的官网下载luarocks, 直接apt-get的已经太老旧, 默认的配置文件有错 luarocks 命令： luarocks build XXX 建立/编译一个包 luarocks download XXX 从rocks服务器下载一个指定文件或者包 luarocks help luarocks帮助 luarocks install XXX 安装包 luarocks make XXX 下载并编译包 luarocks pack 打包 luarocks list 显示已安装的列表 luarocks path 返回包地址 luarocks remove XXX 删除 luarocks search Query the LuaRocks repositories luarocks show Shows information about an installed rock. luarocks unpack Unpack the contents of a rock. Install lua-socket如果有安装 Lua 模块的安装和部署工具 – luarocks， 那么一条指令就能安装部署好 LuaSocket： luarocks install luasocket 关于json如果想安装一个解析 JSON(JavaScript Object Notation) 的模块，可以用 search 参数先搜索一下有什么可安装的解析 JSON 的模块： luarocks search json 假设想安装一个名为 json4lua 模块，可以用 install 参数来安装： luarocks install json4lua Windows环境首先要安装一个微软依赖 : https://www.microsoft.com/en-us/download/details.aspx?id=3387&amp;fa43d42b-25b5-4a42-fe9b-1634f450f5ee=True 然后安装lua for windows : http://www.runoob.com/lua/lua-environment.html 或 http://luaforge.net/projects/luaforwindows/]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令笔记整理之tcpdump]]></title>
    <url>%2F2015%2F11%2F03%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%E4%B9%8Btcpdump%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[强大的抓包工具, 博大精深内容太多, 所以这篇博客整理得只说常用, 具体的参考tcpdump用户手册,tcpdump需要root权限, 所以记得加上sudo 常用参数 -nn选项：意思是说当tcpdump遇到协议号或端口号时，不要将这些号码转换成对应的协议名称或端口名称。比如，大家都知道80是http端口，tcpdump就不会将它显示成http了 -c选项：是Count的含义，这设置了我们希望tcpdump帮我们抓几个包。 -i : 指定哪一张网卡 -l : 使得输出变为行缓冲 -t : 输出时不打印时间戳 -v : 输出更详细的信息 -F : 指定过滤表达式所在的文件, 可以建立了一个filter.txt文本文件来存储过滤表达式，然后通过-F来指定filter.txt -w : 将流量保存到文件中 -r : 读取raw packets文件 . . . 常用过滤规则 通过eth0网卡的，且来源是qq.com服务器或者目的机器是qq.com服务器的网络包sudo tcpdump -i eth0 &#39;host qq.com&#39; 只看到目的机器dst(比如是qq.com)的网络包sudo tcpdump -i eth0 &#39;dst qq.com&#39; 也可以写成 sudo tcpdump -i eth0 &#39;dst host qq.com&#39; 注 : 上述的那个host可以省略 tcpdump支持如下的类型： host：指定主机名或IP地址，例如’host roclinux.cn’或’host 202.112.18.34′ net ：指定网络段，例如’arp net 128.3’或’dst net 128.3′ portrange：指定端口区域，例如’src or dst portrange 6000-6008′ port : 端口如果我们没有设置过滤类型，那么默认是host。 只抓udp的包sudo tcpdump -i eth0 &#39;udp&#39; tcpdump具有根据网络包的协议来进行过滤的能力，我们还可以把udp改为ether、ip、ip6、arp、tcp、rarp等 只抓目的机器的某个端口的包(比如只抓baidu.com的53或者80端口的包)sudo tcpdump -i eth0 &#39;dst baidu.com and (dst port 53 or dst port 80)&#39; 通过eth0网卡的，且qq.com和baidu.com之间通讯的网络包，或者qq.com和sina.cn之间通讯的网络包tcpdump -i eth0 &#39;host qq.com and (baidu.com or sina.cn)&#39; 获取和baidu.com之间建立TCP三次握手中第一个网络包，即带有SYN标记位的网络包sudo tcpdump -i eth0 &#39;tcp[tcpflags] &amp; tcp-syn != 0 and dst host baidu.com&#39; 注 : 因为用proto [ expr : size]语法在写过滤表达式时，你需要把协议格式完全背在脑子里，才能把表达式写对。可这对大多数人来说，可能有些困难。为了让tcpdump工具更人性化一些，有一些常用的偏移量，可以通过一些名称来代替，比如icmptype表示ICMP协议的类型域、icmpcode表示ICMP的code域，tcpflags则表示TCP协议的标志字段域。 更进一步的，对于ICMP的类型域，可以用这些名称具体指代：icmp-echoreply, icmp-unreach, icmp-sourcequench, icmp-redirect, icmp-echo, icmp-routeradvert, icmp-routersolicit, icmp-timxceed, icmp-paramprob, icmp-tstamp, icmp-tstampreply, icmp-ireq, icmp-ireqreply, icmp-maskreq, icmp-maskreply。 而对于TCP协议的标志字段域，则可以细分为tcp-fin, tcp-syn, tcp-rst, tcp-push, tcp-ack, tcp-urg。 输出内容解释以下是在ubuntu上用火狐打开了一个百度时候抓到的 1234567891011121314151617b@b-VirtualBox:~$ sudo tcpdump -i eth0 &apos;host baidu.com&apos;tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes06:46:17.487920 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [S], seq 546310089, win 29200, options [mss 1460,sackOK,TS val 1221546 ecr 0,nop,wscale 7], length 006:46:17.530422 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [S.], seq 3245676077, ack 546310090, win 8192, options [mss 1440,sackOK,nop,nop,nop,nop,nop,nop,nop,nop,nop,nop,nop,wscale 5], length 006:46:17.530458 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 1, win 229, length 006:46:17.530982 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [P.], seq 1:504, ack 1, win 229, length 50306:46:17.576476 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [.], ack 504, win 216, length 006:46:17.577447 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [P.], seq 1:291, ack 504, win 216, length 29006:46:17.577459 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 291, win 237, length 006:46:17.577482 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [P.], seq 291:452, ack 504, win 216, length 16106:46:17.577485 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 452, win 245, length 006:46:17.866950 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [P.], seq 291:452, ack 504, win 216, length 16106:46:17.866966 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 452, win 245, options [nop,nop,sack 1 &#123;291:452&#125;], length 006:46:27.865805 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 452, win 245, length 006:46:27.909962 IP 111.13.101.208.http &gt; 192.168.1.57.60110: Flags [.], ack 504, win 216, length 006:46:37.925624 IP 192.168.1.57.60110 &gt; 111.13.101.208.http: Flags [.], ack 452, win 245, length 0 可以参考tlpi的解释, 如下图 :]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>TcpDump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用文本处理命令笔记整理之sed]]></title>
    <url>%2F2015%2F10%2F23%2Flinux%E5%B8%B8%E7%94%A8%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%E4%B9%8Bsed%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[sed是一种流编辑器，它是文本处理中非常中的工具，能够完美的配合正则表达式使用，功能不同凡响。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。 . . . sed命令 a\ 在当前行下面插入文本。 i\ 在当前行上面插入文本。 c\ 把选定的行改为新的文本。 d 删除，删除选择的行。 D 删除模板块的第一行。 s 替换指定字符 h 拷贝模板块的内容到内存中的缓冲区。 H 追加模板块的内容到内存中的缓冲区。 g 获得内存缓冲区的内容，并替代当前模板块中的文本。 G 获得内存缓冲区的内容，并追加到当前模板块文本的后面。 l 列表不能打印字符的清单。 n 读取下一个输入行，用下一个命令处理新的行而不是用第一个命令。 N 追加下一个输入行到模板块后面并在二者间嵌入一个新行，改变当前行号码。 p 打印模板块的行。 P(大写) 打印模板块的第一行。 q 退出Sed。 b lable 分支到脚本中带有标记的地方，如果分支不存在则分支到脚本的末尾。 r file 从file中读行。 t label if分支，从最后一行开始，条件一旦满足或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。 T label 错误分支，从最后一行开始，一旦发生错误或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。 w file 写并追加模板块到file末尾。 W file 写并追加模板块的第一行到file末尾。 ! 表示后面的命令对所有没有被选定的行发生作用。 = 打印当前行号码。 # 把注释扩展到下一个换行符以前。 sed替换标记 g 表示行内全面替换。 p 表示打印行。 w 表示把行写入一个文件。 x 表示互换模板块中的文本和缓冲区中的文本。 y 表示把一个字符翻译为另外的字符（但是不用于正则表达式） \1 子串匹配标记 &amp; 已匹配字符串标记 sed元字符集 ^ 匹配行开始，如：/^sed/匹配所有以sed开头的行。 $ 匹配行结束，如：/sed$/匹配所有以sed结尾的行。 . 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。 * 匹配0个或多个字符，如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。 [] 匹配一个指定范围内的字符，如/[ss]ed/匹配sed和Sed。 [^] 匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。 (..) 匹配子串，保存匹配的字符，如s/(love)able/\1rs，loveable被替换成lovers。 &amp; 保存搜索字符用来替换其他字符，如s/love/**&amp;**/，love改成**love**。 \&lt; 匹配单词的开始，如:/\ 匹配单词的结束，如/love>/匹配包含以love结尾的单词的行。 x{m} 重复字符x，m次，如：/0{5}/匹配包含5个0的行。 x{m,} 重复字符x，至少m次，如：/0{5,}/匹配至少有5个0的行。 x{m,n} 重复字符x，至少m次，不多于n次，如：/0{5,10}/匹配5~10个0的行。 直接编辑文件选项-i，否则并不会修改源文件 sed常用用法1：增记忆技巧 : 增为a\, \这个符号是用来分隔a和具体要增加的字符串的, a代表的意思是在下一行插入,而i\是在上一行插入, 如果你用过vim的话, 应该很好记忆. 123456b@b-VirtualBox:~/my_temp_test/abc$ cat abc3&amp;&amp; gg&amp;b@b-VirtualBox:~/my_temp_test/abc$ sed -i &apos;/gg/a\hello, my friend&apos; abc3b@b-VirtualBox:~/my_temp_test/abc$ cat abc3&amp;&amp; gg&amp;hello, my friend sed -i ‘/gg/a\hello, my friend’ abc3的含义是：在abc3文件中的“gg”字符串的下一行插入“hello， my friend” sed常用用法2：删123456b@b-VirtualBox:~/my_temp_test/abc$ cat abc3&amp;&amp; gg&amp;hello, my friendb@b-VirtualBox:~/my_temp_test/abc$ sed -i &apos;/gg/d&apos; abc3b@b-VirtualBox:~/my_temp_test/abc$ cat abc3hello, my friend sed -i ‘/gg/d’ abc3的含义是：将abc3文件中所有包含的“gg”字符串的行删除 sed常用用法3：改12345b@b-VirtualBox:~/my_temp_test/abc$ cat abc3hello, my friendb@b-VirtualBox:~/my_temp_test/abc$ sed -i &apos;s/hello/welcome/g&apos; abc3b@b-VirtualBox:~/my_temp_test/abc$ cat abc3welcome, my friend sed -i ‘s/hello/welcome/g’ abc3的含义是：将abc3文件中所有包含的“hello”字符串都修改为“welcome”]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用文本处理命令笔记整理之grep和awk]]></title>
    <url>%2F2015%2F10%2F21%2Flinux%E5%B8%B8%E7%94%A8%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E5%91%BD%E4%BB%A4%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%E4%B9%8Bgrep%E5%92%8Cawk%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[linux常用文本处理的命令的使用率很高， 所以整理了一些之前的笔记，用markdown来记录备忘。首先抛出问题， 带着问题来学记忆知识更有动力： 如何通过一条命令取得eth0的IP4地址 ： 1ifconfig eth0 | grep -w &apos;inet&apos; | awk &apos;&#123;print $2&#125;&apos; | awk -F: &apos;&#123;print $2&#125;&apos; 如何通过一条命令替换当前路径下所有文件中的所有“xxx”为“yyy“ ： 1ls -alF | grep &apos;^-&apos; | awk &apos;&#123;print $NF&#125;&apos; | xargs sed -i &apos;s/xxx/yyy/g&apos; 如何通过一条命令杀掉占用端口34600的进程 ： 1sudo lsof -i:34600 | grep -v &apos;PID&apos; | awk &apos;&#123;print $2&#125;&apos; | xargs kill -9 这些命令它们分别具体是什么意思呢?为何能达到上述效果? . . . grep grep（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 -a 不要忽略二进制数据。 -A&lt;显示列数&gt; 除了显示符合范本样式的那一行之外，并显示该行之后的内容。 -b 在显示符合范本样式的那一行之外，并显示该行之前的内容。 -c 计算符合范本样式的列数。 -C&lt;显示列数&gt;或-&lt;显示列数&gt; 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。 -d&lt;进行动作&gt; 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。 -e&lt;范本样式&gt; 指定字符串作为查找文件内容的范本样式。 -E 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。 -f&lt;范本文件&gt; 指定范本文件，其内容有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每一列的范本样式。 -F 将范本样式视为固定字符串的列表。 -G 将范本样式视为普通的表示法来使用。 -h 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。 -H 在显示符合范本样式的那一列之前，标示该列的文件名称。 -i 忽略字符大小写的差别。 -l 列出文件内容符合指定的范本样式的文件名称。 -L 列出文件内容不符合指定的范本样式的文件名称。 -n 在显示符合范本样式的那一列之前，标示出该列的编号。 -q 不显示任何信息。 -R/-r 此参数的效果和指定“-d recurse”参数相同。 -s 不显示错误信息。 -v 反转查找。 -w 只显示全字符合的列。 -x 只显示全列符合的列。 -y 此参数效果跟“-i”相同。 -o 只输出文件中匹配到的部分。 awk awk是一种编程语言，用于在linux/unix下对文本和数据进行处理。数据可以来自标准输入(stdin)、一个或多个文件，或其它命令的输出。它支持用户自定义函数和动态正则表达式等先进功能，是linux/unix下的一个强大编程工具。它在命令行中使用，但更多是作为脚本来使用。awk有很多内建的功能，比如数组、函数等，这是它和C语言的相同之处，灵活性是awk最大的优势。 常用命令选项 -F fs fs指定输入分隔符（awk默认的分隔符是空格），fs可以是字符串或正则表达式，如-F: -v var=value 赋值一个用户定义变量，将外部变量传递给awk 常用用法123456b@b-VirtualBox:~/my_temp_test/abc$ cat abc3klj;k uu&amp;&amp; ss&amp;b@b-VirtualBox:~/my_temp_test/abc$ cat abc3 | awk &apos;&#123;print $NF&#125;&apos;uuss&amp; cat abc3 | awk ‘{print $NF}’的含义是：输出abc3文件的每一行的最后一列12b@b-VirtualBox:~/my_temp_test/abc$ cat abc3 | grep k | awk -F\; &apos;&#123;print $1&#125;&apos;klj cat abc3 | grep k | awk -F\; ‘{print $1}’的含义是：先输入含有k的那一行（即klj；k）， 然后对那一行以；（\;， 这个分号需要转义）分隔，打印出分隔后的第一列（即klj）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unity中C#调用C++写的DLL之Swig篇]]></title>
    <url>%2F2015%2F09%2F13%2Funity_cpp_swig_csharp%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[近来要用Unity打包到安卓上玩, Unity那边需要用到服务器中用C++写的库,对比了 P/Invoke 和 C++/CLI 两种方式, 都不够省心省力, 决定使用 Swig来撸. 教程基本上按照这篇文章就可以, 文章写得非常详尽, 但文中关于设置 swiglib.i 自定义生成工具的命令行的时候, 他文中的下面一段要注意 : 在常规中选择命令行并且写入： echo on $(SolutionDir)/../../thirdpart/swigwin-3.0.12/swig.exe -c++ -csharp -outdir “$(SolutionDir)/../../../UnityProj/UnityCppLearn/Assets/SwigTools/Interface” “%(FullPath)” echo off 应改成 : 我们在自己填的时候要记得改成自己项目中的路径, 以及把上面这段命令中的中文引号改成英文引号.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Unity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装AndroidSDK的一些坑与注意点]]></title>
    <url>%2F2015%2F09%2F13%2Finstall_android_sdk_jdk%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[近来要用Unity打包到安卓上玩, 需要安装AndroidSDK. 安装教程基本上按照这篇文章A就可以, 遇到不明白的可以拿这篇B对照着看, 以A为准, 但是有几个点要注意 : jdk别装太高版本, 装个jdk-8u161的32位的即可, 别装64位, 也别装高版本的jdk10的64位, 不然 android sdk set up tool 不认识, sdk manager 也会闪退 jdk的环境变量很容易设置错, 比如环境变量JAVA_HOME应该填jdk的安装路径即 : JAVA_HOME=C:/Program Files/Java/jdk1.8.0_11而不是JAVA_HOME=C:/Program Files/Java, 填后者的话, sdk manager 会闪退 为了确保不必要的麻烦最好这样环境变量设置成类似如下 : 123JAVA_HOME=C:/Program Files/Java/jdk1.8.0_11JRE_HOME=C:/Program Files/Java/jre8Path=%JAVA_HOME%;C:... 打安卓包的时候, 如果报file not found debug.keystore 或 Unable to get debug signature key的错, 用管理员权限重新打开Unity即可.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Unity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hash索引btree索引聚簇索引非聚簇索引]]></title>
    <url>%2F2015%2F09%2F12%2Fhash%E7%B4%A2%E5%BC%95btree%E7%B4%A2%E5%BC%95%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%E9%9D%9E%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[索引概要索引是帮助mysql获取数据的数据结构。最常见的索引是 Btree索引 Hash索引 不同的引擎对于索引有不同的支持： Innodb和MyISAM默认的索引是Btree索引； Mermory默认的索引是Hash索引。 Hash索引Mermory默认的索引是Hash索引。 所谓Hash索引，当我们要给某张表某列增加索引时，将这张表的这一列进行哈希算法计算，得到哈希值， 排序在哈希数组上。所以Hash索引可以一次定位，其效率很高，而Btree索引需要经过多次的磁盘IO，但是innodb和myisam之所以没有采用它，是因为它存在着好多缺点. Hash索引的缺点 Hash 索引仅仅能满足”=”,”IN”和”&lt;=&gt;”查询，不能使用范围查询。由于 Hash 索引比较的是进行 Hash 运算之后的 Hash 值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的 Hash 算法处理之后的 Hash 值的大小关系，并不能保证和Hash运算前完全一样。 Hash 索引无法被用来避免数据的排序操作。由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash 值，而且Hash值的大小关系并不一定和 Hash 运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算； Hash 索引不能利用部分索引键查询。对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash 值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用。 Hash 索引在任何时候都不能避免表扫描。前面已经知道，Hash 索引是将索引键通过 Hash 运算之后，将 Hash运算结果的 Hash 值和所对应的行指针信息存放于一个 Hash 表中，由于不同索引键存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash 索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。 Hash 索引遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高。对于选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个 Hash 值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下 Btree索引Innodb和MyISAM默认的索引是Btree索引；至于Btree索引，它是以B+树为存储结构实现的。但是Btree索引的存储结构在Innodb和MyISAM中有很大区别。 btree索引在MyISAM中的实现在MyISAM中，我们如果要对某张表的某列建立Btree索引的话，如图： 所以我们经常会说MyISAM中数据文件和索引文件是分开的。因此MyISAM的索引方式也称为非聚集,至于辅助索引，类似于主索引，唯一区别就是主索引上的值不能重复，而辅助索引可以重复。 因此当我们根据Btree索引去搜索的时候，若key存在，在data域找到其地址，然后根据地址去表中查找数据记录。 btree索引在Innodb中的实现至于Innodb它跟上面又有很大不同，它的叶子节点存储的并不是表的地址，而是数据 我们可以看到这里并没有将地址放入叶子节点，而是直接放入了对应的数据， 这也就是我们平常说到的，Innodb的索引文件就是数据文件， 那么对于Innodb的辅助索引结构跟主索引也相差很多，如图： 我们可以发现，这里叶子节点存储的是主键的信息， 所以我们在利用辅助索引的时候，检索到主键信息， 然后再通过主键去主索引中定位表中的数据，这就可以说明Innodb中主键之所以不宜用过长的字段，由于所有的辅助索引都包含主索引， 所以很容易让辅助索引变得庞大。 我们还可以发现：在Innodb中尽量使用自增的主键， 这样每次增加数据时只需要在后面添加即可， 非单调的主键在插入时会需要维持B+tree特性而进行分裂调整，十分低效。 Btree索引中的最左匹配原则：Btree是按照从左到右的顺序来建立搜索树的。 比如索引是(name,age,sex)， 会先检查name字段，如果name字段相同再去检查后两个字段。 所以当传进来的是后两个字段的数据（age，sex）， 因为建立搜索树的时候是按照第一个字段建立的，所以必须根据name字段才能知道下一个字段去哪里查询。 所以传进来的是（name，sex）时，首先会根据name指定搜索方向，但是第二个字段缺失，所以将name字段正确的都找到后，然后才会去匹配sex的数据。 建立索引的规则： 利用最左前缀：Mysql会一直向右查找直到遇到范围操作（&gt;，&lt;，like、between）就停止匹配。比如a=1 and b=2 and c&gt;3 and d=6；此时如果建立了（a, b, c, d）索引，那么后面的d索引是完全没有用到，当换成了（a, b, d, c）就可以用到。 不能过度索引：在修改表内容的时候，索引必须更新或者重构，所以索引过多时，会消耗更多的时间。 尽量扩展索引而不要新建索引 最适合的索引的列是出现在where子句中的列或连接子句中指定的列。 不同值较少的列不必要建立索引（性别）。 练习题 数据索引的正确是(正确答案A, D) A、一个表只能有一个聚族索引，多个非聚族索引 B、字符串模糊查询不适合索引 C、哈希 索引有利于查询字段用于大小范围的比较查询 D、多余的索引字段会降低性能 Select A,B from Table1 where A between 60 and 100 order by B，下面哪些优化sql性能(正确答案B) A、字段A 建立hash索引，字段 B不建立索引 B、字段 A 建立btree索引，字段 B不建立索引 C、字段A 不建立 索引，字段 B建立btree索引]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GDB多进程多线程调试实战]]></title>
    <url>%2F2015%2F08%2F31%2Fgdb_multi_thread_and_multi_process%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[GDB多进程/多线程调试实战例子在gcc编译的时候，记得附加 -lpthread参数, 否则会出现 undefined reference to ‘pthread_create’ 的错误.(因为在链接的时候，无法找到phread库中哥函数的入口地址，于是链接会失败。) 例程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &lt;stdio.h&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt;void processA();void processB();void *processAworker(void *arg);int main(int argc, const char *argv[])&#123; int pid; pid = fork(); if (pid != 0) processA(); else processB(); return 0;&#125;void processA()&#123; pid_t pid = getpid(); char prefix[] = "ProcessA: "; char tprefix[] = "thread "; int tstatus; pthread_t pt; printf("%s%lu %s\n", prefix, pid, "step1"); tstatus = pthread_create(&amp;pt, NULL, processAworker, NULL); if (tstatus != 0) &#123; printf("ProcessA: Can not create new thread."); &#125; processAworker(NULL); sleep(1);&#125;void *processAworker(void *arg)&#123; pid_t pid = getpid(); pthread_t tid = pthread_self(); char prefix[] = "ProcessA: "; char tprefix[] = "thread "; printf("%s%lu %s%lu %s\n", prefix, pid, tprefix, tid, "step2"); printf("%s%lu %s%lu %s\n", prefix, pid, tprefix, tid, "step3"); return NULL;&#125;void processB()&#123; pid_t pid = getpid(); char prefix[] = "ProcessB: "; printf("%s%lu %s\n", prefix, pid, "step1"); printf("%s%lu %s\n", prefix, pid, "step2"); printf("%s%lu %s\n", prefix, pid, "step3");&#125; 输出[cnwuwil@centos c-lab]$ ./test ProcessA: 802 step1 ProcessB: 803 step1 ProcessB: 803 step2 ProcessB: 803 step3 ProcessA: 802 thread 3077555904 step2 ProcessA: 802 thread 3077555904 step3 ProcessA: 802 thread 3077553008 step2 ProcessA: 802 thread 3077553008 step3 GDB多进程调试命令 set follow-fork-mode [parent|child] set detach-on-fork [on|off] follow-fork-mode detach-on-fork 说明 parent on 只调试主进程（GDB默认） child on 只调试子进程 parent off 同时调试两个进程，GDB跟主进程，子进程block在fork位置 child off 同时调试两个进程，GDB跟子进程，主进程block在fork位置 查询正在调试的进程：info inferiors 切换调试的进程： inferior +inferior number catch fork命令可以捕获进程的创建 attach + pid ， 可以附到一个正在运行的进程上进行调试 . . . GDB多线程调试命令 show scheduler-locking //显示当前scheduler-locking set scheduler-locking [on/off/step] //设置scheduler-locking on：只有当前调试线程运行，其他线程处于暂停状态。 off：当前调试线程外的其他线程一直在正常运行。 step：其他线程跟随当前调试线程运行，但具体怎么协同运行，测试中无法体现。 注意：set scheduler-locking要处于线程运行环境下才能生效，也就是程序已经运行并且暂停在某个断点处，否则会出现“Target ‘exec’ cannot support this command.”这样的错误；而且经过测试，设置后的scheduler-locking值在整个进程内有效，不属于某个线程。 查询线程：info threads 切换调试线程：thread + thread_number 实战调试11.调试主进程，block子进程。 (gdb) set detach-on-fork off (gdb) show detach-on-fork Whether gdb will detach the child of a fork is off. (gdb) catch fork Catchpoint 1 (fork) (gdb) r [Thread debugging using libthread_db enabled] Catchpoint 1 (forked process 3475), 0x00110424 in __kernel_vsyscall () Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.47.el6.i686 (gdb) break test.c:14 Breakpoint 2 at 0x8048546: file test.c, line 14. (gdb) cont [New process 3475] [Thread debugging using libthread_db enabled] Breakpoint 2, main (argc=1, argv=0xbffff364) at test.c:14 Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.47.el6.i686 (gdb) info inferiors Num Description Executable 2 process 3475 /home/cnwuwil/labs/c-lab/test * 1 process 3472 /home/cnwuwil/labs/c-lab/test 2.切换到子进程： (gdb) inferior 2 [Switching to inferior 2 [process 3475] (/home/cnwuwil/labs/c-lab/test)] [Switching to thread 2 (Thread 0xb7fe86c0 (LWP 3475))] #0 0x00110424 in ?? () (gdb) info inferiors Num Description Executable * 2 process 3475 /home/cnwuwil/labs/c-lab/test 1 process 3472 /home/cnwuwil/labs/c-lab/test (gdb) inferior 1 [Switching to inferior 1 [process 3472] (/home/cnwuwil/labs/c-lab/test)] [Switching to thread 1 (Thread 0xb7fe86c0 (LWP 3472))] #0 main (argc=1, argv=0xbffff364) at test.c:14 (gdb) info inferiors Num Description Executable 2 process 3475 /home/cnwuwil/labs/c-lab/test * 1 process 3472 /home/cnwuwil/labs/c-lab/test 3.设断点继续调试主进程，主进程产生两个子线程： (gdb) break test.c:50 Breakpoint 3 at 0x804867d: file test.c, line 50. (2 locations) (gdb) cont ProcessA: 3472 step1 [New Thread 0xb7fe7b70 (LWP 3562)] ProcessA: 3472 thread 3086911168 step2 Breakpoint 3, processAworker (arg=0x0) at test.c:50 (gdb) info inferiors Num Description Executable 2 process 3475 /home/cnwuwil/labs/c-lab/test * 1 process 3472 /home/cnwuwil/labs/c-lab/test (gdb) info threads 3 Thread 0xb7fe7b70 (LWP 3562) 0x00110424 in __kernel_vsyscall () 2 Thread 0xb7fe86c0 (LWP 3475) 0x00110424 in ?? () * 1 Thread 0xb7fe86c0 (LWP 3472) processAworker (arg=0x0) at test.c:50 4.切换到主进程中的子线程，注意：线程2为前面产生的子进程 (gdb) thread 3 [Switching to thread 3 (Thread 0xb7fe7b70 (LWP 3562))]#0 0x00110424 in __kernel_vsyscall () (gdb) cont ProcessA: 3472 thread 3086911168 step3 ProcessA: 3472 thread 3086908272 step2 [Switching to Thread 0xb7fe7b70 (LWP 3562)] Breakpoint 3, processAworker (arg=0x0) at test.c:50 (gdb) info threads * 3 Thread 0xb7fe7b70 (LWP 3562) processAworker (arg=0x0) at test.c:50 2 Thread 0xb7fe86c0 (LWP 3475) 0x00110424 in ?? () 1 Thread 0xb7fe86c0 (LWP 3472) 0x00110424 in __kernel_vsyscall () (gdb) thread 1 实战调试2b@b-VirtualBox:~/Documents/temp_test$ sudo gdb ./o_multi_thread_process [sudo] password for b: GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1 Copyright (C) 2014 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot; and &quot;show warranty&quot; for details. This GDB was configured as &quot;x86_64-linux-gnu&quot;. Type &quot;show configuration&quot; for configuration details. For bug reporting instructions, please see: &lt;http://www.gnu.org/software/gdb/bugs/&gt;. Find the GDB manual and other documentation resources online at: &lt;http://www.gnu.org/software/gdb/documentation/&gt;. For help, type &quot;help&quot;. Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;... Reading symbols from ./o_multi_thread_process...done. (gdb) attach 3027 Attaching to program: /home/b/Documents/temp_test/o_multi_thread_process, process 3027 Reading symbols from /lib/x86_64-linux-gnu/libpthread.so.0...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libpthread-2.19.so...done. done. [New LWP 3029] [Thread debugging using libthread_db enabled] Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;. Loaded symbols for /lib/x86_64-linux-gnu/libpthread.so.0 Reading symbols from /lib/x86_64-linux-gnu/libc.so.6...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libc-2.19.so...done. done. Loaded symbols for /lib/x86_64-linux-gnu/libc.so.6 Reading symbols from /lib64/ld-linux-x86-64.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/ld-2.19.so...done. done. Loaded symbols for /lib64/ld-linux-x86-64.so.2 0x00007f5c9acb8dfd in nanosleep () at ../sysdeps/unix/syscall-template.S:81 81 ../sysdeps/unix/syscall-template.S: No such file or directory. (gdb) set follow-fork-mode parent (gdb) set detach-on-fork off (gdb) catch fork Catchpoint 1 (fork) (gdb) r Starting program: /home/b/Documents/temp_test/o_multi_thread_process [Thread debugging using libthread_db enabled] Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;. Catchpoint 1 (forked process 3002), 0x00007ffff78b7ee4 in __libc_fork () at ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c:130 130 ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c: No such file or directory. (gdb) info inferiors Num Description Executable * 1 process 2998 /home/b/Documents/temp_test/o_multi_thread_process (gdb) b 14 Breakpoint 2 at 0x7ffff78b7f5b: file ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c, line 14. (gdb) info breakpoints Num Type Disp Enb Address What 1 catchpoint keep y fork, process 3002 catchpoint already hit 1 time 2 breakpoint keep y 0x00007ffff78b7f5b in __libc_fork at ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c:14 (gdb) d 2 (gdb) info breakpoints Num Type Disp Enb Address What 1 catchpoint keep y fork, process 3002 catchpoint already hit 1 time (gdb) b multi_thread_process.cpp : 14 Breakpoint 3 at 0x4007f4: file ./multi_thread_process.cpp, line 14. (gdb) c Continuing. [New process 3002] [Thread debugging using libthread_db enabled] Using host libthread_db library &quot;/lib/x86_64-linux-gnu/libthread_db.so.1&quot;. Reading symbols from /usr/lib/debug/lib/x86_64-linux-gnu/libpthread-2.19.so...done. Reading symbols from /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.19.so...done. Reading symbols from /usr/lib/debug/lib/x86_64-linux-gnu/ld-2.19.so...done. Breakpoint 3, main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 15 if(pid != 0) (gdb) info inferiors Num Description Executable 2 process 3002 /home/b/Documents/temp_test/o_multi_thread_process * 1 process 2998 /home/b/Documents/temp_test/o_multi_thread_process (gdb) inferior 2 [Switching to inferior 2 [process 3002] (/home/b/Documents/temp_test/o_multi_thread_process)] [Switching to thread 2 (Thread 0x7ffff7fdf740 (LWP 3002))] 0 0x00007ffff78b7ee4 in __libc_fork () at ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c:130 130 ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c: No such file or directory. (gdb) set scheduler-locking on (gdb) b multi_thread_process.cpp : 50 Breakpoint 4 at 0x400916: multi_thread_process.cpp:50. (2 locations) (gdb) info threads Id Target Id Frame * 2 Thread 0x7ffff7fdf740 (LWP 3002) &quot;o_multi_thread_&quot; 0x00007ffff78b7ee4 in __libc_fork () at ../nptl/sysdeps/unix/sysv/linux/x86_64/../fork.c:130 1 Thread 0x7ffff7fdf740 (LWP 2998) &quot;o_multi_thread_&quot; main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 (gdb) c Continuing. Breakpoint 3, main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 15 if(pid != 0) (gdb) info threads Id Target Id Frame * 2 Thread 0x7ffff7fdf740 (LWP 3002) &quot;o_multi_thread_&quot; main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 1 Thread 0x7ffff7fdf740 (LWP 2998) &quot;o_multi_thread_&quot; main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 (gdb) c Continuing. ProcessB: 3002 step1 ProcessB: 3002 step2 ProcessB: 3002 step3 ^C Program received signal SIGINT, Interrupt. 0x00007ffff78b7de0 in __nanosleep_nocancel () at ../sysdeps/unix/syscall-template.S:81 81 ../sysdeps/unix/syscall-template.S: No such file or directory. (gdb) info threads Id Target Id Frame * 2 Thread 0x7ffff7fdf740 (LWP 3002) &quot;o_multi_thread_&quot; 0x00007ffff78b7de0 in __nanosleep_nocancel () at ../sysdeps/unix/syscall-template.S:81 1 Thread 0x7ffff7fdf740 (LWP 2998) &quot;o_multi_thread_&quot; main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 (gdb) info inferiors Num Description Executable * 2 process 3002 /home/b/Documents/temp_test/o_multi_thread_process 1 process 2998 /home/b/Documents/temp_test/o_multi_thread_process (gdb) inferior 1 [Switching to inferior 1 [process 2998] (/home/b/Documents/temp_test/o_multi_thread_process)] [Switching to thread 1 (Thread 0x7ffff7fdf740 (LWP 2998))] 0 main (argc=1, argv=0x7fffffffe598) at ./multi_thread_process.cpp:15 15 if(pid != 0) (gdb) list 10 { 11 int pid; 12 13 pid = fork(); 14 15 if(pid != 0) 16 processA(); 17 else 18 processB(); 19 (gdb) r The program being debugged has been started already. Start it from the beginning? (y or n) n Program not restarted. (gdb) c Continuing. ProcessA: 2998 step1 [New Thread 0x7ffff77f6700 (LWP 3017)] ^C Program received signal SIGINT, Interrupt. 0x00007ffff78b7dfd in nanosleep () at ../sysdeps/unix/syscall-template.S:81 81 ../sysdeps/unix/syscall-template.S: No such file or directory. (gdb) info threads Id Target Id Frame 3 Thread 0x7ffff77f6700 (LWP 3017) &quot;o_multi_thread_&quot; clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:81 2 Thread 0x7ffff7fdf740 (LWP 3002) &quot;o_multi_thread_&quot; 0x00007ffff78b7de0 in __nanosleep_nocancel () at ../sysdeps/unix/syscall-template.S:81 * 1 Thread 0x7ffff7fdf740 (LWP 2998) &quot;o_multi_thread_&quot; 0x00007ffff78b7dfd in nanosleep () at ../sysdeps/unix/syscall-template.S:81 (gdb) 参考参考]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>GDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Putty配置]]></title>
    <url>%2F2015%2F08%2F23%2Fputty_config%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[平时工作学习必须要使用Windows，在SSH远程连接软件里Putty算是用得比较顺手的，而且很小巧。但是每次输入密码很麻烦，还容易输错，OpenSSH可以利用密钥来自动登陆，如此一来方便了不少。配置过程分为三步：1、生成公钥和私钥先要下载一个叫puttygen的软件（下载见附件），在Windows端生成公钥和私钥。点击Generate开始生成在生成过程中用鼠标在进度条下面的空白处乱晃几下，产生随机性：生成完毕，将私钥保存起来：然后将公钥全选复制。2、远程主机配置我这里使用的是CentOS6.4，已经安装了OpenSSH，如果远程主机没有安装的，先要安装。先连接上远程主机，然后输入命令mkdir .sshchmod 700 .sshvim ~/.ssh/authorized_keys按“i”键进入编辑模式（用过vi/vim的都应该知道吧），然后点鼠标右键将刚才复制的公钥粘贴进去，然后按“Esc”，输入wq&lt;Enter&gt;保存。安全起见，设置验证文件为只读：chmod 400 ~/.ssh/authorized_keys3、Putty端配置先到Connection-Data项设置自己的登陆用户名，如图（我的是root）：再点SSH项下面的Auth，添加第一步保存的私钥然后很重要的是要回去Session项里保存！！！不然下次又得重新添加一遍然后再双击Default Settings里保存的任务，就直接登陆进去了：是不是很棒~最后再优化一下显示设置（转过来的）：字体大小设置Window-&gt;Appearance-&gt;Font settings—&gt;Change按钮设置（我的设置为12）字体颜色设置Window-&gt;Colours-&gt;Default Foreground-&gt;Modify设置（我喜欢绿色设置：R:0 G:255 B:64）此外在默认的黑色背景下 蓝色看不太清楚，可以把Window-&gt;Colours-&gt;ANSI Blue 更改一下设置（我设置为R：255 G：0 B：128）全屏/退出全屏的快捷键设置Window-&gt;Behaviour最下面有个Full screen on Alt-Enter 勾上就可以了。 putty配置导出的方法PuTTY 是一款小巧的开源 Telnet/SSH 客户端，但是它不提供设置的导入导出工具，PuTTY 将设置都保存在注册表中，所以要备份主要就是要备份注册表里的资料。 下面是备份步骤，实质上就是导出相应的注册表键值： 开始-&gt;运行(Win+R)-&gt;regedit 找到 HKEY_CURRENT_USER\Software\SimonTatham 在 SimonTatham 这个节点上点击右键，选择导出，保存。 如果你想恢复配置信息，只需要双击保存的这个文件，导入注册表信息即可。 说明：注册表PuTTY下的Sessions中保存设置连接的项目和设定值，SshHostKeys保存设置过的Remote Host Public Key。 KeepAlive很多远程主机当你一段时间没有输入, 他就会把你踢下线, 所以需要 KeepAlive 功能, 如果填写 0 , 就表示不需要 KeepAlive 功能,填写大于 0 的数, 比如 4, 就意味着每 4 秒就会发送一个空包到远程主机来 KeepAlive .所以建议填写8秒左右的数. SSH 证书登陆配置sudo vi /etc/ssh/sshd_config 取消注释 : #AuthorizedKeysFile .ssh/authorized_keys禁止密码登录 : 修改yes-&gt;no : PasswordAuthentication no 然后重启ssh : sudo service sshd restart Putty server refused our key的三种原因和解决方法server refused our key 是非常容易遇到的错误 1、.ssh文件夹权限错或authorized_keys权限错.ssh 以及其父文件夹（root为/root，普通用户为Home目录）都应该设置为只有该用户可写（比如700）。且 设置 authorized_keys 的权限为 400 chmod 700 .ssh chmod 400 ~/.ssh/authorized_keys 以下为原因：ssh服务器的key方式登录对权限要求严格。 对于客户端: 私钥必须为600权限或者更严格权限(400), 一旦其他用户可读, 私钥就不起作用(如640), 表现为系统认为不存在私钥 对于服务器端: 要求必须公钥其他用户不可写, 一旦其他用户可写(如660), 就无法用key登录, 表现为:Permission denied (publickey). 同时要求.ssh目录其他用户不可写,一旦其他用户可写(如770), 就无法使用key登录, 表现为:Permission denied (publickey). 2、SElinux导致密钥文件不能通过SElinux认证，解决方法如下： # restorecon -R -v /home #root用户为/root 我遇到的就是这种情况，找了好久还找到是这个原因，因为是新装的虚拟机，SElinux还没关闭。这篇博文详细得说明了原因：http://www.toxingwang.com/linux-unix/linux-basic/846.html 3、sshd配置不正确正确配置方法如下：/etc/ssh/sshd_config 1、找到 #StrictModes yes 改成 StrictModes no （去掉注释后改成 no） 2、找到 #PubkeyAuthentication yes 改成 PubkeyAuthentication yes （去掉注释） 3、找到 #AuthorizedKeysFile .ssh/authorized_keys 改成 AuthorizedKeysFile .ssh/authorized_keys （去掉注释） 4、保存 5、/etc/rc.d/init.d/sshd reload 重新加载]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>VBox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 常用SIG信号及其键值]]></title>
    <url>%2F2015%2F08%2F04%2FLinux%20%E5%B8%B8%E7%94%A8SIG%E4%BF%A1%E5%8F%B7%E5%8F%8A%E5%85%B6%E9%94%AE%E5%80%BC%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[01 SIGHUP 挂起（hangup） 02 SIGINT 中断，当用户从键盘按^c键或^break键时 03 SIGQUIT 退出，当用户从键盘按quit键时 04 SIGILL 非法指令 05 SIGTRAP 跟踪陷阱（trace trap），启动进程，跟踪代码的执行 06 SIGIOT IOT指令 07 SIGEMT EMT指令 08 SIGFPE 浮点运算溢出 09 SIGKILL 杀死、终止进程 10 SIGBUS 总线错误 11 SIGSEGV 段违例（segmentation violation），进程试图去访问其虚地址空间以外的位置 12 SIGSYS 系统调用中参数错，如系统调用号非法 13 SIGPIPE 向某个非读管道中写入数据 14 SIGALRM 闹钟。当某进程希望在某时间后接收信号时发此信号 15 SIGTERM 软件终止（software termination） 16 SIGUSR1 用户自定义信号1 17 SIGUSR2 用户自定义信号2 18 SIGCLD 某个子进程死 19 SIGPWR 电源故障]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++链接性之extern和static和const的用法]]></title>
    <url>%2F2015%2F07%2F19%2Fcplusplus_how_to_use_extern_static_const%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[基本解释基本解释：extern可以置于变量或者函数前，以标示变量或者函数的定义在别的文件中，提示编译器遇到此变量和函数时在其他模块中寻找其定义。此外extern也可用来进行链接指定。 也就是说extern有两个作用: 当它与”C”一起连用时，如: extern “C” void fun(int a, int b);则告诉编译器在编译fun这个函数名时按着C的规则去翻译相应的函数名而不是C++的，C++的规则在翻译这个函数名时会把fun这个名字变得面目全非，可能是fun@aBc_int_int#%$也可能是别的，这要看编译器的”脾气”了(不同的编译器采用的方法不一样)，为什么这么做呢，因为C++支持函数的重载啊，在这里不去过多的论述这个问题，如果你有兴趣可以去网上搜索，相信你可以得到满意的解释! 当extern不与”C”在一起修饰变量或函数时，如在头文件中: extern int g_Int; 它的作用就是声明函数或全局变量的作用范围的关键字，其声明的函数和变量可以在本模块或其他模块中使用，记住它是一个声明不是定义!也就是说B模块(编译单元)要是引用模块(编译单元)A中定义的全局变量或函数时，它只要包含A模块的头文件即可,在编译阶段，模块B虽然找不到该函数或变量，但它不会报错，它会在链接时从模块A生成的目标代码中找到此函数。 extern 变量在一个源文件里定义了一个数组：char a[6];在另外一个文件里用下列语句进行了声明：extern char *a；请问，这样可以吗？答案与分析： 不可以，程序运行时会告诉你非法访问。原因在于，指向类型T的指针并不等价于类型T的数组。extern char *a声明的是一个指针变量而不是字符数组，因此与实际的定义不同，从而造成运行时非法访问。应该将声明改为extern char a[ ]。 例子分析如下，如果a[] = “abcd”,则外部变量a=0x61626364 (abcd的ASCII码值)，*a显然没有意义显然a指向的空间（0x61626364）没有意义，易出现非法内存访问。 这提示我们，在使用extern时候要严格对应声明时的格式，在实际编程中，这样的错误屡见不鲜。 extern用在变量声明中常常有这样一个作用，你在.c文件中声明了一个全局的变量，这个全局的变量如果要被引用，就放在.h中并用extern来声明。 extern “c”在C++环境下使用C函数的时候，常常会出现编译器无法找到obj模块中的C函数定义，从而导致链接失败的情况，应该如何解决这种情况呢答案与分析： C++语言在编译的时候为了解决函数的多态问题，会将函数名和参数联合起来生成一个中间的函数名称，而C语言则不会，因此会造成链接时找不到对应函数的情况，此时C函数就需要用extern “C”进行链接指定，这告诉编译器，请保持我的名称，不要给我生成用于链接的中间函数名。下面是一个标准的写法： 1234567891011121314//在.h文件的头上#ifdef __cplusplus#if __cplusplusextern "C"&#123; #endif #endif /* __cplusplus */ … … //.h文件结束的地方 #ifdef __cplusplus #if __cplusplus&#125;#endif#endif /* __cplusplus */ 定义放在头文件还是源文件中?常常见extern放在函数的前面成为函数声明的一部分，那么，C语言的关键字extern在函数的声明中起什么作用？答案与分析： 如果函数的声明中带有关键字extern，仅仅是暗示这个函数可能在别的源文件里定义，没有其它作用。即下述两个函数声明没有明显的区别：extern int f(); 和int f();当然，这样的用处还是有的，就是在程序中取代include “*.h”来声明函数，在一些复杂的项目中，我比较习惯在所有的函数声明前添加extern修饰。关于这样做的原因和利弊可见下面的这个例子：“用extern修饰的全局变量” (1)在test1.h中有下列声明 : 12345#ifndef TEST1H#define TEST1Hextern char g_str[]; // 声明全局变量g_strvoid fun1();#endif (2)在test1.cpp中 : 123#include "test1.h"char g_str[] = "123456"; // 定义全局变量g_strvoid fun1() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; (3)以上是test1模块， 它的编译和链接都可以通过,如果我们还有test2模块也想使用g_str,只需要在原文件中引用就可以了 : 12#include "test1.h"void fun2() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; 以上test1和test2可以同时编译链接通过，如果你感兴趣的话可以用ultraEdit打开test1.obj,你可以在里面找到”123456”这个字符串,但是你却不能在test2.obj里面找到，这是因为g_str是整个工程的全局变量，在内存中只存在一份,test2.obj这个编译单元不需要再有一份了，不然会在链接时报告重复定义这个错误! (4) 有些人喜欢把全局变量的声明和定义放在一起，这样可以防止忘记了定义，如把上面test1.h改为 extern char g_str[] = &quot;123456&quot;; // 这个时候相当于没有extern 然后把test1.cpp中的g_str的定义去掉,这个时候再编译链接test1和test2两个模块时，会报链接错误，这是因为你把全局变量g_str的定义放在了头文件之后， test1.cpp这个模块包含了test1.h所以定义了一次g_str,而test2.cpp也包含了test1.h所以再一次定义了g_str,这个时候链接器在链接test1和test2时发现两个g_str。如果你非要把g_str的定义放在test1.h中的话，那么就把test2的代码中#include “test1.h”去掉 换成: 12extern char g_str[];void fun2() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; 这个时候编译器就知道g_str是引自于外部的一个编译模块了，不会在本模块中再重复定义一个出来，但是我想说这样做非常糟糕，因为你由于无法在test2.cpp中使用#include “test1.h”,那么test1.h中声明的其他函数你也无法使用了，除非也用都用extern修饰，这样的话你光声明的函数就要一大串，而且头文件的作用就是要给外部提供接口使用的，所以 请记住， 只在头文件中做声明，真理总是这么简单。 extern 和 static extern 表明该变量在别的地方已经定义过了,在这里要使用那个变量. static 表示静态的变量，分配内存的时候, 存储在静态区,不存储在栈上面. static 作用范围是内部链接的关系, 和extern有点相反.它和对象本身是分开存储的,extern也是分开存储的,但是extern可以被其他的对象用extern 引用,而static 不可以,只允许对象本身用它. 具体差别首先，static与extern是一对“水火不容”的家伙，也就是说extern和static不能同时修饰一个变量；其次，static修饰的全局变量声明与定义同时进行，也就是说当你在头文件中使用static声明了全局变量后，它也同时被定义了；最后，static修饰全局变量的作用域只能是本身的编译单元，也就是说它的“全局”只对本编译单元有效，其他编译单元则看不到它,如: (1)test1.h : 12345#ifndef TEST1H#define TEST1Hstatic char g_str[] = "123456"; void fun1();#endif (2)test1.cpp : 12#include "test1.h"void fun1() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; (3)test2.cpp : 12#include "test1.h"void fun2() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; 以上两个编译单元可以链接成功, 当你打开test1.obj时，你可以在它里面找到字符串”123456”,同时你也可以在test2.obj中找到它们，它们之所以可以链接成功而没有报重复定义的错误是因为虽然它们有相同的内容，但是存储的物理地址并不一样，就像是两个不同变量赋了相同的值一样，而这两个变量分别作用于它们各自的编译单元。 也许你比较较真，自己偷偷的跟踪调试上面的代码,结果你发现两个编译单元（test1,test2）的g_str的内存地址相同，于是你下结论static修饰的变量也可以作用于其他模块，但是我要告诉你，那是你的编译器在欺骗你，大多数编译器都对代码都有优化功能，以达到生成的目标程序更节省内存，执行效率更高，当编译器在链接各个编译单元的时候，它会把相同内容的内存只拷贝一份，比如上面的”123456”, 位于两个编译单元中的变量都是同样的内容，那么在链接的时候它在内存中就只会存在一份了，如果你把上面的代码改成下面的样子，你马上就可以拆穿编译器的谎言: (1)test1.cpp: 123456#include "test1.h"void fun1()&#123; g_str[0] = 'a'; cout &lt;&lt; g_str &lt;&lt; endl;&#125; (2)test2.cpp : 12#include "test1.h"void fun2() &#123; cout &lt;&lt; g_str &lt;&lt; endl; &#125; (3) : 1234void main() &#123; fun1(); // a23456 fun2(); // 123456&#125; 这个时候你在跟踪代码时，就会发现两个编译单元中的g_str地址并不相同，因为你在一处修改了它，所以编译器被强行的恢复内存的原貌，在内存中存在了两份拷贝给两个模块中的变量使用。正是因为static有以上的特性，所以一般定义static全局变量时，都把它放在原文件中而不是头文件，这样就不会给其他模块造成不必要的信息污染，同样记住这个原则吧！ extern 和constC++中const修饰的全局常量据有跟static相同的特性，即它们只能作用于本编译模块中，但是const可以与extern连用来声明该常量可以作用于其他编译模块中, 如extern const char g_str[];然后在原文件中别忘了定义: const char g_str[] = “123456”; 所以当const单独使用时它就与static相同，而当与extern一起合作的时候，它的特性就跟extern的一样了！所以对const我没有什么可以过多的描述，我只是想提醒你，const char* g_str = &quot;123456&quot; 与 const char g_str[] =&quot;123465&quot;是不同的， 前面那个const 修饰的是 char * 而不是g_str,它的g_str并不是常量，它被看做是一个定义了的全局变量（可以被其他编译单元使用）， 所以如果你像让char*g_str遵守const的全局常量的规则，最好这么定义const char* const g_str=&quot;123456&quot;.]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP的超级全局变量小结]]></title>
    <url>%2F2015%2F07%2F13%2FPHP%E7%9A%84%E8%B6%85%E7%BA%A7%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[PHP 超级全局变量概绍 PHP中预定义了几个超级全局变量（superglobals） ， 这意味着它们在一个脚本的全部作用域中都可用。 你不需要特别说明，就可以在函数及类中使用。 PHP 超级全局变量列表: $GLOBALS $_SERVER $_REQUEST $_POST $_GET $_FILES $_ENV $_COOKIE $_SESSION $GLOBALS$GLOBALS 是PHP的一个超级全局变量组， 在一个PHP脚本的全部作用域中都可以访问。 $GLOBALS 是一个包含了全部变量的全局组合数组。变量的名字就是数组的键。 以下实例介绍了如何使用超级全局变量 $GLOBALS: 123456789101112&lt;?php $x = 75; $y = 25; function addition() &#123; $GLOBALS[&apos;z&apos;] = $GLOBALS[&apos;x&apos;] + $GLOBALS[&apos;y&apos;]; &#125; addition(); echo $z; ?&gt; 以上实例中 z 是一个$GLOBALS数组中的超级全局变量，该变量同样可以在函数外访问。 $_SERVER$_SERVER 是一个包含了诸如头信息(header)、路径(path)、以及脚本位置(script locations)等等信息的数组。 这个数组中的项目由 Web 服务器创建。 不能保证每个服务器都提供全部项目；服务器可能会忽略一些，或 者提供一些没有在这里列举出来的项目。 以下实例中展示了如何使用$_SERVER中的元素: 12345678910111213&lt;?php echo $_SERVER[&apos;PHP_SELF&apos;];echo &quot;&lt;br&gt;&quot;;echo $_SERVER[&apos;SERVER_NAME&apos;];echo &quot;&lt;br&gt;&quot;;echo $_SERVER[&apos;HTTP_HOST&apos;];echo &quot;&lt;br&gt;&quot;;echo $_SERVER[&apos;HTTP_REFERER&apos;];echo &quot;&lt;br&gt;&quot;;echo $_SERVER[&apos;HTTP_USER_AGENT&apos;];echo &quot;&lt;br&gt;&quot;;echo $_SERVER[&apos;SCRIPT_NAME&apos;];?&gt; $_REQUESTPHP $_REQUEST 用于收集HTML表单提交的数据。 以下实例显示了一个输入字段（input）及提交按钮(submit)的表单(form)。 当用户通过点击 “Submit” 按钮提交表单数据时, 表单数据将发送至标签中 action 属性中指定的脚本文件。 在这个实例中，我们指定文件来处理表单数据。 如果你希望其他的PHP文件来处理该数据，你可以修改该指定的脚本文件名。 然后，我们可以使用超级全局变量 $_REQUEST 来收集表单中的 input 字段数据: 123456789101112131415 &lt;html&gt;&lt;body&gt;&lt;form method=&quot;post&quot; action=&quot;&lt;?php echo $_SERVER[&apos;PHP_SELF&apos;];?&gt;&quot;&gt;Name: &lt;input type=&quot;text&quot; name=&quot;fname&quot;&gt;&lt;input type=&quot;submit&quot;&gt;&lt;/form&gt;&lt;?php $name = $_REQUEST[&apos;fname&apos;]; echo $name; ?&gt;&lt;/body&gt;&lt;/html&gt; $_POSTPHP $_POST 被广泛应用于收集表单数据， 在HTML form标签的指定该属性：”method=”post”。 以下实例显示了一个输入字段（input）及提交按钮(submit)的表单(form)。 当用户通过点击 “Submit” 按钮提交表单数据时, 表单数据将发送至标签中 action 属性中指定的脚本文件。 在这个实例中，我们指定文件来处理表单数据。 如果你希望其他的PHP文件来处理该数据，你可以修改该指定的脚本文件名。 然后，我们可以使用超级全局变量 $_POST 来收集表单中的 input 字段数据: 123456789101112131415&lt;html&gt;&lt;body&gt;&lt;form method=&quot;post&quot; action=&quot;&lt;?php echo $_SERVER[&apos;PHP_SELF&apos;];?&gt;&quot;&gt;Name: &lt;input type=&quot;text&quot; name=&quot;fname&quot;&gt;&lt;input type=&quot;submit&quot;&gt;&lt;/form&gt;&lt;?php $name = $_POST[&apos;fname&apos;]; echo $name; ?&gt;&lt;/body&gt;&lt;/html&gt; $_GETPHP $_GET 同样被广泛应用于收集表单数据， 在HTML form标签的指定该属性：”method=”get”。 $_GET 也可以收集URL中发送的数据。 假定我们有一个包含参数的超链接HTML页面：1234567&lt;html&gt;&lt;body&gt;&lt;a href=&quot;test_get.php?subject=PHP&amp;web=runoob.com&quot;&gt;Test $GET&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 当用户点击链接 “Test $GET”, 参数 “subject” 和 “web” 将发送至”test_get.php”, 你可以在 “test_get.php” 文件中使用 $_GET 变量来获取这些数据。 以下实例显示了 “test_get.php” 文件的代码:123456789&lt;html&gt;&lt;body&gt;&lt;?php echo &quot;Study &quot; . $_GET[&apos;subject&apos;] . &quot; at &quot; . $_GET[&apos;web&apos;];?&gt;&lt;/body&gt;&lt;/html&gt; $_REQUEST、$_POST、$_GET的区别和联系小结1. $_REQUESTphp中$_REQUEST可以获取以POST方法和GET方法提交的数据，但是速度比较慢 2. $_GET用来获取由浏览器通过GET方法提交的数据。GET方法他是通过把参数数据加在提交表单的action属性所指的URL中，值和表单内每个字段一一对应，然后在URL中可以看到，但是有如下缺点： 安全性不好，在URL中可以看得到 传送数据量较小，不能大于2KB。 3. $_POST用来获取由浏览器通过POST方法提交的数据。POST方法他是通过HTTP POST机制，将表单的各个字段放置在HTTP HEADER内一起传送到action属性所指的URL地址中，用户看不到这个过程。他提交的大小一般来说不受限制，但是具体根据服务器的不同，还是略有不同。相对于_GET方式安全性略高 4. $_REQUEST、$_POST、$_GET 的区别和联系$_REQUEST[“参数”]具用$_POST[“参数”] $_GET[“参数”]的功能,但是$_REQUEST[“参数”]比较慢。通过post和get方法提交的所有数据都可以通过$_REQUEST数组[“参数”]获得]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python和lua数据类型的比较]]></title>
    <url>%2F2015%2F07%2F11%2Fpython%E5%92%8Clua%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%AF%94%E8%BE%83%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Python比较特殊的数据类型：List []例如 :123456789101112#!/usr/bin/python# -*- coding: UTF-8 -*- list = [ &apos;runoob&apos;, 786 , 2.23, &apos;john&apos;, 70.2 ]tinylist = [123, &apos;john&apos;] print list # 输出完整列表print list[0] # 输出列表的第一个元素print list[1:3] # 输出第二个至第三个的元素 print list[2:] # 输出从第三个开始至列表末尾的所有元素print tinylist * 2 # 输出列表两次print list + tinylist # 打印组合的列表 以上实例输出结果：123456[&apos;runoob&apos;, 786, 2.23, &apos;john&apos;, 70.2]runoob[786, 2.23][2.23, &apos;john&apos;, 70.2][123, &apos;john&apos;, 123, &apos;john&apos;][&apos;runoob&apos;, 786, 2.23, &apos;john&apos;, 70.2, 123, &apos;john&apos;] Tuple（元祖）(),相当于只读列表，不可以二次赋值tuple = ( &#39;runoob&#39;, 786 , 2.23, &#39;john&#39;, 70.2 ), 除了元祖用()而list用[], 而且元祖只是可读的, 其他的跟list一毛一样 dictionary（字典）{}，key值对123456789101112131415#!/usr/bin/python# -*- coding: UTF-8 -*- dict = &#123;&#125;dict[&apos;one&apos;] = &quot;This is one&quot;dict[2] = &quot;This is two&quot; tinydict = &#123;&apos;name&apos;: &apos;john&apos;,&apos;code&apos;:6734, &apos;dept&apos;: &apos;sales&apos;&#125; print dict[&apos;one&apos;] # 输出键为&apos;one&apos; 的值print dict[2] # 输出键为 2 的值print tinydict # 输出完整的字典print tinydict.keys() # 输出所有键print tinydict.values() # 输出所有值 输出结果为:12345This is oneThis is two&#123;&apos;dept&apos;: &apos;sales&apos;, &apos;code&apos;: 6734, &apos;name&apos;: &apos;john&apos;&#125;[&apos;dept&apos;, &apos;code&apos;, &apos;name&apos;][&apos;sales&apos;, 6734, &apos;john&apos;] lua比较特殊的数据类型lua变量 变量在使用前，必须在代码中进行声明，即创建该变量。 编译程序执行代码之前编译器需要知道如何给语句变量开辟存储区，用于存储变量的值。 Lua 变量有三种类型：全局变量、局部变量、表中的域。 Lua 中的变量全是全局变量，那怕是语句块或是函数里，除非用 local 显式声明为局部变量。 局部变量的作用域为从声明位置开始到所在语句块结束。 变量的默认值均为 nil。 test.lua 文件脚本123456789101112131415161718a = 5 -- 全局变量local b = 5 -- 局部变量function joke() c = 5 -- 全局变量 local d = 6 -- 局部变量endjoke()print(c,d) --&gt; 5 nildo local a = 6 -- 局部变量 b = 6 -- 全局变量 print(a,b); --&gt; 6 6endprint(a,b) --&gt; 5 6 执行以上实例输出结果为： 1234$ lua test.lua 5 nil6 65 6 lua的特有的东西table（表）在 Lua 里，table 的创建是通过”构造表达式”来完成， 最简单构造表达式是{}，用来创建一个空表。 也可以在表里添加一些数据，直接初始化表:12345-- 创建一个空的 tablelocal tbl1 = &#123;&#125; -- 直接初始表local tbl2 = &#123;&quot;apple&quot;, &quot;pear&quot;, &quot;orange&quot;, &quot;grape&quot;&#125; Lua 中的表（table）其实是一个”关联数组”（associative arrays），数组的索引可以是数字或者是字符串。 123456789-- table_test.lua 脚本文件a = &#123;&#125;a[&quot;key&quot;] = &quot;value&quot;key = 10a[key] = 22a[key] = a[key] + 11for k, v in pairs(a) do print(k .. &quot; : &quot; .. v)end 脚本执行结果为：123$ lua table_test.lua key : value10 : 33 不同于其他语言的数组把 0 作为数组的初始索引，在 Lua 里表的默认初始索引一般以 1 开始。12345-- table_test2.lua 脚本文件local tbl = &#123;&quot;apple&quot;, &quot;pear&quot;, &quot;orange&quot;, &quot;grape&quot;&#125;for key, val in pairs(tbl) do print(&quot;Key&quot;, key)end 脚本执行结果为：12345$ lua table_test2.lua Key 1Key 2Key 3Key 4 table 不会固定长度大小，有新数据添加时 table 长度会自动增长，没初始的 table 都是 nil。12345678-- table_test3.lua 脚本文件a3 = &#123;&#125;for i = 1, 10 do a3[i] = ienda3[&quot;key&quot;] = &quot;val&quot;print(a3[&quot;key&quot;])print(a3[&quot;none&quot;]) 脚本执行结果为：123$ lua table_test3.lua valnil]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis和hiredis安装教程]]></title>
    <url>%2F2015%2F07%2F11%2Fredis_hiredis_install_tutorial%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[在ubuntu上 redis安装 : sudo apt-get install redis-server hiredis安装 : 先到 https://github.com/redis/hiredis 下载 hiredis , 然后 sudo make sudo make install sudo ldconfig]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis概要之数据类型]]></title>
    <url>%2F2015%2F07%2F11%2Fredis%E6%A6%82%E8%A6%81%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[Redis简介要义 Redis运行在内存中但是可以持久化到磁盘, 重启的时候可以再次加载进行使用 Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行 Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作 Redis支持数据的备份，即master-slave模式的数据备份 Redis数据类型Redis的数据类型很重要, 这是他做很多事情的基础, 不理解的话很难用好 . . . String(字符串)string是redis最基本的类型，你可以理解成与Memcached一模一样的类型， 一个key对应一个value。 string类型是二进制安全的。 二进制安全的意思是redis的string可以包含任何数据。 比如jpg图片或者序列化的对象 。 string类型是Redis最基本的数据类型，一个键最大能存储512MB。 实例 : 1234redis 127.0.0.1:6379&gt; SET name &quot;runoob&quot;OKredis 127.0.0.1:6379&gt; GET name&quot;runoob&quot; Hash（哈希表）Redis hash 是一个键名对集合。Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 实例 :1234567891011redis&gt; HSET people jack &quot;Jack Sparrow&quot;(integer) 1redis&gt; HSET people gump &quot;Forrest Gump&quot;(integer) 1redis&gt; HGETALL people1) &quot;jack&quot; # 域2) &quot;Jack Sparrow&quot; # 值3) &quot;gump&quot;4) &quot;Forrest Gump&quot; 1234567redis&gt; HMSET pet dog &quot;doudou&quot; cat &quot;nounou&quot; # 一次设置多个域OKredis&gt; HMGET pet dog cat fake_pet # 返回值的顺序和传入参数的顺序一样1) &quot;doudou&quot;2) &quot;nounou&quot;3) (nil) # 不存在的域返回nil值 12345678redis&gt; HMSET website google www.google.com yahoo www.yahoo.comOKredis&gt; HGET website google&quot;www.google.com&quot;redis&gt; HGET website yahoo&quot;www.yahoo.com&quot; 12345redis&gt; HSET website google &quot;www.g.cn&quot; # 设置一个新域(integer) 1redis&gt; HSET website google &quot;www.google.com&quot; # 覆盖一个旧域(integer) 0 12345678910111213# 域存在redis&gt; HSET site redis redis.com(integer) 1redis&gt; HGET site redis&quot;redis.com&quot;# 域不存在redis&gt; HGET site mysql(nil) 以上实例中 hash 数据类型存储了包含用户脚本信息的用户对象。每个 hash 可以存储 232 -1 键值对（40多亿）。 List（列表）Redis 列表是简单的字符串列表，按照插入顺序排序。 你可以添加一个元素到列表的头部（左边lpush）或者尾部（右边rpush）。 实例 :1234567891011redis 127.0.0.1:6379&gt; lpush runoob redis(integer) 1redis 127.0.0.1:6379&gt; rpush runoob mongodb(integer) 2redis 127.0.0.1:6379&gt; lpush runoob rabitmq(integer) 3redis 127.0.0.1:6379&gt; lrange runoob 0 101) &quot;rabitmq&quot;2) &quot;redis&quot;3) &quot;mongodb&quot;redis 127.0.0.1:6379&gt; 列表最多可存储 232 - 1 元素 (4294967295, 每个列表可存储40多亿)。 Set（集合）Redis的Set是string类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 sadd 命令添加一个string元素到,key对应的set集合中，成功返回1, 如果元素已经在集合中返回0,key对应的set不存在返回错误。 sadd key member 实例 :12345678910111213redis 127.0.0.1:6379&gt; sadd runoob redis(integer) 1redis 127.0.0.1:6379&gt; sadd runoob mongodb(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 1redis 127.0.0.1:6379&gt; sadd runoob rabitmq(integer) 0redis 127.0.0.1:6379&gt; smembers runoob1) &quot;rabitmq&quot;2) &quot;mongodb&quot;3) &quot;redis&quot; 注意：以上实例中 rabitmq 添加了两次， 但根据集合内元素的唯一性，第二次插入的元素将被忽略。 集合中最大的成员数为 232 - 1(4294967295, 每个集合可存储40多亿个成员)。 zset(sorted set：有序集合)Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。 zadd 命令添加元素到集合，元素在集合中存在则更新对应scorezadd key score member实例 :12345678910111213redis 127.0.0.1:6379&gt; zadd runoob 1 redis(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 3 mongodb(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 4 rabitmq(integer) 1redis 127.0.0.1:6379&gt; zadd runoob 7 rabitmq(integer) 0redis 127.0.0.1:6379&gt; ZRANGEBYSCORE runoob 0 10001) &quot;redis&quot;2) &quot;mongodb&quot;3) &quot;rabitmq&quot;]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[epoll扼要总结]]></title>
    <url>%2F2015%2F06%2F22%2Fepoll%E6%89%BC%E8%A6%81%E6%80%BB%E7%BB%93%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[epoll 编程接口epoll API是Linux系统专有的，在2.6版中新增。 epoll API的核心数据结构称作epoll实例，它和一个打开的文件描述符相关联。这个文件描述符不是用来做I/O操作的，相反，它是内核数据结构的句柄，这些内核数据结构实现了两个目的。 记录了在进程中声明过的感兴趣的文件描述符列表-interest list（兴趣列表）。 维护了处于I/O就绪态的文件描述符列表-ready list（就绪列表）。 ready list中的成员是interest list的子集。 对于由epoll检查的每一个文件描述符，我们可以指定一个位掩码来表示我们感兴趣的事件。这些位掩码同poll()所使用的位掩码有着紧密的关联。 . . . epoll概要需要包含epoll.h头文件, 即 : #include &lt;sys/epoll.h&gt;epoll只有epoll_create, epoll_ctl, epoll_wait 3个系统调用 : 系统调用epoll_create()创建一个epoll实例，返回代表该实例的文件描述符。 系统调用epoll_ctl()操作同epoll实例相关联的兴趣列表。通过epoll_ctl()，我们可以增加新的描述符到列表中，或者将已有的文件描述符从该列表中移除，也可以修改代表文件描述符七事件类型的位掩码。 系统调用epoll_wait()返回与epoll实例相关联的就绪列表中的成员。 epoll_createint epoll_create(int size);Returns file descriptor on success, or -1 on error. 创建一个epoll的句柄。自从linux2.6.8之后，size参数是被忽略的。函数返回代表新创建的 epoll 实例的文件描述符(一般用 epfd表示), 这个文件描述符在其他几个 epoll 系统调用中用来表示 epoll 实例.需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 epoll_ctlint epoll_ctl(int epfd, int op, int fd, struct epoll_event *ev);Returns 0 on success, or -1 on error. epoll的事件注册函数，它不同于select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。 第一个参数是epoll_create()的返回值, 也就是新创建的 epoll 实例的文件描述符。 第二个参数表示动作，用三个宏来表示： EPOLL_CTL_ADD：注册新的fd到epfd中的兴趣列表； EPOLL_CTL_MOD：修改已经注册的fd的设定事件； EPOLL_CTL_DEL：从epfd的兴趣列表中删除一个fd； 第三个参数指要修改兴趣列表中的哪一个文件描述符的设定。 第四个参数是告诉内核需要监听什么事，参数ev是指向结构体epoll_event的指针, struct epoll_event结构如下： 1234struct epoll_event &#123; __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */ &#125;; 结构体epoll_event中的data字段的类型为: 123456typedef union epoll_data &#123; void *ptr; /* Pointer to user-defined data */ int fd; __uint32_t u32; __uint64_t u64; &#125; epoll_data_t; 结构体epoll_event中的events字段是一个位掩码, 他指定了我们为待检查的描述符fd上所感兴趣的事件集合.可以是以下几个宏的集合： EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）； EPOLLOUT：表示对应的文件描述符可以写； EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）； EPOLLERR：表示对应的文件描述符发生错误； EPOLLHUP：表示对应的文件描述符被挂断； EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。(后文会说水平触发和边缘触发的区别) EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 data 字段是一个联合体, 当描述符 fd 稍后成为就绪态时, 联合体的成员可用来指定传回给调用进程的信息 使用 epoll_create() 和 epoll_ctl()的例子使用 epoll_create() 和 epoll_ctl() 1234567891011int epdf;struct epoll_event ev;epfd = epoll_create( 5 );if ( epfd == -1 ) errExit( "epoll_create" );ev.data.fd = fd;ev.events = EPOLLIN;if ( epoll_ctl( epofd, EPOLL_CTL_ADD, fd, ev ) == -1 ) errExit( "epoll_ctl" ); epoll_waitint epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);Returns number of ready file descriptors, 0 on timeout, or -1 on error. epoll_wait收集在epoll监控的事件中已经发送的事件。 参数events是分配好的epoll_event结构体数组，epoll将会把发生的事件赋值到events数组中（events不可以是空指针，内核只负责把数据复制到这个events数组中，不会去帮助我们在用户态中分配内存）。 maxevents告之内核这个events有多大，这个 maxevents的值不能大于创建epoll_create()时的size， 参数timeout是超时时间, 用来确定 epoll_wait() 的阻塞行为, 有如下几种 : 如果 timeout 等于 -1, 调用将一直阻塞, 直到兴趣列表中的文件描述符有事件产生, 或者直到捕获到一个信号为止 如果 timeout 等于 0, 执行一次非阻塞的检查, 立即返回, 看兴趣列表中的文件描述符上产生了哪个事件 如果 timeout 大于 0, 调用将阻塞至多 timeout 毫秒, 知道文件描述符上有事件产生, 或者直到捕获到一个信号为止 TLPI书上的例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114/*************************************************************************\* Copyright (C) Michael Kerrisk, 2017. ** ** This program is free software. You may use, modify, and redistribute it ** under the terms of the GNU General Public License as published by the ** Free Software Foundation, either version 3 or (at your option) any ** later version. This program is distributed without any warranty. See ** the file COPYING.gpl-v3 for details. *\*************************************************************************//* Listing 63-5 *//* epoll_input.c Example of the use of the Linux epoll API. Usage: epoll_input file... This program opens all of the files named in its command-line arguments and monitors the resulting file descriptors for input events. This program is Linux (2.6 and later) specific.*/#include &lt;sys/epoll.h&gt;#include &lt;fcntl.h&gt;#include "tlpi_hdr.h"#define MAX_BUF 1000 /* Maximum bytes fetched by a single read() */#define MAX_EVENTS 5 /* Maximum number of events to be returned from a single epoll_wait() call */intmain(int argc, char *argv[])&#123; int epfd, ready, fd, s, j, numOpenFds; struct epoll_event ev; struct epoll_event evlist[MAX_EVENTS]; char buf[MAX_BUF]; if (argc &lt; 2 || strcmp(argv[1], "--help") == 0) usageErr("%s file...\n", argv[0]); epfd = epoll_create(argc - 1); if (epfd == -1) errExit("epoll_create"); /* Open each file on command line, and add it to the "interest list" for the epoll instance */ for (j = 1; j &lt; argc; j++) &#123; fd = open(argv[j], O_RDONLY); if (fd == -1) errExit("open"); printf("Opened \"%s\" on fd %d\n", argv[j], fd); ev.events = EPOLLIN; /* Only interested in input events */ ev.data.fd = fd; if (epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &amp;ev) == -1) errExit("epoll_ctl"); &#125; numOpenFds = argc - 1; while (numOpenFds &gt; 0) &#123; /* Fetch up to MAX_EVENTS items from the ready list of the epoll instance */ printf("About to epoll_wait()\n"); ready = epoll_wait(epfd, evlist, MAX_EVENTS, -1); if (ready == -1) &#123; if (errno == EINTR) continue; /* Restart if interrupted by signal */ else errExit("epoll_wait"); &#125; printf("Ready: %d\n", ready); /* Deal with returned list of events */ for (j = 0; j &lt; ready; j++) &#123; printf(" fd=%d; events: %s%s%s\n", evlist[j].data.fd, (evlist[j].events &amp; EPOLLIN) ? "EPOLLIN " : "", (evlist[j].events &amp; EPOLLHUP) ? "EPOLLHUP " : "", (evlist[j].events &amp; EPOLLERR) ? "EPOLLERR " : ""); if (evlist[j].events &amp; EPOLLIN) &#123; s = read(evlist[j].data.fd, buf, MAX_BUF); if (s == -1) errExit("read"); printf(" read %d bytes: %.*s\n", s, s, buf); &#125; else if (evlist[j].events &amp; (EPOLLHUP | EPOLLERR)) &#123; /* After the epoll_wait(), EPOLLIN and EPOLLHUP may both have been set. But we'll only get here, and thus close the file descriptor, if EPOLLIN was not set. This ensures that all outstanding input (possibly more than MAX_BUF bytes) is consumed (by further loop iterations) before the file descriptor is closed. */ printf(" closing fd %d\n", evlist[j].data.fd); // 关闭一个文件描述符会自动的将其从所有的 epoll 实例的兴趣列表中移除 if (close(evlist[j].data.fd) == -1) errExit("close"); numOpenFds--; &#125; &#125; &#125; printf("All file descriptors closed; bye\n"); exit(EXIT_SUCCESS);&#125; 水平触发与边缘触发在深入讨论多种可选的机制之前，我们需要先区分两种文件描述符准备就绪的通知模式。 水平触发通知(epoll默认的通知方式)：如果文件描述符上可以非阻塞地执行I/O系统调用，此时认为它已经就绪。 边缘触发通知：如果文件描述符自上次状态检查以来有了新的I/O活动（比如新的输入），此时需要触发通知。 下图总结了I/O多路复用、信号驱动I/O以及epoll所采用的通知模型。epoll API同其他两种I/O模型的区别在于它对水平触发（默认）和边缘触发都支持。 水平触发与边缘触发的区别默认情况下 epoll 提供的是水平触发通知.要使用边缘触发通知，我们在调用epoll_ctl()时在ev．events字段中指定EPOLLET标志. 例如 : 12345struct epoll_event ev;ev.data.fd = fd;ev.events = EPOLLIN | EPOLLET;if (epoll_ctl(epfd, EPOLL_CTL_ADD, fd, ev) == -1) errExit("epoll_ctl"); 我们通过一个例子来说明epoll的水平触发和边缘触发通知之间的区别。假设我们使用epoll来监视一个套接字上的输入（EPOLLIN），接下来会发生如下的事件。 1．套接字上有输入到来。2．我们调用一次epoll_wait()。无论我们采用的是水平触发还是边缘触发通知，该调用都会告诉我们套接字已经处于就绪态了。3．再次调用epoll_wait()。 如果我们采用的是水平触发通知，那么第二个epoll_wait()调用将告诉我们套接字处于就绪态。而如果我们采用边缘触发通知，那么第二个epoll_wait()调用将阻塞，因为自从上一次调用epoll_wait()以来并没有新的输入到来。 边缘触发通知通常和非阻塞的文件描述符结合使用。因而，采用epoll的边缘触发通知机制的程序基本框架如下: 1．让所有待监视的文件描述符都成为非阻塞的。2．通过epoll_ctl()构建epoll的兴趣列表。3．通过如下的循环处理I/O事件 :(a)通过epoll_wait()取得处于就绪态的描述符列表。(b)针对每一个处于就绪态的文件描述符，不断进行I/O处理直到相关的系统调用( 例如read()、write()，recv()、send()或accept() )返回EAGAIN或EWOULDBLOCK错误。 水平触发需要处理的问题使用linux epoll模型，水平触发模式（Level-Triggered）；当socket可写时，会不停的触发socket可写的事件，如何处理？ 第一种最普通的方式： 当需要向socket写数据时，将该socket加入到epoll模型（epoll_ctl）；等待可写事件。 接收到socket可写事件后，调用write()或send()发送数据。。。 当数据全部写完后， 将socket描述符移出epoll模型。 这种方式的缺点是： 即使发送很少的数据，也要将socket加入、移出epoll模型。有一定的操作代价。 第二种方式，（是本人的改进方案， 叫做directly-write） 向socket写数据时，不将socket加入到epoll模型；而是直接调用send()发送； 只有当或send()返回错误码EAGAIN（系统缓存满），才将socket加入到epoll模型，等待可写事件后(表明系统缓冲区有空间可以写了)，再发送数据。 全部数据发送完毕，再移出epoll模型。 这种方案的优点： 当用户数据比较少时，不需要epool的事件处理。 在高压力的情况下，性能怎么样呢？ 对一次性直接写成功、失败的次数进行统计。如果成功次数远大于失败的次数， 说明性能良好。（如果失败次数远大于成功的次数，则关闭这种直接写的操作，改用第一种方案。同时在日志里记录警告） 在我自己的应用系统中，实验结果数据证明该方案的性能良好。 事实上，网络数据可分为两种到达/发送情况： 一是分散的数据包， 例如每间隔40ms左右，发送/接收3-5个 MTU（或更小，这样就没超过默认的8K系统缓存）。 二是连续的数据包， 例如每间隔1s左右，连续发送/接收 20个 MTU（或更多）。 第三种方式： 使用Edge-Triggered（边沿触发），这样socket有可写事件，只会触发一次。 可以在应用层做好标记。以避免频繁的调用 epoll_ctl( EPOLL_CTL_ADD, EPOLL_CTL_MOD)。 这种方式是epoll 的 man 手册里推荐的方式， 性能最高。但如果处理不当容易出错，事件驱动停止。 epoll与select/poll的区别select函数，必须得清楚select跟linux特有的epoll的区别， 有三点(遍内树)： 遍历 ： 每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大；当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd, 每次只需要简单的从列表里取出就行了 内存拷贝 ： select，poll每次调用都要把fd集合从用户态往内核态拷贝一次; epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次 数量限制 ： select默认只支持1024个；epoll并没有最大数目限制 总结： （1）select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。 （2）select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。 常见的epoll编程模型见本博客的: 关于服务器模型总结请转 此文 关于reactor模型总结请转 此文]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>noodle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[socket可读可写条件与非阻塞connect或accept浅析]]></title>
    <url>%2F2015%2F06%2F22%2Fsocket%E5%8F%AF%E8%AF%BB%E5%8F%AF%E5%86%99%E6%9D%A1%E4%BB%B6%E4%B8%8E%E9%9D%9E%E9%98%BB%E5%A1%9Econnect%E6%88%96accept%E6%B5%85%E6%9E%90%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[socket可读的条件: socket的接收缓冲区中的数据字节大于等于该socket的接收缓冲区低水位标记的当前大小。对这样的socket的读操作将不阻塞并返回一个大于0的值(也就是返回准备好读入的数据)。我们可以用SO_RCVLOWAT这个socket选项来设置该socket的低水位标记。对于TCP和UDP的socket而言，其缺省值为1. 该连接的读这一半关闭(也就是接收了FIN的TCP连接)。对这样的socket的读操作将不阻塞并返回0 给监听套接字准备好新连接 有一个socket有异常错误条件待处理.对于这样的socket的读操作将不会阻塞,并且返回一个错误(-1),errno则设置成明确的错误条件.这些待处理的错误也可通过指定socket选项SO_ERROR调用getsockopt来取得并清除; . . . socket可写的条件: socket的发送缓冲区中的可用空间字节数大于等于该socket的发送缓冲区低水位标记的当前大小。对这样的socket的写操作将不阻塞并返回一个大于0的值(也就是返回准备好写入的数据)。我们可以用SO_SNDLOWAT这个socket选项来设置该socket的低水位标记。对于TCP和UDP的socket而言，其缺省值为2048 该连接的写这一半关闭。对这样的socket的写操作将产生SIGPIPE信号，该信号的缺省行为是终止进程。 使用非阻塞connect的套接字已建立连接, 或者connect已经以失败告终 有一个socket异常错误条件待处理.对于这样的socket的写操作将不会阻塞并且返回一个错误(-1),errno则设置成明确的错误条件.这些待处理的错误也可以通过指定socket选项SO_ERROR调用getsockopt函数来取得并清除; 非阻塞connect/accept相关上述的各种条件可以大体总结为下图 注意 : 当socket异常错误的时候socket是可读并可写的, 所以在非阻塞connect(判断是否可写)/accept(判断是否可读)的时候要特别注意这种情况, 要用getsockopt函数, 使用SO_ERROR选项来检查处理.]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>NP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[const和volatile和mutable讲解]]></title>
    <url>%2F2015%2F05%2F24%2Fconst_mutable_volatile%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[const关键字const 是比较常见的关键字, 也是非常好的预防错误的手段. const 修饰普通变量和指针const 修饰变量，一般有两种写法： const TYPE value; TYPE const value; 这两种写法在本质上是一样的。它的含义是：const 修饰的类型为 TYPE 的变量 value 是不可变的。对于一个非指针的类型 TYPE，无论怎么写，都是一个含义，即 value 值不可变。 例如： const int nValue； //nValue 是 const int const nValue； //nValue 是 const . . . 但是对于指针类型的 TYPE，不同的写法会有不同情况： （1） 指针本身是常量不可变 (char*) const pContent; （2）指针所指向的内容是常量不可变 const (char) *pContent; (char) const *pContent; （3） 两者都不可变 const char* const pContent; 识别 const 到底是修饰指针还是指针所指的对象，还有一个较为简便的方法，也就是沿着 * 号划一条线： 如果 const 位于 * 的左侧，则 const 就是用来修饰指针所指向的变量，即指针指向为常量； 如果 const 位于 * 的右侧，const 就是修饰指针本身，即指针本身是常量。 const 修饰函数参数const 修饰函数参数是它最广泛的一种用途，它表示在函数体中不能修改参数的值 (包括参数本身的值或者参数其中包含的值)： void function(const int Var); // 传递过来的参数在函数内不可以改变 (无意义，该函数以传值的方式调用) void function(const char* Var); // 参数指针所指内容为常量不可变 void function(char* const Var); // 参数指针本身为常量不可变 (也无意义，var 本身也是通过传值的形式赋值的) void function(const Class&amp; Var); // 引用参数在函数内不可以改变 参数 const 通常用于参数为指针或引用的情况，若输入参数采用 “值传递” 方式，由于函数将自动产生临时变量用于复制该参数，该参数本就不需要保护，所以不用 const 修饰。 const 修饰类对象 / 对象指针 / 对象引用 const 修饰类对象表示该对象为常量对象，其中的任何成员都不能被修改。对于对象指针和对象引用也是一样。 const 修饰的对象，该对象的任何非 const 成员函数都不能被调用，因为任何非 const 成员函数会有修改成员变量的企图。 例如：1234567891011121314151617181920class AAA&#123; void func1();void func2() const;&#125;const AAA aObj;aObj.func1(); 错误aObj.func2(); 正确const AAA* aObj = new AAA();aObj-&gt;func1(); 错误aObj-&gt;func2(); 正确 const 修饰数据成员 const 数据成员只在某个对象生存期内是常量，而对于整个类而言却是可变的。因为类可以创建多个对象，不同的对象其 const 数据成员的值可以不同。所以不能在类声明中初始化 const 数据成员，因为类的对象未被创建时，编译器不知道 const 数据成员的值是什么，例如：12345class A&#123; const int size = 100; // 正确 int array[size]; // 错误，未知的 size&#125; const 数据成员的初始化只能在类的构造函数的初始化列表中进行。要想建立在整个类中都恒定的常量，可以用类中的枚举常量来实现，例如：12345678910class A&#123;… enum &#123;size1=100, size2 = 200&#125;; int array1[size1]; int array2[size2];…&#125; 枚举常量不会占用对象的存储空间，他们在编译时被全部求值。但是枚举常量的隐含数据类型是整数，其最大值有限，且不能表示浮点数。 const 修饰成员函数const 修饰类的成员函数，用 const 修饰的成员函数不能改变对象的成员变量。一般把 const 写在成员函数的最后：123456789class A&#123; …void function()const; // 常成员函数, 它不改变对象的成员变量. 也不能调用类中任何非 const 成员函数。&#125; 对于 const 类对象 / 指针 / 引用，只能调用类的 const 成员函数。 const 修饰成员函数的返回值 １、一般情况下，函数的返回值为某个对象时，如果将其声明为 const 时，多用于操作符的重载。通常，不建议用 const 修饰函数的返回值类型为某个对象或对某个对象引用的情况。原因如下：如果返回 const 对象，或返回 const 对象的引用，则返回值具有 const 属性，返回实例只能访问类 A 中的公有（保护）数据成员和 const 成员函数，并且不允许对其进行赋值操作，这在一般情况下很少用到。 2、如果给采用 “指针传递” 方式的函数返回值加 const 修饰，那么函数返回值（即指针所指的内容）不能被修改，该返回值只能被赋给加 const 修饰的同类型指针： const char * GetString(void); 如下语句将出现编译错误： char *str=GetString(); 正确的用法是： const char *str=GetString(); 3、函数返回值采用 “引用传递” 的场合不多，这种方式一般只出现在类的赙值函数中，目的是为了实现链式表达。如： 12345678class A&#123;… A &amp;operate= (const A &amp;other); // 赋值函数&#125;A a,b,c; //a,b,c 为 A 的对象…a=b=c; // 正常(a=B)=c; // 不正常，但是合法 若赋值函数的返回值加 const 修饰，那么该返回值的内容不允许修改，上例中 a=b=c 依然正确。(a=b)=c 就不正确了。 const 常量与 define 宏定义的区别 （1） 编译器处理方式不同 define 宏是在预处理阶段展开。 const 常量是编译运行阶段使用。 （2）类型和安全检查不同 define 宏没有类型，不做任何类型检查，仅仅是展开。 const 常量有具体的类型，在编译阶段会执行类型检查。 （3） 存储方式不同 define 宏仅仅是展开，有多少地方使用，就展开多少次，不会分配内存。 const 常量会在内存中分配 (可以是堆中也可以是栈中)。 volatile关键字volatile 的本意是 “易变的”,volatile 关键字是一种类型修饰符，用它声明的类型变量表示可以被某些编译器未知的因素更改，比如操作系统、硬件或者其它线程等。遇到这个关键字声明的变量，编译器对访问该变量的代码就不再进行优化，从而可以提供对特殊地址的稳定访问。 当要求使用 volatile 声明的变量的值的时候，系统总是重新从它所在的内存读取数据，即使它前面的指令刚刚从该处读取过数据。而且读取的数据立刻被寄存。例如：1234567volatile int i=10;int a = i;。。。// 其他代码，并未明确告诉编译器，对 i 进行过操作int b = i; volatile 指出 i 是随时可能发生变化的，每次使用它的时候必须从 i 的地址中读取，因而编译器生成的汇编代码会重新从 i 的地址读取数据放在 b 中。而优化做法是，由于编译器发现两次从 i 读数据的代码之间的代码没有对 i 进行过操作，它会自动把上次读的数据放在 b 中。而不是重新从 i 里面读。这样以来，如果 i 是一个寄存器变量或者表示一个端口数据就容易出错，所以说 volatile 可以保证对特殊地址的稳定访问。 注意，在 vc6 中，一般调试模式没有进行代码优化，所以这个关键字的作用看不出来。下面通过插入汇编代码，测试有无 volatile 关键字，对程序最终代码的影响。首先用 classwizard 建一个 win32 console 工程，插入一个 voltest.cpp 文件，输入下面的代码：12345678910111213141516171819202122232425#include &lt;stdio.h&gt;void main()&#123;int i=10;int a = i;printf("i= %d/n",a);// 下面汇编语句的作用就是改变内存中 i 的值，但是又不让编译器知道__asm &#123; mov dword ptr [ebp-4], 20h&#125;int b = i;printf("i= %d/n",b);&#125; 然后，在调试版本模式运行程序，输出结果如下： i = 10 i = 32 然后，在 release 版本模式运行程序，输出结果如下： i = 10 i = 10 输出的结果明显表明，release 模式下，编译器对代码进行了优化，第二次没有输出正确的 i 值。下面，我们把 i 的声明加上 volatile 关键字，看看有什么变化：1234567891011121314151617181920212223#include &lt;stdio.h&gt;void main()&#123;volatile int i=10;int a = i;printf("i= %d/n",a);__asm &#123; mov dword ptr [ebp-4], 20h&#125;int b = i;printf("i= %d/n",b);&#125; 分别在调试版本和 release 版本运行程序，输出都是： i = 10 i = 32 这说明这个关键字发挥了它的作用！ 关于 volatile 的补充信息一个定义为 volatile 的变量是说这变量可能会被意想不到地改变，这样，编译器就不会去假设这个变量的值了。精确地说就是，优化器在用到这个变量时必须每次都小心地重新读取这个变量的值，而不是使用保存在寄存器里的备份。下面是 volatile 变量的几个例子： 1). 并行设备的硬件寄存器（如：状态寄存器） 2). 一个中断服务子程序中会访问到的非自动变量 (Non-automatic variables) 3). 多线程应用中被几个任务共享的变量 我认为这是区分 C 程序员和嵌入式系统程序员的最基本的问题。嵌入式系统程序员经常同硬件、中断、RTOS 等等打交道，所用这些都要求 volatile 变量。不懂得 volatile 内容将会带来灾难。假设被面试者正确地回答了这是问题（嗯，怀疑这否会是这样），我将稍微深究一下，看一下这家伙是不是直正懂得 volatile 的重要性： 1). 一个参数既可以是 const 还可以是 volatile 吗？解释为什么。 2). 一个指针可以是 volatile 吗？解释为什么。 3). 下面的函数有什么错误： int square(volatile int *ptr) { return *ptr * *ptr; } 下面是答案： 1). 是的。一个例子是只读的状态寄存器。它是 volatile 因为它可能被意想不到地改变。它是 const 因为程序不应该试图去修改它。 2). 是的。尽管这并不很常见。一个例子是当一个中服务子程序修该一个指向一个 buffer 的指针时。 3). 这段代码的有个恶作剧。这段代码的目的是用来返指针 ptr 指向值的平方，但是，由于 ptr 指向一个 volatile 型参数，编译器将产生类似下面的代码： int square(volatile int *ptr) { int a,b; a = *ptr; b = *ptr; return a * b; } 由于 *ptr 的值可能被意想不到地该变，因此 a 和 b 可能是不同的。结果，这段代码可能返不是你所期望的平方值！正确的代码如下： long square(volatile int *ptr) { int a; a = *ptr; return a * a; } mutable关键字mutalbe 的中文意思是 “可变的，易变的”，跟 constant（既 C++ 中的 const）是反义词。在 C++ 中，mutable 也是为了突破 const 的限制而设置的。被 mutable 修饰的变量 (mutable 只能由于修饰类的非静态数据成员)，将永远处于可变的状态，即使在一个 const 函数中。 我们知道，假如类的成员函数不会改变对象的状态，那么这个成员函数一般会声明为 const。但是，有些时候，我们需要在 const 的函数里面修改一些跟类状态无关的数据成员，那么这个数据成员就应该被 mutalbe 来修饰。下面是一个小例子：12345678910111213141516171819202122232425class ClxTest&#123; public: void Output() const;&#125;;void ClxTest::Output() const&#123; cout &lt;&lt; "Output for test!" &lt;&lt; endl;&#125;void OutputTest(const ClxTest&amp; lx)&#123; lx.Output();&#125; 类 ClxTest 的成员函数 Output 是用来输出的，不会修改类的状态，所以被声明为 const。 函数 OutputTest 也是用来输出的，里面调用了对象 lx 的 Output 输出方法，为了防止在函数中调用成员函数修改任何成员变量，所以参数也被 const 修饰。 假如现在，我们要增添一个功能：计算每个对象的输出次数。假如用来计数的变量是普通的变量的话，那么在 const 成员函数 Output 里面是不能修改该变量的值的；而该变量跟对象的状态无关，所以应该为了修改该变量而去掉 Output 的 const 属性。这个时候，就该我们的 mutable 出场了，只要用 mutalbe 来修饰这个变量，所有问题就迎刃而解了。下面是修改过的代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class ClxTest&#123; public: ClxTest(); ~ClxTest(); void Output() const; int GetOutputTimes() const; private: mutable int m_iTimes;&#125;;ClxTest::ClxTest()&#123; m_iTimes = 0;&#125;ClxTest::~ClxTest()&#123;&#125;void ClxTest::Output() const&#123; cout &lt;&lt; "Output for test!" &lt;&lt; endl; m_iTimes++;&#125;int ClxTest::GetOutputTimes() const&#123; return m_iTimes;&#125;void OutputTest(const ClxTest&amp; lx)&#123; cout &lt;&lt;lx.GetOutputTimes() &lt;&lt; endl; lx.Output(); cout &lt;&lt;lx.GetOutputTimes() &lt;&lt; endl;&#125; 计数器 m_iTimes 被 mutable 修饰，那么它就可以突破 const 的限制，在被 const 修饰的函数里面也能被修改。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关系型数据库与NoSQL的爱恨情仇]]></title>
    <url>%2F2015%2F05%2F20%2F%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8ENoSQL%E7%9A%84%E7%88%B1%E6%81%A8%E6%83%85%E4%BB%87%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[NoSQL因关系数据库的不足而生随着互联网的不断发展，各种类型的应用层出不穷，所以导致在这个云计算的时代， 对技术提出了更多的需求，主要体现在下面这四个方面： 低延迟的读写速度：应用快速地反应能极大地提升用户的满意度; 支撑海量的数据和流量：对于搜索这样大型应用而言，需要利用PB级别的数据和能应对百万级的流量; 大规模集群的管理：系统管理员希望分布式应用能更简单的部署和管理; 庞大运营成本的考量：IT经理们希望在硬件成本、软件成本和人力成本能够有大幅度地降低; 目前世界上主流的存储系统大部分还是采用了关系型数据库，其主要有一下优点： 事务处理—保持数据的一致性； 由于以标准化为前提，数据更新的开销很小（相同的字段基本上只有一处）； 可以进行Join等复杂查询。 虽然关系型数据库已经在业界的数据存储方面占据不可动摇的地位，但是由于其天生的几个限制， 使其很难满足上面这几个需求： 扩展困难：由于存在类似Join这样多表查询机制，使得数据库在扩展方面很艰难; 读写慢：这种情况主要发生在数据量达到一定规模时由于关系型数据库的系统逻辑非常复杂，使得其非常容易发生死锁等的并发问题，所以导致其读写速度下滑非常严重; 成本高：企业级数据库的License价格很惊人，并且随着系统的规模，而不断上升; 有限的支撑容量：现有关系型解决方案还无法支撑Google这样海量的数据存储; 业界为了解决上面提到的几个需求，推出了多款新类型的数据库，并且由于它们在设计上和传统的NoSQL数据库相比有很大的不同，所以被统称为“NoSQL”系列数据库。 总的来说，在设计上，它们非常关注对数据高并发地读写和对海量数据的存储等，与关系型数据库相比，它们在架构和数据模型方量面做了“减法”， 而在扩展和并发等方面做了“加法”。 现在主流的NoSQL数据库有MongoDB和Redis以及BigTable、Hbase、Cassandra、SimpleDB、CouchDB、等。 接下来，将关注NoSQL数据库到底存在哪些优缺点。 NoSQL的优缺点在优势方面，主要体现在下面这三点： 简单的扩展：典型例子是Cassandra，由于其架构是类似于经典的P2P，所以能通过轻松地添加新的节点来扩展这个集群; 快速的读写：主要例子有redis，由于其逻辑简单，而且纯内存操作，使得其性能非常出色，单节点每秒可以处理超过10万次读写操作; 低廉的成本：这是大多数分布式数据库共有的特点，因为主要都是开源软件，没有昂贵的License成本; 但瑕不掩瑜，NoSQL数据库还存在着很多的不足，常见主要有下面这几个： 不提供对SQL的支持：如果不支持SQL这样的工业标准，将会对用户产生一定的学习和应用迁移成本; 支持的特性不够丰富：现有产品所提供的功能都比较有限，大多数NoSQL数据库都不支持事务，也不像MS SQL Server和Oracle那样能提供各种附加功能，比如BI和报表等; 现有产品的不够成熟：大多数产品都还处于初创期，和关系型数据库几十年的完善不可同日而语; 上面NoSQL产品的优缺点都是些比较共通的，在实际情况下，每个产品都会根据自己所遵从的数据模型和CAP理念而有所不同.]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译过程]]></title>
    <url>%2F2015%2F05%2F07%2F%E7%BC%96%E8%AF%91%E8%BF%87%E7%A8%8B%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[通常我们使用gcc来生成可执行程序，命令为：gcc hello.c，默认生成可执行文件a.out其实编译（包括链接）的命令：gcc hello.c 可分解为如下4个大的步骤：预处理(Preprocessing)编译(Compilation)汇编(Assembly)链接(Linking)预处理1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 预处理(Preproceessing)预处理的过程主要处理包括以下过程：将所有的#define删除，并且展开所有的宏定义处理所有的条件预编译指令，比如#if #ifdef #elif #else #endif等处理#include&nbsp;预编译指令，将被包含的文件插入到该预编译指令的位置。删除所有注释&nbsp;“//”和”/ /”.添加行号和文件标识，以便编译时产生调试用的行号及编译错误警告行号。保留所有的#pragma编译器指令，因为编译器需要使用它们&nbsp;通常使用以下命令来进行预处理：gcc -E hello.c -o hello.i参数-E表示只进行预处理 或者也可以使用以下指令完成预处理过程cpp hello.c &gt; hello.i &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/ &nbsp;cpp – The C Preprocessor&nbsp; /直接cat hello.i 你就可以看到预处理后的代码&nbsp; 编译2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 编译(Compilation)编译过程就是把预处理完的文件进行一系列的词法分析，语法分析，语义分析及优化后生成相应的汇编代码。$gcc –S hello.i –o hello.s或者$ /usr/lib/gcc/i486-linux-gnu/4.4/cc1 hello.c注：现在版本的GCC把预处理和编译两个步骤合成一个步骤，用cc1工具来完成。gcc其实是后台程序的一些包装，根据不同参数去调用其他的实际处理程序，比如：预编译编译程序cc1、汇编器as、连接器ld可以看到编译后的汇编代码(hello.s)如下： 1234567891011121314151617181920 .file &quot;hello.c&quot; .section .rodata.LC0: .string &quot;Hello, world.&quot; .text.globl main .type main, @functionmain: pushl %ebp movl %esp, %ebp andl $-16, %esp subl $16, %esp movl $.LC0, (%esp) call puts movl $0, %eax leave ret .size main, .-main .ident &quot;GCC: (Ubuntu 4.4.3-4ubuntu5) 4.4.3&quot; .section .note.GNU-stack,&quot;&quot;,@progbits &nbsp; 汇编3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 汇编(Assembly)汇编器是将汇编代码转变成机器可以执行的命令，每一个汇编语句几乎都对应一条机器指令。汇编相对于编译过程比较简单，根据汇编指令和机器指令的对照表一一翻译即可。$ gcc –c hello.c –o hello.o或者$ as hello.s –o hello.co由于hello.o的内容为机器码，不能以普通文本形式的查看（vi 打开看到的是乱码）。&nbsp; 链接4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 链接(Linking)通过调用链接器ld来链接程序运行需要的一大堆目标文件，以及所依赖的其它库文件，最后生成可执行文件。ld -static crt1.o crti.o crtbeginT.o hello.o -start-group -lgcc -lgcc_eh -lc-end-group crtend.o crtn.o (省略了文件的路径名)。&nbsp; 编译器和链接器具体做了什么helloworld的大体编译和链接过程就是这样了，那么编译器和链接器到底做了什么呢？&nbsp;编译过程可分为6步： 词法分析：扫描器（Scanner）将源代的字符序列分割成一系列的记号（Token）。lex工具可实现词法扫描。 语法分析：语法分析器将记号（Token）产生语法树（Syntax Tree）。yacc工具可实现语法分析(yacc: Yet Another Compiler Compiler)。 语义分析：静态语义（在编译器可以确定的语义）、动态语义（只能在运行期才能确定的语义）。 源代码优化：源代码优化器(Source Code Optimizer)，将整个语法书转化为中间代码（Intermediate Code）（中间代码是与目标机器和运行环境无关的）。中间代码使得编译器被分为前端和后端。编译器前端负责产生机器无关的中间代码；编译器后端将中间代码转化为目标机器代码。 目标代码生成：代码生成器(Code Generator). 目标代码优化：目标代码优化器(Target Code Optimizer)。 &nbsp;链接的主要内容是把各个模块之间相互引用的部分处理好，使得各个模块之间能够正确地衔接。链接的主要过程包括：地址和空间分配（Address and Storage Allocation），符号决议（Symbol Resolution），重定位（Relocation）等。 链接分为静态链接和动态链接 静态链接是指在编译阶段直接把静态库加入到可执行文件中去，这样可执行文件会比较大。 动态链接则是指链接阶段仅仅只加入一些描述信息，而程序执行时再从系统中把相应动态库加载到内存中去。 静态链接的大致过程如下图所示：]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>Compile</tag>
        <tag>Make</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Valgrind所报的4种内存丢失]]></title>
    <url>%2F2015%2F05%2F02%2Fvalgrind_tutorial%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[官方解释及分析摘自http://valgrind.org/docs/manual/faq.html#faq.deflost 5.2.With Memcheck’s memory leak detector, what’s the difference between “definitely lost”, “indirectly lost”, “possibly lost”, “still reachable”, and “suppressed”? The details are in the Memcheck section of the user manual.In short: “definitely lost” means your program is leaking memory – fix those leaks! “indirectly lost” means your program is leaking memory in a pointer-based structure. (E.g. if the root node of a binary tree is “definitely lost”, all the children will be “indirectly lost”.) If you fix the “definitely lost” leaks, the “indirectly lost” leaks should go away. “possibly lost” means your program is leaking memory, unless you’re doing unusual things with pointers that could cause them to point into the middle of an allocated block; see the user manual for some possible causes. Use –show-possibly-lost=no if you don’t want to see these reports. “still reachable” means your program is probably ok – it didn’t free some memory it could have. This is quite common and often reasonable. Don’t use –show-reachable=yes if you don’t want to see these reports. “suppressed” means that a leak error has been suppressed. There are some suppressions in the default suppression files. You can ignore suppressed errors. 分析 “definitely lost”：确认丢失。程序中存在内存泄露，应尽快修复。当程序结束时如果一块动态分配的内存没有被释放且通过程序内的指针变量均无法访问这块内存则会报这个错误。 “indirectly lost”：间接丢失。当使用了含有指针成员的类或结构时可能会报这个错误。这类错误无需直接修复，他们总是与”definitely lost”一起出现，只要修复”definitely lost”即可。例子可参考我的例程。 “possibly lost”：可能丢失。大多数情况下应视为与”definitely lost”一样需要尽快修复，除非你的程序让一个指针指向一块动态分配的内存（但不是这块内存起始地址），然后通过运算得到这块内存起始地址，再释放它。例子可参考我的例程。当程序结束时如果一块动态分配的内存没有被释放且通过程序内的指针变量均无法访问这块内存的起始地址，但可以访问其中的某一部分数据，则会报这个错误。 “still reachable”：可以访问，未丢失但也未释放。如果程序是正常结束的，那么它可能不会造成程序崩溃，但长时间运行有可能耗尽系统资源，因此笔者建议修复它。如果程序是崩溃（如访问非法的地址而崩溃）而非正常结束的，则应当暂时忽略它，先修复导致程序崩溃的错误，然后重新检测。 “suppressed”：已被解决。出现了内存泄露但系统自动处理了。可以无视这类错误。这类错误我没能用例程触发，看官方的解释也不太清楚是操作系统处理的还是valgrind，也没有遇到过。所以无视他吧~ 代码示例1234567891011121314151617181920212223242526#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;void *g_p1;int *g_p2;int ** fun1(void)&#123; //付给了局部变量, 函数结束而不释放,为肯定丢失. //把函数尾部语句return p; 改为return 0;更能说明这个问题. int **p=(int **)malloc(16); g_p1=malloc(20); //付给了全局变量, 内存可以访问 g_p2=(int*)malloc(30); g_p2++; //付给了全局变量, 内存可以访问,但是指针被移动过,为可能丢失 p[1]=(int *)malloc(40); //如果p丢失了,则p[1]为间接丢失. return p;&#125;int main(int argc, char *argv[])&#123; int **p=fun1();// free(g_p1); //如果不free, 将会有 still reachable 内存泄露// free(--g_p2);//如果不free, 将会有 possibly lost 内存泄露// free(p[1]); //如果不free, 将会有 indirectly lost 内存泄露// free(p); //如果不free, 将会有 definitely lost内存泄露 return 0;&#125; 执行编译命令g++ val_test.cpp -o v, 然后 当执行valgrind ./v 命令之后的简易内存错误报告 : ==4765== Memcheck, a memory error detector ==4765== Copyright (C) 2002-2013, and GNU GPL&apos;d, by Julian Seward et al. ==4765== Using Valgrind-3.10.1 and LibVEX; rerun with -h for copyright info ==4765== Command: ./v ==4765== ==4765== ==4765== HEAP SUMMARY: ==4765== in use at exit: 106 bytes in 4 blocks ==4765== total heap usage: 4 allocs, 0 frees, 106 bytes allocated ==4765== ==4765== LEAK SUMMARY: ==4765== definitely lost: 16 bytes in 1 blocks ==4765== indirectly lost: 40 bytes in 1 blocks ==4765== possibly lost: 30 bytes in 1 blocks ==4765== still reachable: 20 bytes in 1 blocks ==4765== suppressed: 0 bytes in 0 blocks ==4765== Rerun with --leak-check=full to see details of leaked memory ==4765== ==4765== For counts of detected and suppressed errors, rerun with: -v ==4765== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) b@b-VirtualBox:~/tc/valgrind_test$ valgrind --leak-check=full valgrind: no program specified valgrind: Use --help for more information. 当执行valgrind --leak-check=full ./v 命令之后的详细内存错误报告 : ==4767== Memcheck, a memory error detector ==4767== Copyright (C) 2002-2013, and GNU GPL&apos;d, by Julian Seward et al. ==4767== Using Valgrind-3.10.1 and LibVEX; rerun with -h for copyright info ==4767== Command: ./v ==4767== ==4767== ==4767== HEAP SUMMARY: ==4767== in use at exit: 106 bytes in 4 blocks ==4767== total heap usage: 4 allocs, 0 frees, 106 bytes allocated ==4767== ==4767== 30 bytes in 1 blocks are possibly lost in loss record 2 of 4 ==4767== at 0x4C2AB80: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so) ==4767== by 0x40055E: fun1() (val_test.cpp:12) ==4767== by 0x4005AB: main (val_test.cpp:20) ==4767== ==4767== 56 (16 direct, 40 indirect) bytes in 1 blocks are definitely lost in loss record 4 of 4 ==4767== at 0x4C2AB80: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so) ==4767== by 0x40053F: fun1() (val_test.cpp:10) ==4767== by 0x4005AB: main (val_test.cpp:20) ==4767== ==4767== LEAK SUMMARY: ==4767== definitely lost: 16 bytes in 1 blocks ==4767== indirectly lost: 40 bytes in 1 blocks ==4767== possibly lost: 30 bytes in 1 blocks ==4767== still reachable: 20 bytes in 1 blocks ==4767== suppressed: 0 bytes in 0 blocks ==4767== Reachable blocks (those to which a pointer was found) are not shown. ==4767== To see them, rerun with: --leak-check=full --show-leak-kinds=all ==4767== ==4767== For counts of detected and suppressed errors, rerun with: -v ==4767== ERROR SUMMARY: 2 errors from 2 contexts (suppressed: 0 from 0) 总结 由局部变量指向的内存,如果不释放为肯定丢失, 由此指针而引起的后续内存泄露,为间接丢失. 由全局变量指向的内存如果不被释放,为still reachable, 如果该变量改动过, 为可能丢失. 是啊,局部变量是栈变量,如果你不能把这个栈变量处理好,出了这个函数,指针地址就丢失了,这时那是肯定丢失了. 如果你付给的地址是全局变量,倒是可以访问,叫still reachable 但是如果你这个全局变量的值改动过, 那只有你知道怎样正确访问这块内存,别人可能就访问不到了,这叫可能丢失. 由肯定丢失而引起的进一步的内存丢失为间接丢失. 解决内存泄漏的顺序所以碰到问题你首先要解决什么问题? 肯定丢失,然后是可能丢失,然后间接丢失,然后still reachable!!!]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>Valgrind</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程开发的一些基本概念]]></title>
    <url>%2F2015%2F04%2F27%2Fmulti_thread_dev_base_concept%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[竞态（race condition）软件层面上，竞态是指多个线程或进程读写一个共享资源 (或共享设备) 时的输出结果依赖于线程或进程的先后执行顺序或者时间；（ 更权威的介绍可以看 wiki ) 至于为什么会发生竞态呢？很简单，因为并发，并发使多线程，多进程环境变成可能。 竞态具体场景：假如我们有 2 个进程会对一个全局变量进行 ++ 操作，理想时，程序会这样执行： Thread 1Thread 2 Integer value 0read value ←0increase value 0write back →1 read value←1 increase value 1 write back→2 然而，由于并发的普遍存在，使得情况有时” 不受控制”（不如工程师预期那样工作），可能会变成这样： Thread 1Thread 2 Integer value 0read value ←0 read value←0increase value 0 increase value 0write back →1 write back→1 并发（concurrency）并发 (concurrency) 指的是多个执行单元同时、并行被执行。而并发的执行单元对共享资源 (硬件资源和软件上的全局、静态变量) 的访问则容易导致竞态 (race conditions), 可能导致并发 (即竞态?) 的情况有： SMP（Symmetric Multi-Processing），对称多处理结构。SMP 是一种紧耦合、共享存储的系统模型，它的特点是多个 CPU 使用共同的系统总线，因此可访问共同的外设和存储器。 中断. 中断可以打断正在执行的进程 (哪怕是在中断上下文)，若中断处理程序对共享资源进程访问，则竞态也会发生. 内核抢占.2.6 以后内核提供了内核可抢占特性，虽然是作为一个配置选项，但我们写程序时还是要考虑周全，故内核抢占也是作为伪并发的表现，也可能发生竞态； 临界区（critical section）多个线程进程对共享资源进行访问在软件表现为一个程序片段，如何避免竞态的发生呢？ 一个执行路径在对共享资源进行访问时禁止其他执行路径进行访问，当有一个执行路径（A）对共享资源进行访问时，如有其他执行路径想访问共享资源，须睡眠等待 A 执行路径退出。 那么这时这个程序片段就是临界区。那么具体如何来实现临界区呢？linux 内核提供了多种同步互斥机制.（如信号量，互斥量，自旋锁，RCU，原子操作等）. 什么是RAII技术我们在C++中经常使用new申请了内存空间，但是却也经常忘记delete回收申请的空间，容易造成内存溢出，于是RAII技术就诞生了，来解决这样的问题。 RAII（Resource Acquisition Is Initialization）机制是Bjarne Stroustrup首先提出的，是一种利用对象生命周期来控制程序资源（如内存、文件句柄、网络连接、互斥量等等）的简单技术。 我们知道在函数内部的一些成员是放置在栈空间上的，当函数返回时，这些栈上的局部变量就会立即释放空间，于是Bjarne Stroustrup就想到确保能运行资源释放代码的地方就是在这个程序段（栈）中放置的对象的析构函数了，因为stack winding会保证它们的析构函数都会被执行。RAII就利用了栈里面的变量的这一特点。 RAII 的一般做法是这样的：在对象构造时获取资源，接着控制对资源的访问使之在对象的生命周期内始终保持有效，最后在对象析构的时候释放资源。 借此，我们实际上把管理一份资源的责任托管给了一个存放在栈空间上的局部对象。这种做法有两大好处： (1)不需要显式地释放资源。 (2)采用这种方式，对象所需的资源在其生命期内始终保持有效。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Linux</tag>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL入门三之GroupBy]]></title>
    <url>%2F2015%2F04%2F17%2FMySQL%E5%85%A5%E9%97%A8%E4%B8%89%E4%B9%8BGroupBy%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[SQL GROUP BY 实例我们拥有下面这个 “Orders” 表： O_Id OrderDate OrderPrice Customer 1 2008/12/29 1000 Bush 2 2008/11/23 1600 Carter 3 2008/10/05 700 Bush 4 2008/09/28 300 Bush 5 2008/08/06 2000 Adams 6 2008/07/21 100 Carter 现在，我们希望查找每个客户的总金额（总订单）。我们想要使用 GROUP BY 语句对客户进行组合。我们使用下列 SQL 语句： SELECT Customer,SUM(OrderPrice) FROM Orders GROUP BY Customer 结果集类似这样： Customer SUM(OrderPrice) Bush 2000 Carter 1700 Adams 2000 很棒吧，对不对？让我们看一下如果省略 GROUP BY 会出现什么情况： SELECT Customer,SUM(OrderPrice) FROM Orders 结果集类似这样： Customer SUM(OrderPrice) Bush 5700 Carter 5700 Bush 5700 Bush 5700 Adams 5700 Carter 5700 上面的结果集不是我们需要的。那么为什么不能使用上面这条 SELECT 语句呢？ 解释如下：上面的 SELECT 语句指定了两列（Customer 和 SUM(OrderPrice)）。“SUM(OrderPrice)” 返回一个单独的值（”OrderPrice” 列的总计），而 “Customer” 返回 6 个值（每个值对应 “Orders” 表中的每一行）。因此，我们得不到正确的结果。不过，您已经看到了，GROUP BY 语句解决了这个问题。]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对象模型之内存对齐基础]]></title>
    <url>%2F2015%2F04%2F12%2Fsizeof_struct%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[本文不讨论类的虚函数, 请参考 C++对象模型之虚函数讲解 内存对齐规则首先我们明确内存对齐规则 我们设 A = #pragma pack()指定的数 B = 这个数据成员的自身长度 C = 结构(或联合)中最大数据成员长度 在解释内存对齐的作用前，先来看下内存对齐的规则： 1. 对于结构的各个成员，第一个成员位于偏移为0的位置，以后每个数据成员的偏移量必须是 min( A，B ) 的倍数。 2. 在数据成员完成各自对齐之后，结构(或联合)本身也要进行对齐，对齐将按照 min( A, C) 进行。 问题32位机器上, 下列代码的sizeof(a)的值是多少? 12345678910111213141516171819202122#pragma pack(2)class A&#123; int i; union U &#123; char buff[13]; int i; &#125;u; void foo() &#123; &#125; typedef char* (*f)(void*); enum&#123; red, green, blue &#125; color;&#125;a;#pragma pack() 答案答案是sizeof(a)的值为22. void foo() { } ，typedef char* (f)(void);不占字节， 枚举占4个字节， union按最大的变量所占字节算，占14个字节， int占4个字节， 4+14+4=22。 如果把#pragma pack(2)改为 #pragma pack(4)， sizeof(a)的值就为 24。 解析分为三部分来解析: 枚举所占内存计算方法 #pragma pack用法 共用体(union)所占内存计算方法 枚举所占内存计算方法枚举变量，由枚举类型定义的变量。枚举变量的大小，即枚举类型所占内存的大小。 由于枚举变量的赋值，一次只能存放枚举结构中的某个常数。 所以枚举变量的大小，实质是常数所占内存空间的大小（常数为int类型，当前主流的编译器中一般是32位机器和64位机器中int型都是4个字节），枚举类型所占内存大小也是这样。 #pragma pack用法#pragma pack(a)规定的对齐长度（a可选值为1，2，4，8，16），实际使用的规则是：结构，联合，或者类的数据成员，第一个放在偏移为0的地方，以后每个数据成员的对齐，按照#pragma pack指定的数值和这个数据成员自身长度中，比较小的那个进行。也就是说，当#pragma pack的值等于或超过所有数据成员长度的时候，这个值的大小将不产生任何效果。而结构整体的对齐，则按照结构体中最大的数据成员 和 #pragma pack指定值 之间，较小的那个进行。而 #pragma pack() 表示恢复默认的内存对齐（与#pragma pack(a)指令配对使用） 12345678910111213141516#pragma pack(4)class TestB&#123;public: int aa; //第一个成员，放在[0,3]偏移的位置， char a; //第二个成员，自身长为1，#pragma pack(4),取小值，也就是1，所以这个成员按一字节对齐，放在偏移[4]的位置。 short b; //第三个成员，自身长2，#pragma pack(4)，取2，按2字节对齐，所以放在偏移[6,7]的位置。 char c; //第四个，自身长为1，放在[8]的位置。&#125;;#pragma pack() 这个类实际占据的内存空间是9字节类之间的对齐，是按照类内部最大的成员的长度，和#pragma pack规定的值之中较小的一个对齐的。所以这个例子中，类之间对齐的长度是min(sizeof(int),4)，也就是4。9按照4字节圆整的结果是12，所以sizeof(TestB)是12。 如果12345678910#pragma pack(2)class TestB&#123;public: int aa; //第一个成员，放在[0,3]偏移的位置， char a; //第二个成员，自身长为1，#pragma pack(2),取小值，也就是1，所以这个成员按一字节对齐，放在偏移[4]的位置。 short b; //第三个成员，自身长2，#pragma pack(2)，取2，按2字节对齐，所以放在偏移[6,7]的位置。 char c; //第四个，自身长为1，放在[8]的位置。&#125;;#pragma pack() 可以看出，上面的位置完全没有变化，只是类之间改为按2字节对齐，9按2圆整的结果是10。所以 sizeof(TestB)是10。 现在去掉第一个成员变量为如下代码：123456789#pragma pack(4)class TestC&#123;public: char a;//第一个成员，放在[0]偏移的位置， short b;//第二个成员，自身长2，#pragma pack(4)，取2，按2字节对齐，所以放在偏移[2,3]的位置。 char c;//第三个，自身长为1，放在[4]的位置。&#125;;#pragma pack() 整个类的大小是5字节，按照min(sizeof(short),4)字节对齐，也就是2字节对齐，结果是6，所以sizeof(TestC)是6。 共用体(union)所占内存计算方法共用体又名”联合体”, 英文名为union. 当多个数据需要共享内存或者多个数据每次只取其一时，可以利用联合体(union)。在C Programming Language 一书中对于联合体是这么描述的： 联合体是一个结构； 它的所有成员相对于基地址的偏移量都为0； 此结构空间要大到足够容纳最”宽”的成员； 其对齐方式要适合其中所有的成员； 下面解释这四条描述： 由于联合体中的所有成员是共享一段内存的，因此每个成员的存放首地址相对于于联合体变量的基地址的偏移量为0，即所有成员的首地址都是一样的。为了使得所有成员能够共享一段内存，因此该空间必须足够容纳这些成员中最宽的成员。对于这句“对齐方式要适合其中所有的成员”是指其必须符合所有成员的自身对齐方式。 下面举例说明： 123456union U&#123; char s[9]; int n; double d;&#125;; s占9字节，n占4字节，d占8字节，因此其至少需9字节的空间。然而其实际大小并不是9，用运算符sizeof测试其大小为16.这是因为这里存在字节对齐的问题，9既不能被4整除，也不能被8整除。 因此补充字节到16，这样就符合所有成员的自身对齐了。从这里可以看出联合体所占的空间不仅取决于最宽成员，还跟所有成员有关系，即其大小必须满足两个条件： 大小足够容纳最宽的成员； 大小能被其包含的所有基本数据类型的大小所整除。 若问题为#pragma pack(4)的情况 void foo() { } ，typedef char* (f)(void);不占字节， 枚举占4个字节， union按最大的变量buff[13]所占字节算为13, 在#pragma pack(2)的情况, 得补齐1个字节变为14才能被2整除, 而#pragma pack(4)的情况得补齐3个字节, 总占16个字节，才可以被4整除, int占4个字节 所以#pragma pack(4)的情况, sizeof(A)为4+16+4=24。 练习注意有陷阱, 32位环境下 1234567891011121314151617181920212223242526272829303132# pragma pack(2)class test_class&#123;public: static float i; union test_union &#123; int bb; char aa[13]; short cc; &#125;; enum test_enum &#123; monday, tuesday, sunday &#125;; virtual void testFunc() &#123;&#125; char xmly;&#125;;# pragma pack()int main()&#123; cout &lt;&lt; "sizeof(test_class) : " &lt;&lt; sizeof(test_class) &lt;&lt; endl; return 0;&#125; 请问打印结果? sizeof(test_class) : 6 为什么呢?注意看共用体 test_union 和枚举 test_enum其实并没有声明变量, 如果写成1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;iostream&gt;using namespace std;# pragma pack(2)class test_class&#123;public: static float i; union test_union &#123; int bb; char aa[13]; short cc; &#125;uVar; enum test_enum &#123; monday, tuesday, sunday &#125;eVar; virtual void testFunc() &#123;&#125; char xmly;&#125;;# pragma pack()enum enum_x &#123; x1=5, x2, x3, x4, &#125;; enum enum_x x=x3; int main()&#123; cout &lt;&lt; "sizeof(test_class) : " &lt;&lt; sizeof(test_class) &lt;&lt; endl; cout &lt;&lt; "x : " &lt;&lt; x &lt;&lt; endl; test_class::test_enum i; i = test_class::monday; cout &lt;&lt; "i : " &lt;&lt; i &lt;&lt; endl; test_class test_obj; test_obj.eVar = test_class::sunday; cout &lt;&lt; test_obj.monday &lt;&lt; endl; cout &lt;&lt; test_class::sunday &lt;&lt; endl; return 0;&#125; 打印结果就为 12345sizeof(test_class) : 24x : 7i : 002]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>noodle</tag>
        <tag>ObjectModel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GCC的原子操作函数]]></title>
    <url>%2F2015%2F04%2F11%2Flinux_gcc_atomic%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[linux支持的哪些操作是具有原子特性的？知道这些东西是理解和设计无锁化编程算法的基础。 下面的东西整理自网络。先感谢大家的分享！ 原子操作的api函数gcc从4.1.2以后提供了 __sync_* 系列的下面几类的内嵌函数，提供用于针对数字或布尔型变量的原子操作。 n++类这组返回更新前的值 type __sync_fetch_and_add (type *ptr, type value, ...) type __sync_fetch_and_sub (type *ptr, type value, ...) type __sync_fetch_and_or (type *ptr, type value, ...) type __sync_fetch_and_and (type *ptr, type value, ...) type __sync_fetch_and_xor (type *ptr, type value, ...) type __sync_fetch_and_nand (type *ptr, type value, ...) ++n类这组返回更新后的值 type __sync_add_and_fetch (type *ptr, type value, ...) type __sync_sub_and_fetch (type *ptr, type value, ...) type __sync_or_and_fetch (type *ptr, type value, ...) type __sync_and_and_fetch (type *ptr, type value, ...) type __sync_xor_and_fetch (type *ptr, type value, ...) type __sync_nand_and_fetch (type *ptr, type value, ...) type可以是1,2,4或8字节长度的int类型，即： int8_t / uint8_t int16_t / uint16_t int32_t / uint32_t int64_t / uint64_t 后面的可扩展参数(…)用来指出哪些变量需要memory barrier,因为目前gcc实现的是full barrier（类似于linux kernel 中的mb(),表示这个操作之前的所有内存操作不会被重排序到这个操作之后）,所以可以略掉这个参数。 CAS类CAS 即 compare-and-swap , 下面这两个函数提供原子的比较和交换，如果 *ptr == oldval,就将 newval 写入 *ptr 此函数在相等并写入的情况下返回 true bool __sync_bool_compare_and_swap (type *ptr, type oldval, type newval, ...) /* 对应的伪代码 */ { if (*ptr == oldval) { *ptr = newval; return true; } else { return false; } } 此函数在返回 oldval type __sync_val_compare_and_swap (type *ptr, type oldval, type newval, ...) /* 对应的伪代码 */ { if (*ptr == oldval) { *ptr = newval; } return oldval; } 其他原子操作type __sync_lock_test_and_set (type *ptr, type value, ...) 将 *ptr 设为value并返回 *ptr 操作之前的值。 void __sync_lock_release (type *ptr, ...) 将 *ptr 置 0 内存栅障内存栅障主要是处理不同cpu运作时（主要是执行不同线程代码时），一个cpu对内存的操作的原子性，也就保证其他cpu见到的内存单元数据的正确性。 内存栅障介绍如果有对某一变量上写锁， 就不能在不获得相应的锁时对其进行读取操作。内存栅的作用在于保证内存操作的相对顺序， 但并不保证内存操作的严格时序。 内存栅并不保证 CPU 将本地快取缓存或存储缓冲的内容刷写回内存， 而是在锁释放时确保其所保护的数据， 对于能看到刚释放的那个锁的 CPU 或设备可见。持有内存栅的 CPU 可以在其快取缓存或存储缓冲中将数据保持其所希望的、 任意长的时间， 但如果其它 CPU 在同一数据元上执行原子操作，则第一个 CPU 必须保证， 其所更新的数据值， 以及内存栅所要求的任何其它操作， 对第二个 CPU 可见。 __sync_synchronize (...) 发出一个full barrier. 内存栅障应用对于执行一条指令，操作到4个寄存器时，如： write1(dev.register_size,size); write1(dev.register_addr,addr); write1(dev.register_cmd,READ); write1(dev.register_control,GO); 最后一个寄存器是控制寄存器，在所有的参数都设置好之后向其发出指令，设备开始读取参数. 如果最后一条write1被换到了前几条语句之前，那么肯定不是我们所期望的，这时候我们可以在最后一条语句之前加入一个memory barrier,强制cpu执行完前面的写入以后再执行最后一条： write1(dev.register_size,size); write1(dev.register_addr,addr); write1(dev.register_cmd,READ); __sync_synchronize(); write1(dev.register_control,GO); memory barrier有几种类型： acquire barrier : 不允许将barrier之后的内存读取指令移到barrier之前（linux kernel中的wmb()）。 release barrier : 不允许将barrier之前的内存读取指令移到barrier之后 (linux kernel中的rmb())。 full barrier : 以上两种barrier的合集(linux kernel中的mb())。 原子操作应用范围原子操作只允许一次更新或读一个内存单元。 需要原子地更新多个单元时， 就必须使用锁来代替它了。 例如， 如果需要更新两个相互关联的计数器时， 就必须使用锁， 而不是两次单独的原子操作了。 原子操作例子例子代码： 123456789101112131415161718192021222324252627282930#include &lt;stdio.h&gt; #include &lt;pthread.h&gt; #include &lt;stdlib.h&gt; static int count = 0; void *test_func(void *arg) &#123; int i=0; for(i=0;i&lt;20000;++i)&#123; __sync_fetch_and_add(&amp;count,1); &#125; return NULL; &#125; int main(int argc, const char *argv[]) &#123; pthread_t id[20]; int i = 0; for(i=0;i&lt;20;++i)&#123; pthread_create(&amp;id[i],NULL,test_func,NULL); &#125; for(i=0;i&lt;20;++i)&#123; pthread_join(id[i],NULL); &#125; printf("%d\n",count); return 0; &#125; 原子操作封装使用根据常用的原子操作，封装一些实用的接口 : 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697//原子操作 //设置值 int lock_set(volatile int &amp;a, int value) &#123; __sync_val_compare_and_swap(&amp;a, a, value); return a; &#125; //加1 int lock_inc(volatile int &amp;n) &#123; return __sync_fetch_and_add(&amp;n, 1); &#125; //减1 int lock_dec(volatile int &amp;n) &#123; return __sync_fetch_and_sub(&amp;n, 1); &#125; //加值value int lock_add(volatile int &amp;n, int value) &#123; return __sync_fetch_and_add(&amp;n, value); &#125; //减值value int lock_sub(volatile int &amp;n, int value) &#123; return __sync_fetch_and_sub(&amp;n, value); &#125; //位或value int lock_or(volatile int &amp;n, int value) &#123; return __sync_fetch_and_or(&amp;n, value); &#125; //位与value int lock_and(volatile int &amp;n, int value) &#123; return __sync_fetch_and_and(&amp;n, value); &#125; //异或value int lock_xor(volatile int &amp;n, int value) &#123; return __sync_fetch_and_xor(&amp;n, value); &#125; //无符号类型（函数重载） //设置值 unsigned int lock_set(volatile unsigned int &amp;a, unsigned int value) &#123; __sync_val_compare_and_swap(&amp;a, a, value); return a; &#125; //加1 unsigned int lock_inc(volatile unsigned int &amp;n) &#123; return __sync_fetch_and_add(&amp;n, 1); &#125; //减1 unsigned int lock_dec(volatile unsigned int &amp;n) &#123; return __sync_fetch_and_sub(&amp;n, 1); &#125; //加值value unsigned int lock_add(volatile unsigned int &amp;n, unsigned int value) &#123; return __sync_fetch_and_add((int*)&amp;n, value); &#125; //减值value unsigned int lock_sub(volatile unsigned int &amp;n, unsigned int value) &#123; return __sync_fetch_and_sub((int*)&amp;n, value); &#125; //位或value unsigned int lock_or(volatile unsigned int &amp;n, unsigned int value) &#123; return __sync_fetch_and_or((int*)&amp;n, value); &#125; //位与value unsigned int lock_and(volatile unsigned int &amp;n, unsigned int value) &#123; return __sync_fetch_and_and((int*)&amp;n, value); &#125; //异或value unsigned int lock_xor(volatile unsigned int &amp;n, unsigned int value) &#123; return __sync_fetch_and_xor((int*)&amp;n, value); &#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Linux</tag>
        <tag>MultiThread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GCC的分支预测优化__builtin_expect]]></title>
    <url>%2F2015%2F04%2F11%2Flinux_gcc_builtin_expect%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[1. 为什么需要分支预测优化 将流水线引入cpu，可以提高cpu的效率。更简单的说，让cpu可以预先取出下一条指令，可以提供cpu的效率。如下图所示： 取指令 执行指令 输出结果 取指令 执行 可见，cpu流水钱可以减少cpu等待取指令的耗时，从而提高cpu的效率。如果存在跳转指令，那么预先取出的指令就无用了。cpu在执行当前指令时，从内存中取出了当前指令的下一条指令。执行完当前指令后，cpu发现不是要执行下一条指令,而是执行offset偏移处的指令。cpu只能重新从内存中取出offset偏移处的指令。因此，跳转指令会降低流水线的效率，也就是降低cpu的效率。 综上，在写程序时应该尽量避免跳转语句。那么如何避免跳转语句呢？答案就是使用__builtin_expect。 这个指令是gcc引入的，作用是”允许程序员将最有可能执行的分支告诉编译器”。 这个指令的写法为：builtin_expect(EXP, N)。意思是：EXP==N的概率很大。一般的使用方法是将builtin_expect指令封装为LIKELY和UNLIKELY宏。这两个宏的写法如下。 12#define LIKELY(x) __builtin_expect(!!(x), 1) //x很可能为真#define UNLIKELY(x) __builtin_expect(!!(x), 0) //x很可能为假 在很多源码如Linux内核、Glib等,我们都能看到likely()和unlikely()这两个宏,通常这两个宏定义是下面这样的形式。可以看出这2个宏都是使用函数 builtin_expect()实现的, builtin_expect()函数是GCC的一个内建函数(build-in function). 2. 函数声明函数__builtin_expect()是GCC v2.96版本引入的, 其声明如下: long __builtin_expect(long exp, long c); 2.1. 功能描述由于大部分程序员在分支预测方面做得很糟糕，所以GCC 提供了这个内建函数来帮助程序员处理分支预测. 你期望 exp 表达式的值等于常量 c, 看 c 的值, 如果 c 的值为0(即期望的函数返回值), 那么 执行 if 分支的的可能性小, 否则执行 else 分支的可能性小(函数的返回值等于第一个参数 exp). GCC在编译过程中，会将可能性更大的代码紧跟着前面的代码，从而减少指令跳转带来的性能上的下降, 达到优化程序的目的. 通常，你也许会更喜欢使用 gcc 的一个参数 ‘-fprofile-arcs’ 来收集程序运行的关于执行流程和分支走向的实际反馈信息,但是对于很多程序来说,数据是很难收集的。 2.2. 参数详解 exp exp 为一个整型表达式, 例如: (ptr != NULL) c c 必须是一个编译期常量, 不能使用变量 2.3. 返回值 返回值等于 第一个参数 exp 2.4. 使用方法与关键字if一起使用.首先要明确一点就是 if (value) 等价于 if (__builtin_expert(value, x)), 与x的值无关. 例子如下: 例子1 : 期望 x == 0, 所以执行func()的可能性小 12345678if (__builtin_expect(x, 0))&#123; func();&#125;else&#123; //do someting&#125; 例子2 : 期望 ptr !=NULL这个条件成立(1), 所以执行func()的可能性小 12345678if (__builtin_expect(ptr != NULL, 1))&#123; //do something&#125;else&#123; func();&#125; 例子3 : 引言中的likely()和unlikely()宏 首先,看第一个参数!!(x), 他的作用是把(x)转变成”布尔值”, 无论(x)的值是多少 !(x)得到的是true或false, !!(x)就得到了原值的”布尔值” 使用 likely() ，执行 if 后面的语句 的机会更大，使用 unlikely()，执行 else 后面的语句的机会更大。 12345678910111213141516171819202122#define likely(x) __builtin_expect(!!(x), 1)#define unlikely(x) __builtin_expect(!!(x), 0)int main(char *argv[], int argc)&#123; int a; /* Get the value from somewhere GCC can't optimize */ a = atoi (argv[1]); if (unlikely (a == 2)) &#123; a++; &#125; else &#123; a--; &#125; printf ("%d\n", a); return 0;&#125; 3. RATIONALE(原理)if else 句型编译后, 一个分支的汇编代码紧随前面的代码,而另一个分支的汇编代码需要使用JMP指令才能访问到. 很明显通过JMP访问需要更多的时间, 在复杂的程序中,有很多的if else句型,又或者是一个有if else句型的库函数,每秒钟被调用几万次, 通常程序员在分支预测方面做得很糟糕, 编译器又不能精准的预测每一个分支,这时JMP产生的时间浪费就会很大, 函数 __builtin_expert() 就是用来解决这个问题的.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能指针笔记]]></title>
    <url>%2F2015%2F04%2F11%2Fsmart_pointer_note%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[有些错误是编译器查不到的, 这种错误是最可怕的, 当项目大了之后,即使用 Valgrind 也很难定位,因为裸指针在团队合作中使用很容易导致其他成员忘记释放或多次释放, 所以在团队合作中一般使用智能指针. 而智能指针用的不好, 结果可能适得其反. 所以我们聊一下智能指针的几点注意事项. 总结自 C++ Primer. 一个简单的包含删除器的例子演示12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include&lt;iostream&gt;#include&lt;functional&gt;#include&lt;memory&gt;using std::cout;using std::endl;using std::bind;using namespace std::placeholders;int testBind( int* a, int b, int c )&#123; cout &lt;&lt; *a + b + c &lt;&lt; endl; return *a;&#125;struct Foo&#123; Foo() = default; Foo( const Foo &amp; a ) &#123; data = a.data; std::cout &lt;&lt; "复制构造" &lt;&lt; std::endl; &#125; void print_sum( int n1, int n2 ) &#123; std::cout &lt;&lt; n1 + n2 &lt;&lt; '\n'; &#125; int data = 10;&#125;;int main()&#123; //绑定类成员函数用对象的指针 Foo foo; auto f3 = std::bind( &amp;Foo::print_sum, &amp;foo, 95, _1 ); f3( 5 ); auto check_testBind = std::bind( testBind, std::placeholders::_1, 3, 9 ); int * p = new int( 7 ); cout &lt;&lt; check_testBind( p ) &lt;&lt; endl; shared_ptr&lt;int&gt; pi( new int(), check_testBind ); *pi = 88; shared_ptr&lt;int&gt; pii( new int( 12 ), std::bind( testBind, std::placeholders::_1, 32, 19 ) ); std::function&lt; int( int* ) &gt; test_function_bind = std::bind( testBind, std::placeholders::_1, 331, 9 ); cout &lt;&lt; "test_function_bind( p, 331, 9 ) = " &lt;&lt; test_function_bind( p ) &lt;&lt; endl;; shared_ptr&lt;int&gt; piii( new int( 112 ), test_function_bind ); return 0;&#125; 打印结果 : 100 119 7 347 test_function_bind( p, 331, 9 ) = 7 452 63 100 请按任意键继续. . . 智能指针陷阱智能指针可以提供对动态分配的内存安全而又方便的管理，但这建立在正确使用的前提下 。 为了正确使用智能指针，我们必须坚持一些基本规范 : 不使用相同的内置指针值初始化(或 reset) 多个智能指针 。 不 delete get ( ) 返 回的指针 。 不使用 get () 初始化或 reset 另 一 个智能指针 。 如果你使用 get () 返 回的指针，记住当最后一个对应的智能指针销 毁 后， 你 的指 针就 变为无 效 了 。 如果你使用智能指针管理的资源不是 new 分配的内存 ， 记住传递给它一个删除器( 参见 12. 1.4 节 ， 第 415 页和 12. 1.5 节 ， 第 419 页)。 尽量用make_shared而非newshared_ptr 可以协调对象的析构 ， 但这仅限于其自身的拷贝 ( 也 是 shared_ptr)之间。 这也是为什么我们推荐使用 make_shared 而不是 new 的原因 。 这样 ， 我们就能在分配对象的同时就将 shared_ptr 与之绑定，从而避免了无意中将同一块内存绑定到多个独立创建的 shared_ptr 上 。(这是最容易犯的错) 总结 : 所以我们要尽量一开始就用make_shared来分配动态内存, 而不是先new一个出来, 再找机会将它转为智能指针. 考虑下面对 shared_ptr 进行操作的函数 : 12345// 在函数被调用时 ptr 被创建并初始化void process(shared_ptr&lt;int&gt; ptr)&#123; // 使用 ptr&#125; // ptr 离 开作用域，被销毁 process 的参数是传值方式传递的，因此实参会被拷贝到 ptr 巾 。 拷贝 一 个 shared_ptr会递增其引用讨数，因此， 在 process 运行过程中，引用七| 数值至少为 2 。 当 process结束时， ptr 的引用计数会边喊，但不会变为 0 。 因此，当用音11变 11 ptr 被销毁时， ptr指向的内存不会被释放 。 使用此函数的正确方法是传递给它一个 shared_ptr : 123shared_ptr&lt;int&gt; p(new int(42)) ; // 引用计数为 1process(p); // 拷贝 p 会递增它的引用计数 ;在 process 中引用计数位为 2int i = *p; // 正确:引用计数位为 1 虽然不能传递给 process 一 个内置指针，但可以传递 给它 一 个(临时的)shared_ptr ， 这个 shared_ptr 是用 一个内 置指针显式构造的 。 但是，这样做很可能会导致错误 : 1234int *x(new int(1024)); // 危险 x 是一个普通指针，不是一个智能指针process(x) ; // 错误 : 不能将 int* 转换为 一个 shared_ptr&lt;int&gt;process(shared_ptr&lt;int&gt;(x)); // 合法的，但内存会被释放!int j = *x ; //未定义的 x 是一个空悬指针! 在上面的调用中 ， 我们将一个临时 shared_ptr 传递给 process 。 当这个调用所在的表达式结束时，这个临时对象就被销毁了 。 销毁这个临时变量会递减引用计数，此时引用计数就变为 0 了 。 因此，当临时对象被销毁时 ， 它所指向的内存会被释放 。但 x 继续指 向 (已经释放的)内存，从而变成一个空悬指针。如果试图使用 x 的值，其行为是未定义的 。 当将一个 shared_ptr 绑定到一个普通指针时 ， 我们就将内存的管理责任交给了这个 shared_ptr 一旦这样做 了 ， 我们就不应该再使用内置指针来访问 shared_ptr 所指向的内存了 不要使用 get 初始化另一个智能指针或为智能指针赋值智能指针类型定义了 一个名为 get 的函数(参见表 1 2. J )，它返回一个内置指针，指向智能指针管理的对象 。 此函数是为了这样一种情况而设计的 : 我们需要向不能使用智能指针的代码传递一个内置指针。使用 get 返回的指告| 的代码不能 delete 此指针 。虽然编译器不会给出错误信息 ， 但将另一个智能指针也绑定到 get 返回的指针上是错误的 : 1234567shared_ptr&lt;int&gt; p(new int(42)) ; // 引 用计数为 1int *q = p . get() ; // 正确 · 但使用 q 时妥注意，不要让它管理的指针被释放&#123; // 新程序块// 未定义:两个独立的 shared_ptr 指 向 相同的内存 shared ptr&lt;int&gt; (q) ;&#125; // 程序块结束， q 被销毁 ， 它指向的内存被待放int foo = *p ; // 未定义 p 指向的内存 已 经被释放了 在本例中， p 和 q 指 向相同的内存。由于它们是相互独立创建的，因此各自的引用计数都是 1。 当 q 所在的程序块结束时 ， q 被销毁 ， 这会导致 q 指向的内存被释放 。 从而 p 变成一个空悬指针，意味着当我们试图使用 p 时，将发生未定义的行为 。 而且 ， 当 p 被销毁时 ，这块内存会被第二次 delete 。 get 用来将指针的访问权限传递给代码，你只有在确定代码不会 get.特别是，永远不要用 get 初始化另一个智能指针 del ete 指针或者为另一个智能指针赋值.]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux一些不要想当然的事(一)之目录权限]]></title>
    <url>%2F2015%2F03%2F18%2Flinux_directory_permission%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[目录的可读/可写/可执行权限不要把目录的这几个权限和档案的这几个权限混淆了, 不要想当然的以为是差不多的, 差很多!记忆技巧 : 档案的rwx是针对于档案的内容来设计的, 而目录的rwx是针对于目录的文件名列表来设计的 . . . 目录可读r 目录可读权限r : 只能获得文件列表 特别注意:如果一个目录为非空, 却没有r权限, 即使你有wx的权限, 你用rm -r也是删不掉的, 因为没有r权限拿不到这个目录的文件列表, rm -r 自然也就不晓得要删除什么东西了.只有求助root了 123456789101112131415161718192021b@b-VirtualBox:~/my_temp_test/abc$ mkdir tempb@b-VirtualBox:~/my_temp_test/abc$ touch temp/ddb@b-VirtualBox:~/my_temp_test/abc$ ls tempddb@b-VirtualBox:~/my_temp_test/abc$ chmod 444 tempb@b-VirtualBox:~/my_temp_test/abc$ ls templs: cannot access temp/dd: Permission deniedddb@b-VirtualBox:~/my_temp_test/abc$ cd temp/bash: cd: temp/: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ cat temp/dd cat: temp/dd: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ touch temp/yytouch: cannot touch ‘temp/yy’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ rm temp/dd rm: cannot remove ‘temp/dd’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ rm -r temprm: descend into write-protected directory ‘temp’? yrm: cannot remove ‘temp/dd’: Permission deniedrm: remove write-protected directory ‘temp’? yrm: cannot remove ‘temp’: Directory not empty 目录可写w 目录可写权限w : 代表可以在目录下增加或删除档案和目录和改名(但是必须得有目录可执行权限x的支持才可以, 所以一般有w就会有x) 不要和档案的可写权限混淆了, 即使没有目录可写权限, 有目录可执行x也是可以修改目录下的档案的, 只要拥有要修改的那个档案的可写权限既可. 但也要注意的是: 档案的w是针对于档案的内容来说的, 你可以编辑修改他的内容, 但是如果想删除这个档案, 你需要这个档案所在的目录的w权限. 1234567891011121314b@b-VirtualBox:~/my_temp_test/abc$ chmod 222 tempb@b-VirtualBox:~/my_temp_test/abc$ mkdir temp/uumkdir: cannot create directory ‘temp/uu’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ touch temp/ootouch: cannot touch ‘temp/oo’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ chmod 333 temp b@b-VirtualBox:~/my_temp_test/abc$ mkdir temp/uub@b-VirtualBox:~/my_temp_test/abc$ touch temp/oob@b-VirtualBox:~/my_temp_test/abc$ rm -r temprm: cannot remove ‘temp’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ rm -r temp/uub@b-VirtualBox:~/my_temp_test/abc$ rm temp/oob@b-VirtualBox:~/my_temp_test/abc$ ls templs: cannot open directory temp: Permission denied 目录可执行x 目录可执行权限x : 有进入目录的权限, 有在这个目录下执行命令的权限. 但不可以删除或者增加档案和目录(因为不具备目录的可写权限w) 1234567891011121314151617b@b-VirtualBox:~/my_temp_test/abc$ chmod 111 temp/b@b-VirtualBox:~/my_temp_test/abc$ ls templs: cannot open directory temp: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ echo &quot;xxd&quot; &gt; temp/ddb@b-VirtualBox:~/my_temp_test/abc$ cat temp/ddxxdb@b-VirtualBox:~/my_temp_test/abc$ touch temp/yytouch: cannot touch ‘temp/yy’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ rm temp/ddrm: cannot remove ‘temp/dd’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ rm -r temprm: descend into write-protected directory ‘temp’? yrm: remove write-protected directory ‘temp’? yrm: cannot remove ‘temp’: Permission deniedb@b-VirtualBox:~/my_temp_test/abc$ cd tempb@b-VirtualBox:~/my_temp_test/abc/temp$ lsls: cannot open directory .: Permission denied]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用运维命令(df和free和top)笔记整理(三)]]></title>
    <url>%2F2015%2F03%2F11%2Flinux_command_df_free_top%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[df df命令用于显示磁盘分区上的可使用的磁盘空间。默认显示单位为KB。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 -a或–all：包含全部的文件系统； –block-size=&lt;区块大小&gt;：以指定的区块大小来显示区块数目； -h或–human-readable：以可读性较高的方式来显示信息； -H或–si：与-h参数相同，但在计算时是以1000 Bytes为换算单位而非1024 Bytes； -i或–inodes：显示inode的信息； -k或–kilobytes：指定区块大小为1024字节； -l或–local：仅显示本地端的文件系统； -m或–megabytes：指定区块大小为1048576字节； –no-sync：在取得磁盘使用信息前，不要执行sync指令，此为预设值； -P或–portability：使用POSIX的输出格式； –sync：在取得磁盘使用信息前，先执行sync指令； -t&lt;文件系统类型&gt;或–type=&lt;文件系统类型&gt;：仅显示指定文件系统类型的磁盘信息； -T或–print-type：显示文件系统的类型； -x&lt;文件系统类型&gt;或–exclude-type=&lt;文件系统类型&gt;：不要显示指定文件系统类型的磁盘信息； –help：显示帮助； –version：显示版本信息 . . . df常用用法：df -h12345678910b@b-VirtualBox:~$ df -hFilesystem Size Used Avail Use% Mounted onudev 990M 4.0K 990M 1% /devtmpfs 201M 968K 200M 1% /run/dev/sda1 8.8G 4.1G 4.3G 49% /none 4.0K 0 4.0K 0% /sys/fs/cgroupnone 5.0M 0 5.0M 0% /run/locknone 1001M 76K 1001M 1% /run/shmnone 100M 36K 100M 1% /run/user/dev/sr0 57M 57M 0 100% /media/b/VBOXADDITIONS_5.1.22_115126 free free命令可以显示当前系统未使用的和已使用的内存数目，还可以显示被内核使用的内存缓冲区。 free常用用法：free -m或者free -g12345b@b-VirtualBox:~$ free -m total used free shared buffers cachedMem: 2000 1231 768 9 72 456-/+ buffers/cache: 702 1297Swap: 1021 0 1021 top PID：进程的ID USER：进程所有者 PR：进程的优先级别，越小越优先被执行 NInice：值 VIRT：进程占用的虚拟内存 RES：进程占用的物理内存 SHR：进程使用的共享内存 S：进程的状态。S表示休眠，R表示正在运行，Z表示僵死状态，N表示该进程优先值为负数 %CPU：进程占用CPU的使用率 %MEM：进程使用的物理内存和总内存的百分比 TIME+：该进程启动后占用的总的CPU时间，即占用CPU使用时间的累加值。 COMMAND：进程启动命令名称 另 : 1：使用 ps -ef|grep xxx 命令查找需要查看的进程，xxx是进程名字 2：top -p pid 查看程序的情况 3：ps -aux | grep process_name 4：cat /proc/pid/status这里会打印出当前进程详细的情况，其中，内存是 VmRSS。( 注：pid是要替换成一个id数字的。)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用运维命令(netstat和lsof)笔记整理(二)]]></title>
    <url>%2F2015%2F03%2F09%2Flinux_command_netstat_lsof%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[netstat netstat命令用来打印Linux中网络系统的状态信息，可让你得知整个Linux系统的网络情况。 -a或–all：显示所有连线中的Socket； -A&lt;网络类型&gt;或–&lt;网络类型&gt;：列出该网络类型连线中的相关地址； -c或–continuous：持续列出网络状态； -C或–cache：显示路由器配置的快取信息； -e或–extend：显示网络其他相关信息； -F或–fib：显示FIB； -g或–groups：显示多重广播功能群组组员名单； -h或–help：在线帮助； -i或–interfaces：显示网络界面信息表单； -l或–listening：显示监控中的服务器的Socket； -M或–masquerade：显示伪装的网络连线； -n或–numeric：直接使用ip地址，而不通过域名服务器； -N或–netlink或–symbolic：显示网络硬件外围设备的符号连接名称； -o或–timers：显示计时器； -p或–programs：显示正在使用Socket的程序识别码和程序名称； -r或–route：显示Routing Table； -s或–statistice：显示网络工作信息统计表； -t或–tcp：显示TCP传输协议的连线状况； -u或–udp：显示UDP传输协议的连线状况； -v或–verbose：显示指令执行过程； -V或–version：显示版本信息； -w或–raw：显示RAW传输协议的连线状况； -x或–unix：此参数的效果和指定”-A unix”参数相同； –ip或–inet：此参数的效果和指定”-A inet”参数相同。 . . . netstat常用用法：netstat -anlpnetstat -anlpt的含义是 ： 列出所有处于使用tcp协议的 Sockets123456789b@b-VirtualBox:~$ sudo netstat -anlptActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 127.0.1.1:53 0.0.0.0:* LISTEN 1075/dnsmasq tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 935/sshd tcp 0 0 127.0.0.1:631 0.0.0.0:* LISTEN 2271/cupsd tcp6 0 0 :::22 :::* LISTEN 935/sshd tcp6 0 0 ::1:631 :::* LISTEN 2271/cupsd tcp6 1 0 ::1:50654 ::1:631 CLOSE_WAIT 1027/cups-browsed 查看udp的就是netstat -anlpu；只查看tcp和udp的就是netstat -anlptu lsof （list open files） lsof命令用于查看你进程打开的文件，打开文件的进程，进程打开的端口(TCP、UDP)。找回/恢复删除的文件。是十分方便的系统监视工具，因为lsof命令需要访问核心内存和各种文件，所以需要root用户执行。 在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。所以如传输控制协议 (TCP) 和用户数据报协议 (UDP) 套接字等，系统在后台都为该应用程序分配了一个文件描述符，无论这个文件的本质如何，该文件描述符为应用程序与基础操作系统之间的交互提供了通用接口。因为应用程序打开文件的描述符列表提供了大量关于这个应用程序本身的信息，因此通过lsof工具能够查看这个列表对系统监测以及排错将是很有帮助的。 -a：列出打开指定文件的进程； -c&lt;进程名&gt;：列出指定进程所打开的文件； -g：列出GID号进程详情； -d&lt;文件号&gt;：列出占用该文件号的进程； +d&lt;目录&gt;：列出目录下被打开的文件； +D&lt;目录&gt;：递归列出目录下被打开的文件； -n&lt;目录&gt;：列出使用NFS的文件； -i&lt;条件&gt;：列出符合条件的进程。（4、6、协议、:端口、 @ip ） -p&lt;进程号&gt;：列出指定进程号所打开的文件； -u：列出UID号进程详情； -h：显示帮助信息； -v：显示版本信息 -R: 显示PPID（父进程ID） lsof常用用法1：lsof -pps -ef |grep sshd|grep -v grep| awk ‘{print $2}’|xargs sudo lsof -p的含义是：列出sshd进程打开的所有文件描述符123456789101112131415161718192021222324252627282930313233343536373839b@b-VirtualBox:~$ ps -ef |grep sshd|grep -v grep| awk &apos;&#123;print $2&#125;&apos;|xargs sudo lsof -plsof: WARNING: can&apos;t stat() fuse.gvfsd-fuse file system /run/user/1000/gvfs Output information may be incomplete.COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 935 root cwd DIR 8,1 4096 2 /sshd 935 root rtd DIR 8,1 4096 2 /sshd 935 root txt REG 8,1 770944 301274 /usr/sbin/sshdsshd 935 root mem REG 8,1 43616 136982 /lib/x86_64-linux-gnu/libnss_files-2.19.sosshd 935 root mem REG 8,1 47760 136992 /lib/x86_64-linux-gnu/libnss_nis-2.19.sosshd 935 root mem REG 8,1 39824 136978 /lib/x86_64-linux-gnu/libnss_compat-2.19.sosshd 935 root mem REG 8,1 101240 137033 /lib/x86_64-linux-gnu/libresolv-2.19.sosshd 935 root mem REG 8,1 14256 136950 /lib/x86_64-linux-gnu/libkeyutils.so.1.4sshd 935 root mem REG 8,1 43672 403209 /usr/lib/x86_64-linux-gnu/libkrb5support.so.0.1sshd 935 root mem REG 8,1 186824 403203 /usr/lib/x86_64-linux-gnu/libk5crypto.so.3.1sshd 935 root mem REG 8,1 31792 137035 /lib/x86_64-linux-gnu/librt-2.19.sosshd 935 root mem REG 8,1 141574 137027 /lib/x86_64-linux-gnu/libpthread-2.19.sosshd 935 root mem REG 8,1 252032 137010 /lib/x86_64-linux-gnu/libpcre.so.3.13.1sshd 935 root mem REG 8,1 14664 136924 /lib/x86_64-linux-gnu/libdl-2.19.sosshd 935 root mem REG 8,1 97296 136976 /lib/x86_64-linux-gnu/libnsl-2.19.sosshd 935 root mem REG 8,1 1840928 136907 /lib/x86_64-linux-gnu/libc-2.19.sosshd 935 root mem REG 8,1 14592 136916 /lib/x86_64-linux-gnu/libcom_err.so.2.1sshd 935 root mem REG 8,1 831616 403207 /usr/lib/x86_64-linux-gnu/libkrb5.so.3.3sshd 935 root mem REG 8,1 290520 403037 /usr/lib/x86_64-linux-gnu/libgssapi_krb5.so.2.2sshd 935 root mem REG 8,1 43368 136917 /lib/x86_64-linux-gnu/libcrypt-2.19.sosshd 935 root mem REG 8,1 100728 137070 /lib/x86_64-linux-gnu/libz.so.1.2.8sshd 935 root mem REG 8,1 10680 137062 /lib/x86_64-linux-gnu/libutil-2.19.sosshd 935 root mem REG 8,1 1934624 136919 /lib/x86_64-linux-gnu/libcrypto.so.1.0.0sshd 935 root mem REG 8,1 281552 136921 /lib/x86_64-linux-gnu/libdbus-1.so.3.7.6sshd 935 root mem REG 8,1 14536 440884 /usr/lib/x86_64-linux-gnu/libck-connector.so.0.0.0sshd 935 root mem REG 8,1 134296 137037 /lib/x86_64-linux-gnu/libselinux.so.1sshd 935 root mem REG 8,1 55856 136999 /lib/x86_64-linux-gnu/libpam.so.0.83.1sshd 935 root mem REG 8,1 104936 136897 /lib/x86_64-linux-gnu/libaudit.so.1.0.0sshd 935 root mem REG 8,1 36632 137067 /lib/x86_64-linux-gnu/libwrap.so.0.7.6sshd 935 root mem REG 8,1 149120 136883 /lib/x86_64-linux-gnu/ld-2.19.sosshd 935 root 0u CHR 1,3 0t0 6 /dev/nullsshd 935 root 1u CHR 1,3 0t0 6 /dev/nullsshd 935 root 2u CHR 1,3 0t0 6 /dev/nullsshd 935 root 3u IPv4 10479 0t0 TCP *:ssh (LISTEN)sshd 935 root 4u IPv6 10481 0t0 TCP *:ssh (LISTEN) ps -ef | grep sshd | grep -v grep : 获取ps打印出来的列表中的sshd进程所在的那一行（grep -v grep的含义是清除掉包含“grep”字符串的那一行）, 即为： 12b@b-VirtualBox:~$ ps -ef | grep sshd | grep -v greproot 935 1 0 17:37 ? 00:00:00 /usr/sbin/sshd -D awk ‘{print $2}’ : 获取上述命令打印出来结果的第2列（上述结果的第二列为sshd的pid， 是935） xargs sudo lsof -p ： 列出上述结果pid为935的进程打开的所有文件描述符， 等价于sudo lsof -p 935的结果 因为在 Linux 中一切皆为文件, socket 不也例外, 我们在上面的例子的最后两行可以看到12sshd 935 root 3u IPv4 10479 0t0 TCP *:ssh (LISTEN)sshd 935 root 4u IPv6 10481 0t0 TCP *:ssh (LISTEN) 10479 和 10481 就是 ssh 打开的两个socket文件描述符了. 用命令 ls -l /proc/命令ID/fd , 也可查看所打开的文件.本例中pid为 935 , 则相应的命令为 ls -l /proc/935/fd 查看指定进程的所有连接信息例如我想要查看进程 frps 当前的所有连接信息，先获得进程的 pid： ps -ef|grep frps 结果为： wcl 4721 1 0 10:27 ? 00:00:01 ./frps可以看到进程 pid 为 4721，之后通过 lsof 命令查看所有 TCP 连接信息： lsof -p 4721 -nP | grep TCP 显示结果为： 12345678frps 4721 wcl 4u IPv6 117051764 0t0 TCP *:7000 (LISTEN)frps 4721 wcl 6u IPv6 117051765 0t0 TCP *:7003 (LISTEN)frps 4721 wcl 7u IPv6 117092563 0t0 TCP 139.129.11.120:7000-&gt;116.231.70.223:61545 (ESTABLISHED)frps 4721 wcl 8u IPv6 117092565 0t0 TCP *:6000 (LISTEN)frps 4721 wcl 9u IPv6 117334426 0t0 TCP 139.129.11.120:7000-&gt;116.237.93.230:64898 (ESTABLISHED)frps 4721 wcl 10u IPv6 117053538 0t0 TCP 139.129.11.120:7000-&gt;115.231.20.123:41297 (ESTABLISHED)frps 4721 wcl 11u IPv6 117053540 0t0 TCP *:6005 (LISTEN)frps 4721 wcl 12u IPv6 117334428 0t0 TCP *:6004 (LISTEN) 从 lsof 的输出结果中可以清楚的看到 frps 进程监听了 5 个端口，并且在 7000 端口上建立了 3 个连接，连接两端的 ip 信息也都可以查到。 lsof 的 -nP 参数用于将 ip 地址和端口号显示为正常的数值类型，否则可能会用别名表示。 lsof常用用法：lsof -i:sudo lsof -i:22含义为列出占用22的进程1234b@b-VirtualBox:~$ sudo lsof -i:22COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEsshd 935 root 3u IPv4 10479 0t0 TCP *:ssh (LISTEN)sshd 935 root 4u IPv6 10481 0t0 TCP *:ssh (LISTEN)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用运维命令(iostat)笔记整理(一)]]></title>
    <url>%2F2015%2F03%2F07%2Flinux_command_iostat%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[在linux服务器开发过程中， 经常需要各种命令配合来查看各种状态，所以整理了一些老的笔记来备忘。 iostat iostat主要用于监控系统设备的IO负载情况，iostat首次运行时显示自系统启动开始的各项统计信息，之后运行iostat将显示自上次运行该命令以后的统计信息。用户可以通过指定统计的次数和时间来获得所需的统计信息 -c 仅显示CPU统计信息.与-d选项互斥. -d 仅显示磁盘统计信息.与-c选项互斥. -k 以K为单位显示每秒的磁盘请求数,默认单位块. -t 在输出数据时,打印搜集数据的时间. -V 打印版本号和帮助信息. -x 输出扩展信息. . . . iostat常用用法1：iostat -d指定采样时间间隔与采样次数 我们可以以”iostat interval [count] ”形式指定iostat命令的采样间隔和采样次数：12345678910linux # iostat -d 1 2Linux 2.6.16.60-0.21-smp (linux) 06/13/12Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnsda 0.55 8.93 36.27 6737086 27367728sdb 0.00 0.00 0.00 928 0Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnsda 2.00 0.00 72.00 0 72sdb 0.00 0.00 0.00 0 0 以上命令输出Device的信息，采样时间为1秒，采样2次，若不指定采样次数，则iostat会一直输出采样信息，直到按”ctrl+c”退出命令。注意，第1次采样信息与单独执行iostat的效果一样，为从系统开机到当前执行时刻的统计信息。 iostat常用用法2： iostat -xdk123456linux # iostat -xdk 1Linux 2.6.16.60-0.21-smp (linux) 06/13/12……Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await svctm %utilsda 0.00 9915.00 1.00 90.00 4.00 34360.00 755.25 11.79 120.57 6.33 57.60 以上各列的含义如下： rrqm/s: 每秒对该设备的读请求被合并次数，文件系统会对读取同块(block)的请求进行合并 wrqm/s: 每秒对该设备的写请求被合并次数 r/s: 每秒完成的读次数 w/s: 每秒完成的写次数 rkB/s: 每秒读数据量(kB为单位) wkB/s: 每秒写数据量(kB为单位) avgrq-sz:平均每次IO操作的数据量(扇区数为单位) avgqu-sz: 平均等待处理的IO请求队列长度 await: 平均每次IO请求等待时间(包括等待时间和处理时间，毫秒为单位) svctm: 平均每次IO请求的处理时间(毫秒为单位) %util: 采用周期内用于IO操作的时间比率，即IO队列非空的时间比率 对于以上示例输出，我们可以获取到以下信息： 每秒向磁盘上写30M左右数据(wkB/s值)每秒有91次IO操作(r/s+w/s)，其中以写操作为主体平均每次IO请求等待处理的时间为120.57毫秒，处理耗时为6.33毫秒等待处理的IO请求队列中，平均有11.79个请求驻留]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的struct模块]]></title>
    <url>%2F2015%2F03%2F02%2Fpython%E7%9A%84struct%E6%A8%A1%E5%9D%97%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[struct, 这玩意c/c++也有, 顾名思义, 能联想到这玩意是啥了 模块的主要作用就是对python基本类型值与 用python字符串格式表示的C struct类型间 的转化（This module performs conversions between Python values and C structs represented as Python strings.） 基本用法123456789101112import structimport binasciivalues = (1, 'abc', 2.7)s = struct.Struct('I3sf')packed_data = s.pack(*values)unpacked_data = s.unpack(packed_data) print 'Original values:', valuesprint 'Format string :', s.formatprint 'Uses :', s.size, 'bytes'print 'Packed Value :', binascii.hexlify(packed_data)print 'Unpacked Type :', type(unpacked_data), ' Value:', unpacked_data 输出为:12345Original values: (1, &apos;abc&apos;, 2.7) Format string : I3sf Uses : 12 bytes Packed Value : 0100000061626300cdcc2c40 Unpacked Type : &lt;type &apos;tuple&apos;&gt; Value: (1, &apos;abc&apos;, 2.700000047683716) 代码中， 首先定义了一个元组数据， 包含int、string、float三种数据类型， 然后定义了struct对象，并制定了format‘I3sf’， I 表示int， 3s表示三个字符长度的字符串， f 表示 float。最后通过struct的pack和unpack进行打包和解包。通过输出结果可以发现， value被pack之后， 转化为了一段二进制字节串， 而unpack可以把该字节串再转换回一个元组， 但是值得注意的是对于float的精度发生了改变， 这是由一些比如操作系统等客观因素所决定的。打包之后的数据所占用的字节数与C语言中的struct十分相似。定义format可以参照官方api提供的对照表： 字节序设置另一方面，打包的后的字节顺序默认上是由操作系统的决定的， 当然struct模块也提供了自定义字节顺序的功能， 可以指定大端存储、小端存储等特定的字节顺序， 对于底层通信的字节顺序是十分重要的， 不同的字节顺序和存储方式也会导致字节大小的不同。在format字符串前面加上特定的符号即可以表示不同的字节顺序存储方式， 例如采用小端存储 s = struct.Struct(‘&lt;I3sf’)就可以了。官方api library 也提供了相应的对照列表：]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL入门二之一些小注意点]]></title>
    <url>%2F2015%2F03%2F02%2FMySQL%E5%85%A5%E9%97%A8%E4%BA%8C%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[distinct关键字distinct是应用于所有列的, 而不是某一个列 1234567891011121314151617181920212223242526272829mysql&gt; select * from test_table;+------+------+| one | two |+------+------+| 56 | 12 || 52 | 10 || 56 | 12 || 56 | 13 |+------+------+4 rows in set (0.00 sec)mysql&gt; select distinct one, two from test_table;+------+------+| one | two |+------+------+| 56 | 12 || 52 | 10 || 56 | 13 |+------+------+3 rows in set (0.00 sec)mysql&gt; select distinct one from test_table;+------+| one |+------+| 56 || 52 |+------+2 rows in set (0.00 sec) . . . and关键字and的组合优先级比or高 12345678910111213141516171819202122232425262728293031323334353637mysql&gt; select * from test_table;+------+------+| one | two |+------+------+| 56 | 12 || 52 | 10 || 56 | 12 || 56 | 13 || NULL | NULL |+------+------+5 rows in set (0.00 sec)mysql&gt; select one, two from test_table where one = 52 or one = 56 and two &gt; 12;+------+------+| one | two |+------+------+| 52 | 10 || 56 | 13 |+------+------+2 rows in set (0.00 sec)mysql&gt; select one, two from test_table where one = 52 or (one = 56 and two &gt; 12);+------+------+| one | two |+------+------+| 52 | 10 || 56 | 13 |+------+------+2 rows in set (0.00 sec)mysql&gt; select one, two from test_table where (one = 52 or one = 56) and two &gt; 12;+------+------+| one | two |+------+------+| 56 | 13 |+------+------+1 row in set (0.00 sec) NULLnull和空字符是不一样的, 找到他和删除他的方式也比较特别 1234567891011121314151617181920212223242526272829303132333435363738mysql&gt; insert into test_table(one , two) values (null, null);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_table;+------+------+| one | two |+------+------+| NULL | NULL |+------+------+1 row in set (0.00 sec)mysql&gt; delete from test_table where one = NULL;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test_table;+------+------+| one | two |+------+------+| NULL | NULL |+------+------+1 row in set (0.00 sec)mysql&gt; delete from test_table where one = &apos; &apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test_table;+------+------+| one | two |+------+------+| NULL | NULL |+------+------+1 row in set (0.00 sec)mysql&gt; delete from test_table where isnull(one);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_table;Empty set (0.00 sec) rollback 并不是什么都可以回滚的, 典型的如创建表和删除表这些都是不能回退的. 事务是用来管理 insert,update,delete 语句的123456789101112131415161718192021222324252627282930313233343536mysql&gt; set autocommit = 0;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test_tab;+-----+-----+-------+| one | two | three |+-----+-----+-------+| 3 | 4 | 5 |+-----+-----+-------+1 row in set (0.00 sec)mysql&gt; set autocommit = 0;Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into test_tab value (4, 4, 5);Query OK, 1 row affected (0.00 sec)mysql&gt; rollback;Query OK, 0 rows affected (0.01 sec)mysql&gt; select * from test_tab;+-----+-----+-------+| one | two | three |+-----+-----+-------+| 3 | 4 | 5 |+-----+-----+-------+1 row in set (0.00 sec)mysql&gt; drop table test_tab;Query OK, 0 rows affected (0.02 sec)mysql&gt; rollback;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test_tab;ERROR 1146 (42S02): Table &apos;b_test_database.test_tab&apos; doesn&apos;t exist]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL入门一之增删查改与关联]]></title>
    <url>%2F2015%2F02%2F27%2FMySQL%E5%85%A5%E9%97%A8%E4%B8%80%E4%B9%8B%E5%A2%9E%E5%88%A0%E6%9F%A5%E6%94%B9%E4%B8%8E%E5%85%B3%E8%81%94%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[增删改查 INSERT INTO table_name (列1, 列2,…) VALUES (值1, 值2,….) DELETE FROM 表名称 WHERE 列名称 = 值 UPDATE 表名称 SET 列名称 = 新值 WHERE 列名称 = 某值 SELECT 列名称 FROM 表名称 关联SQL join 用于根据两个或多个表中的列之间的关系，从这些表中查询数据。 Join和Key概绍有时为了得到完整的结果，我们需要从两个或更多的表中获取结果。我们就需要执行 join。数据库中的表可通过键将彼此联系起来。主键（Primary Key）是一个列，在这个列中的每一行的值都是唯一的。在表中，每个主键的值都是唯一的。这样做的目的是在不重复每个表中的所有数据的情况下，把表间的数据交叉捆绑在一起。请看 “Persons” 表： Id_P LastName FirstName Address City 1 Adams John Oxford Street London 2 Bush George Fifth Avenue New York 3 Carter Thomas Changan Street Beijing 请注意，”Id_P” 列是 Persons 表中的的主键。这意味着没有两行能够拥有相同的 Id_P。即使两个人的姓名完全相同，Id_P 也可以区分他们。接下来请看 “Orders” 表： Id_O OrderNo Id_P 1 77895 3 2 44678 3 3 22456 1 4 24562 1 5 34764 65 请注意，”Id_O” 列是 Orders 表中的的主键，同时，”Orders” 表中的 “Id_P” 列用于引用 “Persons” 表中的人，而无需使用他们的确切姓名。请留意，”Id_P” 列把上面的两个表联系了起来。 下面列出了您可以使用的 JOIN 类型，以及它们之间的差异。 JOIN(INNER JOIN): 如果左右表中都有至少一个匹配，则返回行 LEFT JOIN: 即使右表中没有匹配，也从左表返回所有的行 RIGHT JOIN: 即使左表中没有匹配，也从右表返回所有的行 FULL JOIN: 只要其中一个表中存在匹配，就返回行 注 : JOIN使用on的, 而不是where. 使用Join(INNER JOIN)除了上面的方法，我们也可以使用关键词 JOIN 来从两个表中获取数据。如果我们希望列出所有人的定购，可以使用下面的 SELECT 语句：12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName 结果集： LastName FirstName OrderNo Adams John 22456 Adams John 24562 Carter Thomas 77895 Carter Thomas 44678 使用Left Join现在，我们希望列出所有的人，以及他们的定购 - 如果有的话。您可以使用下面的 SELECT 语句：12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsLEFT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 结果集： LastName FirstName OrderNo Adams John 22456 Adams John 24562 Carter Thomas 77895 Carter Thomas 44678 Bush George - - - - LEFT JOIN 关键字会从左表 (Persons) 那里返回所有的行，即使在右表 (Orders) 中没有匹配的行。 使用Right Join现在，我们希望列出所有的定单，以及定购它们的人 - 如果有的话。您可以使用下面的 SELECT 语句：12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsRIGHT JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 结果集： LastName FirstName OrderNo Adams John 22456 Adams John 24562 Carter Thomas 77895 Carter Thomas 44678 34764 RIGHT JOIN 关键字会从右表 (Orders) 那里返回所有的行，即使在左表 (Persons) 中没有匹配的行。 使用Full Join现在，我们希望列出所有的人，以及他们的定单，以及所有的定单，以及定购它们的人。您可以使用下面的 SELECT 语句：12345SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsFULL JOIN OrdersON Persons.Id_P=Orders.Id_PORDER BY Persons.LastName 结果集： LastName FirstName OrderNo Adams John 22456 Adams John 24562 Carter Thomas 77895 Carter Thomas 44678 Bush George 34764 FULL JOIN 关键字会从左表 (Persons) 和右表 (Orders) 那里返回所有的行。如果 “Persons” 中的行在表 “Orders” 中没有匹配，或者如果 “Orders” 中的行在表 “Persons” 中没有匹配，这些行同样会列出。]]></content>
      <categories>
        <category>DB</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ProtoBuf的安装与使用]]></title>
    <url>%2F2015%2F02%2F23%2Fprotobuf_tutorial%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[介绍与 JSON 相比， Protobuf 的序列化和反序列化的速度更快，而且传输的数据会先压缩，使得传输的效率更高些 。Protobuf ， 全称 Protocol Buffer ， 是 Google 公司内部的混合语言数据标准，是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。 它很适合做数据存储或 RPC 数据交换格式 。 Protobuf是可用于通信协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式 。 安装谷歌的东西要想在大陆安装起来总是有点那啥, 你懂的. 需要的依赖sudo apt-get install curl sudo apt-get install autoconf autogen sudo apt-get install libtool 安装步骤下载自github的代码需要首先执行 $ ./autogen.sh 生成configure文件 注意autogen.sh 需要gtest包，默认是从 googletest.googlecode.com下载，国内需要翻墙才能访问，很多人问autogen.sh运行失败，这里我补充一下 修改一下autogen.sh , 将这段:123echo &quot;Google Test not present. Fetching gtest-1.5.0 from the web...&quot;curl http://googletest.googlecode.com/files/gtest-1.5.0.tar.bz2 | tar jxmv gtest-1.5.0 gtest 修改为:123wget https://github.com/google/googletest/archive/release-1.5.0.tar.gztar xzvf release-1.5.0.tar.gzmv googletest-release-1.5.0 gtest 再执行 autogen.sh，这样就不会报错了 $ ./configure $ make $ make check $ make install 默认是装在 usr/local/bin usr/local/lib, usr/local/include 检查是否安装成功protoc --version 如果安装成功,会出现版本号 如 libprotoc 2.6.1 如果有问题，会输出错误内容, 最后我安装完成,用上述命令检查版本号时出现如下问题 protoc: error while loading shared libraries: libprotocbuf.so.9: cannot open shared 错误原因 protobuf的默认安装路径是/usr/local/lib,而/usr/local/lib不在ubuntu体系默认的LD_LIBRARY_PATH里,所以就找不到lib 解决办法 : 1 - 在 /etc/ld.so.conf.d/目录下创建文件 bprotobuf.conf文件,文件内容如下 /usr/local/lib 2 - 输入命令 sudo ldconfig 这时,再输入protoc --version就可以正常看到版本号了 使用Writer.cpp123456789101112131415#include&lt;iostream&gt;#include&lt;fstream&gt;#include "Mymessage.pb.h"using namespace std;int main()&#123; Im::Content msg1; msg1.set_id(101); msg1.set_str("ggsmd"); fstream output("./log", ios::out | ios::trunc | ios::binary); if (!msg1.SerializeToOstream(&amp;output)) &#123; cerr &lt;&lt; "Failed to write msg." &lt;&lt; endl; return -1; &#125; return 0; &#125; Reader.cpp123456789101112131415161718#include&lt;iostream&gt;#include&lt;fstream&gt;#include "Mymessage.pb.h"using namespace std;void ListMsg(const Im::Content &amp; msg)&#123; cout &lt;&lt; msg.id() &lt;&lt; endl; cout &lt;&lt; msg.str() &lt;&lt; endl;&#125; int main(int argc, char* argv[])&#123; Im::Content msg1; fstream input("./log", ios::in | ios::binary); if (!msg1.ParseFromIstream(&amp;input)) &#123; cerr &lt;&lt; "Failed to parse address book." &lt;&lt; endl; return -1; &#125; ListMsg(msg1); return 0;&#125; makefile1234567891011121314151617181920INC=/usr/local/includeLIB=/usr/local/liblib=protobufall:Writer ReaderWriter.o:Writer.cpp g++ -g -c Writer.cpp -I$(INC) -L$(LIB) -l$(lib)Reader.o:Reader.cpp g++ -g -c Reader.cpp -I$(INC) -L$(LIB) -l$(lib) Writer:Writer.o Mymessage.pb.o g++ -g -o Writer Writer.o Mymessage.pb.o -I$(INC) -L$(LIB) -l$(lib)Reader:Reader.o Mymessage.pb.o g++ -g -o Reader Reader.o Mymessage.pb.o -I$(INC) -L$(LIB) -l$(lib)Mymessage.pb.o:Mymessage.pb.cc g++ -g -c Mymessage.pb.cc -I$(INC) -L$(LIB) -l$(lib) clean:Writer Reader Writer.o Reader.o Mymessage.pb.o rm Writer Reader Writer.o Reader.o Mymessage.pb.o Mymessage.proto1234567package Im; message Content &#123; required int32 id = 1; // ID required string str = 2; // str optional int32 opt = 3; //optional field &#125; 打印结果执行protoc -I=./ --cpp_out=./ Mymessage.proto 命令后，会生成 Mymessage.pb.h 和 Mymessage.pb.cc 文件。 再执行 make 命令，生成Writer 和 Reader 文件 。 执行 ./Writer 命令后，再执行./Reader 命令，终端上输出： b@b-VirtualBox:~/tc$ protoc -I=./ --cpp_out=./ Mymessage.proto b@b-VirtualBox:~/tc$ ll total 44 drwxrwxr-x 2 b b 4096 5月 19 22:43 ./ drwxr-xr-x 4 b b 4096 5月 19 22:35 ../ -rw-rw-r-- 1 b b 647 5月 19 22:36 makefile -rw-rw-r-- 1 b b 12214 5月 19 22:43 Mymessage.pb.cc -rw-rw-r-- 1 b b 7762 5月 19 22:43 Mymessage.pb.h -rw-rw-r-- 1 b b 161 5月 19 22:36 Mymessage.proto -rw-rw-r-- 1 b b 421 5月 19 22:36 Reader.cpp -rw-rw-r-- 1 b b 340 5月 19 22:35 Writer.cpp b@b-VirtualBox:~/tc$ make g++ -g -c Writer.cpp -I/home/sharexu/charpter13/1302/include -L/home/sharexu/charpter13/1302/lib -lprotobuf g++ -g -c Mymessage.pb.cc -I/home/sharexu/charpter13/1302/include -L/home/sharexu/charpter13/1302/lib -lprotobuf g++ -g -o Writer Writer.o Mymessage.pb.o -I/home/sharexu/charpter13/1302/include -L/home/sharexu/charpter13/1302/lib -lprotobuf g++ -g -c Reader.cpp -I/home/sharexu/charpter13/1302/include -L/home/sharexu/charpter13/1302/lib -lprotobuf g++ -g -o Reader Reader.o Mymessage.pb.o -I/home/sharexu/charpter13/1302/include -L/home/sharexu/charpter13/1302/lib -lprotobuf b@b-VirtualBox:~/tc$ ll total 772 drwxrwxr-x 2 b b 4096 5月 19 22:43 ./ drwxr-xr-x 4 b b 4096 5月 19 22:35 ../ -rw-rw-r-- 1 b b 647 5月 19 22:36 makefile -rw-rw-r-- 1 b b 12214 5月 19 22:43 Mymessage.pb.cc -rw-rw-r-- 1 b b 7762 5月 19 22:43 Mymessage.pb.h -rw-rw-r-- 1 b b 244112 5月 19 22:43 Mymessage.pb.o -rw-rw-r-- 1 b b 161 5月 19 22:36 Mymessage.proto -rwxrwxr-x 1 b b 188430 5月 19 22:43 Reader* -rw-rw-r-- 1 b b 421 5月 19 22:36 Reader.cpp -rw-rw-r-- 1 b b 57656 5月 19 22:43 Reader.o -rwxrwxr-x 1 b b 184244 5月 19 22:43 Writer* -rw-rw-r-- 1 b b 340 5月 19 22:35 Writer.cpp -rw-rw-r-- 1 b b 59232 5月 19 22:43 Writer.o b@b-VirtualBox:~/tc$ ./Writer b@b-VirtualBox:~/tc$ ./Reader 101 ggsmd]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>ProtoBuf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++对象模型之虚函数实例讲解]]></title>
    <url>%2F2015%2F02%2F21%2Fcpp_object_model_conclusion%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[介绍因为c++只规定了 虚继承/ 虚函数/ 多继承/ 的行为, 但将实现方法留给编译器作者. 所以各个平台的实现并不相同, 得出的结果也不尽相同. 经测试, vs和gcc目前比较统一的情况只有2种 : 无继承+无虚函数 无继承+虚函数 故本文只讨论这2种, 以及了解虚函数和虚继承的含义. . . . 关于虚函数当类中声明了虚函数（不管是1个还是多个），那么在实例化对象时，编译器会自动在对象里安插一个指针vPtr指向虚函数表VTable； 关于虚继承当涉及到虚继承，会增加vbPtr指针指向虚基表vbTable 单继承对象模型类的继承关系为：class Derived : public Base 无继承但有虚函数示例测试环境为Windows/VS, 32位. 1234567891011121314151617181920212223242526class A &#123; &#125;; class B &#123; char ch; virtual void func0() &#123; &#125; &#125;; class C &#123; char ch1; char ch2; virtual void func() &#123; &#125; virtual void func1() &#123; &#125; &#125;; int main(void) &#123; cout&lt;&lt;"A="&lt;&lt;sizeof(A)&lt;&lt;endl; //result=1 cout&lt;&lt;"B="&lt;&lt;sizeof(B)&lt;&lt;endl; //result=8 cout&lt;&lt;"C="&lt;&lt;sizeof(C)&lt;&lt;endl; //result=8 return 0; &#125; 再测试一个复杂点的情况: 123456789101112131415161718192021222324struct MyStructA&#123;&#125;;struct MyStructB&#123; int value(); &#125;;struct MyStructC&#123; virtual ~MyStructC(); int x; &#125;;struct MyStructD&#123; virtual ~ MyStructD(); virtual void value(); &#125;;struct MyStructE : public MyStructC&#123; virtual ~MyStructE(); &#125;;struct MyStructF : public MyStructC, public MyStructD&#123; virtual ~MyStructF(); &#125;;int main(int argc, char* argv[])&#123; &#123; std::cout &lt;&lt; sizeof(MyStructA) &lt;&lt; std::endl; // 1 std::cout &lt;&lt; sizeof(MyStructB) &lt;&lt; std::endl; // 1 std::cout &lt;&lt; sizeof(MyStructC) &lt;&lt; std::endl; // 8 std::cout &lt;&lt; sizeof(MyStructD) &lt;&lt; std::endl; // 4 std::cout &lt;&lt; sizeof(MyStructE) &lt;&lt; std::endl; // 8 std::cout &lt;&lt; sizeof(MyStructF) &lt;&lt; std::endl; // 12 getchar(); return 0; &#125;&#125; 总结首先，平时所声明的类只是一种类型定义，它本身是没有大小可言的。 因此，如果用sizeof运算符对一个类型名操作，那得到的是具有该类型实体的大小。计算一个类对象的大小时的规律： 空类、单一继承的空类、多重继承的空类所占空间大小为：1（字节，下同）；一个类中，虚函数本身、成员函数（包括静态与非静态）和静态数据成员都是不占用类对象的存储空间的；因此一个对象的大小≥所有非静态成员大小的总和； 这是为什么呢?实际上，这是类结构体实例化的原因，空的类或结构体同样可以被实例化，如果定义对空的类或者结构体取sizeof()的值为0，那么该空的类或结构体实例化出很多实例时，在内存地址上就不能区分该类实例化出的实例，，，所以，为了实现每个实例在内存中都有一个独一无二的地址，编译器往往会给一个空类隐含的加一个字节，这样空类在实例化后在内存得到了独一无二的地址，所以空类所占的内存大小是1个字节。 类对象的大小 =各非静态数据成员（包括父类的非静态数据成员但都不包括所有的成员函数）的总和 +虚函数表指针(多继承有几个含有虚函数的父类就有几个虚函数表指针)+虚基表指针(多继承下可能不止一个, 不赘述了, 虚继承的对象的内存布局，在不同编译器实现有所区别) +编译器因为要内存对齐而额外增加的字节。 详情可参考: 陈皓的 C++ 虚函数表解析 测试常用平台的不同测试了 Windows10 / VS2015 和 Ubuntu14.04.3 / gcc4.8.4 , 都是64位 测试有多继承的情况12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;iostream&gt;using std::cout;using std::endl;class A&#123;&#125;;class B&#123; char ch; virtual void func0() &#123; &#125;&#125;;class C&#123; char ch1; char ch2; virtual void func() &#123; &#125; virtual void func1() &#123; &#125;&#125;;class D : public A, public C&#123; int d; virtual void func() &#123; &#125; virtual void func1() &#123; &#125;&#125;;class E : public B, public C&#123; int e; virtual void func0() &#123; &#125; virtual void func1() &#123; &#125;&#125;;int main(void)&#123; cout &lt;&lt; "A=" &lt;&lt; sizeof(A) &lt;&lt; endl; cout &lt;&lt; "B=" &lt;&lt; sizeof(B) &lt;&lt; endl; cout &lt;&lt; "C=" &lt;&lt; sizeof(C) &lt;&lt; endl; cout &lt;&lt; "D=" &lt;&lt; sizeof(D) &lt;&lt; endl; cout &lt;&lt; "E=" &lt;&lt; sizeof(E) &lt;&lt; endl; return 0;&#125; 打印对比如下 : Windows10/VS2015123456A=1B=16C=16D=24E=40请按任意键继续. . . Ubuntu14.04.3/gcc4.8.412345A=1B=16C=16D=16E=32 测试有虚拟继承的情况123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113#include &lt;iostream&gt;using std::cout;using std::endl;class Base&#123;public: Base() &#123; mBase = 11; &#125; virtual void funcA() &#123; cout &lt;&lt; "Base::funcA()" &lt;&lt; endl; &#125; virtual void funcX() &#123; cout &lt;&lt; "Base::funcX()" &lt;&lt; endl; &#125;protected: int mBase;&#125;;class Base1 : virtual public Base&#123;public: Base1() : Base() &#123; mBase1 = 101; &#125; virtual void funcA() &#123; cout &lt;&lt; "Base1::funcA()" &lt;&lt; endl; &#125; virtual void funcB() &#123; cout &lt;&lt; "Base1::funcB()" &lt;&lt; endl; &#125;private: int mBase1;&#125;;class Base2 : virtual public Base&#123;public: Base2() : Base() &#123; mBase2 = 102; &#125; virtual void funcA() &#123; cout &lt;&lt; "Base2::funcA()" &lt;&lt; endl; &#125; virtual void funcC() &#123; cout &lt;&lt; "Base2::funcC()" &lt;&lt; endl; &#125;private: int mBase2;&#125;;class Base3 : virtual public Base&#123;public: Base3() : Base() &#123; mBase3 = 102; &#125; virtual void funcA() &#123; cout &lt;&lt; "Base3::funcA()" &lt;&lt; endl; &#125; virtual void funcX() &#123; cout &lt;&lt; "Base3::funcC()" &lt;&lt; endl; &#125;private: int mBase3;&#125;;class Derived : public Base1, public Base2&#123;public: Derived() : Base1(), Base2() &#123; mDerived = 1001; &#125; virtual void funcD() &#123; cout &lt;&lt; "Derived::funcD()" &lt;&lt; endl; &#125; virtual void funcA() &#123; cout &lt;&lt; "Derived::funcA()" &lt;&lt; endl; &#125;private: int mDerived;&#125;;int main(void)&#123; cout &lt;&lt; "Derived's size is " &lt;&lt; sizeof(Derived) &lt;&lt; endl; cout &lt;&lt; "Base's size is " &lt;&lt; sizeof(Base) &lt;&lt; endl; cout &lt;&lt; "Base1's size is " &lt;&lt; sizeof(Base1) &lt;&lt; endl; cout &lt;&lt; "Base2's size is " &lt;&lt; sizeof(Base2) &lt;&lt; endl; cout &lt;&lt; "Base3's size is " &lt;&lt; sizeof(Base3) &lt;&lt; endl; return 0;&#125; 打印对比如下 : Windows10/VS2015123456Derived&apos;s size is 80Base&apos;s size is 16Base1&apos;s size is 48Base2&apos;s size is 48Base3&apos;s size is 40请按任意键继续. . . Ubuntu14.04.3/gcc4.8.412345Derived&apos;s size is 48Base&apos;s size is 16Base1&apos;s size is 32Base2&apos;s size is 32Base3&apos;s size is 32 测试总结这两个都还算是比较常用的平台了, 测试之后发现vs和gcc目前比较统一的情况只有2种 : 无继承+无虚函数 无继承+虚函数 参考 陈皓的 C++ 虚函数表解析 C++对象模型之详述C++对象的内存布局 C++对象模型之简述C++对象的内存布局]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>noodle</tag>
        <tag>ObjectModel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的__name__和__main()__]]></title>
    <url>%2F2015%2F02%2F10%2Fpython%E4%B8%AD%E7%9A%84__name__%E5%92%8C__main()__%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[12345678#hello.pydef sayHello(): str="hello" print(str);if __name__ == "__main__": print ('This is main of module "hello.py"') sayHello() python作为一种脚本语言，我们用python写的各个module都可以包含以上那么一个累死c中的main函数，只不过python中的这种__main__与c中有一些区别，类似于php的魔术那一套, 主要体现在： 1、当单独执行该module时，比如单独执行以上hello.py： python hello.py，则输出 12This is main of module &quot;hello.py&quot;hello 可以理解为&quot;if __name__==&quot;__main__&quot;:&quot;这一句与c中的main()函数所表述的是一致的，即作为入口； 2、当该module被其它module 引入使用时，其中的&quot;if __name__==&quot;__main__&quot;:&quot; 所表示的Block不会被执行, 这是因为此时module被其它module引用时， 其__name__的值将发生变化，__name__的值将会是module的名字。 比如在python shell中import hello后，查看hello.__name__： 123import hellohello.__name__'hello' 3、因此，在python中，当一个module作为整体被执行时,moduel.name的值将是&quot;__main__&quot;； 而当一个module被其它module引用时，module.__name__将是module自己的名字， 当然一个module被其它module引用时，其本身并不需要一个可执行的入口main了。]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件编译指令之#if和#ifdef和#if defined之间的区别]]></title>
    <url>%2F2015%2F02%2F09%2F%E6%9D%A1%E4%BB%B6%E7%BC%96%E8%AF%91%E6%8C%87%E4%BB%A4%E4%B9%8Bif%E5%92%8Cifdef%E5%92%8Cifdefined%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[#if的使用说明#if的后面接的是表达式 : 123#if (MAX==10)||(MAX==20) code...#endif 它的作用是：如果(MAX==10)||(MAX==20)成立，那么编译器就会把其中的#if 与 #endif之间的代码编译进去（注意：是编译进去，不是执行！！） #if defined的使用#if后面接的是一个宏, 而#if define(x)的使用如下 : 123#if defined (x) ...code...#endif 这个#if defined它不管里面的“x”的逻辑是“真”还是“假”它只管这个程序的前面的宏定义里面有没有定义“x”这个宏，如果定义了x这个宏，那么，编译器会编译中间的…code…否则不直接忽视中间的…code…代码。另外 #if defined(x)也可以取反，也就用 #if !defined(x) #ifdef的使用 #ifdef的使用和#if defined()的用法一致 #ifndef又和#if !defined()的用法一致。 最后强调两点： 这几个宏定义只是决定代码块是否被编译！ 别忘了#endif]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Compile</tag>
        <tag>Make</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua特别之处笔记]]></title>
    <url>%2F2015%2F02%2F03%2Flua_special_part%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[2.2 Booleans两个取值 false 和 true。但要注意 Lua 中所有的值都可以作为条件。在控制结构的条件中除了 false 和 nil 为假，其他值都为真。所以 Lua 认为 0 和空串都是真。 2.5 table我们用一个疑问来引入table的介绍 传的是值还是引用?lua的函数调用传的是值还是引用? . . . 测试代码123456789101112131415161718192021222324252627282930313233tTableForTest = &#123;&#125;tTableForTest[1] = 9function testTable(tTable) tTable[1] = 11endprint("tTableForTest[1]".." : "..tTableForTest[1])testTable(tTableForTest)print("tTableForTest[1]".." : "..tTableForTest[1])print("\n==================\n")nNumberForTest = 1function TestNumber( nNumber ) nNumber = 99endprint("nNumberForTest".." : "..nNumberForTest)TestNumber(nNumberForTest)print("nNumberForTest".." : "..nNumberForTest)print("\n==================\n")nStringForTest = "hi"function TestNumber( nString ) nString = "hello"endprint("nStringForTest".." : "..nStringForTest)TestNumber(nStringForTest)print("nStringForTest".." : "..nStringForTest) 打印结果tTableForTest[1] : 9 tTableForTest[1] : 11 ================== nNumberForTest : 1 nNumberForTest : 1 ================== nStringForTest : hi nStringForTest : hi 结论 table传引用 number传值 string传值 为什么lua中table会不一样在Lua中, table 既不是值也不是变量而是对象. 程序仅持有一个对他们的引用, Lua不会暗中产生table的副本或创建新的table. 事实上, table的创建是通过构造表达式完成的, 最简单的构造表达式就是{} 123456789101112131415161718a = &#123;&#125;k = "x"a[k] = 10a[20] = "great"print(a["x"]) --&gt; 10k = 20print(a[k]) --&gt; "great"a["x"] = a["x"] + 1print(a["x"]) --&gt; 11b = a -- b与a引用了同一个tableprint(b["x"]) --&gt; 10b["x"] = 20print(a["x"]) --&gt; 20a = nil -- 现在只有b还在引用tableb = nil -- 再也没有对table的引用了 当一个程序再也没有对一个table的引用时, Lua的垃圾收集器最终会删除该table, 并复用它的内存. a[x]和a.x是不同的a.x表示a[&quot;x&quot;] , 表示以字符串&quot;x&quot;来索引table a[x] 是以变量x的值来索引table 123456a = &#123;&#125;x = "y"a[x] = 10print(a[x]) --&gt; 表示a["y"], 即 10print(a.x) --&gt; 表示a["x"], 没有定义这个, 所以是 nilprint(a.y) --&gt; 表示a["y"], 即 10 用table作为数组时就Lua的习惯而言, 数组通常以1作为索引的起始值, 并且还有不少lua机制依赖于这个惯例, 大多数lua内置的函数都假设数组起始于索引1, 这跟c语言以0为起始是不同的. Lua5.1以上, 可以使用 # 来返回一个数组或者线性表的最后一个索引值或者其大小. 1234-- 打印所有的行for i=1, #a do print(a[i])end]]></content>
      <categories>
        <category>Script</category>
      </categories>
      <tags>
        <tag>Lua</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GDB基础教程]]></title>
    <url>%2F2015%2F02%2F02%2Fgdb_tutorial%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[GDB 操作提示在编译可执行文件时需要给 gcc 加上 “-g” 选项，这样它才会为生成的可执行文件加入额外的调试信息。不要使用编译器的优化选项，比如： “-O”，”-O2”。因为编译器会为了优化而改变程序流程，那样不利于调试。在 GDB 中执行 shell 命令可以使用：shell commandGDB 命令可以使用 TAB 键来补全。按两次 TAB 键可以看到所有可能的匹配。GDB 命令缩写：例如 info bre 中的 bre 相当于 breakpoints。 启动GDB gdb executable gdb -e executable -c core-file gdb executable -pid process-id （使用ps相关命令可以查看进程的 pid） GDB常用命令 help 列出 gdb 帮助信息。 info+subcommand , 比如 : info breakpoints : 列出断点。 info watchpoints : 列出观察点。 info threads : 列出当前的线程。 info locals : 列出Local variables of current stack frame step(简写一个s也可) 进入下一行代码的执行，会进入函数内部。 next(简写一个n也可) 执行下一行代码。但不会进入函数内部。 finish 跳出当前代码（之前跳入调试） continue(c) 继续执行直到下一个断点或观察点。 b 断点 kill 停止程序执行。 quit(q) 退出 GDB调试器 run(r) 从头开始执行程序，也允许进行重定向。 print(p) variable 打印指定变量的值。 p variable p file::variable p ‘file’::variables backtrace(简写bt), 显示函数调用栈 bt : 显示所有函数调用栈 bt n : 显示程序的调用栈信息，只显示栈顶n桢(frame) bt -n : 显示程序的调用栈信息，只显示栈底部n桢(frame) set backtrace limit n : 设置bt显示的最大桢层数 where 和 info stack ： 都是bt的别名，功能一样 细说断点给 test.c 的第10行设置一个断点 : b test.c:10 断点删除断点的删除与断点的设置同样的重要。删除断点的命令有两个: delete clear delete用法：delete [breakpoints num] [range…]delete可删除单个断点，也可删除一个断点的集合，这个集合用连续的断点号来描述。例如： delete 5 delete 1-10 clear用法: clear 删除所选定的环境中所有的断点 clear location location描述具体的断点。 clear 删除断点是基于行的，不是把所有的断点都删除。例如： clear list_insert //删除函数的所有断点 clear list.c:list_delet //删除文件：函数的所有断点 clear 12 //删除行号的所有断点 clear list.c:12 //删除文件：行号的所有断点 断点的使能和禁止对断点的控制除了建立和删除外，还可以通过使能和禁止来控制，后一种方法更灵活。 断点的四种使能操作： enable [breakpoints] [range…] 完全使能 enable //激活所有断点 enable 4 //激活4断点 enable 5-6 //激活5～6断点 disable [breakpoints] [range…] 禁止 enable once [breakpoints] [range…] 使能一次，触发后禁止 enable delete [breakpoints] [range…]使能一次，触发后删除 用法举例： diable //禁止所有断点 disble 2 //禁止第二个断点 disable 1-5 //禁止第1到第5个断点 GDB帮助GDB的命令很多, 有些用得少的命令记不住的话, 可以在进入GDB之后敲 “help”, 然后再敲 “help + command_class”,比如 : (gdb) helpList of classes of commands: aliases – Aliases of other commandsbreakpoints – Making program stop at certain pointsdata – Examining datafiles – Specifying and examining filesinternals – Maintenance commandsobscure – Obscure featuresrunning – Running the programstack – Examining the stackstatus – Status inquiriessupport – Support facilitiestracepoints – Tracing of program execution without stopping the programuser-defined – User-defined commands Type “help” followed by a class name for a list of commands in that class.Type “help all” for the list of all commands.Type “help” followed by command name for full documentation.Type “apropos word” to search for commands related to “word”.Command name abbreviations are allowed if unambiguous. (gdb) help running Running the program. List of commands: advance – Continue the program up to the given location (same form as args for break command)attach – Attach to a process or file outside of GDBcontinue – Continue program being debuggeddetach – Detach a process or file previously attacheddetach checkpoint – Detach from a checkpoint (experimental)detach inferiors – Detach from inferior ID (or list of IDS)disconnect – Disconnect from a targetfinish – Execute until selected stack frame returnshandle – Specify how to handle signalsinferior – Use this command to switch between inferiorsinterrupt – Interrupt the execution of the debugged programjump – Continue program being debugged at specified line or addresskill – Kill execution of program being debuggedkill inferiors – Kill inferior ID (or list of IDs)next – Step programnexti – Step one instructionreverse-continue – Continue program being debugged but run it in reversereverse-finish – Execute backward until just before selected stack frame is calledreverse-next – Step program backwardreverse-nexti – Step backward one instructionreverse-step – Step program backward until it reaches the beginning of another source linereverse-stepi – Step backward exactly one instructionrun – Start debugged program…]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>GDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式的析构问题和线程安全问题]]></title>
    <url>%2F2015%2F02%2F02%2Fsingleton_pattern%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[在某些应用环境下面，一个类只允许有一个实例，这就是著名的单例模式。单例模式分为 懒汉模式 饿汉模式 饿汉模式在实例化 m_instance 变量时，直接调用类的构造函数。顾名思义，在还未使用变量时，已经对 m_instance 进行赋值，就像很饥饿的感觉。在main开始前就初始化好了， 所以是线程安全的。 没有考虑析构问题饿汉模式的示例代码首先给出没有考虑析构问题的饿汉模式的实现 1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;using namespace std;class singleton&#123;protected: singleton() &#123;&#125;;private: singleton(const singleton&amp;) &#123;&#125;; singleton&amp; operator=(const singleton&amp;) &#123;&#125;; static singleton* m_instance;public: static singleton* GetInstance(); ~singleton() &#123; printf("Singleton destruction\n"); &#125;&#125;;singleton* singleton::GetInstance()&#123; return m_instance;&#125;singleton* singleton::m_instance = new singleton;int main()&#123; singleton *ct = singleton::GetInstance(); return 0;&#125; 饿汉模式的优点 线程安全 实现简单，容易维护 饿汉模式的缺点 不适合部分场景。如：因为性能问题，希望懒加载；需要运行时才能知道，是否生成实例 由于在main开始前就必须初始化，几乎不可能给类传入任何参数。 懒汉模式懒汉模式下，在定义m_instance变量时先等于NULL，在调用GetInstance()方法时，在判断是否要赋值。这种模式，并非是线程安全的，因为多个线程同时调用GetInstance()方法，就可能导致有产生多个实例。 没有考虑线程安全与析构问题的懒汉模式的示例代码下面是没有考虑线程安全以及析构问题的懒汉模式的代码实现 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;using namespace std;class singleton&#123;protected: singleton() &#123;&#125;;private: singleton(const singleton&amp;) &#123;&#125;; singleton&amp; operator=(const singleton&amp;) &#123;&#125;; static singleton* m_instance;public: static singleton* GetInstance(); ~singleton() &#123; printf("Singleton destruction\n"); &#125;&#125;;singleton* singleton::GetInstance()&#123; if (m_instance == NULL) &#123; m_instance = new singleton; &#125; return m_instance;&#125;singleton* singleton::m_instance = NULL;int main()&#123; singleton *ct = singleton::GetInstance(); return 0;&#125; 解决懒汉模式线程安全问题的几种方法有下面几种方法 : 使用局部静态变量 加锁 pthread_once DCL 使用局部静态变量使用局部静态变量。局部静态变量的初始化是线程安全的，这一点由编译器保证.（http://gcc.gnu.org/ml/gcc-patches/2004-09/msg00265.html，这是一个 GCC 的 patch，专门解决这个问题）。会在程序退出的时候自动销毁。见此处 这个方法适合 C++11，C++11保证静态局部变量的初始化是线程安全的。如果是 C++98 就不能用这个方法。 非摸板版本12345678910111213class S&#123; public: static S&amp; getInstance() &#123; static S instance; return instance; &#125; private: S() &#123;&#125; S(S const&amp;); // Don't Implement. void operator=(S const&amp;); // Don't implement &#125;; 或者模板版本123456789101112131415161718template&lt;typename T&gt;class Singleton&#123;public: static T&amp; GetInstance() &#123; static T instance; return instance; &#125; Singleton(T&amp;&amp;) = delete; Singleton(const T&amp;) = delete; void operator= (const T&amp;) = delete;protected: Singleton() = default; virtual ~Singleton() = default;&#125;; 加锁线程安全，但每次都有开销。 1234567891011121314151617181920// singleton.hclass Singleton &#123; public: static Singleton *GetInstance() &#123; lock(); if (p == NULL) &#123; p = new Singleton; &#125; unlock(); return p; &#125; private: static Singleon *p; Singleton() &#123;&#125; Singleton(const Singleton &amp;); Singleton&amp; operator= (const Singleton &amp;);&#125;;// singleton.ccSingleton *Singleton::p = NULL; pthread_once陈硕推荐的做法 123456789101112131415161718class Singleton &#123; public: static Singleton *GetInstance() &#123; pthread_once(&amp;ponce_, &amp;Singleton::init); return value_; &#125; private: Singleton() &#123;&#125; Singleton(const Singleton &amp;); Singleton&amp; operator= (const Singleton &amp;); static void init() &#123; value_ = new T(); &#125; static pthread_once_t ponce_; static Singleton *value_;&#125;;pthread_once_t SIngleton::ponce_ = PTHREAD_ONCE_INIT;Singleton* Singleton::value_ = NULL; DCLdouble check locking. 只能用内存屏障，其他做法都是有问题的。参见论文： http://www.aristeia.com/Papers/DDJ_Jul_Aug_2004_revised.pdf普通的 double check 之所以错，是因为乱序执行和多处理器下，不同 CPU 中间 cache 刷往内存并对其他 CPU 可见的顺序无法保障（cache coherency problem）。Singleton&lt;T&gt; *p = new Singleton&lt;T&gt;;, 那么实际有 3 步： 分配内存 构造 赋值给 p 2 和 3 的顺序是未定的（乱序执行！）。因此，如果直接赋值给 p 那么很可能构造还没完成。此时另一个线程调用 GetInstance，在 lock 外面 check 了一下，发现 p!=NULL，于是直接返回 p，使用了未初始化完成的实例，跪了。 那么，如果用中间变量转一下呢？用 tmp_p 转了下以后，tmp_p 赋值给 p 的时候，显然 p 指向的实例是构造完成了的。然而，这个 tmp_p 在编译器看来明显没什么用，会被优化掉。 关于不能自动调用析构的问题上面的两个示例代码( 没有考虑析构问题饿汉模式的示例代码 和 没有考虑线程安全与析构问题的懒汉模式的示例代码 ) 都有不能自动调用析构的问题. 当你运行这两个示例代码之后, 你都会发现并没有打印 “Singleton destruction”, 也就是说程序结束时并没有调用 singleton 类的析构函数的, 为什么呢? 因为 m_instance = new singleton;, new出来的东西需要delete掉, 如果加上一句 delete ct; ct = NULL;, 就会调用析构函数了.但这种手动调用很容易忘啊, 怎么才能自动调用它的析构呢? 我们想要的是 : 自动化的正常删除该实例。 有两种方法, 我给他划分为: 不需要加GC(垃圾回收)内嵌类的单例模式(推荐) 需要加GC(垃圾回收)内嵌类的单例模式 需要加GC内嵌类的单例模式我们先看第二种, 我们知道，程序在结束的时候，系统会自动析构所有的全局变量。事实上，系统也会析构所有的类的静态成员变量，就像这些静态成员也是全局变量一样。利用这个特征，我们可以在单例类中定义一个这样的静态成员变量，而它的唯一工作就是在析构函数中删除单例类的实例。那就是定义一个内部垃圾回收GC类，并且在 singleton 中定义一个此类的静态成员。程序结束时，系统会自动析构此静态成员，此时，在此类的析构函数中析构 singleton 实例，就可以实现 m_instance 的自动释放。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;iostream&gt;using namespace std;class singleton&#123;protected: singleton() &#123;&#125;;private: singleton(const singleton&amp;) &#123;&#125;; singleton&amp; operator=(const singleton&amp;) &#123;&#125;; static singleton* m_instance;public: static singleton* GetInstance(); ~singleton() &#123; printf("Singleton destruction\n"); &#125; class GC &#123; public: ~GC() &#123; printf("GC destruction\n"); if (m_instance) &#123; delete m_instance; m_instance = NULL; &#125; &#125; &#125;; static GC gc_singleton;&#125;;singleton::GC singleton::gc_singleton;singleton* singleton::GetInstance()&#123; if (m_instance == NULL) &#123; m_instance = new singleton(); &#125; return m_instance;&#125;singleton* singleton::m_instance = NULL;int main()&#123; singleton *ct = singleton::GetInstance(); return 0;&#125; 当然还有更好的方法.那就是下面这个不需要加GC内嵌类的单例模式. 不需要加GC内嵌类的单例模式在 GetInstance 方法里放一个 m_instance 的局部静态变量, 然后返回他的地址, 他就可以在程序结束自动调用析构函数.而且这种方法在C++11也能保证线程安全. 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;using namespace std;class singleton&#123;protected: singleton() &#123;&#125;;private: singleton(const singleton&amp;) &#123;&#125;; singleton&amp; operator=(const singleton&amp;) &#123;&#125;;public: static singleton* GetInstance(); ~singleton() &#123; printf("Singleton destruction\n"); &#125;&#125;;singleton* singleton::GetInstance()&#123; static singleton m_instance; return &amp;m_instance;&#125;int main()&#123; singleton *ct = singleton::GetInstance(); return 0;&#125; 总结既要考虑线程安全又要考虑析构问题的话, 有下面几种方法 : 饿汉模式+GC内嵌类 懒汉模式+GC内嵌类, 然后加锁，但每个线程缓存了返回的指针，调用一次有用缓存的指针即可。 懒汉模式+GC内嵌类, 然后 pthread_once 如果是C++11的话, 则可以使用局部静态变量, 因为C++11保证静态局部变量的初始化是线程安全的(C++98不保证), 而且也没有析构问题.]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[new和delete详解]]></title>
    <url>%2F2015%2F01%2F22%2Fnew%E5%92%8Cdelete%E8%AF%A6%E8%A7%A3%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[c++中对new申请的内存的释放方式有delete和delete[]两种方式，到底这两者有什么区别呢？ 疑问我们通常从教科书上看到这样的说明：delete 释放new分配的单个对象指针指向的内存delete[] 释放new分配的对象数组指针指向的内存那么，按照教科书的理解，我们看下下面的代码： 123int *a = new int[10];delete a; //方式1delete [] a; //方式2 肯定会有很多人说方式1肯定存在内存泄漏，是这样吗？ 针对基本数据类型针对简单类型 使用new分配后的不管是数组还是非数组形式内存空间用两种方式均可 如：123int *a = new int[10];delete a;delete [] a; 此种情况中的释放效果相同，原因在于：分配简单类型内存时，内存大小已经确定，系统可以记忆并且进行管理，在析构时，系统并不会调用析构函数，它直接通过指针可以获取实际分配的内存空间，哪怕是一个数组内存空间(在分配过程中 系统会记录分配内存的大小等信息，此信息保存在结构体_CrtMemBlockHeader中，具体情况可参看VC安装目录下CRT\SRC\DBGDEL.cpp) 针对复杂数据类型针对类Class，两种方式体现出具体差异当你通过下列方式分配一个类对象数组： 123456789101112131415class A&#123;private: char *m_cBuffer; int m_nLen;public: A()&#123; m_cBuffer = new char[m_nLen]; &#125; ~A() &#123; delete [] m_cBuffer; &#125;&#125;;A *a = new A[10]; // delete a 仅释放了a指针指向的这个数组本身的全部内存空间, 而且只调用了a[0]对象的析构函数, // 但是剩下的从a[1]到a[9]这9个用户自行分配的m_cBuffer对应内存空间没有释放 从而造成内存泄漏 // 所以只有a和A[0]会变为野指针delete a; delete [] a; //调用使用类对象的析构函数释放用户自己分配内存空间并且释放了a指针指向的全部内存空间 再看下面代码: 12345class A&#123;//...&#125;;A *pa = new A();A *pas = new A[NUM](); delete []pas; //详细流程答案见下文总结 delete []pa; //会发生什么答案是调用未知次数的A的析构函数. 因为delete[]会去通过pa+offset找一个基于pa的偏移量找一个内存里的数据, 他假定这个内存里放了要调用析构的次数n这个数据, 而这个内存里到底是多少是未知的. delete pas; //哪些指针会变成野指针答案是pas和A[0]中的指针会变成野指针. 因为只有这两个指针指向的内存被释放了, 也就是说, 仅释放了pas指针指向的这个数组的全部内存空间, 以及只调用了a[0]对象的析构函数 总结所以总结下就是，关于 new[] 和 delete[]，其中又分为两种情况： 基本数据类型对于像int/char/long/int*/struct等等简单数据类型，由于对象没有destructor，所以用delete 和delete [] 是一样的！ 复杂数据类型类型 delete ptr 代表用来释放内存，且只用来释放ptr指向的内存。 delete[] rg 用来释放rg指向的内存！！还逐一调用数组中每个对象的destructor！！]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>noodle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统设计概要笔记-四]]></title>
    <url>%2F2015%2F01%2F10%2Fdistributed_system_design_note_four%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[分布式系统设计实践基本的理论和策略简单介绍这么多，后面本人会从工程的角度，细化说一下”数据分布“、”副本控制”和”高可用协议” 在分布式系统中，无论是计算还是存储，处理的对象都是数据，数据不存在于一台机器或进程中， . . . 这就牵扯到如何多机均匀分发数据的问题，此小结主要讨论”哈希取模”，”一致性哈希“，”范围表划分“，”数据块划分“ 哈希取模：哈希方式是最常见的数据分布方式，实现方式是通过可以描述记录的业务的id或key(比如用户 id)， 通过Hash函数的计算求余。 余数作为处理该数据的服务器索引编号处理。 如图： 这样的好处是只需要通过计算就可以映射出数据和处理节点的关系，不需要存储映射。 难点就是如果id分布不均匀可能出现计算、存储倾斜的问题，在某个节点上分布过重。 并且当处理节点宕机时，这种”硬哈希“的方式会直接导致部分数据异常，还有扩容非常困难，原来的映射关系全部发生变更。 此处，如果是”无状态“型的节点，影响比较小，但遇到”有状态“的存储节点时，会发生大量数据位置需要变更，发生大量数据迁移的问题。 这个问题在实际生产中，可以通过按2的幂的机器数，成倍扩容的方式来缓解，如图： 不过扩容的数量和方式后收到很大限制。 下面介绍一种”自适应“的方式解决扩容和容灾的问题。 一致性哈希：一致性哈希 – Consistent Hash 是使用一个哈希函数计算数据或数据特征的哈希值，令该哈希函数的输出值域为一个封闭的环，最大值+1=最小值。 将节点随机分布到这个环上，每个节点负责处理从自己开始顺时针至下一个节点的全部哈希值域上的数据，如图： 一致性哈希的优点在于可以任意动态添加、删除节点，每次添加、删除一个节点仅影响一致性哈希环上相邻的节点。 为了尽可能均匀的分布节点和数据，一种常见的改进算法是引入虚节点的概念，系统会创建许多虚拟节点，个数远大于当前节点的个数，均匀分布到一致性哈希值域环上。 读写数据时，首先通过数据的哈希值在环上找到对应的虚节点，然后查找到对应的real节点。 这样在扩容和容错时，大量读写的压力会再次被其他部分节点分摊，主要解决了压力集中的问题。 如图： 数据范围划分：有些时候业务的数据id或key分布不是很均匀，并且读写也会呈现聚集的方式。 比如某些id的数据量特别大，这时候可以将数据按Group划分，从业务角度划分比如id为0~10000，已知8000以上的id可能访问量特别大，那么分布可以划分为[[0~8000],[8000~9000],[9000~1000]]。 将小访问量的聚集在一起。 这样可以根据真实场景按需划分，缺点是由于这些信息不能通过计算获取，需要引入一个模块存储这些映射信息。 这就增加了模块依赖，可能会有性能和可用性的额外代价。 数据块划分：许多文件系统经常采用类似设计，将数据按固定块大小(比如HDFS的64MB)，将数据分为一个个大小固定的块，然后这些块均匀的分布在各个节点，这种做法也需要外部节点来存储映射关系。 由于与具体的数据内容无关，按数据量分布数据的方式一般没有数据倾斜的问题，数据总是被均匀切分并分布到集群中。 当集群需要重新负载均衡时，只需通过迁移数据块即可完成。 如图：]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统设计概要笔记-三]]></title>
    <url>%2F2015%2F01%2F07%2Fdistributed_system_design_note_three%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[分布式系统设计策略重试机制一般情况下，写一段网络交互的代码，发起rpc或者http，都会遇到请求超时而失败情况。 可能是网络抖动(暂时的网络变更导致包不可达，比如拓扑变更)或者对端挂掉。 这时一般处理逻辑是将请求包在一个重试循环块里，如下：[cpp] view plain copy print?int retry = 3;while(!request() &amp;&amp; retry–)sched_yield(); // or usleep(100) . . . 此种模式可以防止网络暂时的抖动，一般停顿时间很短，并重试多次后，请求成功！但不能防止对端长时间不能连接(网络问题或进程问题) 心跳机制心跳顾名思义，就是以固定的频率向其他节点汇报当前节点状态的方式。 收到心跳，一般可以认为一个节点和现在的网络拓扑是良好的。 当然，心跳汇报时，一般也会携带一些附加的状态、元数据信息，以便管理。 如下图： 但心跳不是万能的，收到心跳可以确认ok，但是收不到心跳却不能确认节点不存在或者挂掉了，因为可能是网络原因倒是链路不通但是节点依旧在工作。 所以切记，”心跳“只能告诉你正常的状态是ok，它不能发现节点是否真的死亡，有可能还在继续服务。 (后面会介绍一种可靠的方式 – Lease机制) 副本副本指的是针对一份数据的多份冗余拷贝，在不同的节点上持久化同一份数据，当某一个节点的数据丢失时，可以从副本上获取数据。 数据副本是分布式系统解决数据丢失异常的仅有的唯一途径。 当然对多份副本的写入会带来一致性和可用性的问题，比如规定副本数为3，同步写3份，会带来3次IO的性能问题。 还是同步写1份，然后异步写2份，会带来一致性问题，比如后面2份未写成功其他模块就去读了(下个小结会详细讨论如果在副本一致性中间做取舍)。 中心化/无中心化系统模型这方面，无非就是两种：中心节点，例如mysql的MSS单主双从、MongDB Master、HDFS NameNode、MapReduce JobTracker等，有1个或几个节点充当整个系统的核心元数据及节点管理工作，其他节点都和中心节点交互。 这种方式的好处显而易见，数据和管理高度统一集中在一个地方，容易聚合，就像领导者一样，其他人都服从就好。 简单可行。 但是缺点是模块高度集中，容易形成性能瓶颈，并且如果出现异常，就像群龙无首一样。 无中心化的设计，例如cassandra、zookeeper，系统中不存在一个领导者，节点彼此通信并且彼此合作完成任务。 好处在于如果出现异常，不会影响整体系统，局部不可用。 缺点是比较协议复杂，而且需要各个节点间同步信息。]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[观察者模式]]></title>
    <url>%2F2015%2F01%2F07%2Fobserver_pattern%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[概述一些面向对象的编程方式，提供了一种构建对象间复杂网络互连的能力。当对象们连接在一起时，它们就可以相互提供服务和信息。 通常来说，当某个对象的状态发生改变时，你仍然需要对象之间能互相通信。但是出于各种原因，你也许并不愿意因为代码环境的改变而对代码做大的修改。也许，你只想根据你的具体应用环境而改进通信代码。或者，你只想简单的重新构造通信代码来避免类和类之间的相互依赖与相互从属。 问题当一个对象的状态发生改变时，你如何通知其他对象？是否需要一个动态方案――一个就像允许脚本的执行一样，允许自由连接的方案？ 解决方案观察者模式 ：定义对象间的一种一对多的依赖关系, 当一个对象的状态发生改变时, 所有依赖于它的对象都得到通知并被自动更新。 观察者模式允许一个对象关注其他对象的状态，并且，观察者模式还为被观测者提供了一种观测结构，或者说是一个主体和一个客体。主体，也就是被观测者，可以用来联系所有的观测它的观测者。客体，也就是观测者，用来接受主体状态的改变 观测就是一个可被观测的类（也就是主题）与一个或多个观测它的类（也就是客体）的协作。不论什么时候，当被观测对象的状态变化时，所有注册过的观测者都会得到通知。 观察者模式将被观测者（主体）从观测者（客体）种分离出来。这样，每个观测者都可以根据主体的变化分别采取各自的操作。（观察者模式和 Publish/Subscribe 模式一样，也是一种有效描述对象间相互作用的模式。）观察者模式灵活而且功能强大。对于被观测者来说，那些查询哪些类需要自己的状态信息和每次使用那些状态信息的额外资源开销已经不存在了。另外，一个观测者可以在任何合适的时候进行注册和取消注册。你也可以定义多个具体的观测类，以便在实际应用中执行不同的操作。 将一个系统分割成一系列相互协作的类有一个常见的副作用：需要维护相关对象间的一致性。我们不希望为了维持一致性而使各类紧密耦合，因为这样降低了它们的可重用性。 适用性在以下任一情况下可以使用观察者模式: 当一个抽象模型有两个方面, 其中一个方面依赖于另一方面。将这二者封装在独立的对象中以使它们可以各自独立地改变和复用。 当对一个对象的改变需要同时改变其它对象 , 而不知道具体有多少对象有待改变。 当一个对象必须通知其它对象，而它又不能假定其它对象是谁。换言之 , 你不希望这些对象是紧密耦合的。 结构 模式的组成观察者模式包含如下角色： 目标（Subject）: 目标知道它的观察者。可以有任意多个观察者观察同一个目标。 提供注册和删除观察者对象的接口。 具体目标（ConcreteSubject）: 将有关状态存入各 ConcreteObserver 对象。 观察者 (Observer): 为那些在目标发生改变时需获得通知的对象定义一个更新接口。当它的状态发生改变时, 向它的各个观察者发出通知。 具体观察者 (ConcreteObserver): 维护一个指向 ConcreteSubject 对象的引用。存储有关状态，这些状态应与目标的状态保持一致。实现 O b s e r v e r 的更新接口以使自身状态与目标的状态保持一致。 观察者模式的优缺点Observer 模式允许你独立的改变目标和观察者。你可以单独复用目标对象而无需同时复用其观察者, 反之亦然。它也使你可以在不改动目标和其他的观察者的前提下增加观察者。 下面是观察者模式其它一些优点 : 观察者模式可以实现表示层和数据逻辑层的分离, 并定义了稳定的消息更新传递机制，抽象了更新接口，使得可以有各种各样不同的表示层作为具体观察者角色。 在观察目标和观察者之间建立一个抽象的耦合 ：一个目标所知道的仅仅是它有一系列观察者 , 每个都符合抽象的 Observer 类的简单接口。目标不知道任何一个观察者属于哪一个具体的类。这样目标和观察者之间的耦合是抽象的和最小的。因为目标和观察者不是紧密耦合的, 它们可以属于一个系统中的不同抽象层次。一个处于较低层次的目标对象可与一个处于较高层次的观察者通信并通知它 , 这样就保持了系统层次的完整。如果目标和观察者混在一块 , 那么得到的对象要么横贯两个层次 (违反了层次性), 要么必须放在这两层的某一层中 (这可能会损害层次抽象)。 支持广播通信 : 不像通常的请求, 目标发送的通知不需指定它的接收者。通知被自动广播给所有已向该目标对象登记的有关对象。目标对象并不关心到底有多少对象对自己感兴趣 ; 它唯一的责任就是通知它的各观察者。这给了你在任何时刻增加和删除观察者的自由。处理还是忽略一个通知取决于观察者。 观察者模式符合 “开闭原则” 的要求。 观察者模式的缺点 : 如果一个观察目标对象有很多直接和间接的观察者的话，将所有的观察者都通知到会花费很多时间。 如果在观察者和观察目标之间有循环依赖的话，观察目标会触发它们之间进行循环调用，可能导致系统崩溃。 观察者模式没有相应的机制让观察者知道所观察的目标对象是怎么发生变化的，而仅仅只是知道观察目标发生了变化。 意外的更新 因为一个观察者并不知道其它观察者的存在 , 它可能对改变目标的最终代价一无所知。在目标上一个看似无害的的操作可能会引起一系列对观察者以及依赖于这些观察者的那些对象的更新。此外 , 如果依赖准则的定义或维护不当，常常会引起错误的更新 , 这种错误通常很难捕捉。 简单的更新协议不提供具体细节说明目标中什么被改变了 , 这就使得上述问题更加严重。如果没有其他协议帮助观察者发现什么发生了改变，它们可能会被迫尽力减少改变。 实现在 php 的 SPL 支持观察者模式，SPL 提供了 SplSubject 和 SplObserver 接口。 SplSubject 接口提供了 attach()、detach()、notify() 三个方法。而 SplObserver 接口则提供了 update() 方法。 SplSubject 派生类维护了一个状态，当状态发生变化时 - 比如属性变化等，就会调用 notify() 方法，这时，之前在 attach() 方法中注册的所有 SplObserver 实例的 update() 方法就会被调用。接口定义如下： 1234567891011121314&lt;?php /** * 这一模式的概念是 SplSubject 类维护了一个特定状态，当这个状态发生变化时，它就会调用 notify() 方法。 * 调用 notify() 方法时，所有之前使用 attach() 方法注册的 SplObserver 实例的 update 方法都会被调用。 * */ interface SplSubject&#123; public function attach(SplObserver $observer);// 注册观察者 public function detach(SplObserver $observer);// 释放观察者 public function notify();// 通知所有注册的观察者 &#125; interface SplObserver&#123; public function update(SplSubject $subject);// 观察者进行更新状态 &#125; 实现代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;?php /** * 具体目标 * */ class ConcreteSubject implements SplSubject &#123; private $observers, $value; public function __construct() &#123; $this-&gt;observers = array(); &#125; public function attach(SplObserver $observer) &#123; // 注册观察者 $this-&gt;observers[] = $observer; &#125; public function detach(SplObserver $observer) &#123; // 释放观察者 if($idx = array_search($observer,$this-&gt;observers,true)) &#123; unset($this-&gt;observers[$idx]); &#125; &#125; public function notify() &#123; // 通知所有观察者 foreach($this-&gt;observers as $observer) &#123; $observer-&gt;update($this); &#125; &#125; public function setValue($value) &#123; $this-&gt;value = $value; $this-&gt;notify(); &#125; public function getValue() &#123; return $this-&gt;value; &#125; &#125; /** * 具体观察者 * */ class ConcreteObserver1 implements SplObserver &#123; public function update(SplSubject $subject) &#123; echo 'ConcreteObserver1 value is',$subject-&gt;getValue(), '&lt;br&gt;'; &#125; &#125; /** * 具体观察者 * */ class ConcreteObserver2 implements SplObserver &#123; public function update(SplSubject $subject) &#123; echo 'ConcreteObserver2 value is', $subject-&gt;getValue(), '&lt;br&gt;'; &#125; &#125; $subject = new ConcreteSubject(); $observer1 = new ConcreteObserver1(); $observer2 = new ConcreteObserver2(); $subject-&gt;attach($observer1); $subject-&gt;attach($observer2); $subject-&gt;setValue(5); ?&gt; 我们扩展上面的例子，根据目标状态而更新不同的观察者： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136&lt;?php /** * 具体目标 * */ class ConcreteSubject implements SplSubject &#123; private $observers, $_state; public function __construct() &#123; $this-&gt;observers = array(); &#125; /** * 注册观察者 * * @param SplObserver $observer */ public function attach(SplObserver $observer) &#123; $this-&gt;observers[] = $observer; &#125; /** * // 释放观察者 * * @param SplObserver $observer */ public function detach(SplObserver $observer) &#123; if($idx = array_search($observer,$this-&gt;observers,true)) &#123; unset($this-&gt;observers[$idx]); &#125; &#125; /** * 通知所有观察者 * */ public function notify() &#123; /** * 只要状态改变，就通知观察者 */ foreach($this-&gt;observers as $observer) &#123; if ($observer-&gt;getState() == $this-&gt;_state) &#123; $observer-&gt;update($this); &#125; &#125; &#125; /** * 设置状态 * * @param unknown_type $state */ public function setState($state) &#123; $this-&gt;_state = $state; $this-&gt;notify(); &#125; public function getState() &#123; return $this-&gt;_state; &#125; &#125; /** * 抽象观摩者 * */ abstract class observer&#123; private $_state; function __construct($state) &#123; $this-&gt;_state = $state; &#125; public function setState($state) &#123; $this-&gt;_state = $state; $this-&gt;notify(); &#125; public function getState() &#123; return $this-&gt;_state; &#125; &#125; /** * 具体观察者 1 * */ class ConcreteObserver1 extends observer implements SplObserver &#123; function __construct($state) &#123; parent::__construct($state); &#125; public function update(SplSubject $subject) &#123; echo 'ConcreteObserver1 state is',$subject-&gt;getState(), '&lt;br&gt;'; &#125; &#125; /** * 具体观察者 2 * */ class ConcreteObserver2 extends observer implements SplObserver &#123; function __construct($state) &#123; parent::__construct($state); &#125; public function update(SplSubject $subject) &#123; echo 'ConcreteObserver2 state is', $subject-&gt;getState(), '&lt;br&gt;'; &#125; &#125; /** * 具体观察者 3 * */ class ConcreteObserver3 extends observer implements SplObserver &#123; function __construct($state) &#123; parent::__construct($state); &#125; public function update(SplSubject $subject) &#123; echo 'ConcreteObserver3 state is', $subject-&gt;getState(), '&lt;br&gt;'; &#125; &#125; $subject = new ConcreteSubject(); $observer1 = new ConcreteObserver1(1); $observer2 = new ConcreteObserver2(1); $observer3 = new ConcreteObserver3(2); $subject-&gt;attach($observer1); $subject-&gt;attach($observer2); $subject-&gt;attach($observer3); echo 'Subject state is 1', '&lt;br&gt;'; $subject-&gt;setState(1); echo 'Subject state is 2', '&lt;br&gt;'; $subject-&gt;setState(2); ?&gt; 与其他相关模式 终结者模式 Mediator: 通过封装复杂的更新语义 , ChangeManager 充当目标和观察者之间的中介者。 单例模式 Singleton: ChangeManager 可使用 Singleton 模式来保证它是唯一的并且是可全局访问的。 总结与分析通过 Observer 模式，把一对多对象之间的通知依赖关系的变得更为松散，大大地提高了程序的可维护性和可扩展性，也很好的符合了开放 - 封闭原则。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工厂模式]]></title>
    <url>%2F2015%2F01%2F07%2Ffactory_pattern%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[分类工厂模式主要是为创建对象提供过渡接口，以便将创建对象的具体过程屏蔽隔离起来，达到提高灵活性的目的。 工厂模式可以分为三类： 简单工厂模式 （Simple Factory, 简单工厂模式可看为工厂方法模式的一种特例，两者归为一类。 ） 工厂方法模式 （Factory Method） 抽象工厂模式 （Abstract Factory） 这三种模式从上到下逐步抽象，并且更具一般性。 GOF在《设计模式》一书中将工厂模式分为两类：工厂方法模式（Factory Method）与抽象工厂模式（Abstract Factory）。 区别 简单工厂模式 ： 一个工厂类, 这个工厂类能创建多个具体产品类的实例。 一个抽象产品类，可以派生出多个具体产品类。 工厂方法模式 ： 一个抽象工厂类，可以派生出多个具体工厂类。 一个抽象产品类，可以派生出多个具体产品类。 每个具体工厂类只能创建一个具体产品类的实例。 抽象工厂模式 ： 一个抽象工厂类，可以派生出多个具体工厂类。 多个抽象产品类，每个抽象产品类可以派生出多个具体产品类。 每个具体工厂类可以创建多个具体产品类的实例。 区别： 工厂方法模式只有一个抽象产品类，而抽象工厂模式有多个。 工厂方法模式的具体工厂类只能创建一个具体产品类的实例，而抽象工厂模式可以创建多个。 简单工厂模式产品类1234567891011121314151617181920&lt;?php/** 车子系列 */abstract Class BWM&#123; function construct($pa) &#123; &#125;&#125;Class BWM320 extends BWM&#123; function construct($pa) &#123; &#125;&#125;Class BMW523 extends BWM&#123; function construc($pb)&#123; &#125;&#125; 工厂类12345678910111213141516/** 工厂创建车 */class Factory &#123; static function createBMW($type)&#123; switch ($type) &#123; case 320: return new BWM320(); case 523: return new BMW523(); //…. &#125;&#125; 客户类12345678910/** 客户通过工厂获取车 */class Customer &#123; private $BMW; function getBMW($type)&#123; $this¬-&gt; BMW = Factory::createBMW($type); &#125;&#125; 工厂方法模式产品类1234567891011121314151617181920&lt;?php/** 车子系列 /abstract Class BWM&#123;function construct($pa) &#123;&#125;&#125;Class BWM320 extends BWM&#123;function construct($pa) &#123;&#125;&#125;Class BMW523 extends BWM&#123; function construc($pb)&#123;&#125;&#125; 创建工厂类123456789101112131415161718192021222324252627282930/** 创建工厂的接口 */interface FactoryBMW &#123; function createBMW();&#125;/** 创建BWM320车 **/class FactoryBWM320 implements FactoryBMW &#123; function createBMW($type)&#123; return new BWM320(); &#125;&#125;/** 创建BWM523车 **/class FactoryBWM523 implements FactoryBMW &#123; function createBMW($type)&#123; return new BMW523(); &#125;&#125; 客户类12345678910111213141516171819/** 客户得到车 */class Customer &#123; private $BMW; function getBMW($type)&#123; switch ($type) &#123; case 320: $BWM320 = new FactoryBWM320(); return $BWM320-&gt;createBMW(); case 523: $BWM523 = new FactoryBWM523(); return $BWM320-&gt;createBMW(); //…. &#125; &#125;&#125; 抽象工厂模式产品类123456789101112131415161718192021222324252627&lt;?php/** 车子系列以及型号 */abstract class BWM&#123;&#125;class BWM523 extends BWM &#123;&#125;class BWM320 extends BWM &#123;&#125;/* 空调 */abstract class aircondition&#123;&#125;class airconditionBWM320 extends aircondition &#123;&#125;class airconditionBWM52 extends aircondition &#123;&#125; 创建工厂类123456789101112131415161718192021222324252627282930313233343536/* 创建工厂的接口 */interface FactoryBMW &#123; function createBMW(); function createAirC();&#125;/** 创建BWM320车 /class FactoryBWM320 implements FactoryBMW &#123; function createBMW()&#123; return new BWM320();&#125;function createAirC()&#123; //空调 return new airconditionBWM320();&#125;&#125;/* 创建BWM523车 */class FactoryBWM523 implements FactoryBMW &#123; function createBMW()&#123; return new BWM523();&#125;function createAirC()&#123; return new airconditionBWM523();&#125;&#125; 客户1234567891011121314/* 客户得到车 */class Customer &#123; private $BMW; private $airC; function getBMW($type)&#123; $class = new ReflectionClass(‘FactoryBWM’ .$type );//建立 Person这个类的反射类 $instance = $class-&gt;newInstanceArgs();//相当于实例化Person 类 $this-&gt;BMW = $instance-&gt;createBMW(); $this-&gt;airC = $instance-&gt;createAirC(); &#125;&#125;]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>DesignPattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统设计概要笔记-二]]></title>
    <url>%2F2015%2F01%2F05%2Fdistributed_system_design_note_two%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[分布式系统特性CAP是分布式系统里最著名的理论，wiki百科如下 Consistency(all nodes see the same data at the same time) Availability (a guarantee that every request receives a response about whether it was successful or failed) Partition tolerance (the system continues to operate despite arbitrary message loss or failure of part of the system)(摘自 ：http://en.wikipedia.org/wiki/CAP_theorem) . . . 早些时候，国外的大牛已经证明了CAP三者是不能兼得，很多实践也证明了。 本人就不挑战权威了，感兴趣的同学可以自己Google。 本人以自己的观点总结了一下： 一致性可以参考陈皓的博文&lt;&lt;分布式事务处理&gt;&gt; 描述当前所有节点存储数据的统一模型，分为强一致性和弱一致性：强一致性描述了所有节点的数据高度一致，无论从哪个节点读取，都是一样的。 无需担心同一时刻会获得不同的数据。 是级别最高的，实现的代价比较高如图： 弱一致性又分为单调一致性和最终一致性： 1、单调一致性强调数据是按照时间的新旧，单调向最新的数据靠近，不会回退，如： 数据存在三个版本v1-&gt;v2-&gt;v3，获取只能向v3靠近(如取到的是v2，就不可能再次获得v1) 2、最终一致性强调数据经过一个时间窗口之后，只要多尝试几次，最终的状态是一致的，是最新的数据 如图： 强一致性的场景，就好像交易系统，存取钱的+/-操作必须是马上一致的，否则会令很多人误解。 弱一致性的场景，大部分就像web互联网的模式，比如发了一条微博，改了某些配置，可能不会马上生效，但刷新几次后就可以看到了，其实弱一致性就是在系统上通过业务可接受的方式换取了一些系统的低复杂度和可用性。 可用性保证系统的正常可运行性，在请求方看来，只要发送了一个请求，就可以得到恢复无论成功还是失败（不会超时）! 分区容忍性在系统某些节点或网络有异常的情况下，系统依旧可以继续服务。 这通常是有负载均衡和副本来支撑的。 例如计算模块异常可通过负载均衡引流到其他平行节点，存储模块通过其他几点上的副本来对外提供服务。 扩展性扩展性是融合在CAP里面的特性，我觉得此处可以单独讲一下。 扩展性直接影响了分布式系统的好坏，系统开发初期不可能把系统的容量、峰值都考虑到，后期肯定牵扯到扩容，而如何做到快而不太影响业务的扩容策略，也是需要考虑的。 (后面在介绍数据分布时会着重讨论这个问题)]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统设计概要笔记-一]]></title>
    <url>%2F2015%2F01%2F04%2Fdistributed_system_design_note_one%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[分布式系统中的概念最简单的分布式系统分布式可繁也可以简，最简单的分布式就是大家最常用的， 在负载均衡服务器后加一堆web服务器，然后在上面搞一个缓存服务器来保存临时状态， 后面共享一个数据库，其实很多号称分布式专家的人也就停留于此， 大致结构如下图所示： 这种环境下真正进行分布式的只是web server而已， 并且web server之间没有任何联系，所以结构和实现都非常简单。 最完备的分布式体系的模块组成有些情况下，对分布式的需求就没这么简单， 在每个环节上都有分布式的需求， 比如Load Balance、DB、Cache和文件等等， 并且当分布式节点之间有关联时， 还得考虑之间的通讯， 另外， 节点非常多的时候， 得有监控和管理来支撑。这样看起来， 分布式是一个非常庞大的体系， 只不过你可以根据具体需求进行适当地裁剪。按照最完备的分布式体系来看， 可以由以下模块组成： 分布式任务处理服务：负责具体的业务逻辑处理 分布式节点注册和查询：负责管理所有分布式节点的命名和物理信息的注册与询，是节点之间联系的桥梁 分布式DB：分布式结构化数据存取 分布式Cache：分布式缓存数据（非持久化）存取 分布式文件：分布式文件存取 网络通信：节点之间的网络数据通信 监控管理：搜集、监控和诊断所有节点运行状态 分布式编程语言：用于分布式环境下的专有编程语言，比如Elang、Scala 分布式算法：为解决分布式环境下一些特有问题的算法，比如解决一致性问题的Paxos算法 三元组其实，分布式系统说白了，就是很多机器组成的集群，靠彼此之间的网络通信，担当的角色可能不同，共同完成同一个事情的系统。 . . . 如果按”实体“来划分的话，就是如下这几种： 1、节点 – 系统中按照协议完成计算工作的一个逻辑实体，可能是执行某些工作的进程或机器 2、网络 – 系统的数据传输通道，用来彼此通信。 通信是具有方向性的。 3、存储 – 系统中持久化数据的数据库或者文件存储。 状态特性各个节点的状态可以是“无状态”或者“有状态的”, 一般认为，节点是偏计算和通信的模块，一般是无状态的。 这类应用一般不会存储自己的中间状态信息，比如Nginx，一般情况下是转发请求而已，不会存储中间信息。 另一种“有状态”的，如MySQL等数据库，状态和数据全部持久化到磁盘等介质。 “无状态”的节点一般我们认为是可随意重启的，因为重启后只需要立刻工作就好。 “有状态”的则不同，需要先读取持久化的数据，才能开始服务。 所以，“无状态”的节点一般是可以随意扩展的，“有状态”的节点需要一些控制协议来保证扩展。 系统异常异常，可认为是节点因为某种原因不能工作，此为节点异常。 还有因为网络原因，临时、永久不能被其他节点所访问，此为网络异常。 在分布式系统中，要有对异常的处理，保证集群的正常工作。 分布式系统与单节点的不同从linux write()系统调用说起众所周知，在unix/linux/mac(类Unix)环境下，两个机器通信，最常用的就是通过socket连接对方。 传输数据的话，无非就是调用write()这个系统调用，把一段内存缓冲区发出去。 但是可以进一步想一下，write()之后能确认对方收到了这些数据吗？ 答案肯定是不能，原因就是发送数据需要走内核-&gt;网卡-&gt;链路-&gt;对端网卡-&gt;内核，这一路径太长了，所以只能是异步操作。 write()把数据写入内核缓冲区之后就返回到应用层了，具体后面何时发送、怎么发送、TCP怎么做滑动窗口、流控都是tcp/ip协议栈内核的事情了。 所以在应用层，能确认对方受到了消息只能是对方应用返回数据，逻辑确认了这次发送才认为是成功的。 这就却别与单系统编程，大部分系统调用、库调用只要返回了就说明已经确认完成了。 TCP/IP协议是“不可靠”的教科书上明确写明了互联网是不可靠的，TCP实现了可靠传输。 何来“不可靠”呢？先来看一下网络交互的例子，有A、B两个节点，之间通过TCP连接，现在A、B都想确认自己发出的任何一条消息都能被对方接收并反馈，于是开始了如下操作：A-&gt;B发送数据，然后A需要等待B收到数据的确认，B收到数据后发送确认消息给A，然后B需要等待A收到数据的确认，A收到B的数据确认消息后再次发送确认消息给B，然后A又去需要等待B收到的确认。 死循环了！！ 其实，这就是著名的“拜占庭将军”问题 所以，通信双方是“不可能”同时确认对方受到了自己的信息。 而教科书上定义的其实是指“单向”通信是成立的，比如A向B发起Http调用， 收到了HttpCode 200的响应包，这只能确认，A确认B收到了自己的请求，并且B正常处理了，不能确认的是B确认A受到了它的成功的消息。 不可控的状态在单系统编程中，我们对系统状态是非常可控的。 比如函数调用、逻辑运算，要么成功，要么失败，因为这些操作被框在一个机器内部，cpu/总线/内存都是可以快速得到反馈的。 开发者可以针对这两个状态很明确的做出程序上的判断和后续的操作。 而在分布式的网络环境下，这就变得微妙了。 比如一次rpc、http调用，可能成功、失败，还有可能是“超时”，这就比前者的状态多了一个不可控因素，导致后面的代码不是很容易做出判断。 试想一下，用A用支付宝向B转了一大笔钱，当他按下“确认”后，界面上有个圈在转啊转，然后显示请求超时了，然后A就抓狂了，不知道到底钱转没转过去，开始确认自己的账户、确认B的账户、打电话找客服等等。 所以分布式环境下，我们的其实要时时刻刻考虑面对这种不可控的“第三状态”设计开发，这也是挑战之一。 视异常为正常单系统下，进程/机器的异常概率十分小。 即使出现了问题，可以通过人工干预重启、迁移等手段恢复。 但在分布式环境下，机器上千台，每几分钟都可能出现宕机、死机、网络断网等异常，出现的概率很大。 所以，这种环境下，进程core掉、机器挂掉都是需要我们在编程中认为随时可能出现的，这样才能使我们整个系统健壮起来，所以”容错“是基本需求。 异常可以分为如下几类： 节点错误：一般是由于应用导致，一些coredump和系统错误触发，一般重新服务后可恢复。 硬件错误：由于磁盘或者内存等硬件设备导致某节点不能服务，需要人工干预恢复。 网络错误：由于点对点的网络抖动，暂时的访问错误，一般拓扑稳定后或流量减小可以恢复。 网络分化： 网络中路由器、交换机错误导致网络不可达，但是网络两边都正常，这类错误比较难恢复，并且需要在开发时特别处理。 【这种情况也会比较前面的问题较难处理】]]></content>
      <categories>
        <category>NP</category>
      </categories>
      <tags>
        <tag>Distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么不推荐递归以及什么是尾递归]]></title>
    <url>%2F2015%2F01%2F02%2Fwhy_not_recursion_and_what_is_tail_recursion%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[为什么不推荐递归递归的调试难度奇高，就决定了实际项目中很少用递归。 而且递归确实运行效率低，因为函数一层一层调用存在调用栈， 在切换到更深层的函数时要产生断点，为了保证回来时继续运行， 必须保存现在所处函数的各种状态，回来时恢复状态，这样一层层下去性能损失就不断增加。 大量开辟在栈区的内存 ，直到每一层的递归结束或整个递归结束才释放 且这个内存空间可能呈几何级数增加， 空间效率不佳， 有可能会栈溢出 而要知道什么是尾递归， 首先得指到什么是尾调用 . . . 尾调用尾调用的概念非常简单，一句话就能说清楚，就是指某个函数的最后一步是调用另一个函数 123function f(x)&#123; return g(x);&#125; 上面代码中，函数f的最后一步是调用函数g，这就叫尾调用。 以下两种情况，都不属于尾调用。 12345678910// 情况一function f(x)&#123; let y = g(x); return y;&#125;// 情况二function f(x)&#123; return g(x) + 1;&#125; 上面代码中，情况一是调用函数g之后，还有别的操作，所以不属于尾调用，即使语义完全一样。情况二也属于调用后还有操作，即使写在一行内。 尾调用不一定出现在函数尾部，只要是最后一步操作即可。 123456function f(x) &#123; if (x &gt; 0) &#123; return m(x) &#125; return n(x);&#125; 上面代码中，函数m和n都属于尾调用，因为它们都是函数f的最后一步操作。 为什么推荐尾调用Lua 中函数的另一个有趣的特征是可以正确的处理尾调用（proper tail recursion，一些书使用术语“尾递归”，虽然并未涉及到递归的概念）。尾调用是一种类似在函数结尾的 goto 调用，当函数最后一个动作是调用另外一个函数时，我们称这种调用尾调用。例如：123function f(x) return g(x)end g 的调用是尾调用。例子中 f 调用 g 后不会再做任何事情，这种情况下当被调用函数 g 结束时程序不需 要返回到调用者 f； 所以尾调用之后程序不需要在栈中保留关于调用者的任何信息。一些编译器比如 Lua 解释器利用这种特性在处理尾调用时不使用额外的栈，我们称这种语言支持正确的尾调用. 由于尾调用不需要使用栈空间，那么尾调用递归的层次可以无限制的。例如下面调用不论 n 为何值不会导致栈溢出。123function foo (n) if n &gt; 0 then return foo(n - 1) endend 什么是尾递归函数调用自身，称为递归。如果尾调用自身，就称为尾递归。 尾递归具体例子112345long Rescuvie(long n) &#123; return (n == 1) ? 1 : n Rescuvie(n - 1);&#125; 尾递归: 123456789101112long TailRescuvie(long n, long a) &#123; return (n == 1) ? a : TailRescuvie(n - 1, a * n);&#125;long TailRescuvie(long n) &#123;//封装用的 return (n == 0) ? 1 : TailRescuvie(n, 1);&#125; 当n = 5时 对于线性递归, 他的递归过程如下: Rescuvie(5) {5 Rescuvie(4)} {5 {4 Rescuvie(3)}} {5 {4 {3 Rescuvie(2)}}} {5 {4 {3 {2 Rescuvie(1)}}}} {5 {4 {3 {2 1}}}} {5 {4 {3 2}}} {5 {4 6}} {5 * 24} 120 对于尾递归, 他的递归过程如下: TailRescuvie(5) TailRescuvie(5, 1) TailRescuvie(4, 5) TailRescuvie(3, 20) TailRescuvie(2, 60) TailRescuvie(1, 120) 120 很容易看出, 普通的线性递归比尾递归更加消耗资源, 在实现上说, 每次重复的过程 调用都使得调用链条不断加长. 系统不得不使用栈进行数据保存和恢复.而尾递归就 不存在这样的问题, 因为他的状态完全由n和a保存. 尾递归具体例子2具体事例2 快速排序算法实施尾递归优化1234567891011121314151617void quickSort(SqList * list , int low ,int high)&#123; int pivot; while(low&lt;high) &#123; pivot=Partition(list,low,high); quickSort(list, low,pivot - 1); //quickSort(list,low,pivot-1); 原递归调用 //quickSort(list,pivot+1,high); low = pivot+1; /*尾递归*/ &#125;&#125;]]></content>
      <categories>
        <category>Misc</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据结构四之链表进阶]]></title>
    <url>%2F2014%2F12%2F22%2Fadvanced_linked_list%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[只谈一下单链表, 链表实在是太重要, 是前面两篇说算法博客的基础, 了解了其应用和衍生, 再去了解其本身就有动力了 存储结构12345typedef struct Node&#123; DataType data; struct Node *next;&#125;Node, *Node_Ptr; 链表中第一个结点的存储位置叫做头指针 头指针和头结点不同，头结点即第一个结点，头指针是指向第一个结点的指针。链表中可以没有头结点，但不能没有头指针。 如果链表有头结点，那么头指针就是指向头结点数据域的指针。 单链表也可以没有头结点 头结点的优点: 头结点是为了操作的统一与方便而设立的，放在第一个元素结点之前，其数据域一般无意义（当然有些情况下也可存放链表的长度、用做监视哨等等）。 有了头结点后，对在第一个元素结点前插入结点和删除第一个结点，其操作与对其它结点的操作统一了。 有头结点和无头结点的建立链表方法头插法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;typedef struct Link &#123; int elem; struct Link *next;&#125;link;//无头结点链表的头插法实现函数link * creatLink(int * arc, int length) &#123; int i; //最初状态下，头指针 H 没有任何结点，所以，插入第一个元素，就相当于是创建结点 H link * H =(link*)malloc(sizeof(link)); H-&gt;elem = arc[0]; H-&gt;next = NULL; //如果采用头插法插入超过 1 个元素，则可添加到第一个结点 H 之前 for (i = 1; i&lt;length; i++) &#123; link * a = (link*)malloc(sizeof(link)); a-&gt;elem = arc[i]; //插入元素时，首先将插入位置后的链表链接到新结点上 a-&gt;next = H; //然后再链接头指针 H H = a; &#125; return H;&#125;//有头结点链表的头插法实现函数link * HcreatLink(int * arc, int length) &#123; int i; //创建头结点 H，其链表的头指针也是 H link * H = (link*)malloc(sizeof(link)); H-&gt;elem = 0; H-&gt;next = NULL; //采用头插法创建链表 for (i = 0; i&lt;length; i++) &#123; link * a = (link*)malloc(sizeof(link)); a-&gt;elem = arc[i]; //首先将插入位置之后的链表链接到新结点 a 上 a-&gt;next = H-&gt;next; //将新结点 a 插入到头结点之后的位置 H-&gt;next = a; &#125; return H;&#125;//链表的输出函数void display(link *p) &#123; while (p) &#123; printf("%d ", p-&gt;elem); p = p-&gt;next; &#125; printf("\n");&#125;int main() &#123; int a[3] = &#123; 1,2,3 &#125;; //采用头插法创建无头结点链表 link * H = creatLink(a, 3); display(H); //采用头插法创建有头结点链表 link * head = HcreatLink(a, 3); display(head); //使用完毕后，释放即可 free(H); free(head); return 0;&#125; 运行结果：123 2 10 3 2 1 提示：没有 0 的为无头结点的头插法输出结果，有 0 的为有头结点的头插法的输出结果 . . . 在O(1)时间删除链表结点给定单链表的头指针和一个结点指针, 定义一个函数在 O(1)时间删除该结点. 思路 : 如果要遍历找到该结点的前一个结点p, 来改变结点p的下一个结点指向的这种解法肯定是O(n)时间复杂度了.那是不是一定需要得到被删除的结点的前一个结点呢？答案是否定的。我们可以很方便地得到要删除的结点的下一个结点。如果我们把下一个结点的内容复制到需要删除的结点上覆盖原有的内容，再把下一个结点删除，那是不是就相当于把当前需要删除的结点删除了？ 找一个单链表的中间结点算法思想 : (快慢指针的使用)设置两个指针，一个每次移动两个位置，一个每次移动一个位置，当第一个指针到达尾节点时，第二个指针就达到了中间节点的位置 判断链表中是否有环算法思想 : (快慢指针的使用)链表中有环，其实也就是自相交. 用两个指针pslow和pfast从头开始遍历链表，pslow每次前进一个节点，pfast每次前进两个结点，若存在环，则pslow和pfast肯定会在环中相遇，若不存在，则pslow和pfast能正常到达最后一个节点 判断两个链表是否相交, 假设两个链表均不带环算法思想 : 如果两个链表相交于某一节点，那么在这个相交节点之后的所有节点都是两个链表所共有的。也就是说，如果两个链表相交，那么最后一个节点肯定是共有的。先遍历第一个链表，记住最后一个节点，然后遍历第二个链表，到最后一个节点时和第一个链表的最后一个节点做比较，如果相同，则相交，否则不相交。 从尾到头打印链表有两种解法 : 反转链表解法 : 反转链表之后再从头到尾打印 (这样会改变原来的链表) 栈存储解法(比较简单, 本文不详讲了) : 用栈存储之后再逐个出栈一一打印 (这样不会改变原来的链表) 反转链表解法比如一个链表:头结点-&gt;A-&gt;B-&gt;C-&gt;D-&gt;E反转成为:头结点-&gt;E-&gt;D-&gt;C-&gt;B-&gt;A 算法思想 :第一轮 : 头结点-&gt;A-&gt;B-&gt;C-&gt;D-&gt;E第二轮 : 头结点-&gt;B-&gt;A-&gt;C-&gt;D-&gt;E第三轮 : 头结点-&gt;C-&gt;B-&gt;A-&gt;D-&gt;E第四轮 : 头结点-&gt;D-&gt;C-&gt;B-&gt;A-&gt;E第五轮 : 头结点-&gt;E-&gt;D-&gt;C-&gt;B-&gt;A 算法cpp实现：手写的代码， 已经跑过了，可直接用下面代码中反转函数为 ReverseList ， 且有详细注释以及总结 LinkedList.h12345678910111213141516#pragma oncestruct TList&#123; struct TList *pNext; void *pData;&#125;;typedef struct TList *LPTLIST;void AppendElem(LPTLIST *ppstHead);void ReverseList(LPTLIST *ppstHead);void PrintList(LPTLIST *ppstHead);void DestroyList(LPTLIST *ppstHead); LinkedList.cpp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162#include "LinkedList.h"#include &lt;iostream&gt;using std::cout;using std::endl;using std::cin;void AppendElem(LPTLIST *ppstHead)&#123; if (!ppstHead) &#123; cout &lt;&lt; "ppstHead is null" &lt;&lt; endl; return; &#125; if (!*ppstHead) &#123; *ppstHead = new TList; if (!*ppstHead) &#123; cout &lt;&lt; "*ppstHead malloc error" &lt;&lt; endl; return; &#125; (*ppstHead)-&gt;pData = nullptr; (*ppstHead)-&gt;pNext = nullptr; &#125; LPTLIST temp_elem_ptr = *ppstHead; cout &lt;&lt; "input '.' to finish" &lt;&lt; endl; char key_data = '.'; while (1) &#123; cin &gt;&gt; key_data; if (key_data != '.') &#123; char * temp_key_data = new char; if (!temp_key_data) &#123; cout &lt;&lt; "temp_key_data malloc error" &lt;&lt; endl; return; &#125; *temp_key_data = key_data; LPTLIST new_elem_ptr = new TList; if (!new_elem_ptr) &#123; cout &lt;&lt; "new_elem malloc error" &lt;&lt; endl; return; &#125; new_elem_ptr-&gt;pData = temp_key_data; new_elem_ptr-&gt;pNext = nullptr; temp_elem_ptr-&gt;pNext = new_elem_ptr; temp_elem_ptr = new_elem_ptr; &#125; else &#123; break; &#125; &#125;&#125;/* 将单链表反转, 要求只能扫描链表一次.*@param ppstHead 指向链表首节点的指针*/void ReverseList(LPTLIST *ppstHead)&#123; if (!ppstHead) &#123; cout &lt;&lt; "ppstHead is null" &lt;&lt; endl; return; &#125; if (!*ppstHead) &#123; cout &lt;&lt; "*ppstHead is null" &lt;&lt; endl; return; &#125; // 我们只用上述算法思想中第二轮来说明一下此算法, 即为 "第二轮 : 头结点-&gt;B-&gt;A-&gt;C-&gt;D-&gt;E" // origin_first_elem_ptr指针一直指向着原来链表头指针后面的那个元素 //（即原第一个元素， 这个指针的指向一直都不会变， 一直都是指向A） LPTLIST origin_first_elem_ptr = (*ppstHead)-&gt;pNext; LPTLIST temp_elem_ptr = nullptr; // 需要两个判断, 不然当 origin_first_elem_ptr 为NULL的时候会出错, // 且 origin_first_elem_ptr -&gt;pNext为NULL的时候也没必要继续循环了 while (origin_first_elem_ptr &amp;&amp; origin_first_elem_ptr-&gt;pNext) &#123; // 临时保存一下元素A后面的后面那个元素C temp_elem_ptr = origin_first_elem_ptr-&gt;pNext-&gt;pNext; // 让B指向A : B-&gt;A (第1步) origin_first_elem_ptr-&gt;pNext-&gt;pNext = (*ppstHead)-&gt;pNext; // 把目前第一个元素A替换为原第一个元素的后面那个元素B : 头结点-&gt;B (第2步) (*ppstHead)-&gt;pNext = origin_first_elem_ptr-&gt;pNext; // 原第一个元素A的pnext指到它后面的后面那个元素C : A-&gt;C (第3步) origin_first_elem_ptr-&gt;pNext = temp_elem_ptr; &#125; // 综上所述只需要3步, 链表反转需要两个指针， // 见上面两个指针, 一个 origin_first_elem_ptr, 一个 temp_elem_ptr // 且要注意while条件中循环的是origin_first_elem_ptr, 而非ppstHead&#125;void PrintList(LPTLIST *ppstHead)&#123; if (!ppstHead) &#123; cout &lt;&lt; "ppstHead is null" &lt;&lt; endl; return; &#125; if (!*ppstHead) &#123; cout &lt;&lt; "*ppstHead is null" &lt;&lt; endl; return; &#125; LPTLIST temp_elem_ptr = *ppstHead; while (temp_elem_ptr = temp_elem_ptr-&gt;pNext) &#123; cout &lt;&lt; *(char *)(temp_elem_ptr-&gt;pData) &lt;&lt; "-&gt;"; &#125; cout &lt;&lt; endl;&#125;void DestroyList(LPTLIST *ppstHead)&#123; if (!ppstHead) &#123; cout &lt;&lt; "ppstHead is null" &lt;&lt; endl; return; &#125; if (!*ppstHead) &#123; cout &lt;&lt; "*ppstHead is null" &lt;&lt; endl; return; &#125; LPTLIST temp_elem_ptr = *ppstHead, temp_next_ptr = nullptr; while (temp_elem_ptr) &#123; temp_next_ptr = temp_elem_ptr-&gt;pNext; delete (char *)(temp_elem_ptr-&gt;pData); temp_elem_ptr-&gt;pData = nullptr; delete temp_elem_ptr; temp_elem_ptr = temp_next_ptr; &#125; cout &lt;&lt; "DestroyList finished." &lt;&lt; endl;&#125; main.cpp123456789101112131415#include "LinkedList.h"int main()&#123; TList *test_list = nullptr; AppendElem(&amp;test_list); PrintList(&amp;test_list); ReverseList(&amp;test_list); PrintList(&amp;test_list); DestroyList(&amp;test_list); return 0;&#125;]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>DataStructure</tag>
        <tag>Algo</tag>
        <tag>noodle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVN的UpdateItemToThisRevision和RevertToThisRevision和UpdateItemToRevision的区别]]></title>
    <url>%2F2014%2F12%2F13%2FSVN%E7%9A%84UpdateItemToThisRevision%E5%92%8CRevertToThisRevision%E5%92%8CUpdateItemToRevision%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[前言使用SVN在管理代码的时候免不了进行代码的合并和还原，特别是当前版本的修改发现有重大问题的时候，还原是避免不了的，那么究竟应该怎样操作呢？ 内容使用SVN查看文件或目录的日志的时候，右键单击日志记录会弹出下面这个界面，今天我们来着重了解一下被红圈标记的三个选项——“Update item to this version”，“Revert to this version”，“Revert changes from this version”，这三个选项对于刚接触SVN的人确实不太好区分，一开始我也搞不懂，直到亲自试验一下才搞清楚这三个选项的用法。 在讲解这三个选项的作用之前，我们还是先来假定一个使用情景，假设我们的项目文件一共有8个版本，它版本号分别是1，2，3，4，5，6，7，8。 Update item to this version这个选项的作用是将文件版本更新到对应所选的版本（当然内容也修改到了相应的版本）。如果我们是在版本4这里点击“Update item to this version”，表示5~8版本所作的修改全部作废，这个文件的历史回退到了版本4那个时代，但是需要注意的是，此时文件的版本是4，并不是最新的。我们知道SVN工具中如果文件不是最新版本就无法上传，所以说这个功能只是用来暂时还原一下版本，来查询某个问题的，不能将还原后的文件上传。这个特别是当你服务器启动不了了, 把版本退回一个可以启动版本的情况 Revert to this version这个选项的作用是将文件的内容更新到对应的版本，版本号没有发生变化。如果我们是在版本4这里点击“Revert to this version”，表示5~8版本所作的修改全部被还原，此时svn里会有5-8被还原的文件改动可以提交, 此时文件和版本4的文件一模一样，但需要注意的是这项操作相当于我们把版本4这个文件拷贝了一份赋值给了当前目录下的文件，此时的文件版本还是8，并且是可以提交的，提交以后文件的版本变成了9，增加了一个新的版本，虽然这个版本和版本4的内容是一样的。 Revert changes from this version这个选项的作用是将对应版本的修改还原，文件的版本号不发生变化，相当于在当前本版本上剔除某些版本所作的改变。如果我们是在版本4这里点击“Revert changes from this version”，表示版本4所作的修改被抹杀了，只剩下除版本4以外的7个修改了，但是此时文件是可以上传的，并且会生成新的版本9，只是版本9只包括除版本4以外的7次修改。这个选项是可以选择多个版本的，如果我们选择4,5,6,7这四个版本点击“Revert changes from this revision”，那么这几次修改都会被抹杀。如果我们选择5,6,7,8这四个版本点击“Revert changes from this revision”，表示取消这几个版本的修改，实际上和在版本4这里点击“Revert to this version”的作用是一样的。]]></content>
      <categories>
        <category>Misc</category>
      </categories>
      <tags>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些常见的笔试题]]></title>
    <url>%2F2014%2F09%2F29%2Fsome_common_examination%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[考察cpp的静态绑定123456789101112131415161718struct MMPA&#123; int value() const &#123; return this-&gt;v_; &#125; int tvalue() const &#123; return 1; &#125;public: int v_;&#125;;int main(int argc, char* argv[])&#123; &#123; const MMPA* p = nullptr;// std::cout &lt;&lt; p-&gt;v_ &lt;&lt; std::endl; std::cout &lt;&lt; p-&gt;tvalue() &lt;&lt; std::endl; std::cout &lt;&lt; p-&gt;value() &lt;&lt; std::endl; getchar(); return 0; &#125;&#125; 会打印什么? 答案以及分析答案: 打印1之后崩溃 真正的原因是： 因为对于非虚成员函数，Ｃ++这门语言是静态绑定的。这也是Ｃ++语言和其它语言Java, Python的一个显著区别。以此下面的语句为例：somenull-&gt;foo();这语句的意图是：调用对象somenull的foo成员函数。如果这句话在Java或Python等动态绑定的语言之中，编译器生成的代码大概是：找到somenull的foo成员函数，调用它。 （注意，这里的找到是程序运行的时候才找的，这也是所谓动态绑定的含义：运行时才绑定这个函数名与其对应的实际代码。有些地方也称这种机制为迟绑定，晚绑定。）但是对于C++。为了保证程序的运行时效率，Ｃ++的设计者认为凡是编译时能确定的事情，就不要拖到运行时再查找了。 所以C++的编译器看到这句话会这么干： 查找somenull的类型，发现它有一个非虚的成员函数叫foo。（编译器干的） 找到了，在这里生成一个函数调用，直接调B::foo(somenull)。 所以到了运行时，由于foo()函数里面并没有任何需要解引用somenull指针的代码，所以真实情况下也不会引发segment fault。这里对成员函数的解析，和查找其对应的代码的工作都是在编译阶段完成而非运行时完成的，这就是所谓的静态绑定，也叫早绑定。正确理解C++的静态绑定可以理解一些特殊情况下C++的行为。 求最大公约数求 a 和 b 的最大公约数 123456789101112131415161718192021int measure(int a, int b)&#123; int product = a * b; if (a == 0 || b == 0) &#123; return -1; &#125; if (a &lt; b) &#123; int temp = a; a = b; b = temp; &#125; while (int mod = a % b) &#123; a = b; b = mod; &#125; //return b; // 最大公约数 return product / b; // 记住这个公式： a*b=最小公倍数*最大公约数&#125; 棋盘/格子问题在如下7*5的棋盘中，请计算从A移动到B一共有多少走法？要求每次只能向上或向右移动一格，并且不能经过P。(答案为492) 给定一个M*N的格子或棋盘，从左下角走到右上角的走法总数（每次只能向右或向上移动一个方格边长的距离） 运用动态规划来解答 :我们可以把棋盘的左下角看做二维坐标的原点(0,0)，把棋盘的右上角看做二维坐标(M,N)(坐标系的单位长度为小方格的变长)用f(i,j)表示移动到坐标f(i,j)的走法总数，其中0=&lt;i,j&lt;=n，设f(m,n)代表从坐标（0,0）到坐标（m,n）的移动方法，则 f(m,n)=f(m-1,n)+f(m,n-1).于是状态f(i,j)的状态转移方程为： f(i,j)=f(i-1,j)+f(i,j-1) if i,j&gt;0 f(i,j)=f(i,j-1) if i=0 f(i,j)=f(i-1,j) if j=0 初始情况就为：f(0,0)=0, f(0,1)=1, f(1,0)=1，这个问题可以在时间O(n^2)，空间O(n^2)内求解，非递归解. 所以答案为 492 = 1SumWaysOfMoveOnChessBoard(7, 5) - SumWaysOfMoveOnChessBoard(3, 3) * SumWaysOfMoveOnChessBoard(7 - 3, 5 - 3) 递归解12345678int SumWaysOfMoveOnChessBoard_Recursion(int m, int n) &#123; if (m == 0 &amp;&amp; n == 0) return 0; if (m==0 || n==0) return 1; return SumWaysOfMoveOnChessBoard_Recursion(m, n - 1) + SumWaysOfMoveOnChessBoard_Recursion(m - 1, n);&#125; 非递归解12345678910111213141516171819202122232425262728293031323334353637383940414243444546int SumWaysOfMoveOnChessBoard_NonRecursion_RawArray(int m, int n)&#123; if (m == 0 || n == 0) return 1; if (m == 0 &amp;&amp; n == 0) return 0; int xSize = m + 1; int ySize = n + 1; int** arr = new int*[xSize]; for (int i = 0; i &lt; xSize; ++i) arr[i] = new int[ySize]; arr[0][0] = 0; for (int i = 0; i &lt; xSize; ++i) arr[i][0] = 1; for (int i = 0; i &lt; ySize; ++i) arr[0][i] = 1; for (int i = 1; i &lt; xSize; ++i) for (int j = 1; j &lt; ySize; ++j) arr[i][j] = arr[i - 1][j] + arr[i][j - 1]; for (int i = 0; i &lt; xSize; ++i) delete[] arr[i]; delete[] arr; return arr[m][n];&#125;int SumWaysOfMoveOnChessBoard_NonRecursion_STL(int m, int n)&#123; if (m == 0 &amp;&amp; n == 0) return 0; int xSize = m + 1; int ySize = n + 1; std::vector&lt; vector&lt;int&gt; &gt; ChessBoardArray(xSize, vector&lt;int&gt;(ySize));; ChessBoardArray[0][0] = 0; for (int i = 0; i &lt; xSize; ++i) ChessBoardArray[i][0] = 1; for (int j = 0; j &lt; ySize; ++j) ChessBoardArray[0][j] = 1; for (int i = 1; i &lt; xSize; ++i) for (int j = 1; j &lt; ySize; ++j) ChessBoardArray[i][j] = ChessBoardArray[i][j - 1] + ChessBoardArray[i - 1][j]; return ChessBoardArray[m][n];&#125; 大数加法/乘法大数加法思路 :模拟小学列竖式 9 8 + 2 1 ------------- (1)(1)(9) 大数乘法思路 : 模拟乘法累加 - 改进简单来说，方法二就是先不算任何的进位，也就是说，将每一位相乘，相加的结果保存到同一个位置，到最后才计算进位。 例如：计算98×21,步骤如下 9 8 × 2 1 ------------- (9)(8) &lt;---- 第1趟: 98×1的每一位结果 (18)(16) &lt;---- 第2趟: 98×2的每一位结果 ------------- (18)(25)(8) &lt;---- 这里就是相对位的和，还没有累加进位 这里唯一要注意的便是进位问题，我们可以先不考虑进位，当所有位对应相加，产生结果之后，再考虑。从右向左依次累加，如果该位的数字大于10，那么我们用取余运算，在该位上只保留取余后的个位数，而将十位数进位（通过模运算得到）累加到高位便可，循环直到累加完毕。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129void BigIntAddition(char* bigIntA, char* bigIntB)&#123; if (!bigIntA || !bigIntB) return; size_t strlenA = strlen(bigIntA); size_t strlenB = strlen(bigIntB); size_t biggerStrlen = strlenA &gt; strlenB ? strlenA : strlenB; int* reversedA = new int[biggerStrlen]; int* reversedB = new int[biggerStrlen]; // 先将例子中的 1234 和 98765 逆序存储, 不够的补零, 方便计算 for (size_t i = 0; i &lt; biggerStrlen; ++i) &#123; //cout &lt;&lt; int(strlenA - 1 - i) &lt;&lt; endl; reversedA[i] = (int(strlenA - 1 - i) &gt;= 0) ? (bigIntA[strlenA - 1 - i] - '0') : 0; reversedB[i] = (int(strlenB - 1 - i) &gt;= 0) ? (bigIntB[strlenB - 1 - i] - '0') : 0; &#125; for (size_t i = 0; i &lt; biggerStrlen; ++i) cout &lt;&lt; reversedA[i]; cout &lt;&lt; endl; // --&gt; 43210 for (size_t i = 0; i &lt; biggerStrlen; ++i) cout &lt;&lt; reversedB[i]; cout &lt;&lt; endl; // --&gt; 98765 int* bigIntSum = new int[biggerStrlen + 1]; int x = 0; // 进位 // 模拟小学的列竖式加法, 满10进1 for (size_t i = 0; i &lt; biggerStrlen; ++i) &#123; bigIntSum[i] = reversedA[i] + reversedB[i] + x; x = bigIntSum[i] / 10; bigIntSum[i] %= 10; &#125; size_t printLen = biggerStrlen; // 查看最后一个进位是否 &gt; 0, 大于零则最高位为1 if (x &gt; 0) &#123; bigIntSum[biggerStrlen] = x; printLen = biggerStrlen + 1; &#125; for (size_t i = 0; i &lt; printLen; ++i) cout &lt;&lt; bigIntSum[printLen - 1 - i]; // --&gt; 58023 cout &lt;&lt; endl; delete[] bigIntSum;&#125;void BigIntMultiplication(char* bigIntA, char* bigIntB)&#123; if (!bigIntA || !bigIntB) return; int strlenA = static_cast&lt;int&gt;(strlen(bigIntA)); int strlenB = static_cast&lt;int&gt;(strlen(bigIntB)); cout &lt;&lt; strlenA &lt;&lt; ", " &lt;&lt; strlenB &lt;&lt; endl; int biggerStrlen = strlenA &gt; strlenB ? strlenA : strlenB; int* reversedA = new int[biggerStrlen]; int* reversedB = new int[biggerStrlen]; // 先将例子中的 1234 和 98765 逆序存储, 不够的补零, 方便计算 for (int i = 0; i &lt; biggerStrlen; ++i) &#123; reversedA[i] = (int(strlenA - 1 - i) &gt;= 0) ? (bigIntA[strlenA - 1 - i] - '0') : 0; reversedB[i] = (int(strlenB - 1 - i) &gt;= 0) ? (bigIntB[strlenB - 1 - i] - '0') : 0; &#125; for (int i = 0; i &lt; biggerStrlen; ++i) cout &lt;&lt; reversedA[i]; cout &lt;&lt; endl; // --&gt; 43210 for (int i = 0; i &lt; biggerStrlen; ++i) cout &lt;&lt; reversedB[i]; cout &lt;&lt; endl; // --&gt; 98765 // 分配一个空间，用来存储运算的结果，num1长的数 * num2长的数， // 结果不会超过num1+num2长 int* bigIntProduct = new int[strlenA + strlenB]; // 比如防止下面执行 bigIntSum[i + j] += reversedA[i] * reversedB[j]; 这句的时候 // i+j = 0 时 出错, 因为 bigIntSum[0] 为一个未初始化的值 for (int i = 0; i &lt; strlenA + strlenB; ++i) bigIntProduct[i] = 0; int carry = 0; // 进位 // 先不考虑进位问题，根据竖式的乘法运算， // num1的第i位与num2的第j位相乘，结果应该存放在结果的第i+j位上 for (int i = 0; i &lt; strlenA; ++i) for (int j = 0; j &lt; strlenB; ++j) bigIntProduct[i + j] += reversedA[i] * reversedB[j]; for (int i = 0; i &lt; strlenA + strlenB; ++i) cout &lt;&lt; bigIntProduct[i] &lt;&lt; ", "; // --&gt; 3659707060341650 cout &lt;&lt; endl; //单独处理进位 for (int i = 0; i &lt; strlenA + strlenB - 1; ++i) &#123; bigIntProduct[i] += carry; carry = bigIntProduct[i] / 10; bigIntProduct[i] %= 10; &#125; for (int i = 0; i &lt; strlenA + strlenB; ++i) cout &lt;&lt; bigIntProduct[i] &lt;&lt; ", "; // --&gt; 626770070 cout &lt;&lt; endl; int printLen = strlenA + strlenB - 1; // 查看最后一个进位是否 &gt; 0, 大于零则最高位为1 if (carry &gt; 0) &#123; bigIntProduct[strlenA + strlenB - 1] = carry; printLen = strlenA + strlenB; &#125; for (int i = 0; i &lt; printLen; ++i) cout &lt;&lt; bigIntProduct[printLen - 1 - i]; // --&gt; 70077626 cout &lt;&lt; endl; delete[] bigIntProduct;&#125;int main()&#123; char *bigIntA = "1234"; char *bigIntB = "56789"; BigIntAddition(bigIntA, bigIntB); BigIntMultiplication(bigIntA, bigIntB); return 0;&#125; 最长公共子串问题：有两个字符串str和str2，求出两个字符串中最长公共子串长度。 比如：str=acbcbcef，str2=abcbced，则str和str2的最长公共子串为bcbce，最长公共子串长度为5。 算法思路： 1、把两个字符串分别以行和列组成一个二维矩阵。2、比较二维矩阵中每个点对应行列字符中否相等，相等的话值设置为1，否则设置为0。3、通过查找出值为1的最长对角线就能找到最长公共子串。 针对于上面的两个字符串我们可以得到的二维矩阵如下： 从上图可以看到，str和str2共有5个公共子串，但最长的公共子串长度为5。 为了进一步优化算法的效率，我们可以再计算某个二维矩阵的值的时候顺便计算出来当前最长的公共子串的长度，即某个二维矩阵元素的值由 item[i][j]=1 演变为 item[i][j]=1 +item[i-1][j-1] ，这样就避免了后续查找对角线长度的操作了。修改后的二维矩阵如下： 故状态转移方程 X[i] == Y[j]，dp[i][j] = dp[i-1][j-1] + 1 X[i] != Y[j]，dp[i][j] = 0 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061int LongestCommonSubstring(char* strA, char* strB)&#123; if (!strA || !strB) return -1; int maxLen = 0; int strlenA = static_cast&lt;int&gt;(strlen(strA)); int strlenB = static_cast&lt;int&gt;(strlen(strB)); int biggerStrlen = strlenA &gt; strlenB ? (strlenA + 1) : (strlenB + 1); char * lcs = new char[biggerStrlen]; int lcsMaxIndex = 0; int** temp = new int*[strlenA]; for (int i = 0; i &lt; strlenA; ++i) temp[i] = new int[strlenB]; for (int i = 0; i &lt; strlenA; ++i) &#123; for (int j = 0; j &lt; strlenB; ++j) &#123; if (strA[i] == strB[j]) &#123; if (i &gt; 0 &amp;&amp; j &gt; 0) temp[i][j] = temp[i - 1][j - 1] + 1; else temp[i][j] = 1; &#125; else &#123; temp[i][j] = 0; &#125; if (temp[i][j] &gt; maxLen) &#123; maxLen = temp[i][j]; lcsMaxIndex = i; &#125; &#125; &#125; for (int i = 0;i &lt; maxLen; ++i) *(lcs + maxLen - i - 1) = strA[lcsMaxIndex--]; *(lcs+maxLen) = '\0'; cout &lt;&lt; lcs &lt;&lt; endl; for (int i = 0; i &lt; strlenA; ++i) delete[] temp[i]; delete[] temp; delete[] lcs; return maxLen;&#125;int main()&#123; cout &lt;&lt; "maxLen = " &lt;&lt; LongestCommonSubstring("wwwadfabcdeasdf", "wwweoruqpeorqabcdezcvnz") &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>noodle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[STL之队列和双端队列和栈的比较]]></title>
    <url>%2F2014%2F09%2F25%2Fdeque_queue_stack_comparison%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[队列和双端队列的比较队列(queue)是一种是相对于栈的一种数据结构，它是先进先出(First In First Out)。它只可以在尾部添加元素。 双端队列(deque double ended queue（双端队列）)是一种相对于队列的一种数据结构。它可以在尾部和头部插入、移除和获取。 三者成员函数的比较通过他们各自的成员函数我们能一目了然的看出区别 栈的成员函数 stack&lt;Type&gt; s : 定义一个stack的变量 s.push(x) : 入栈，如例 s.pop() : 出栈，如例 . 注意，出栈操作只是删除栈顶元素，并不返回该元素。 s.top() : 访问栈顶，如例 s.empty() : 判断栈空，如例，当栈空时，返回true。 s.size() : 访问栈中的元素个数，如例 队列的成员函数 queue&lt;Type&gt; M : 定义一个queue的变量 M.empty() : 查看是否为空范例 是的话返回1，不是返回0; M.push() : 从已有元素后面增加元素 M.size() : 输出现有元素的个数 M.front() : 显示第一个元素 M.back() : 显示最后一个元素 M.pop() : 清除第一个元素 双端队列的成员函数 deque&lt;Type&gt; c : 定义一个deque的变量 c.pop_back() : 删除最后一个数据。 c.pop_front() : 删除头部数据。 c.push_back(elem) : 在尾部加入一个数据。 c.push_front(elem) : 在头部插入一个数据。 c.clear() : 移除容器中所有数据。 c.front() : 传回地一个数据。 c.back() : 传回最后一个数据，不检查这个数据是否存在。 c.size() : 返回容器中实际数据的个数。 c.empty() : 判断容器是否为空。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构三之二叉搜索树的增删查]]></title>
    <url>%2F2014%2F09%2F24%2Fbinary_search_tree_insert_delete_search%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[有了二叉树的基础, 我们继续学习二叉搜索树. 二叉搜索树的定义二叉查找树(Binary Search Tree, 简称”BST”), 又名”二叉搜索树”或”二叉排序树”:它或者是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉排序树。 . . . 注 : 我们本文中二叉树的二叉链式存储方案的代码表示为123456typedef struct BinaryTreeNode&#123; void *data; BinaryTreeNode *LeftNode; BinaryTreeNode *RightNode;&#125;BTN, *BTNP; 二叉搜索树的查询123456789101112131415161718192021BTNP SearchBST(BTNP btnp, char key_data)&#123; while (btnp) &#123; if ( key_data &lt; (*(char *)(btnp-&gt;data)) ) &#123; btnp = btnp-&gt;LeftNode; &#125; else if ( key_data &gt;(*(char *)(btnp-&gt;data)) ) &#123; btnp = btnp-&gt;RightNode; &#125; else &#123; cout &lt;&lt; "found" &lt;&lt; endl; return btnp; &#125; &#125; cout &lt;&lt; "not found" &lt;&lt; endl; return nullptr;&#125; 二叉搜索树的插入 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849void InsertBST(BTNP &amp;btnp, char key_data)&#123; char * temp_key_data = new char; *temp_key_data = key_data; BTNP new_btnp = new BTN; new_btnp-&gt;data = temp_key_data; new_btnp-&gt;LeftNode = nullptr; new_btnp-&gt;RightNode = nullptr; if (!btnp) &#123; btnp = new_btnp; return; &#125; BTNP temp_btnp = btnp; BTNP parent_btnp = nullptr; bool is_left = true; while (temp_btnp) &#123; parent_btnp = temp_btnp; if (key_data &lt; (*(char *)(temp_btnp-&gt;data))) &#123; temp_btnp = temp_btnp-&gt;LeftNode; is_left = true; &#125; else if (key_data &gt; (*(char *)(temp_btnp-&gt;data))) &#123; temp_btnp = temp_btnp-&gt;RightNode; is_left = false; &#125; else &#123; cout &lt;&lt; "already has a same key_data" &lt;&lt; endl; return; &#125; &#125; if (is_left == true) &#123; parent_btnp-&gt;LeftNode = new_btnp; &#125; else &#123; parent_btnp-&gt;RightNode = new_btnp; &#125;&#125; 二叉搜索树之删除某个结点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173void DeleteBinaraySearchTree(BTNP &amp;btnp, char key)&#123; // 第一步 : 查找是否有这个key BTNP parentBTNP = NULL; bool isLeft = true; BTNP tempBNTP = btnp; while (tempBNTP) &#123; if (*(char*)tempBNTP-&gt;data &gt; key) &#123; parentBTNP = tempBNTP; tempBNTP = tempBNTP-&gt;LeftNode; isLeft = true; &#125; else if (*(char*)tempBNTP-&gt;data &lt; key) &#123; parentBTNP = tempBNTP; tempBNTP = tempBNTP-&gt;RightNode; isLeft = false; &#125; else &#123; break; &#125; &#125; if (!tempBNTP) &#123; cout &lt;&lt; "not found this key!!" &lt;&lt; endl; return; &#125; cout &lt;&lt; "found this key!!" &lt;&lt; endl; // 第二步 : 我们得处里key结点没有父结点的情况 if (!parentBTNP) &#123; delete (char *)btnp-&gt;data; btnp-&gt;data = NULL; delete btnp; btnp = NULL; cout &lt;&lt; "test" &lt;&lt; endl; return; &#125; else &#123; // 第三步 : 我们得处里key结点有父结点的4种情况 // 情况1 if (tempBNTP-&gt;LeftNode == NULL &amp;&amp; tempBNTP-&gt;RightNode == NULL) &#123; if (isLeft) &#123; parentBTNP-&gt;LeftNode = NULL; &#125; else &#123; parentBTNP-&gt;RightNode = NULL; &#125; &#125; // 情况2 else if (tempBNTP-&gt;LeftNode == NULL) &#123; if (isLeft) &#123; parentBTNP-&gt;LeftNode = tempBNTP-&gt;RightNode; &#125; else &#123; parentBTNP-&gt;RightNode = tempBNTP-&gt;RightNode; &#125; &#125; // 情况3 else if (tempBNTP-&gt;RightNode == NULL) &#123; if (isLeft) &#123; parentBTNP-&gt;LeftNode = tempBNTP-&gt;LeftNode; &#125; else &#123; parentBTNP-&gt;RightNode = tempBNTP-&gt;LeftNode; &#125; &#125; // 情况4 else if (tempBNTP-&gt;LeftNode != NULL &amp;&amp; tempBNTP-&gt;RightNode != NULL) &#123; // 情况4比较复杂, 我们得找到key结点的后继 // (后继 : 一个结点x的后继是大于x.key的最小关键字的结点) // 因为情况4中的key结点左孩子和右孩子都不为空, // 所以key结点的后继 successorBTNP 肯定位于key结点的右子树中, // 且 successorBTNP 没有左孩子( 不然 successorBTNP 的左孩子就是key结点的后继了嘛) BTNP tempTempBNTP = tempBNTP-&gt;RightNode; // key结点的后继 BTNP successorBTNP = NULL; // key结点的后继的父结点 BTNP successorParentBTNP = NULL; while (tempTempBNTP) &#123; successorBTNP = tempTempBNTP; if (tempTempBNTP &amp;&amp; tempTempBNTP-&gt;LeftNode &amp;&amp; tempTempBNTP-&gt;LeftNode-&gt;LeftNode == NULL) &#123; successorParentBTNP = tempTempBNTP; &#125; tempTempBNTP = tempTempBNTP-&gt;LeftNode; &#125; // 情况4又分两种情况, 如下 : // 第一种情况 A 是 successorBTNP 是 key 的右孩子; // 第二种情况 B 是 successorBTNP 在 key 的右子树中, 但并不是 successorBTNP 本身并不是 key 的右孩子 // 情况 A : if (successorBTNP == tempBNTP-&gt;RightNode) &#123; // 用 key 结点的后继 successorBTNP 来替代 key 结点 if (isLeft) &#123; parentBTNP-&gt;LeftNode = successorBTNP; &#125; else &#123; parentBTNP-&gt;RightNode = successorBTNP; &#125; &#125; // 情况 B : else &#123; // 如果 successorBTNP 有右子树, 则用 successorBTNP 的右子树 代替 原来 successorBTNP 的位置. if (successorBTNP-&gt;RightNode) &#123; successorParentBTNP-&gt;LeftNode = successorBTNP-&gt;RightNode; &#125; // 用 key 结点的后继 successorBTNP 的data来替换 key 结点的data char * tempChar = new char; *tempChar = *(char *)successorBTNP-&gt;data; tempBNTP-&gt;data = tempChar; tempBNTP = successorBTNP; // 下面注释的这块代码可以用上面这两4行代替 // if (isLeft) // &#123; // parentBTNP-&gt;LeftNode = successorBTNP; // successorBTNP-&gt;RightNode = tempBNTP-&gt;RightNode; // successorBTNP-&gt;LeftNode = tempBNTP-&gt;LeftNode; // &#125; // else // &#123; // parentBTNP-&gt;RightNode = successorBTNP; // successorBTNP-&gt;RightNode = tempBNTP-&gt;RightNode; // successorBTNP-&gt;LeftNode = tempBNTP-&gt;LeftNode; // &#125; &#125; &#125; // 记得释放 key 结点 delete (char *)tempBNTP-&gt;data; tempBNTP-&gt;data = NULL; delete tempBNTP; tempBNTP = NULL; &#125;&#125; 二叉搜索树之找最低公共祖先给定二叉搜索树（BST）中两节点，找出他们的最低公共祖先(LeastCommonAncestors, 简称LCA)。 例如对于本文第一张图的LCA为 : LCA(4， 14)=8; LCA(8， 10)=8. 思路: 利用BST的性质, 假设n1,n2都在BST中，并且n1 &lt; n2。则有 : 在遍历过程中，遇到的第一个值介于n1和n2之间的节点n，也即n1 =&lt; n &lt;= n2, 就是n1和n2的LCA。 在遍历过程中，如果节点的值比n1和n2都大，那么LCA在节点的左子树。 在遍历过程中，如果节点的值比n1和n2都小，那么LCA在节点的右子树。 123456789101112131415161718192021222324252627282930313233343536373839404142434445void LeastCommonAncestorsBinaraySearchTree(BTNP btnp, char key1, char key2)&#123; if (!btnp) &#123; cout &lt;&lt; "the BST is null" &lt;&lt; endl; return; &#125; if (key1 == key2) &#123; cout &lt;&lt; "key1 == key2, so this is not a BST" &lt;&lt; endl; return; &#125; if (!SearchBinarySearchTree(btnp, key1)) &#123; cout &lt;&lt; "key1 not found" &lt;&lt; endl; return; &#125; if (!SearchBinarySearchTree(btnp, key2)) &#123; cout &lt;&lt; "key2 not found" &lt;&lt; endl; return; &#125; while (btnp) &#123; char curChar = *(char *)(btnp-&gt;data); if (curChar &lt; key1 &amp;&amp; curChar &lt; key2) &#123; btnp = btnp-&gt;RightNode; &#125; else if (curChar &gt; key1 &amp;&amp; curChar &gt; key2) &#123; btnp = btnp-&gt;LeftNode; &#125; else &#123; cout &lt;&lt; "LeastCommonAncestors is " &lt;&lt; curChar &lt;&lt; endl; return; &#125; &#125;&#125; 参考&lt;&lt; 算法导论 &gt;&gt;]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>DataStructure</tag>
        <tag>Algo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构二之二叉树的遍历和交换左右孩子]]></title>
    <url>%2F2014%2F09%2F23%2Fbinary_tree_traverse_and_swap%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[二叉树的二叉链式存储方案的代码表示： 123456typedef struct BinTreeNode&#123; BinTreeNode( char Data ) : data_( Data ), left_( nullptr ), right_( nullptr ) &#123;&#125; char data_; struct BinTreeNode *left_, *right_;&#125;btn, *btnp; 二叉树的遍历. . . 二叉树的广度优先遍历 12345678910111213141516171819202122void BreadthFirstTraverse( btnp bTreeNode )&#123; if ( !bTreeNode ) &#123; return; &#125; std::queue&lt;btnp&gt; tempQueue; tempQueue.push( bTreeNode ); while ( !tempQueue.empty() ) &#123; cout &lt;&lt; tempQueue.front()-&gt;data_ &lt;&lt; endl; if ( tempQueue.front()-&gt;left_ ) &#123; tempQueue.push( tempQueue.front()-&gt;left_ ); &#125; if ( tempQueue.front()-&gt;right_ ) &#123; tempQueue.push( tempQueue.front()-&gt;right_ ); &#125; tempQueue.pop(); &#125;&#125; 深度优先的递归式遍历递归式遍历的代码实现非常简洁, 但生产环境一般不允许递归, 因为怕栈溢出. 12345678910111213141516171819202122232425262728293031323334353637typedef struct BinaryTreeNode&#123; void *data; BinaryTreeNode *LeftNode; BinaryTreeNode *RightNode;&#125;BTN, *BTNP;void PreOrderTraverse_Recursion(BTNP btnp)&#123; if (btnp) &#123; cout &lt;&lt; *(char *)btnp-&gt;data &lt;&lt; endl; PreOrderTraverse_Recursion(btnp-&gt;LeftNode); PreOrderTraverse_Recursion(btnp-&gt;RightNode); &#125;&#125;void InOrderTraverse_Recursion(BTNP btnp)&#123; if (btnp) &#123; PreOrderTraverse_Recursion(btnp-&gt;LeftNode); cout &lt;&lt; *(char *)btnp-&gt;data &lt;&lt; endl; PreOrderTraverse_Recursion(btnp-&gt;RightNode); &#125;&#125;void PostOrderTraverse_Recursion(BTNP btnp)&#123; if (btnp) &#123; PreOrderTraverse_Recursion(btnp-&gt;LeftNode); PreOrderTraverse_Recursion(btnp-&gt;RightNode); cout &lt;&lt; *(char *)btnp-&gt;data &lt;&lt; endl; &#125;&#125; 深度优先的迭代式遍历递归的本质就是出栈入栈, 所以我们用栈来模拟递归, 写出以下三种迭代式遍历 迭代的二叉树三种遍历方式其实思想是统一的 : 都是从左子树的各个结点依次入栈, 当左边已经走到头了, 就开始走右边, 在适当的条件就出栈, 只是每个遍历方式的出栈条件不一样而已, 或者是打印结点的时机不同而已. 迭代式先序遍历代码实现用栈(Stack)的思路来处理问题。 前序遍历的顺序为根-左-右，具体算法为： 把根节点 push 到栈中 循环检测栈是否为空，若不空，则取出栈顶元素，保存其值 看其右子节点是否存在，若存在则 push 到栈中(因为前序遍历的顺序为根-左-右, 而stack是先进后出的, 所以要先push右子节点) 看其左子节点，若存在，则 push 到栈中。 这个cpp版本没有本文下面的go版写得好, 那个更容易理解123456789101112131415161718192021222324void PreOrderTraverseNonRecursion( btnp bTreeNode )&#123; if ( !bTreeNode ) &#123; return; &#125; std::stack&lt;btnp&gt; tempStack; while ( !tempStack.empty() || bTreeNode ) &#123; while ( bTreeNode ) &#123; cout &lt;&lt; bTreeNode-&gt;data_ &lt;&lt; endl; tempStack.push( bTreeNode ); bTreeNode = bTreeNode-&gt;left_; &#125; if ( !tempStack.empty() ) &#123; bTreeNode = tempStack.top()-&gt;right_; tempStack.pop(); &#125; &#125;&#125; 迭代式中序遍历代码实现123456789101112131415161718192021222324void InOrderTraverseNonRecursion( btnp bTreeNode )&#123; if ( !bTreeNode ) &#123; return; &#125; std::stack&lt;btnp&gt; tempStack; while ( !tempStack.empty() || bTreeNode ) &#123; while ( bTreeNode ) &#123; tempStack.push( bTreeNode ); bTreeNode = bTreeNode-&gt;left_; &#125; if ( !tempStack.empty() ) &#123; cout &lt;&lt; tempStack.top()-&gt;data_ &lt;&lt; endl; bTreeNode = tempStack.top()-&gt;right_; tempStack.pop(); &#125; &#125;&#125; 迭代式后序遍历代码实现后序遍历的出栈条件有点不一样, 因为后序是先左后右再中的, 比如某个结点p要出栈, 需要遍历完了p的所有右子树之后才能出栈, 而不能第一次就出栈, 所以专门构造了一个结构体PostOrderBT来记录他是否是第一次出栈 (PostOrderBT结构体里有个 isFirstTime_ 的数据来记录) 所以我们代码中的思路就是 : 把每个将要入栈的结点的 isFirstTime_ 标志都置为 true , 当第一次遍历到结点p的时候, 不使p出栈, 但使p的 isFirstTime_ 标志变为 false, 然后 “bTreeNode = 栈顶-&gt;右孩子” 开始遍历他的右子树. 当p的右子树都遍历完了之后(也就是p的右子树都依次出栈了之后)又会遍历到p自己, 不过这一次他的 isFirstTime_ 标志已经为 false 了, 我们通过这个标志知道不是第一次遍历到p了, 所以这时我们使p出栈( 此时p的右子树都已经遍历完了, 所以不用像之前一样再 “bTreeNode = 栈顶-&gt;右孩子” 了 ) 12345678910111213141516171819202122232425262728293031323334353637383940void PostOrderTraverseNonRecursion( btnp bTreeNode )&#123; if ( !bTreeNode ) &#123; return; &#125; typedef struct PostOrderBTreeNode &#123; PostOrderBTreeNode( btnp OriginBT, bool IsFirstTime ) : bt_( OriginBT ), isFirstTime_( IsFirstTime ) &#123;&#125; btnp bt_; bool isFirstTime_; &#125;pobtn, *pobtnp; std::stack&lt;pobtn&gt; tempStack; while ( !tempStack.empty() || bTreeNode ) &#123; while ( bTreeNode ) &#123; pobtn np( bTreeNode, true ); tempStack.push( np ); bTreeNode = bTreeNode-&gt;left_; &#125; if ( !tempStack.empty() ) &#123; if ( tempStack.top().isFirstTime_ ) &#123; tempStack.top().isFirstTime_ = false; bTreeNode = tempStack.top().bt_-&gt;right_; &#125; else &#123; cout &lt;&lt; tempStack.top().bt_-&gt;data_ &lt;&lt; endl; tempStack.pop(); &#125; &#125; &#125;&#125; 遍历测试代码 如上图得到的相应的三种深度优先遍历的序列分别为 ： 先(根)序遍历 ： ABCDEGF 中(根)序遍历 ： CBEGDFA 后(根)序遍历 ： CGEFDBA 而得到的广度优先遍历的序列为 : ABCDEFG 遍历测试代码CPP版参照上图构建如下二叉树 : 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;queue&gt;#include &lt;stack&gt;using std::cout;using std::endl;using std::stack;using std::queue;int main()&#123; btn testbt = btn( 'A' ); auto testB = btn( 'B' ); auto testC = btn( 'C' ); auto testD = btn( 'D' ); auto testE = btn( 'E' ); auto testG = btn( 'G' ); auto testF = btn( 'F' ); testbt.left_ = &amp;testB; testbt.left_-&gt;left_ = &amp;testC; testbt.left_-&gt;right_ = &amp;testD; testbt.left_-&gt;right_-&gt;left_ = &amp;testE; testbt.left_-&gt;right_-&gt;left_-&gt;right_ = &amp;testG; testbt.left_-&gt;right_-&gt;right_ = &amp;testF; cout &lt;&lt; "BreadthFirstTraverse : \n"; BreadthFirstTraverse( &amp;testbt ); cout &lt;&lt; "PreOrderTraverseNonRecursion : \n"; PreOrderTraverseNonRecursion( &amp;testbt ); cout &lt;&lt; "InOrderTraverseNonRecursion : \n"; InOrderTraverseNonRecursion( &amp;testbt ); cout &lt;&lt; "PostOrderTraverseNonRecursion : \n"; PostOrderTraverseNonRecursion( &amp;testbt ); return 0;&#125; 遍历测试代码GO版1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package mainimport ( "fmt")func main() &#123; testPreorderTraversal()&#125;type TreeNode struct &#123; Left *TreeNode Right *TreeNode Val string&#125;func preorderTraversal(root *TreeNode) []string &#123; if root == nil &#123; //当树为空树时，直接返回一个空list return nil &#125; result := make([]string, 0) //非递归前序遍历，需要借助栈 stack := make([]*TreeNode, 0) //第一步是将根节点压入栈中 stack = append(stack, root) // 当栈不为空时，出栈的元素插入list尾部。 // 当它的孩子不为空时，将孩子压入栈，一定是先压右孩子再压左孩子 // 因为前序遍历的顺序为根-左-右, 而stack是先进后出的, 所以要先push右子节点 for len(stack) != 0 &#123; //此处的root只是一个变量的复用 root = stack[len(stack)-1] stack = stack[:len(stack)-1] result = append(result, root.Val) if root.Right != nil &#123; stack = append(stack, root.Right) &#125; if root.Left != nil &#123; stack = append(stack, root.Left) &#125; &#125; return result&#125;func testBinaryTreeTraversal() &#123; testbtA := new(TreeNode) testbtA.Val = "A" testbtB := new(TreeNode) testbtB.Val = "B" testbtC := new(TreeNode) testbtC.Val = "C" testbtD := new(TreeNode) testbtD.Val = "D" testbtE := new(TreeNode) testbtE.Val = "E" testbtF := new(TreeNode) testbtF.Val = "F" testbtG := new(TreeNode) testbtG.Val = "G" testbtA.Left = testbtB testbtA.Left.Left = testbtC testbtA.Left.Right = testbtD testbtA.Left.Right.Left = testbtE testbtA.Left.Right.Left.Right = testbtG testbtA.Left.Right.Right = testbtF // 标准答案: // 先(根)序遍历 ： ABCDEGF // 中(根)序遍历 ： CBEGDFA // 后(根)序遍历 ： CGEFDBA fmt.Println(preorderTraversal(testbtA))&#125; 深度优先的迭代式遍历之总结请仔细看完上述代码再看总结, 才更有体会. 出栈条件的不同: 前序遍历的出栈条件都是左边走到头了就让栈顶元素出栈 中序遍历同上 后序遍历的出栈条件是遍历到同一个栈顶元素第二次要出栈的时候才让他出栈 打印结点时机的不同: 前序遍历的打印结点时机是入栈时就打印 中序遍历的打印结点时机是第一次出栈时就打印 中序遍历的打印结点时机是第二次出栈时就打印 交换所有左右孩子非递归方式Swap1234567891011121314151617181920212223242526272829303132void SwapBT( btnp bTreeNode )&#123; if (!bTreeNode) return; std::stack&lt;btnp&gt; tempStack; tempStack.push( bTreeNode ); btnp tempForTop = nullptr; btnp tempForSwap = nullptr; while ( !tempStack.empty() ) &#123; tempForTop = tempStack.top(); // swap tempForSwap = tempForTop-&gt;left_; tempForTop-&gt;left_ = tempForTop-&gt;right_; tempForTop-&gt;right_ = tempForSwap; tempStack.pop(); if ( tempForTop-&gt;left_ ) &#123; tempStack.push( tempForTop-&gt;left_ ); &#125; if ( tempForTop-&gt;right_ ) &#123; tempStack.push( tempForTop-&gt;right_ ); &#125; &#125;&#125; 递归方式Swap交换左右孩子用递归很容易做到 123456789101112void SwapBinaryTree(BTNP btnp)&#123; if (btnp) &#123; BTNP tempBTNP = btnp-&gt;LeftNode; btnp-&gt;LeftNode = btnp-&gt;RightNode; btnp-&gt;RightNode = tempBTNP; SwapBinaryTree(btnp-&gt;LeftNode); SwapBinaryTree(btnp-&gt;RightNode); &#125;&#125;]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>DataStructure</tag>
        <tag>Algo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构一之二叉树的创建和销毁]]></title>
    <url>%2F2014%2F09%2F22%2Fbinary_tree_create_and_destroy%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[接着上一篇， 上一篇主要说了各种排序算法， 但对几个常用的数据结构还未提及，所以这一篇主要讲二叉树, 二叉树已经包括很多链表的知识了。所有代码都是测试过的, 可以直接撸. 二叉树这里不举太多数字方面的东西， 我们直接看图， 直观感性的认识满二叉树和完全二叉树： 有一点性质需要牢记：具有n个结点的完全二叉树的最大高度为log2n+1 二叉树的二叉链式存储二叉树的二叉链式存储方案的代码表示： 123456typedef struct BinaryTreeNode&#123; void *data; BinaryTreeNode *LeftNode; BinaryTreeNode *RightNode;&#125;BTN, *BTNP; . . . 二叉树的创建下面代码写法是基于二叉树的先序遍历来创建二叉树的.基于中序或者后序的写法都类似. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950void CreateBT(BTNP &amp;btnp)&#123; char input_data = 0; cin &gt;&gt; input_data; // 检查是否为叶子结点, 我们把输入为'.'的字符认为是叶子结点 if (input_data == '.') &#123; btnp = nullptr; return; &#125; char * temp_char = new char; if (!temp_char) &#123; return; &#125; *temp_char = input_data; btnp = new BTN; if (!btnp) &#123; return; &#125; // 注意这里不能写成 btnp-&gt;data = &amp;inputData; 因为inputData是分配在栈上的 btnp-&gt;data = temp_char; CreateBT(btnp-&gt;LeftNode); CreateBT(btnp-&gt;RightNode);&#125;void DestroyBT(BTNP btnp)&#123; if (!btnp) &#123; return; &#125; DestroyBT(btnp-&gt;LeftNode); DestroyBT(btnp-&gt;RightNode); // 安全释放void指针 : 将void *转换为原来类型的指针，然后再调用delete释放指针 delete (char *)btnp-&gt;data; btnp-&gt;data = nullptr; delete btnp; btnp = nullptr;&#125;]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>DataStructure</tag>
        <tag>Algo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向对象的四大特征以及五大原则]]></title>
    <url>%2F2014%2F09%2F12%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%9B%9B%E5%A4%A7%E7%89%B9%E5%BE%81%E4%BB%A5%E5%8F%8A%E4%BA%94%E5%A4%A7%E5%8E%9F%E5%88%99%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[面向对象的四大特征 封装：封装是把过程和数据包围起来，对数据的访问只能通过已定义的界面，面向对象计算始于这个基本概念，即现实世界可以被描绘成一系列完全自治，封装的对象，这些对象通过一个受保护的接口访问其他对象 继承：继承是一种联结类的层次模型，并且允许和鼓励类的重用，它提供了一种明确表达共性的方法，对象的一个新类可以从现在的类中派生，这个过程成为继承，新类继承了原始类的特性，新类成为原始类的派生类，而原始类称为新类的基类，派生类可以从他的基类那里继承方法和实例变量，并且类可以修改或增加新的方法使之更加适合特殊的需求 抽象：抽象就是忽略一个主题中与当前目标无关的那些方面，以便充分的注意与当前目标有关的方面，抽象包括两个方面，一个是过程抽象，二是数据抽象 多态性：多态性是指允许不同类的对象对同一消息作出响应，多态性包括参数化多态性和包含多态性，多态性语言具有灵活，抽象，行为共享，代码共享的优势，很好的解决了应用程序函数同名的问题 面向对象的五大原则记忆口诀 : 替开依(“凯隐”, 一个英雄联盟英雄名字)接单 单一职责原则（Single-Resposibility Principle）其核心思想为：一个类，最好只做一件事，只有一个引起它的变化。单一职责原则可以看做是低耦合、高内聚在面向对象原则上的引申，将职责定义为引起变化的原因，以提高内聚性来减少引起变化的原因。职责过多，可能引起它变化的原因就越多，这将导致职责依赖，相互之间就产生影响，从而大大损伤其内聚性和耦合度。通常意义下的单一职责，就是指只有一种单一功能，不要为类实现过多的功能点，以保证实体只有一个引起它变化的原因。专注，是一个人优良的品质；同样的，单一也是一个类的优良设计。交杂不清的职责将使得代码看起来特别别扭牵一发而动全身，有失美感和必然导致丑陋的系统错误风险。 开放封闭原则（Open-Closed principle）其核心思想是：软件实体应该是可扩展的，而不可修改的。也就是，对扩展开放，对修改封闭的。开放封闭原则主要体现在两个方面1、对扩展开放，意味着有新的需求或变化时，可以对现有代码进行扩展，以适应新的情况。2、对修改封闭，意味着类一旦设计完成，就可以独立完成其工作，而不要对其进行任何尝试的修改。实现开开放封闭原则的核心思想就是对抽象编程，而不对具体编程，因为抽象相对稳定。让类依赖于固定的抽象，所以修改就是封闭的；而通过面向对象的继承和多态机制，又可以实现对抽象类的继承，通过覆写其方法来改变固有行为，实现新的拓展方法，所以就是开放的。“需求总是变化”没有不变的软件，所以就需要用封闭开放原则来封闭变化满足需求，同时还能保持软件内部的封装体系稳定，不被需求的变化影响。 替换原则（Liskov-Substituion Principle）其核心思想是：子类必须能够替换其基类。这一思想体现为对继承机制的约束规范，只有子类能够替换基类时，才能保证系统在运行期内识别子类，这是保证继承复用的基础。在父类和子类的具体行为中，必须严格把握继承层次中的关系和特征，将基类替换为子类，程序的行为不会发生任何变化。同时，这一约束反过来则是不成立的，子类可以替换基类，但是基类不一定能替换子类。Liskov替换原则，主要着眼于对抽象和多态建立在继承的基础上，因此只有遵循了Liskov替换原则，才能保证继承复用是可靠地。实现的方法是面向接口编程：将公共部分抽象为基类接口或抽象类，通过Extract Abstract Class，在子类中通过覆写父类的方法实现新的方式支持同样的职责。Liskov替换原则是关于继承机制的设计原则，违反了Liskov替换原则就必然导致违反开放封闭原则。Liskov替换原则能够保证系统具有良好的拓展性，同时实现基于多态的抽象机制，能够减少代码冗余，避免运行期的类型判别。 依赖倒置原则（Dependecy-Inversion Principle）其核心思想是：依赖于抽象。具体而言就是高层模块不依赖于底层模块，二者都同依赖于抽象；抽象不依赖于具体，具体依赖于抽象。我们知道，依赖一定会存在于类与类、模块与模块之间。当两个模块之间存在紧密的耦合关系时，最好的方法就是分离接口和实现：在依赖之间定义一个抽象的接口使得高层模块调用接口，而底层模块实现接口的定义，以此来有效控制耦合关系，达到依赖于抽象的设计目标。抽象的稳定性决定了系统的稳定性，因为抽象是不变的，依赖于抽象是面向对象设计的精髓，也是依赖倒置原则的核心。依赖于抽象是一个通用的原则，而某些时候依赖于细节则是在所难免的，必须权衡在抽象和具体之间的取舍，方法不是一层不变的。依赖于抽象，就是对接口编程，不要对实现编程。 接口隔离原则（Interface-Segregation Principle）其核心思想是：使用多个小的专门的接口，而不要使用一个大的总接口。具体而言，接口隔离原则体现在：接口应该是内聚的，应该避免“胖”接口。一个类对另外一个类的依赖应该建立在最小的接口上，不要强迫依赖不用的方法，这是一种接口污染。接口有效地将细节和抽象隔离，体现了对抽象编程的一切好处，接口隔离强调接口的单一性。而胖接口存在明显的弊端，会导致实现的类型必须完全实现接口的所有方法、属性等；而某些时候，实现类型并非需要所有的接口定义，在设计上这是“浪费”，而且在实施上这会带来潜在的问题，对胖接口的修改将导致一连串的客户端程序需要修改，有时候这是一种灾难。在这种情况下，将胖接口分解为多个特点的定制化方法，使得客户端仅仅依赖于它们的实际调用的方法，从而解除了客户端不会依赖于它们不用的方法。分离的手段主要有以下两种：1、委托分离，通过增加一个新的类型来委托客户的请求，隔离客户和接口的直接依赖，但是会增加系统的开销。2、多重继承分离，通过接口多继承来实现客户的需求，这种方式是较好的。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++很基础的易混淆点一]]></title>
    <url>%2F2014%2F09%2F09%2Fcplusplus_confused_points_one%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[2.1.1 : C++标准规定的各种算术类型的尺寸的最小值, 同时允许编译器赋予这些类型更大的尺寸. 比如char的最小尺寸为8位 执行浮点数运算选用double ，这是因为float 通常精度不够而且双精度浮点数和单精度浮点数的计算代价相差无儿。事实上， 对于某些机器来说，双精度运算甚至比单精度还快 2.1.2 : 当我们赋给无符号类型一个超出它表示范围的值时，结果是初始值对无符号类型表示数值总数取模后的余数。例如， 8 比特大小的 unsigned char 可以表示0 至255 区间内的值，如果我们赋了一个区间以外的值，则实际的结果是该值对256取模后所得的余数。因此，把 -1 赋给8 比特大小的 unsigned char 所得的结果是255 当我们赋给带符号类型一个超出它表示范围的值时，结果是未定义的( undefined )。此时， 程序，可能继续工作、可能崩溃，也可能生成垃圾数据。 如果表达式里既有带符号类型又有无符号类型， 当带符号类型取值为负时会出现异常结果， 这是因为带符号数会自动地转换成无符号数。例如，在一个形如 a*b 的式子中，如果a = -1 , b = 1 ，而且a 和b 都是int ，则表达式的值显然为- 1. 然而，如果a 是int ， 而b 是unsigned ， 则结果须视在当前机器上int 所占位数而定。在32环境里，因为2的32次方是4294967296, 所以a*b结果是4294967295 . . . 2.2.1 : 如果是内置类型的变量未被显式初始化，它的值由定义的位置决定。定义于任何函数体之外的变量被初始化为0 。然而如6. 1. 1 节(第1 85 页)所示， 一种例外情况是，定义在函数体内部的内置类型变量将不被初始化。一个未被初始化的内置类型变量的值是未定义的(参见2. 1. 2 节， 第33 页) ，如果试图拷贝或以其他形式访问此类值将引发错误 定义于函数体内的内置类型的对象如果没有初始化，值未定义。类的对象如果没有显式地初始化，则由类确定。 2.2.2 :为了支持分离式编译， C++语言将声明和定义区分开来。声明( declaration ) 使得名字为程序所知， 一个文件如果想使用别处定义的名字则必须包含对那个名字的声明。而定义( definition ) 负责创建与名字关联的实体。变量声明规定了变量的类型和名字， 在这一点上定义与之相同。但是除此之外，定义还申请存储空间，也可能会为变量赋一个初始值。如果想声明一个变量;而非定义它，就在变量名前添加关键字extern ，而且不要显式地初始化变量:12extern int i ; // 声明i 而非定义iint j ; / / 声明并定义j 任何何包含了显式初始化的声明即成为定义。我们能给由extern 关键字标记的变量赋一个初始值，但是这么做也就抵消了extern 的作用。extern 语句如果包含初始值就不再是声明，而变成定义了:extern doub1e pi = 3 . 1416 ; // 定义 变量能且只能定义一次, 但可以多次声明. 在函数体内部，如果试图初始化一个由extern 关键字标记的变量， 将引发错误. 2.2.3 : C++也为标准库保留了一些名字。用户在自定义的标识符中不能连续出现两个下划线，也不能以下划线紧连大写字句开头。此外，定义在函数体外的标识稍不能以下划线开头。 比如: int _ = 3;是合法的 2.3.1 : 其他所有引用的类型都要和与之绑定的对象严格匹配。而且，引用只能绑定在对象上，而不能与字面值或某个表达式的计算结果绑定在一起， 相关原因将在2 .4 . 1 节详述: 123int &amp;refVa14 = 10 ; //错误· 引用类型的初始值必须是一个对象double dval = 3.14 ;int &amp;refVa15 = dva1 ; // 错误: 此处引用类型的初始位必须是int 型对象 2.3.2 : 因为引用不是对象， 没有实际地址，所以不能定义指向引用的指针。 2.4 :默认状态下， const 对象仅在文件内有效 当以编译时初始化的方式定义一个const 对象时，就如对bufSize 的定义一样: const int bufSize = 512; //输入缓冲区大小 编译器将在编译过程中把用到该变量的地方都替换成对应的值。也就是说，编译器会找到代码中所有用到 bufSize 的地方，然后用512 替换。为了执行上述替换， 编译器必须知道变量的初始值。如果程序包含多个文件，则每个用了const 对象的文件都必须得能访问到它的初始值才行。要做到这一点，就必须在每一个用到变量的文件中都有对它的定义(参见2.2.2 节， 第4 1 页)。为了支持这一用法，同时避免对同一变量的重复定义，默认情况下， const 对象被设定为仅在文件内有效。多个文件中出现了同名的const 变量时，其实等同于在不同文件中分别定义了独立的变量。某些时候有这样一种const 变量，它的初始值不是一个常量表达式，但又确实有必要在文件间共享。这种情况， 我们不希望编译器为每个文件分别生成独立的变量。相反，我们想让这类const 对象像其他(非常量)对象一样工作，也就是说，只在一个文件中定义const ，而在其他多个文件中声明并使用它。解决的办法是，对于 const 变量不管是声明还是定义都添加extern 关键字， 这样只需定义一次就可以了: 1234// file 1 . cc 定义并初始化了一个常量，该常量能被其他文件访问extern const int bufSize = fcn();// file 1 . h 头文件extern const int bufSize ; /1 与f ile 1. cc 中定义的bufSize 是同一个 如上述程序所示， file 1. cc 定义并初始化了bufSize 。因为这条语句包含了初始值，所以它(显然〉是一次定义。然而，因为bufSize 是个常量，必须用extern 加以限定使其被其他文件使用。 file 1. h 头文件中的声明也由extern 做了限定，其作用是指明bufSize 并非file 1独有，它的定义将在别处出现。 2.4.1初始化和对const 的引用 2.3.1节(第46 页)提到， 号| 用的类型必须与其所引用对象的类型→致，但是有两个例外。第一种例外情况就是在初始化常量引用时允许用任意表达式作为初始值，只要该表达式的结果能转换成(参见2.1 .2 节，第3 2 页)引用的类型叩可。尤其，允许为一个常量引用绑定非常量的对象、字面值， 甚至是个-般表达式: 12345int i= 42 ; .const int &amp;r1 = i ; // ft许将const int &amp;r1 定到一个普通int 对象上const int &amp;r2 = 42; // 正确r1 是一个常量引用const int &amp;r3 = r1 * 2 ; // 正确r3 是一个常量引用int &amp;r4 = r1 * 2 ; // 错误r4 是一个普通的非常量引用 要想、理解这种例外情况的原因，陆简单的办法是弄清楚当一个常量引用被绑定到另外一种类型上时到底发生了什么: 12doub1e dval = 3 . 14 ;const int &amp;ri = dva1 ; 此处ri 引用了一个int 型的数。对口的操作应该是整数运算，但dval 却是一个双精度浮点数而非整数。因此为了确保让rl 绑定一个整数，编译器把上述代码变成了如下形式: 12const int temp = dval; 1/ 由双精度浮点数生成一个临时的整型常量const int &amp;ri = temp ; 11 让rl 绑定这个临时受 在这种情况下， ri 绑定了一个临时量对象。所谓临时量对象就是当编译器而要一个空间来暂存表达式的求值结果时临时创建的一个未命名的对象。c++程序员们常常把临时量对象简称为临时量。接下来探讨当ri 不是常量时，如果执行了类似于上面的初始化过程将带来什么样的后果。如果且不是常量，就允许对ri 赋值，这样就会改变ri 所引用对象的值。注意，此时绑定的对象是一个临时量;而dvalo 程序员既然让rl 引用dval ， 就肯定想通过ri 改变dval 的值，否则干什么要给ri 赋值l呢?如此看来， 既然大家基本上不会想着把引用绑定到临时量上， c++语言也就把这种行为归为非法。 2.5.2 和原来另一些只对应一种特定类型的说明符(比如double) 不|司. auto 让编译器通过初始值来推算变量的类型。显然. auto 定义的变量必须有初始值: 使用auto 也能在一条语句中声明多个变量。因为一条声明语句只能有一个基本数据类型，所以该语句中所有变量的初始基本数据类型都必须一样: 123int i = 0; const int ci = i;auto &amp;n = i, *p = &amp;ci // 错误 : i 的类型是int 而&amp; ci 的类型是const int]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法四之谈一谈堆排序]]></title>
    <url>%2F2014%2F08%2F24%2Fheap_sort%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[此文用的是最大堆, 最大堆的堆排序之后的数组是升序, 最小堆反之.堆排序 HeapSort 由 以下两部分组成 : 堆化 MaxHeapify 建堆 BuildMaxHeap. . . . 堆化MaxHeapify具体过程如下图 :]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>CPP</tag>
        <tag>Algo</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法三之谈一谈快排优化和二分查找]]></title>
    <url>%2F2014%2F08%2F22%2Fquick_sort_and_binary_search%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[快速排序与归并排序一样， 快排也是用了分治的思想。 特别注意 : 快排的核心模块是Partition, 而Partition的复杂度为O(N). 你可以想象一个两副牌然后随意取出一张牌pivot，其他的所有牌都跟这张pivot牌比较， 大的放右边那一摞A，小的放左边B。接着再从左边这一摞B再随意取出一张牌pivot，其他的所有牌都跟这张pivot牌比较， 大的放右边那一摞，小的放左边，递归下去。A也重复上述步骤递归。 递归结束之后， 左边的都比右边的小， 而且是有序的。 效率很差的情况对于分治算法，当每次划分时，算法若都能分成两个等长的子序列时，那么分治算法效率会达到最大。也就是说，基准的选择是很重要的。选择基准的方式决定了两个分割后两个子序列的长度，进而对整个算法的效率产生决定性影响所以当如果一个有序递增序列, 每次选基准都选最后一个, 那肯定效率很差了啊 算法导论的快排C++实现版本 . . . 这个c++实现版本主要用于说明算法思想, 而对于代码鲁棒性有太多关注,下面有个我自己手写的命名清晰版本会比较多的关注鲁棒性以及易读性 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657void swap(int *a, int *b)&#123; int temp = 0; temp = *a; *a = *b; *b = temp;&#125;int partition(int *array, int p, int r)&#123; int i = 0, j = 0, pivot = 0; pivot = array[r]; i = p-1; for(j=p; j&lt;=r-1; j++) &#123; if(array[j] &lt;= pivot) &#123; i++; swap(&amp;array[i], &amp;array[j]); &#125; &#125; swap(&amp;array[i+1], &amp;array[r]); return i+1;&#125;/*通常，我们可以向一个算法中加入随机化成分，以便对于所有输入，它均能获得较好的平均情况性能。将这种方法用于快速排序时，不是始终采用A[r]作为主元，而是从子数组A[p..r]中随机选择一个元素，即将A[r]与从A[p..r]中随机选出的一个元素交换。*/ int rand_patition(int test_arr[], int p, int r) &#123; srand(static_cast&lt;unsigned&gt;(time(nullptr))); int rand_index = (rand() % (r - p) ) + p + 1; swap(&amp;test_arr[rand_index], &amp;test_arr[r]); return partition(test_arr, p, r); &#125;void quick_sort(int *array, int p, int r)&#123; int q = 0; /*if(p &lt; r) &#123; q = rand_patition(array, p, r); quick_sort(array, p, q-1); quick_sort(array, q+1, r); &#125;*/ // 以上的注释部分可以写成尾递归的方式来优化 while (p &lt; r) &#123; q = rand_patition(array, p, r); quick_sort(array, p, q-1); p = q + 1; &#125;&#125;]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>Algo</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法二之谈一谈冒泡插入归并桶排序]]></title>
    <url>%2F2014%2F08%2F20%2Fbubble_insert_merge_bucket_sort%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[注：以下所有代码皆可以直接运行， 都已经测试过。 冒泡排序(可略过, 不实用)想象这里有很多泡泡，最大的泡泡每次循环之后浮到数组的最后面]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>Algo</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈一谈各类算法的复杂度和常用数据结构]]></title>
    <url>%2F2014%2F08%2F19%2Fintroduction_of_sort_algorithm_complexity_and_data_structure%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[因为之前的笔记和书籍相关知识都是零零散散的， 没有一个汇总， 所以写了这篇博客。有些算法很简单，复杂度一眼都能看得出来， 几乎不需要记忆 ， 但是有些算法或者数据结构的操作的复杂度就不是一眼可以看得出来， 推导也是很费时间的， 所谓常识就是应该熟记于心且被认可的知识。 必须掌握的知识 DataStructure 链表 二叉树 二叉搜索树 栈 队列 散列表 算法 二分查找 快速排序 归并排序 堆排序 插入排序 树的插入/查找/删除 广度优先搜索 深度优先搜索 . . . 该注意的点 实用的排序算法有四种：插入、快速、归并、堆。其余的都不值得深究。这几个排序算法都有其特点，涵盖了常见的使用场景，在其特定的使用场景下是效率最高的。 插入排序 : 在小数据量或者数据都较为有序的时候比起归并和快速排序有更佳的时间效率, 插入排序在这种情况下，只需要从头到尾扫描一遍，交换、移动少数元素即可；时间复杂度近乎 o(N)))。 快速排序 : 时间复杂度依赖数据打乱的程度 快排最差情形的时间复杂度是O(n2), 平均是O(nlogn) 就地快速排序使用的空间是O(1)的，也就是个常数级；而真正消耗空间的就是递归调用了，因为每次递归就要保持一些数据； 最优的情况下空间复杂度为：O(logn) ；每一次都平分数组的情况 最差的情况下空间复杂度为：O( n ) ；退化为冒泡排序的情况 选择基准的方式决定了两个分割后两个子序列的长度，进而对整个算法的效率产生决定性影响, 比如当如果一个有序递增序列, 每次选基准都选最后一个, 那肯定效率 很差了啊 归并排序 : 时间复杂度稳定但是占用2N的内存 归并的空间复杂度就是那个临时的数组和递归时压入栈的数据占用的空间：n + logn；所以空间复杂度为: O(n) 还有一种空间复杂度为O(1)的归并排序的实现 堆排序 : 在不能一次排序N个数据并要求选出前M个数据时使用。 插入排序、堆排序、快速排序等都是原址排序。归并排序是非原址的。 插入排序、归并排序是稳定的, 堆排序、快速排序是不稳定的。 内省排序: std的sort就是用的内省排序. 此算法首先从快速排序开始，当递归深度超过一定深度（深度为排序元素数量的对数值即logN）后转为堆排序。采用这个方法，内省排序既能在常规数据集上实现快速排序的高性能，又能在最坏情况下仍保持o(NlogN)的时间复杂度。 为什么在平均情况下快速排序比堆排序要优秀堆排序是渐进最优的比较排序算法，达到了O(nlgn)这一下界，而快排有一定的可能性会产生最坏划分，时间复杂度可能为O(n^2)，那为什么快排在实际使用中通常优于堆排序？ 虽然quick_sort会n^2（其实有稳定的nlgn的版本），但这毕竟很少出现。heap_sort大多数情况下比较次数都多于quick_sort，尽管大家都是nlgn。那就让倒霉蛋倒霉好了，大多数情况下快才是硬道理。 堆排比较的几乎都不是相邻元素，对cache极不友好，这才是很少被采用的原因。数学上的时间复杂度不代表实际运行时的情况.快排是分而治之，每次都在同一小段进行比较，最后越来约接近局部性。反观堆排，堆化过程中需要一直拿index的当前元素A和处于index*2 + 1 的子元素B比较, 两个元素距离较远。(局部性原理是指CPU访问存储器时，无论是存取指令还是存取数据，所访问的存储单元都趋于聚集在一个较小的连续区域中。) 如何解决快排和堆排都不够好的问题?使用 内省排序 , std的sort就是用的内省排序. 各类算法的复杂度汇总表 算法与数据结构-综合提-C++版 课程网址 GitHub代码仓库网址 第一章：当我们在讨论算法的时候，我们在讨论什么？ 1-1 我们究竟为什么要学习算法 1-2 课程介绍 第二章：排序基础 2-1 选择排序法 2-2 使用模板（泛型）编写算法 2-3 随机生成算法测试用例 2-4 测试算法的性能 2-5 插入排序法 2-6 插入排序法的改进 2-7 更多关于O（n*2）排序算法的思考 第三章：高级排序问题 3-1 归并排序法 3-2 归并排序法的实现 3-3 归并排序法的优化 3-4 自底向上的归并排序算法 3-5 快速排序法 3-6 随机化快速排序法 3-7 双路快速排序法 3-8 三路快速排序法 3-9 归并排序和快速排序的衍生问题 第四章：堆和堆排序 4-1 为什么使用堆 4-2 堆的基本存储 4-3 Shift Up 4-4 Shift Down 4-5 基础堆排序和Heapify 4-6 优化的堆排序 4-7 排序算法总结 4-8 索引堆 4-9 索引堆的优化 4-10 和堆相关的其他问题 第五章：二分搜索树 5-1 二分查找法 5-2 二分搜索树基础 5-3 二分搜索树的节点插入 5-4 二分搜索书的查找 5-5 二分搜索树的遍历（深度优先遍历） 5-6 层序遍历（广度优先遍历） 5-7 删除最大值，最小值 5-8 二分搜索树的删除 5-9 二分搜索树的顺序性 5-10 二分搜索树的局限性 5-11 树形问题和更多树。 第六章：并查集 6-1 并查集基础 6-2 Qucik Find 6-3 Quick Union 6-4 基于size的优化 6-5 基于rank的优化 6-6 路径压缩 第七章：图论 7-1 图论基础 7-2 图的表示 7-3 相邻点迭代器 7-4 图的算法框架 7-5 深度优先遍历和连通分量 7-6 寻路 7-7 广度优先遍历和最短路径 7-8 迷宫生成，ps抠图–更多无权图的应用 第八章：最小生成树 8-1 有权图 8-2 最小生成树问题和切分定理 8-3 Prim算法的第一个实现 8-4 Prim算法的优化 8-5 优化后的Prim算法的实现 8-6 Krusk算法 8-7 最小生成树算法的思考 第九章：最短路径 9-1 最短路径问题和松弛操作 9-2 Dijkstra算法的思想 9-3 实现Dijkstra算法 9-4 负权边和Bellman-Ford算法 9-5 实现Bellman-Ford算法 9-6 更多和最短路径相关的思考 第十章：结束语 10-1 总结，算法思想，大家加油！ 玩转算法面试_从真题到思维全面提升算法思维 课程网址 GitHub代码仓库网址 第1章 算法面试到底是什么鬼?一提起算法面试，很多同学就会心有余悸。可其实，大多数企业的算法面试，并没有那么可怕。并不是一定要啃完整本《算法导论》，才能玩儿转算法面试；也并不是只有ACM参赛选手，才能笑傲算法面试。恰恰相反，大多数算法面试关注的算法思维，其实很基础。在这一章，和大家聊一聊，算法面试，到底是什么鬼？… 1-1 算法面试不仅仅是正确的回答问题试看1-2 算法面试只是面试的一部分试看1-3 如何准备算法面试试看1-4 如何回答算法面试问题 第2章 面试中的复杂度分析很多同学一提起复杂度分析就头疼，马上想起了《算法导论》中复杂的数学推导。但其实在一般的企业面试中，对复杂度的分析要求并没有那么高，但也是绕不过去的坎儿。在这一章，和大家介绍一下，面试中需要掌握的复杂度分析。… 2-1 究竟什么是大O（Big O）2-2 对数据规模有一个概念2-3 简单的复杂度分析2-4 亲自试验自己算法的时间复杂度2-5 递归算法的复杂度分析2-6 均摊时间复杂度分析（Amortized Time Analysis）2-7 避免复杂度的震荡 第3章 数组中的问题其实最常见面试中的算法问题，有很多并不需要复杂的数据结构支撑。就是用数组，就能考察出很多东西了。其实，经典的排序问题，二分搜索等等问题，就是在数组这种最基础的结构中处理问题的。在这一章中，我们学习常见的数组中处理问题的方法。… 3-1 从二分查找法看如何写出正确的程序3-2 改变变量定义，依然可以写出正确的算法 3-3 在LeetCode上解决第一个问题 Move Zeros3-4 即使简单的问题，也有很多优化的思路3-5 三路快排partition思路的应用 Sort Color3-6 对撞指针 Two Sum II - Input Array is Sorted3-7 滑动窗口 Minimum Size Subarray Sum3-8 在滑动窗口中做记录 Longest Substring Without Repeating Characters 第4章 查找表相关问题查找，是使用计算机处理问题时的一个最基本的任务，因此也是面试中非常常见的一类问题。很多算法问题的本质，就是要能够高效查找。学会使用系统库中的map和set，就已经成功了一半。 4-1 set的使用 Intersection of Two Arrays4-2 map的使用 Intersection of Two Arrays II4-3 set和map不同底层实现的区别4-4 使用查找表的经典问题 Two Sum4-5 灵活选择键值 4Sum II4-6 灵活选择键值 Number of Boomerangs4-7 查找表和滑动窗口 Contain Duplicate II4-8 二分搜索树底层实现的顺序性 Contain Duplicate III 第5章 在链表中穿针引线链表是一种特殊的线性结构，由于不能像数组一样进行随机的访问，所以和链表相关的问题有他自身的特点。我将之称为穿针引线。我们在这一章，就来看一看，如何在链表中穿针引线。 5-1 链表，在节点间穿针引线 Reverse Linked List5-2 测试你的链表程序5-3 设立链表的虚拟头结点 Remove Linked List Elements5-4 复杂的穿针引线 Swap Nodes in Pairs5-5 不仅仅是穿针引线 Delete Node in a Linked List5-6 链表与双指针 Remove Nth Node Form End of List 第6章 栈，队列，优先队列栈和队列虽然是简单的数据结构，但是使用这些简单的数据结构所解决的算法问题不一定简单。在这一章里，我们将来探索，和栈与队列相关的算法问题。 6-1 栈的基础应用 Valid Parentheses6-2 栈和递归的紧密关系 Binary Tree Preorder, Inorder and Postorder Traversal6-3 运用栈模拟递归6-4 队列的典型应用 Binary Tree Level Order Traversal6-5 BFS和图的最短路径 Perfect Squares6-6 优先队列6-7 优先队列相关的算法问题 Top K Frequent Elements 第7章 二叉树和递归递归，是使用计算机解决问题的一种重要的思考方式。而二叉树由于其天然的递归结构，使得基于二叉树的算法，均拥有着递归性质。使用二叉树，是研究学习递归算法的最佳入门方式。在这一章里，我们就来看一看二叉树中的递归算法。… 7-1 二叉树天然的递归结构, 104, 111, 100, 101, 222, 1107-2 一个简单的二叉树问题引发的血案 Invert Binary Tree7-3 注意递归的终止条件 Path Sum, 112, 111, 4047-4 定义递归问题 Binary Tree Path, 257, 113, 1297-5 稍复杂的递归逻辑 Path Sum III, 4377-6 二分搜索树中的问题 Lowest Common Ancestor of a Binary Search Tree, 235, 98, 450, 108, 230, 236 第8章 递归和回溯法回溯法是解决很多算法问题的常见思想，甚至可以说是传统人工智能的基础方法。其本质依然是使用递归的方法在树形空间中寻找解。在这一章，我们来具体看一下将递归这种技术使用在非二叉树的结构中，从而认识回溯这一基础算法思想。… 8-1 树形问题 Letter Combinations of a Phone Number8-2 什么是回溯, 93, 1318-3 排列问题 Permutations, 478-4 组合问题 Combinations, 77, 39, 40, 216, 78, 90, 4018-5 回溯法解决组合问题的优化8-6 二维平面上的回溯法 Word Search, 798-7 floodfill算法，一类经典问题 Number of Islands-8-8 回溯法是经典人工智能的基础 N Queens 第9章 动态规划基础很多同学听到“动态规划”的名称可能会望而生畏，觉得动态规划的问题都很复杂。但其实，动态规划本质依然是递归算法，只不过是满足特定条件的递归算法。在这一章里，我们就来逐步解开动态规划的神秘面纱 9-1 什么是动态规划 9-2 第一个动态规划问题 Climbing Stairs, 70, 120, 649-3 发现重叠子问题 Integer Break, 343, 279, 91, 62, 639-4 状态的定义和状态转移 House Robber, 198, 213, 337, 309,9-5 0-1背包问题9-6 0-1背包问题的优化和变种9-7 面试中的0-1背包问题 Partition Equal Subset Sum, 416, 322, 377, 474, 139, 4949-8 LIS问题 Longest Increasing Subsequence9-9 LCS，最短路，求动态规划的具体解以及更多 第10章 贪心算法通常同学们可能会认为贪心算法比较简单。确实，通常贪心算法的实现非常容易，但是，一个问题是否能够使用贪心算法，是一定要小心的。我们在这一章来看一看，贪心算法可能会有哪些坑。 10-1 贪心基础 Assign Cookies, 455, 39210-2 贪心算法与动态规划的关系 Non-overlapping Intervals, 43510-3 贪心选择性质的证明 第11章 课程结语看完整个课程，我不能保证所有的同学都能百分百地对每一个算法面试问题应答自如，但认真学习的同学对大部分问题都应该已经有了一个合理的思维路径。在最后一章，我们再来简单地总结一下，并祝每一位同学都能找到自己喜欢的工作，大展宏图：）… 11-1 结语]]></content>
      <categories>
        <category>Algo</category>
      </categories>
      <tags>
        <tag>Algo</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三种编程命名规范]]></title>
    <url>%2F2014%2F05%2F09%2F%E4%B8%89%E7%A7%8D%E7%BC%96%E7%A8%8B%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[匈牙利命名法：开头字母用变量类型的缩写，其余部分用变量的英文或英文的缩写，要求单词第一个字母大写。 ex:int iMyAge; “i”是int类型的缩写；char cMyName[10]; “c”是char类型的缩写；float fManHeight; “f”是float类型的缩写； 其他：前缀类型 a b by c cb cr cx,cy dw fn h i l lp m_ n np p s sz w （一一对应关系） 数组 (Array) 布尔值 (Boolean) 字节 (Byte) 有符号字符 (Char) 无符号字符 (Char Byte，没有多少人用) 颜色参考值 (ColorRef) 坐标差（长度 ShortInt） Double Word 函数 Handle（句柄） 整型 长整型 (Long Int) Long Pointer 类的成员 短整型 (Short Int) Near Pointer Pointer 字符串型 以 null 做结尾的字符串型 (String with Zero End) Word . . . 驼峰式命名法：又叫小驼峰式命名法。第一个单词首字母小写，后面其他单词首字母大写。 ex:int myAge;char myName[10];float manHeight; 帕斯卡命名法：又叫大驼峰式命名法。每个单词的第一个字母都大写。 ex:int MyAge;char MyName[10];float ManHeight; 下划线命名法还有些许其他的命名规范，如：下划线命名法，但是不是太常用，个人感觉可能是因为下划线位置太偏的事，不方便大量使用。 总结综合各方面考虑，匈牙利命名法比较好，优势明显，不过书写较为繁琐，但阅读相当便利, 目前使用驼峰式命名法的人比较多些, 个人推荐匈牙利命名法。]]></content>
      <categories>
        <category>CPP</category>
      </categories>
      <tags>
        <tag>CPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bash定制]]></title>
    <url>%2F2014%2F02%2F01%2Fbash_enhance%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[profile和bashrc和bash_profile的区别 /etc/profile:此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.并从 /etc/profile.d 目录的配置文件中搜集shell的设置.所以如果你有对/etc/profile有修改的话必须得重启你的修改才会生效，此修改对每个用户都生效。 /etc/bashrc:为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取.如果你想对所有的使用bash的用户修改某个配置并在以后打开的bash都生效的话可以修改这个文件，修改这个文件不用重启，重新打开一个bash即可生效。 ~/.bash_profile:每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件.此文件类似于/etc/profile，也是需要需要重启才会生效，/etc/profile对所有用户生效，~/.bash_profile只对当前用户生效。 ~/.bashrc:该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取.（每个用户都有一个.bashrc文件，在用户目录下）此文件类似于/etc/bashrc，不需要重启生效，重新打开一个bash即可生效， /etc/bashrc对所有用户新打开的bash都生效，但~/.bashrc只对当前用户新打开的bash生效。 ~/.bash_logout:当每次退出系统(退出bash shell)时,执行该文件. 另外,/etc/profile中设定的变量(全局)的可以作用于任何用户,而~/.bashrc等中设定的变量(局部)只能继承/etc/profile中的变量,他们是”父子”关系. ~/.bash_profile 是交互式、login 方式进入bash 运行的；~/.bashrc 是交互式 non-login 方式进入bash 运行的； 通常二者设置大致相同，所以通常 ~/.bash_profile 会在其文件中加入以下代码来调用 ~/.bashrc :1234# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi . . . 设置core文件格式/proc/sys/kernel/core_pattern可以设置格式化的core文件保存位置或文件名，比如原来文件内容是core-%e , 可以这样修改: echo &quot;/corefile/core-%e-%p-%t&quot; &gt; /proc/sys/kernel/core_pattern 将会控制所产生的core文件会存放到/corefile目录下，产生的文件名为core-命令名-pid-时间戳 ,以下是参数列表: %p - insert pid into filename 添加pid %u - insert current uid into filename 添加当前uid %g - insert current gid into filename 添加当前gid %s - insert signal that caused the coredump into the filename 添加导致产生core的信号 %t - insert UNIX time that the coredump occurred into filename 添加core文件生成时的unix时间 %h - insert hostname where the coredump happened into filename 添加主机名 %e - insert coredumping executable name into filename 添加命令名 安全的rmrm -rf 慎用命令敲得多了，常在河边走，难免会湿鞋昨天，一个手误，敲错了命令，把原本想要留的文件夹给rm -rf掉了几天心血全木有了，靠，死的心都有了 几点教训： rm 特别是rm -rf之前，小心，三思，或者直接将命令改写掉 做好备份，有便捷的备份脚本 做好定时备份，有个前辈搞定时脚本，每天定时自个执行，即使删错了也不会那么悲催 在~下 .bashrc或者.bash_profile加入 1234567891011121314151617181920212223242526272829# the following lines if you want `ls' to be colorized:export LS_OPTIONS='--color=auto'eval "`dircolors`"alias ls='ls $LS_OPTIONS'alias ll='ls $LS_OPTIONS -alF'alias l='ls $LS_OPTIONS -lA'# Some more alias to avoid making mistakes:alias rm='rm -i'alias cp='cp -i'alias mv='mv -i'alias ll='ls -alF'# mkdir and enter italias mc=mkdircdmkdircd()&#123; mkdir -p "$1" &amp;&amp; cd "$1"&#125;# cd and ls -alF diralias cd=cdllcdll()&#123; builtin cd "$@" &amp;&amp; ll;&#125;ulimit -c unlimited 建立一个简易回收站搞个回收站得在~下 .bashrc或者.bash_profile加入 123456789101112131415161718192021222324252627282930313233343536373839404142434445mkdir -p ~/.trash# original rmalias or='/bin/rm'alias rm=trashalias r=trashalias lr='ls -alF ~/.trash'alias cr='cd ~/.trash'alias ur=undelfilealias er=emptytrashundelfile()&#123; mv -i ~/.trash/$@ ./&#125;trash()&#123; for TARGET_NAME in $@ do TARGET_WITH_NO_LAST_SLASH=~/.trash/$&#123;TARGET_NAME%*/&#125; if [ -d $&#123;TARGET_WITH_NO_LAST_SLASH&#125; ]; then # or -rf $&#123;TARGET_WITH_NO_LAST_SLASH&#125; mv $&#123;TARGET_WITH_NO_LAST_SLASH&#125; $&#123;TARGET_WITH_NO_LAST_SLASH&#125;_`date '+%x%X'` echo "rename the old one with the sameName_currentTime successfully." fi if [ -f $&#123;TARGET_WITH_NO_LAST_SLASH&#125; ]; then # or -rf $&#123;TARGET_WITH_NO_LAST_SLASH&#125; mv $&#123;TARGET_WITH_NO_LAST_SLASH&#125; $&#123;TARGET_WITH_NO_LAST_SLASH&#125;_`date '+%x%X'` echo "rename the old one with the sameName_currentTime successfully." fi if [ -L $&#123;TARGET_WITH_NO_LAST_SLASH&#125; ]; then # or -rf $&#123;TARGET_WITH_NO_LAST_SLASH&#125; mv $&#123;TARGET_WITH_NO_LAST_SLASH&#125; $&#123;TARGET_WITH_NO_LAST_SLASH&#125;_`date '+%x%X'` echo "rename the old one with the sameName_currentTime successfully." fi done ORIGIN_TARGET=$@ mv $&#123;ORIGIN_TARGET%*/&#125; ~/.trash/ #$&#123;ORIGIN_TARGET%*/&#125; for removing the last slash&#125;emptytrash()&#123; read -p "clear sure?[n]" confirm [ $confirm == 'y' ] || [ $confirm == 'Y' ] &amp;&amp; or -rf ~/.trash/*&#125; 如果想清空回收站彻底删除所有, 用er就可以了. 上文中的 alias or=&#39;/bin/rm&#39;中的 /bin/rm 因系统而异,你可以敲 whereis rm 命令来查看你的系统的rm在哪儿,比如我的就是 [b@host ~]$ whereis rm rm: /bin/rm /usr/share/man/man1/rm.1.gz 而有些人的却是 /usr/bin/rm , 那就要改成 alias or=&#39;/usr/bin/rm&#39; 了 命令补全增强首先找到 .inputrc 文件, 通过 sudo find / -name inputrc 来找到它, 如果没找到就在自己的home目录下新建一个 .inputrc 文件, 然后在.inputrc文件末尾加上常用的Bash定制 : 1234567891011set completion-ignore-case on #For single press Tab results for when a partial or no completion is possible set show-all-if-ambiguous on #For results when no completion is possible set show-all-if-unmodified on #History completion bound to arrow keys (down, up) "\e[A": history-search-backward "\e[B": history-search-forward 解释 : show-all-if-ambiguous : 默认情况下，按下两次 才会出现提示，现在只需要一次了。 completion-ignore-case : 在自动补全时忽略大小写 history-search-* : 输入某个命令的一部分时，按上下箭头，会匹配关于这个这命令最近的使用历史。比如：输入 vim ，然后按”上“键，此时，可以显示上一次运行vim时的那条命令，非常的方便！ Bash中快速移动光标bash有两种输入模式vi模式和emacs模式，其中emacs是默认模式，而且操作起来也比vi模式要快捷。可以通过 set -o vi和set -o emacs来转换。 命令行中移动Alt+f和Alt+b 是前后移动一个单词的距离这个很快比如你输入了 删除 Alt+d 往右边删除一个单词 Alt+Backspace 往左删除一个单词 Ctrl+u 往左删除到行首 Ctrl+k 往右删除到行末 其中这些删除都放入了删除环里面，可以使用Ctrl+y找回，Alt+y在删除环里面移动也就是说命令行里面可以使用剪切和粘贴了。上面的几条如果用熟练了效率能提高很多。 而如果还想了解更多的快捷键绑定，敲如下命令 bind -P 发现有些你需要的功能而没有快捷键绑定的话可以如下绑定，比如我绑定了两个函数 bind -m emacs ‘“/M-w”: kill-region’ bind -m emacs ‘“/M-W”: copy-region-as-kill’ 在命令历史中查找PS : 其实如果使用了上述的Bash定制中的 history-search-* 就不需要这个 ctrl + r 了 使用 Ctrl+r， 这个键组合是反向增量查找消息历史。很好用。 比如你很久以前输入过某个命令如。 gcc -c -DKKT - Dnnn 等等，一长串， 用上下方向键来找比较困难，这时候可以Ctrl+r，然后输入gcc很快找到该命令，重复按Ctrl+r将查找更早的历史。 bash脚本基础 -e filename 如果 filename存在，则为真 [ -e /var/log/syslog ] -d filename 如果 filename为目录，则为真 [ -d /tmp/mydir ] -f filename 如果 filename为常规文件，则为真 [ -f /usr/bin/grep ] -L filename 如果 filename为符号链接，则为真 [ -L /usr/bin/grep ] -r filename 如果 filename可读，则为真 [ -r /var/log/syslog ] -w filename 如果 filename可写，则为真 [ -w /var/mytmp.txt ] -x filename 如果 filename可执行，则为真 [ -L /usr/bin/grep ] filename1-nt filename2 如果 filename1比 filename2新，则为真 [ /tmp/install/etc/services -nt /etc/services ] filename1-ot filename2 如果 filename1比 filename2旧，则为真 [ /boot/bzImage -ot arch/i386/boot/bzImage ] 一个例子 : 1234567891011121314151617181920212223#!/bin/shBUILD_DIR=../buildif [ ! -d $&#123;BUILD_DIR&#125; ]; then mkdir $&#123;BUILD_DIR&#125;else echo "BUILD_DIR is already exist."fiRTS_PATH=`cd .. &amp;&amp; pwd`RTS_SYMBOLIC_LINK=~/rtsif [ ! -L "$RTS_SYMBOLIC_LINK" ]; then ln -s $RTS_PATH $RTS_SYMBOLIC_LINK;else echo "RTS_SYMBOLIC_LINK is already exist."fiexit 0]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始搭建一台简易Ubuntu服务器三]]></title>
    <url>%2F2013%2F08%2F27%2Fhow_to_build_a_simple_ubuntu_server_three%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[安装MySQL执行命令安装MySQL：sudo apt-get install mysql-server安装的时候会提示填入一个root的初始密码，先输入个8做初始密码吧 导入客户数据库base_accountmysql -uroot -p &lt; *.sql(某个sql文件) 安装svn并checkout一个svn服务器上的目录 进入 /data/www目录下 ：cd /data/www 执行命令安装：sudo apt-get install subversion checkout一个目录（比如svn://112.124.26.188/myapp/td/01CServer_PHP/errorMsg），执行命令：svn checkout svn://112.124.26.188/myapp/td/01CServer_PHP/errorMsg （或者 svn co svn://112.124.26.188/myapp/td/01CServer_PHP/errorMsg） . . . 测试是否可以访问并将相应数据传入数据库中访问 localhost/errorMsg/ErrorMsg.php?data=3_2&amp;error_msg=zhangnimashuai然后查看数据库相应表里是否增加了数据 证书登陆 (可选)#对dev 将id_rsa私钥和id_rsa.pub公钥以及authorized_keys授权文件拷贝至~dev/.ssh/目录chmod 600 id_rsa; chmod 644 id_rsa.pub; chmod 644 authorized_keys SSH 证书登陆配置sudo vi /etc/ssh/sshd_config取消注释 : #AuthorizedKeysFile .ssh/authorized_keys修改yes-&gt;no : PasswordAuthentication nosudo service ssh restart 重启服务 测试登陆sudo service nginx restartsudo service php5-fpm restart 测试成功后去除dev用户的sudo权限 （可选）sudo visudo 删除：dev ALL=(ALL:ALL) ALL]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>VBox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始搭建一台简易Ubuntu服务器二]]></title>
    <url>%2F2013%2F08%2F25%2Fhow_to_build_a_simple_ubuntu_server_two%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[安装nginx如若有有不明白，还可以前往参考 执行命令安装nginx：sudo apt-get install nginx测试是否安装成功：在本机的浏览器里访问 localhost ;如果现实”Welcome to nginx!”，表明你的 Nginx 服务器安装成功！启动 Nginx：sudo /etc/init.d/nginx start关闭 Nginx：sudo /etc/init.d/nginx stop重启 nginx：sudo /etc/init.d/nginx restart 或者 sudo service nginx restartsudo service apache2 stop (如果之前装了apache2则需要sudo apt-get remove apache2 卸载掉apache2然后执行这个stop命令) . . . 修改nginx的server配置文件 执行命令：sudo vi /etc/nginx/sites-available/default 然后将default文件里的内容全部删除，把下面的内容粘贴进去： 12345678910111213141516171819202122232425262728server &#123; listen 80 default_server; root /data/www; #这里表示nginx根目录 index index.php index.html index.htm; server_name localhost; chunked_transfer_encoding off; location / &#123; try_files $uri $uri/ =404; &#125; error_page 404 /index.html; location ~ \.php$ &#123; #加上这个代码块就可以用php访问了，这个代码块还有fastcgi的相关内容 try_files $uri =404; fastcgi_split_path_info ^(.+\.php)(/.+)$; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi_params; &#125;&#125; 安装以及启动fastcgi 执行命令安装：sudo apt-get install spawn-fcgi 启动fastcgi ：spawn-fcgi -a 127.0.0.1 -p 9000 -C 10 -u www-data -f /usr/bin/php-cgi 为了让php-cgi开机自启动： Ubuntu开机之后会执行/etc/rc.local文件中的脚本 所以我们可以直接在/etc/rc.local中添加启动脚本。将 spawn-fcgi -a 127.0.0.1 -p 9000 -C 10 -u www-data -f /usr/bin/php-cgi 添加到语句：exit 0 前面才行 安装PHP执行命令安装PHP：sudo apt-get install php5-cli php5-cgi php5-fpm php5-mcrypt php5-mysql php设置sudo vi /etc/php5/fpm/php.ini #设置cgi.fix_pathinfo=0sudo service php5-fpm restart sudo vi /etc/php5/fpm/pool.d/www.conflisten.owner = www-datalisten.group = www-datalisten.mode = 0660 （去掉原www.conf里“listen.mode = 0660”前的分号，那个分号是注释的意思）sudo service php5-fpm restart 测试是否可以访问 在nginx根目录也就是 上面server配置文件里的 /data/www(若没有这个目录就建一个，并改变权限，执行sudo chown dev:dev data/)文件夹里新建index.html（不建此文件将不能在本机访问localhost）将下面的内容粘贴到index.html文件里 12345678&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=&quot;white&quot; text=&quot;black&quot;&gt;&lt;center&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 第1步完成后将可以在浏览器访问 http://localhost 再在 /data/www文件夹里新建test.php将下面的内容粘贴到test.php文件里&lt;?php phpinfo(); ?&gt; 第3步完成后将可以在浏览器访问 http://localhost/test.php 注：如果没有启动fastcgi，访问之后将会下载此php文件。若启动了fastcgi，则访问phpinfo的网页;如果出现No input file specified就在上面的server配置文件里的下述地方加入这条语句：fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;如下：12345678location ~ \.php$ &#123; try_files $uri =404; fastcgi_split_path_info ^(.+\.php)(/.+)$; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params;&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>VBox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始搭建一台简易Ubuntu服务器一]]></title>
    <url>%2F2013%2F08%2F23%2Fhow_to_build_a_simple_ubuntu_server_one%2F</url>

    <encrypted>0</encrypted>

    <content type="text"><![CDATA[创建dev用户sudo adduser dev 增加dev权限sudo visudo 添加：dev ALL=(ALL:ALL) ALL . . . 关于Ubuntu的root密码Ubuntu的默认root密码是随机的，即每次开机都有一个新的root密码。我们可以在终端输入命令 sudo passwd，然后输入当前用户的密码，enter，终端会提示我们输入新的密码并确认，此时的密码就是root新密码。修改成功后，输入命令 su root，再输入新的密码就ok了。 安装ssh服务端先更换源， 然后执行sudo apt-get update执行命令：sudo apt-get install openssh-server测试是否安装成功：ssh localhost]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>VBox</tag>
      </tags>
  </entry>
</search>
